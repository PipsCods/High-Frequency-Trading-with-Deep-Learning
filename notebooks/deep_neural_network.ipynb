{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed_value=42):\n",
    "    \"\"\"\n",
    "    Set random seed across NumPy, Python, and PyTorch to ensure reproducibility.\n",
    "\n",
    "    Args:\n",
    "        seed_value (int): The seed value to use. Default is 42, a commonly used arbitrary number.\n",
    "\n",
    "    This function ensures that experiments produce the same results across different runs,\n",
    "    which is critical for debugging, comparing models, and scientific reproducibility.\n",
    "\n",
    "    It sets the seed for:\n",
    "        - NumPy (used for numerical ops like matrix generation)\n",
    "        - Python's built-in random module (used in random sampling, shuffling, etc.)\n",
    "        - PyTorch (both CPU and GPU)\n",
    "\n",
    "    For GPU reproducibility:\n",
    "        - It manually sets the CUDA seeds (for single and multi-GPU setups)\n",
    "        - It disables the CUDA backend benchmarking feature to ensure deterministic behavior\n",
    "          (at the potential cost of performance).\n",
    "    \"\"\"\n",
    "\n",
    "    # Set seed for NumPy (used in data shuffling, batch generation, etc.)\n",
    "    np.random.seed(seed_value)\n",
    "\n",
    "    # Set seed for PyTorch operations on CPU\n",
    "    torch.manual_seed(seed_value)\n",
    "\n",
    "    # Set seed for Python's built-in random module (e.g., random.shuffle, random.randint)\n",
    "    random.seed(seed_value)\n",
    "\n",
    "    # Set seeds for PyTorch operations on GPU\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"Cuda is available. Setting GPU seed.\")\n",
    "        # Set seed for single-GPU\n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "\n",
    "        # Set seed for all available GPUs (multi-GPU training)\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "        # Ensures that CUDA uses deterministic algorithms\n",
    "        # This disables non-deterministic optimizations and ensures reproducible behavior\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "        # Disables cuDNN auto-tuner which selects the best algorithm for each configuration\n",
    "        # When disabled, it uses deterministic algorithms, but this might make training slower\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Set the seed globally so every run starts from the same state\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I) Create target and predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_target_predictors(dataframe: pd.DataFrame, target_col: str = \"RETURN\", n_lags = 10):\n",
    "\n",
    "    for lag in range(1, n_lags + 1):\n",
    "        dataframe[f'lag_{lag}'] = dataframe[target_col].shift(lag)\n",
    "\n",
    "    dataframe.dropna(inplace=True)\n",
    "\n",
    "    predictors = dataframe[[f'lag_{lag}' for lag in range(1, n_lags + 1)]].copy()\n",
    "    predictors = predictors.values\n",
    "    target = dataframe[target_col].values\n",
    "\n",
    "    return predictors, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II) Train and test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_and_test_data(signals, labels, split_percentage):\n",
    "        \n",
    "    split = int(split_percentage * signals.shape[0])\n",
    "\n",
    "    train_labels = labels[:split]\n",
    "    test_labels = labels[split:]\n",
    "\n",
    "    train_data = signals[:split, :]\n",
    "    test_data = signals[split:, :]\n",
    "\n",
    "    return train_data, test_data, train_labels, test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III) Use the real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"/Users/nicolobaldovin/Desktop/EPFL/MachineLearninginFinance/High-Frequency-Trading-with-Deep-Learning/data/high_10m.parquet\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[\"SYMBOL\"] == \"A\"]\n",
    "df = df.drop(columns=[\"DATE\", \"SYMBOL\", \"TIME\", \"ALL_EX\", \"MID_OPEN\", \"SUM_DELTA\"])\n",
    "df.head()\n",
    "\n",
    "predictors, labels = create_target_predictors(df, target_col=\"RETURN\", n_lags=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = get_train_and_test_data(predictors, labels, split_percentage=0.8)\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# Wrap the input and label tensors into TensorDatasets for PyTorch\n",
    "# This allows easy batching and iteration\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# DataLoaders make it easy to loop over the dataset in mini-batches\n",
    "# - batch_size=32: standard default, can be tuned\n",
    "# - shuffle=True: randomly shuffles training data each epoch for better generalization\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# For evaluation, we typically do not shuffle test data\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV) Class for the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Intraday_trading(nn.Module):\n",
    "    def __init__(self, layers: list, scale: float = 1.0, bias_scale: float = 0.0, activation=nn.GELU()):\n",
    "        \"\"\"\n",
    "        Initialize a customizable Multi-Layer Perceptron (MLP) with flexible architecture and initialization.\n",
    "\n",
    "        Args:\n",
    "            layers (list of int): A list of integers where each value defines the size of each layer.\n",
    "                                  For example, layers=[10, 64, 32, 1] defines a network with input dimension 10,\n",
    "                                  two hidden layers with sizes 64 and 32, and output dimension 1.\n",
    "            scale (float): Scaling factor for weight initialization. Controls the standard deviation of\n",
    "                           the normal distribution used in initializing weights. Recommended to be 1.0 for LeCun initialization.\n",
    "            bias_scale (float): Scaling factor for bias initialization. Often set to 0.0 to start with no initial bias.\n",
    "            activation (nn.Module): Activation function applied after each linear transformation except the last layer.\n",
    "                                    Defaults to nn.GELU(), but can be any activation like nn.ReLU(), nn.Tanh(), etc.\n",
    "        \"\"\"\n",
    "        # Call the constructor of the parent class (nn.Module) to initialize all internal PyTorch machinery.\n",
    "        # This is crucial because nn.Module handles a lot of behind-the-scenes logic like:\n",
    "        # - registering parameters (weights and biases) for automatic optimization\n",
    "        # - setting up .to(device), .eval(), .train(), etc.\n",
    "        # - tracking submodules (layers, activations, etc.)\n",
    "        #\n",
    "        # If you omit this line, the module will NOT work correctly in PyTorch:\n",
    "        # things like model.cuda(), model.parameters(), model.state_dict(), etc. will all break.\n",
    "        #\n",
    "        # The super() call here:\n",
    "        # - FlexibleMLP is our class\n",
    "        # - nn.Module is the parent class\n",
    "        # - self.__init__() is the method we want to call from the parent\n",
    "        super(Intraday_trading, self).__init__()   # Chiamo l'__init__ del nn.Module e lo metto nel self di FlexibleMLP\n",
    "\n",
    "        # Save arguments as attributes for reuse in reset_parameters\n",
    "        self.layer_sizes = layers\n",
    "        self.scale = scale\n",
    "        self.bias_scale = bias_scale\n",
    "        self.activation_fn = activation\n",
    "\n",
    "        # Create containers to hold layers and activations\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.activations = nn.ModuleList()\n",
    "\n",
    "        # Build network structure (but not weights yet)\n",
    "        self._build_layers()\n",
    "\n",
    "        # Initialize weights and biases\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def _build_layers(self):\n",
    "        \"\"\"\n",
    "        Build the linear layers and corresponding activations (except for initialization).\n",
    "        \"\"\"\n",
    "        for i in range(len(self.layer_sizes) - 1):\n",
    "            # Create a linear layer from layer i to layer i+1\n",
    "            layer = nn.Linear(self.layer_sizes[i], self.layer_sizes[i + 1])\n",
    "            self.layers.append(layer)\n",
    "\n",
    "            # Add an activation function unless it's the final layer\n",
    "            if i < len(self.layer_sizes) - 2:\n",
    "                self.activations.append(self.activation_fn)\n",
    "            else:\n",
    "                # Final layer doesn't use activation (use Identity to keep list structure consistent)\n",
    "                self.activations.append(nn.Identity())\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"\n",
    "        Apply custom initialization to all layers using the given scale and bias_scale.\n",
    "        \"\"\"\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            # Apply LeCun-style initialization for better gradient behavior\n",
    "            nn.init.normal_(layer.weight, mean=0.0, std=self.scale * np.sqrt(1 / self.layer_sizes[i]))\n",
    "            nn.init.normal_(layer.bias, mean=0.0, std=self.bias_scale * np.sqrt(1 / self.layer_sizes[i]))\n",
    "\n",
    "    def forward(self, x, return_last_hidden=False):\n",
    "        \"\"\"\n",
    "        Perform a forward pass through the network.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, input_dim).\n",
    "            return_last_hidden (bool): If True, returns both the final output and the last hidden layer's output.\n",
    "                                       Useful for feature extraction, analysis, or interpretability.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output of the final layer.\n",
    "            torch.Tensor (optional): Output of the last hidden layer (before final linear layer),\n",
    "                                     if return_last_hidden is set to True.\n",
    "        \"\"\"\n",
    "        last_hidden = None  # Will store the output of the last hidden layer\n",
    "\n",
    "        # Apply all but the last layer with activation\n",
    "        for layer, activation in zip(self.layers[:-1], self.activations[:-1]):\n",
    "            x = activation(layer(x))  # Apply linear transformation and activation\n",
    "            # x = activation(layer.forward(x))\n",
    "            last_hidden = x  # Save the last hidden output\n",
    "\n",
    "        # Final layer (linear transformation only, no activation)\n",
    "        x = self.layers[-1](x)\n",
    "\n",
    "        if return_last_hidden:\n",
    "            return x, last_hidden\n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V) Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(num_epochs: int,\n",
    "                train_loader: torch.utils.data.DataLoader,\n",
    "                criterion,\n",
    "                optimizer,\n",
    "                model: torch.nn.Module,\n",
    "                ridge_penalty: float = 0.001):\n",
    "    \"\"\"\n",
    "    Train a PyTorch model using mini-batch gradient descent with optional L2 (ridge) regularization.\n",
    "\n",
    "    Args:\n",
    "        num_epochs (int): Number of full passes through the training dataset.\n",
    "        train_loader (DataLoader): PyTorch DataLoader containing training data in mini-batches.\n",
    "        criterion: Loss function (e.g., nn.MSELoss).\n",
    "        optimizer: PyTorch optimizer (e.g., torch.optim.Adam or SGD).\n",
    "        model (nn.Module): The model to train.\n",
    "        ridge_penalty (float): L2 regularization strength (default: 0.001). Helps reduce overfitting.\n",
    "\n",
    "    Returns:\n",
    "        None. Prints training loss every 20 epochs.\n",
    "    \"\"\"\n",
    "    for epoch in range(num_epochs):\n",
    "        for inputs, targets in train_loader:\n",
    "            # Forward pass: get model predictions for this batch\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Compute loss = prediction error + ridge penalty (L2 regularization)\n",
    "            # The penalty term prevents overfitting by discouraging large weights\n",
    "            # sum(p.abs().pow(2).sum()) is equivalent to ||W||Â² (squared L2 norm)\n",
    "            loss = criterion(outputs, targets) + ridge_penalty * sum(p.pow(2.0).sum() for p in model.parameters())\n",
    "\n",
    "            # Backward pass and optimization:\n",
    "            optimizer.zero_grad()   # Clear previous gradients to prevent accumulation\n",
    "            loss.backward()         # Backpropagate the current loss\n",
    "            optimizer.step()        # Perform a parameter update (gradient descent step)\n",
    "\n",
    "        # Print progress every 20 epochs\n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VI) Do the DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_sizes = [10, 32, 1]\n",
    "model = Intraday_trading(layer_sizes, scale=2., bias_scale=0.)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the model is in evaluation mode\n",
    "# Function to get predictions\n",
    "def get_predictions(loader, model):\n",
    "    model.eval()\n",
    "    targets = []\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            outputs = model(inputs)\n",
    "            targets.extend(labels.numpy())\n",
    "            predictions.extend(outputs.numpy())\n",
    "\n",
    "    return np.array(targets).flatten(), np.array(predictions).flatten()\n",
    "\n",
    "# Get predictions for both training and test sets\n",
    "train_sample = get_predictions(train_loader, model)\n",
    "test_sample = get_predictions(test_loader, model)\n",
    "train_targets = train_sample[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_targets = test_sample[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions = train_sample[1]\n",
    "test_predictions = test_sample[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Training set subplot\n",
    "axs[0].scatter(train_sample[1], y_train)\n",
    "axs[0].plot([train_targets.min(), train_targets.max()], [train_targets.min(), train_targets.max()], 'k--', lw=2)  # Diagonal line\n",
    "axs[0].set_xlabel('Actual values')\n",
    "axs[0].set_ylabel('Predicted values')\n",
    "\n",
    "# Test set subplot\n",
    "axs[1].scatter(test_sample[1], y_test) #.mean(1))\n",
    "axs[1].plot([test_targets.min(), test_targets.max()], [test_targets.min(), test_targets.max()], 'k--', lw=2)  # Diagonal line\n",
    "axs[1].set_xlabel('Actual values')\n",
    "axs[1].set_ylabel('Predicted values')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLassignment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
