{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75ce5ebb",
   "metadata": {},
   "source": [
    "Here we will follow professor's colab in order to create a MLP. Here is the link to the colab:https://colab.research.google.com/drive/1gmm92ZzBF_pB9QcHA91dhRte2saC7pf1. No further explanation is needed. Attention is all you need.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "9f1e6af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch                     # for all things PyTorch\n",
    "import pandas as pd\n",
    "import torch.nn as nn            # for torch.nn.Module, the parent object for PyTorch models\n",
    "import torch.nn.functional as F  # for the activation function\n",
    "import numpy as np\n",
    "import random\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "75eba4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlexibleMLP(nn.Module):\n",
    "    def __init__(self, layers: list, scale: float = 1.0, bias_scale: float = 0.0, activation=nn.GELU()):\n",
    "        \"\"\"\n",
    "        Initialize a customizable Multi-Layer Perceptron (MLP) with flexible architecture and initialization.\n",
    "\n",
    "        Args:\n",
    "            layers (list of int): A list of integers where each value defines the size of each layer.\n",
    "                                  For example, layers=[10, 64, 32, 1] defines a network with input dimension 10,\n",
    "                                  two hidden layers with sizes 64 and 32, and output dimension 1.\n",
    "            scale (float): Scaling factor for weight initialization. Controls the standard deviation of\n",
    "                           the normal distribution used in initializing weights. Recommended to be 1.0 for LeCun initialization.\n",
    "            bias_scale (float): Scaling factor for bias initialization. Often set to 0.0 to start with no initial bias.\n",
    "            activation (nn.Module): Activation function applied after each linear transformation except the last layer.\n",
    "                                    Defaults to nn.GELU(), but can be any activation like nn.ReLU(), nn.Tanh(), etc.\n",
    "        \"\"\"\n",
    "        # Call the constructor of the parent class (nn.Module) to initialize all internal PyTorch machinery.\n",
    "        # This is crucial because nn.Module handles a lot of behind-the-scenes logic like:\n",
    "        # - registering parameters (weights and biases) for automatic optimization\n",
    "        # - setting up .to(device), .eval(), .train(), etc.\n",
    "        # - tracking submodules (layers, activations, etc.)\n",
    "        #\n",
    "        # If you omit this line, the module will NOT work correctly in PyTorch:\n",
    "        # things like model.cuda(), model.parameters(), model.state_dict(), etc. will all break.\n",
    "        #\n",
    "        # The super() call here:\n",
    "        # - FlexibleMLP is our class\n",
    "        # - nn.Module is the parent class\n",
    "        # - self.__init__() is the method we want to call from the parent\n",
    "        super(FlexibleMLP, self).__init__()\n",
    "\n",
    "        # Save arguments as attributes for reuse in reset_parameters\n",
    "        self.layer_sizes = layers\n",
    "        self.scale = scale\n",
    "        self.bias_scale = bias_scale\n",
    "        self.activation_fn = activation\n",
    "\n",
    "        # Create containers to hold layers and activations\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.activations = nn.ModuleList()\n",
    "\n",
    "        # Build network structure (but not weights yet)\n",
    "        self._build_layers()\n",
    "\n",
    "        # Initialize weights and biases\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def _build_layers(self):\n",
    "        \"\"\"\n",
    "        Build the linear layers and corresponding activations (except for initialization).\n",
    "        \"\"\"\n",
    "        for i in range(len(self.layer_sizes) - 1):\n",
    "            # Create a linear layer from layer i to layer i+1\n",
    "            layer = nn.Linear(self.layer_sizes[i], self.layer_sizes[i + 1])\n",
    "            self.layers.append(layer)\n",
    "\n",
    "            # Add an activation function unless it's the final layer\n",
    "            if i < len(self.layer_sizes) - 2:\n",
    "                self.activations.append(self.activation_fn)\n",
    "            else:\n",
    "                # Final layer doesn't use activation (use Identity to keep list structure consistent)\n",
    "                self.activations.append(nn.Identity())\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"\n",
    "        Apply custom initialization to all layers using the given scale and bias_scale.\n",
    "        \"\"\"\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            # Apply LeCun-style initialization for better gradient behavior\n",
    "            nn.init.normal_(layer.weight, mean=0.0, std=self.scale * np.sqrt(1 / self.layer_sizes[i]))\n",
    "            nn.init.normal_(layer.bias, mean=0.0, std=self.bias_scale * np.sqrt(1 / self.layer_sizes[i]))\n",
    "\n",
    "    def forward(self, x, return_last_hidden=False):\n",
    "        \"\"\"\n",
    "        Perform a forward pass through the network.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, input_dim).\n",
    "            return_last_hidden (bool): If True, returns both the final output and the last hidden layer's output.\n",
    "                                       Useful for feature extraction, analysis, or interpretability.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output of the final layer.\n",
    "            torch.Tensor (optional): Output of the last hidden layer (before final linear layer),\n",
    "                                     if return_last_hidden is set to True.\n",
    "        \"\"\"\n",
    "        last_hidden = None  # Will store the output of the last hidden layer\n",
    "\n",
    "        # Apply all but the last layer with activation\n",
    "        for layer, activation in zip(self.layers[:-1], self.activations[:-1]):\n",
    "            x = activation(layer(x))  # Apply linear transformation and activation\n",
    "            last_hidden = x  # Save the last hidden output\n",
    "\n",
    "        # Final layer (linear transformation only, no activation)\n",
    "        x = self.layers[-1](x)\n",
    "\n",
    "        if return_last_hidden:\n",
    "            return x, last_hidden\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "ecfe088c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Module.parameters at 0x40828fcf0>\n",
      "Parameter containing:\n",
      "tensor([[-2.1611e-01,  9.3741e-01, -4.9680e-01, -4.9965e-01,  5.5516e-01,\n",
      "         -7.1713e-01,  1.4691e+00, -6.2890e-01, -9.2800e-03,  4.4954e-01],\n",
      "        [-1.1728e-01, -3.6335e-02, -2.9497e-01,  5.4442e-01,  2.8203e-01,\n",
      "         -1.3733e+00,  6.0638e-01, -3.0763e-01,  9.0883e-01, -7.7506e-01],\n",
      "        [-7.4259e-01,  4.6882e-02,  2.8148e-02,  1.3506e+00, -1.6789e-01,\n",
      "          8.9621e-01, -1.2210e-01, -5.5355e-01,  8.0468e-01,  3.5682e-01],\n",
      "        [ 2.7760e-01, -9.6584e-01, -3.5006e-01, -8.9517e-02, -1.3101e+00,\n",
      "          6.9359e-02,  1.6711e+00,  8.4339e-01, -9.7516e-01, -5.4338e-01],\n",
      "        [ 2.2063e-01, -5.2109e-01, -1.7779e+00,  4.8113e-01,  1.0327e+00,\n",
      "          5.6263e-01,  3.8973e-01, -6.8264e-01,  3.2480e-01, -8.3416e-01],\n",
      "        [-6.1412e-01,  9.6624e-01,  5.0090e-01,  3.7787e-01,  1.4081e-01,\n",
      "          9.2216e-01,  2.3723e-01, -1.4694e-01,  9.2946e-01,  5.7497e-01],\n",
      "        [ 2.2277e-01,  7.8742e-01,  1.6386e-01, -8.4906e-01, -4.7635e-01,\n",
      "          2.2122e-01,  7.3743e-01, -2.7915e-01,  6.6780e-01, -1.2767e+00],\n",
      "        [ 6.1432e-02, -6.8906e-03, -4.6184e-01, -3.1058e-01,  1.1663e-01,\n",
      "         -4.2256e-01, -1.9397e-02,  7.0531e-01,  2.1129e-01, -2.3295e-01],\n",
      "        [-3.3646e-01,  5.7749e-01,  2.4709e-01,  5.1097e-01,  3.5601e-01,\n",
      "         -2.4116e-01, -9.6617e-01,  1.1545e+00, -1.0521e+00,  3.7684e-01],\n",
      "        [-1.2780e-01, -5.7015e-01, -1.6829e-01,  5.1102e-02,  6.3553e-01,\n",
      "          3.8778e-01,  1.1850e+00,  6.1720e-01, -6.9003e-04, -1.8880e+00],\n",
      "        [ 3.6097e-01, -1.1231e+00, -5.6261e-01,  6.2480e-01, -4.7065e-01,\n",
      "         -3.6428e-01,  6.5766e-03, -1.8592e-01, -5.7179e-01, -1.7087e-01],\n",
      "        [ 3.5756e-01, -2.1738e-01, -1.1828e+00,  5.2437e-01,  9.8570e-01,\n",
      "          5.3949e-01,  5.6672e-01, -1.2393e+00, -6.6523e-01, -6.8661e-01],\n",
      "        [ 9.6167e-01,  2.9928e-01, -8.4563e-01,  4.3349e-01,  5.0742e-01,\n",
      "         -5.6119e-01,  2.2849e-01, -8.4983e-02, -2.7323e-01, -1.1690e+00],\n",
      "        [-7.3120e-02, -3.5969e-01, -8.0689e-01,  1.8305e-01, -2.9788e-01,\n",
      "          8.0139e-01, -7.3853e-01,  1.7224e+00, -7.9798e-02, -2.2026e-01],\n",
      "        [ 2.5803e-01, -1.0793e-01,  2.4975e-04, -1.0369e+00,  2.1795e-01,\n",
      "          3.7200e-01,  8.2238e-01, -7.5764e-01,  4.5925e-01, -7.4617e-01],\n",
      "        [-2.2721e-01, -3.2755e-01, -2.1552e-01,  3.7863e-01,  1.6261e-01,\n",
      "          7.6919e-01,  2.6553e-01, -1.0101e+00,  1.4004e-01,  4.3442e-01],\n",
      "        [-8.9922e-01,  6.9637e-01,  1.6539e-01,  7.8016e-02,  1.1308e+00,\n",
      "         -5.5496e-01,  8.2848e-01,  4.8573e-02,  6.2459e-01, -3.4284e-01],\n",
      "        [-1.1413e-01, -1.2307e-01, -2.5897e-01,  8.3546e-01,  7.1141e-01,\n",
      "         -2.4558e-01, -7.1132e-01, -8.9267e-02,  6.6135e-01, -5.6367e-01],\n",
      "        [-1.5544e-01,  2.0623e-01, -5.6166e-01,  9.2947e-01,  9.2766e-01,\n",
      "         -5.7917e-01, -2.1386e-01,  4.4628e-01,  1.4945e-01,  4.8450e-01],\n",
      "        [ 1.3730e-01,  2.5387e-01, -1.1702e+00, -5.5723e-01, -5.9739e-01,\n",
      "          1.0385e+00, -5.8252e-01, -1.6047e-02, -7.3044e-01,  2.7016e-01],\n",
      "        [ 2.9440e-01, -1.3535e+00,  6.6769e-01,  1.3444e+00,  8.8072e-01,\n",
      "          1.1934e+00,  1.4418e+00,  2.5084e-01,  5.6934e-01, -2.2444e-01],\n",
      "        [-4.3769e-01,  8.3882e-01,  1.7785e-01,  4.8623e-03, -8.3278e-04,\n",
      "          9.5798e-01,  1.3950e-01, -1.0885e+00, -2.6802e-01,  5.4309e-01],\n",
      "        [ 2.0682e-01, -4.0443e-01,  8.0128e-01, -1.3117e+00, -3.6605e-01,\n",
      "         -2.4966e-02, -1.3074e-01, -8.8153e-01,  1.2180e-01, -2.3367e-02],\n",
      "        [ 2.1494e-01, -4.0251e-01, -3.6489e-03, -1.2964e+00,  4.5569e-01,\n",
      "         -4.3920e-01,  9.0781e-01,  3.0096e-01, -4.1354e-01,  4.6795e-01],\n",
      "        [-1.7023e-01, -4.3314e-01, -5.5937e-01,  8.8078e-01, -4.5413e-01,\n",
      "         -7.4070e-01, -4.4323e-01,  4.1764e-01, -1.4875e-01, -6.3343e-01],\n",
      "        [-3.6744e-01, -2.1523e-01, -1.3613e-01, -2.0273e-01,  1.9302e-01,\n",
      "          4.3718e-02, -8.5808e-01, -7.9872e-01, -6.3829e-01, -1.6222e-01],\n",
      "        [-1.5148e+00,  3.6743e-01,  1.0083e-01,  9.7079e-01, -4.6381e-01,\n",
      "          1.0612e+00, -4.8019e-01, -2.5877e-01,  2.9204e-01,  7.8894e-01],\n",
      "        [ 1.2539e+00,  2.6051e-01, -5.3866e-01, -3.5700e-01,  6.0788e-01,\n",
      "         -3.8889e-01, -1.1190e-02,  8.1449e-01,  3.8688e-01, -1.2252e-02],\n",
      "        [ 1.6906e-02, -5.5453e-01, -1.2440e+00,  2.4597e-01, -7.9445e-02,\n",
      "         -7.2566e-01, -1.4108e+00, -3.0510e-01,  1.1129e+00, -1.9825e-01],\n",
      "        [-2.7324e-01, -1.2942e+00,  2.5229e-01, -1.9318e-01,  1.2818e-01,\n",
      "          4.7638e-01, -2.5587e-01,  5.8072e-02,  8.1921e-01, -2.4344e-01],\n",
      "        [ 1.5511e-01, -1.1681e-01,  2.3764e-01,  7.8804e-01,  2.6792e-01,\n",
      "          5.3029e-01,  1.1530e+00, -1.3974e-01,  1.2264e+00, -7.1378e-02],\n",
      "        [-2.7857e-01, -1.1635e-01, -2.1092e-01, -2.7181e-01,  2.6258e-01,\n",
      "         -8.4653e-01,  5.3699e-02,  9.5531e-02, -1.7753e-02, -1.1401e+00],\n",
      "        [ 5.5475e-01,  2.3026e-01,  4.9392e-02, -1.1474e-01, -1.2598e+00,\n",
      "          6.4153e-01,  9.9787e-01,  2.3180e-01,  1.8308e-01, -5.5083e-02],\n",
      "        [-9.4580e-02,  5.2114e-01, -3.4075e-01, -7.7338e-01, -3.2364e-01,\n",
      "          1.6331e-01, -6.7079e-02,  6.1105e-01, -1.2497e+00,  8.2840e-01],\n",
      "        [ 3.1850e-01,  1.3676e-01, -1.5409e+00,  1.1728e-01, -4.7747e-01,\n",
      "          7.5812e-01, -3.8480e-01,  1.4552e-01, -6.3653e-01,  7.5327e-01],\n",
      "        [-3.2633e-01,  2.7780e-01, -6.4869e-01, -2.5624e-01, -4.6860e-01,\n",
      "         -1.7335e-01, -3.6765e-01, -4.5691e-01, -4.4632e-01,  1.3581e+00],\n",
      "        [-4.5886e-01,  4.1409e-01,  6.7020e-02,  4.8626e-01,  1.0929e+00,\n",
      "          6.8277e-01, -4.1469e-01,  4.8686e-01, -3.0064e-01,  6.8660e-01],\n",
      "        [-1.7887e-01,  4.4569e-01,  3.1590e-01, -4.5920e-01, -5.0640e-01,\n",
      "         -3.1034e-01, -5.8266e-02,  9.8648e-01, -5.9225e-01,  5.7173e-01],\n",
      "        [-8.9355e-01,  2.1886e-01,  1.1237e+00,  5.4311e-01, -8.3139e-01,\n",
      "         -1.0624e+00, -6.8106e-01, -3.6235e-01,  2.4605e-01,  2.8528e-02],\n",
      "        [ 4.3891e-01, -2.7119e-01, -4.9770e-01,  1.5914e+00, -5.8676e-01,\n",
      "          1.0875e+00,  6.9948e-02,  6.1984e-01,  5.3760e-02, -2.1948e-01],\n",
      "        [-2.5780e-01, -1.2785e+00, -4.5880e-02, -2.6381e-01, -1.8159e-02,\n",
      "          9.2927e-03, -1.1613e+00,  2.4935e-01,  2.0007e-01, -3.0215e-01],\n",
      "        [-1.0030e+00,  3.5863e-01, -5.9422e-01, -2.0653e-01,  4.7487e-01,\n",
      "         -4.4345e-01, -1.6435e-01,  3.9174e-01,  7.4360e-02, -5.3576e-01],\n",
      "        [-8.5849e-01,  6.2425e-01, -2.6612e-01,  8.3160e-01, -1.5428e-01,\n",
      "         -2.3084e-01, -6.9896e-01,  4.3578e-01,  4.0992e-01,  1.0907e+00],\n",
      "        [-2.2107e-01, -6.7682e-01,  8.6615e-01, -6.6634e-01,  8.2959e-01,\n",
      "          7.2530e-01,  5.6485e-01,  2.3720e-01, -3.9372e-03, -1.3654e+00],\n",
      "        [-3.7645e-01,  1.1971e+00, -5.2361e-01, -4.4657e-01,  2.8052e-01,\n",
      "          1.0128e-02,  6.4579e-01,  4.4930e-01, -4.2662e-01,  9.7103e-01],\n",
      "        [ 5.7386e-01, -4.5218e-01,  1.1827e+00,  3.9949e-01, -2.1274e-01,\n",
      "         -2.3297e-01, -8.7436e-01, -2.1357e-01,  1.0140e+00, -1.7656e-01],\n",
      "        [ 1.9997e-01, -7.7584e-01, -2.8302e-01,  2.3995e-01, -3.8363e-01,\n",
      "          3.5999e-01,  3.5440e-02,  6.0538e-01, -6.1777e-01,  1.0289e+00],\n",
      "        [ 2.5610e-01,  1.5262e-01,  1.0043e+00, -9.2384e-01,  1.2042e+00,\n",
      "         -5.2186e-01,  4.2523e-01, -1.3709e-01,  9.0444e-01, -2.0032e-01],\n",
      "        [-3.3640e-01,  7.5766e-01, -1.0751e+00, -2.5245e-01,  8.7445e-01,\n",
      "         -1.1162e+00, -5.2646e-01,  4.3137e-01, -3.8566e-01,  9.8132e-01],\n",
      "        [-2.0309e-01, -9.5028e-01,  2.5848e-01,  1.9077e-01, -9.4710e-02,\n",
      "          3.8061e-01, -3.2006e-01,  6.3079e-01, -1.0118e-01,  2.5933e-01],\n",
      "        [-4.2488e-01,  6.0788e-01,  5.0738e-01,  9.9169e-01,  1.2969e+00,\n",
      "          4.4007e-01,  1.1713e-01,  4.9904e-01, -7.3071e-01, -1.1028e+00],\n",
      "        [ 2.7054e-01,  1.1103e+00,  2.4115e-01, -6.3160e-01,  1.5738e+00,\n",
      "         -1.4288e+00,  4.5086e-02, -1.4247e-01, -1.0493e-01,  9.3259e-01],\n",
      "        [ 1.0703e+00,  1.1272e-02, -1.4330e-01, -6.4845e-01,  3.8117e-01,\n",
      "         -1.1161e+00,  5.8019e-02, -3.1124e-01,  7.2818e-02,  2.1396e-01],\n",
      "        [-2.5441e-01,  3.1331e-01,  1.4070e-01,  1.8497e-01,  3.6099e-01,\n",
      "         -3.4544e-01,  3.2017e-01,  6.1897e-01, -4.3603e-02,  3.1131e-01],\n",
      "        [-5.7782e-01, -3.3906e-01, -9.2563e-02, -5.0784e-01, -3.4687e-01,\n",
      "          6.0402e-01, -4.3335e-01,  9.9642e-01,  6.6080e-01, -5.6487e-01],\n",
      "        [-4.2409e-01, -1.8123e-01, -6.0168e-01, -2.4669e-01,  2.3579e-01,\n",
      "         -6.8818e-01, -8.7372e-01, -1.1224e+00,  4.8241e-01,  3.1855e-01],\n",
      "        [ 6.2679e-01,  4.4222e-01,  1.7123e-01,  1.0079e-01, -2.6342e-01,\n",
      "          4.6873e-01,  1.0804e+00,  9.6932e-01,  1.8050e-01,  5.4870e-01],\n",
      "        [ 6.1276e-01, -1.6492e-02, -2.5943e-01, -3.3830e-01,  7.4665e-01,\n",
      "         -2.0340e-01, -9.9454e-01, -7.7137e-01, -4.9737e-01, -9.6590e-01],\n",
      "        [-3.3333e-01,  1.5905e-01, -2.6453e-01,  3.0809e-01, -9.5384e-01,\n",
      "          1.3046e-01, -1.4555e-02, -1.4001e-01, -3.1523e-01,  5.0177e-01],\n",
      "        [-9.2774e-01,  6.0696e-02,  9.9203e-01,  8.5049e-02,  5.8074e-01,\n",
      "         -7.3869e-01, -8.9106e-01, -8.7710e-01, -6.4707e-01,  2.9452e-01],\n",
      "        [ 8.4261e-02, -2.6736e-01, -1.6855e+00,  4.7846e-01, -1.4151e-01,\n",
      "          1.7955e-01,  5.5554e-01, -4.5685e-01,  8.2978e-02,  1.3623e-01],\n",
      "        [ 4.6246e-01, -2.5338e-01, -3.8573e-01, -5.9069e-01,  5.2625e-01,\n",
      "         -1.1346e+00, -4.1557e-01, -9.0106e-02, -4.6717e-01, -3.5049e-01],\n",
      "        [ 7.9013e-03,  5.9451e-02,  4.9737e-01, -3.5035e-01, -5.7397e-01,\n",
      "         -2.7406e-01,  3.8792e-01, -7.2076e-01,  8.2090e-01, -1.1668e+00],\n",
      "        [-3.5945e-01,  4.2578e-01,  2.3869e-01, -5.3365e-01, -1.3687e+00,\n",
      "          7.2544e-01, -8.1884e-01, -1.5412e-01,  2.4472e-01, -8.0351e-01]],\n",
      "       requires_grad=True)\n",
      "torch.Size([64, 10])\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "torch.Size([64])\n",
      "Parameter containing:\n",
      "tensor([[ 0.2128,  0.1265, -0.1268,  ...,  0.6012, -0.2180, -0.1271],\n",
      "        [-0.0765,  0.1326, -0.0489,  ...,  0.0324, -0.1918,  0.0188],\n",
      "        [-0.3437,  0.1148,  0.4440,  ...,  0.2547, -0.2691,  0.1916],\n",
      "        ...,\n",
      "        [-0.0846, -0.0915,  0.1660,  ...,  0.2572, -0.0830,  0.3555],\n",
      "        [ 0.3733,  0.1767, -0.7163,  ..., -0.0443, -0.0677, -0.3783],\n",
      "        [ 0.3787, -0.0820,  0.1001,  ...,  0.1754,  0.4007, -0.3328]],\n",
      "       requires_grad=True)\n",
      "torch.Size([32, 64])\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n",
      "torch.Size([32])\n",
      "Parameter containing:\n",
      "tensor([[ 8.4060e-01,  4.5753e-02,  2.6276e-01,  3.3760e-01, -2.9659e-01,\n",
      "          1.7666e-01, -5.3529e-03, -3.6732e-01, -4.8772e-02,  2.5793e-01,\n",
      "         -1.9758e-01,  1.5682e-01, -1.7431e-01,  7.9886e-01, -8.2271e-03,\n",
      "         -3.0147e-01, -3.2941e-01,  1.2620e-02, -1.3555e-01,  2.8279e-02,\n",
      "         -4.3118e-01, -6.9640e-01,  1.5745e-01,  3.0318e-01, -3.5616e-01,\n",
      "         -3.4931e-01, -2.9572e-01, -5.6402e-02, -1.8087e-01,  6.7924e-01,\n",
      "          9.0749e-02,  4.5542e-04]], requires_grad=True)\n",
      "torch.Size([1, 32])\n",
      "Parameter containing:\n",
      "tensor([0.], requires_grad=True)\n",
      "torch.Size([1])\n",
      "Total number of parameters: 2817\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "# Create an MLP with input size of 10, two hidden layers with 64 and 32 neurons,\n",
    "# and an output size of 5.\n",
    "layer_sizes = [10, 64, 32, 1] # use this to see parameters: [2, 3, 2, 1]#\n",
    "model = FlexibleMLP(layer_sizes, scale=2., bias_scale=0.)\n",
    "print(model.parameters())\n",
    "\n",
    "num_param = 0\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "  print(param)\n",
    "  print(param.shape)\n",
    "  num_param += param.numel()\n",
    "\n",
    "print(f'Total number of parameters: {num_param}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "00532aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed_value=42):\n",
    "    \"\"\"\n",
    "    Set random seed across NumPy, Python, and PyTorch to ensure reproducibility.\n",
    "\n",
    "    Args:\n",
    "        seed_value (int): The seed value to use. Default is 42, a commonly used arbitrary number.\n",
    "\n",
    "    This function ensures that experiments produce the same results across different runs,\n",
    "    which is critical for debugging, comparing models, and scientific reproducibility.\n",
    "\n",
    "    It sets the seed for:\n",
    "        - NumPy (used for numerical ops like matrix generation)\n",
    "        - Python's built-in random module (used in random sampling, shuffling, etc.)\n",
    "        - PyTorch (both CPU and GPU)\n",
    "\n",
    "    For GPU reproducibility:\n",
    "        - It manually sets the CUDA seeds (for single and multi-GPU setups)\n",
    "        - It disables the CUDA backend benchmarking feature to ensure deterministic behavior\n",
    "          (at the potential cost of performance).\n",
    "    \"\"\"\n",
    "\n",
    "    # Set seed for NumPy (used in data shuffling, batch generation, etc.)\n",
    "    np.random.seed(seed_value)\n",
    "\n",
    "    # Set seed for PyTorch operations on CPU\n",
    "    torch.manual_seed(seed_value)\n",
    "\n",
    "    # Set seed for Python's built-in random module (e.g., random.shuffle, random.randint)\n",
    "    random.seed(seed_value)\n",
    "\n",
    "    # Set seeds for PyTorch operations on GPU\n",
    "    if torch.cuda.is_available():\n",
    "        # Set seed for single-GPU\n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "\n",
    "        # Set seed for all available GPUs (multi-GPU training)\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "        # Ensures that CUDA uses deterministic algorithms\n",
    "        # This disables non-deterministic optimizations and ensures reproducible behavior\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "        # Disables cuDNN auto-tuner which selects the best algorithm for each configuration\n",
    "        # When disabled, it uses deterministic algorithms, but this might make training slower\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Set the seed globally so every run starts from the same state\n",
    "set_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "60e922ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_lagged_mlp_data(df: pd.DataFrame, symbol: str, max_lag: int = 10, test_size: float = 0.8, seed: int = 42):\n",
    "    \"\"\"\n",
    "    Prepare lagged feature matrix and target vector from stock returns,\n",
    "    and split into PyTorch-ready training and test sets.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): must contain ['SYMBOL', 'DATE', 'TIME', 'RETURN']\n",
    "        symbol (str): asset to process\n",
    "        max_lag (int): number of lagged returns as features\n",
    "        test_size (float): fraction of data to allocate to test set\n",
    "        seed (int): random seed for reproducibility\n",
    "\n",
    "    Returns:\n",
    "        train_loader, test_loader: PyTorch DataLoader objects\n",
    "        input_dim: int, number of input features (should be max_lag)\n",
    "    \"\"\"\n",
    "    group = df[df['SYMBOL'] == symbol].sort_values(by=['DATE', 'TIME']).reset_index(drop=True)\n",
    "    returns = group['RETURN'].values.astype(np.float32)\n",
    "\n",
    "    if len(returns) < max_lag + 1:\n",
    "        raise ValueError(\"Not enough data to build lagged features.\")\n",
    "\n",
    "    # Build lagged features (X) and corresponding target (y)\n",
    "    X = np.column_stack([returns[i:len(returns)-max_lag+i] for i in range(max_lag)])\n",
    "    y = returns[max_lag:]\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=seed, shuffle=False\n",
    "    )\n",
    "    X_scaler = StandardScaler()\n",
    "    X_train_scaled = X_scaler.fit_transform(X_train)\n",
    "    X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "    y_scaler = StandardScaler()\n",
    "    y_train_scaled = y_scaler.fit_transform(y_train.reshape(-1, 1))\n",
    "    y_test_scaled = y_scaler.transform(y_test.reshape(-1, 1))\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "    X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train_scaled, dtype=torch.float32)\n",
    "    y_test_tensor = torch.tensor(y_test_scaled, dtype=torch.float32)\n",
    "\n",
    "    # Wrap in datasets\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "    # DataLoaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "71322071",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_parquet('/Users/emanueledurante/Desktop/LGMB/lausanne/epfl/MLfinance/High-Frequency-Trading-with-Deep-Learning/data/high_10m.parquet')\n",
    "symbol = 'GS'\n",
    "input_dim = 10\n",
    "train_loader, test_loader = prepare_lagged_mlp_data(df, symbol, max_lag= input_dim)\n",
    "\n",
    "model = FlexibleMLP([input_dim, 64, 32, 32, 32, 1], scale=1.0, bias_scale=0.0)\n",
    "#model = FlexibleMLP([input_dim, 64, 32, 1], scale=1.0, bias_scale=0.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "7ff5cafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)  # Fixing the seed\n",
    "criterion = nn.MSELoss()  # For prediction, MSE is the standard objective; but other, custom objective might be better;\n",
    "# choose loss appropriate for your task\n",
    "# experiment with learning rates, lr = 0.02, 0.01, 0.001\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=0.001) # this is one of the most popular gradient descent algorithms\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01) # experiment with 0.1, 0.2, 0.5. 0.5 is super interesting, achives well OOS!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "2cee9e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10000] Avg train loss: 1.053831\n",
      "Epoch [2/10000] Avg train loss: 0.988070\n",
      "Epoch [3/10000] Avg train loss: 0.954107\n",
      "Epoch [4/10000] Avg train loss: 0.960592\n",
      "Epoch [5/10000] Avg train loss: 0.967099\n",
      "Epoch [6/10000] Avg train loss: 1.006209\n",
      "Epoch [7/10000] Avg train loss: 0.988119\n",
      "Epoch [8/10000] Avg train loss: 0.996069\n",
      "Epoch [9/10000] Avg train loss: 0.993638\n",
      "Epoch [10/10000] Avg train loss: 0.984519\n",
      "Epoch [11/10000] Avg train loss: 0.983537\n",
      "Epoch [12/10000] Avg train loss: 0.977199\n",
      "Epoch [13/10000] Avg train loss: 0.971811\n",
      "Epoch [14/10000] Avg train loss: 0.976694\n",
      "Epoch [15/10000] Avg train loss: 0.973569\n",
      "Epoch [16/10000] Avg train loss: 0.968315\n",
      "Epoch [17/10000] Avg train loss: 0.962638\n",
      "Epoch [18/10000] Avg train loss: 0.964646\n",
      "Epoch [19/10000] Avg train loss: 0.962326\n",
      "Epoch [20/10000] Avg train loss: 0.955505\n",
      "Epoch [21/10000] Avg train loss: 0.947819\n",
      "Epoch [22/10000] Avg train loss: 0.943707\n",
      "Epoch [23/10000] Avg train loss: 0.952305\n",
      "Epoch [24/10000] Avg train loss: 0.947788\n",
      "Epoch [25/10000] Avg train loss: 0.942821\n",
      "Epoch [26/10000] Avg train loss: 0.950341\n",
      "Epoch [27/10000] Avg train loss: 0.952482\n",
      "Epoch [28/10000] Avg train loss: 0.947886\n",
      "Epoch [29/10000] Avg train loss: 0.942030\n",
      "Epoch [30/10000] Avg train loss: 0.939986\n",
      "Epoch [31/10000] Avg train loss: 0.937244\n",
      "Epoch [32/10000] Avg train loss: 0.933202\n",
      "Epoch [33/10000] Avg train loss: 0.929042\n",
      "Epoch [34/10000] Avg train loss: 0.924827\n",
      "Epoch [35/10000] Avg train loss: 0.922888\n",
      "Epoch [36/10000] Avg train loss: 0.924598\n",
      "Epoch [37/10000] Avg train loss: 0.920162\n",
      "Epoch [38/10000] Avg train loss: 0.918172\n",
      "Epoch [39/10000] Avg train loss: 0.915262\n",
      "Epoch [40/10000] Avg train loss: 0.911460\n",
      "Epoch [41/10000] Avg train loss: 0.908508\n",
      "Epoch [42/10000] Avg train loss: 0.904523\n",
      "Epoch [43/10000] Avg train loss: 0.900430\n",
      "Epoch [44/10000] Avg train loss: 0.899809\n",
      "Epoch [45/10000] Avg train loss: 0.901014\n",
      "Epoch [46/10000] Avg train loss: 0.897606\n",
      "Epoch [47/10000] Avg train loss: 0.899347\n",
      "Epoch [48/10000] Avg train loss: 0.895935\n",
      "Epoch [49/10000] Avg train loss: 0.892260\n",
      "Epoch [50/10000] Avg train loss: 0.888252\n",
      "Epoch [51/10000] Avg train loss: 0.885431\n",
      "Epoch [52/10000] Avg train loss: 0.881951\n",
      "Epoch [53/10000] Avg train loss: 0.878487\n",
      "Epoch [54/10000] Avg train loss: 0.877114\n",
      "Epoch [55/10000] Avg train loss: 0.874214\n",
      "Epoch [56/10000] Avg train loss: 0.871905\n",
      "Epoch [57/10000] Avg train loss: 0.867623\n",
      "Epoch [58/10000] Avg train loss: 0.867902\n",
      "Epoch [59/10000] Avg train loss: 0.865092\n",
      "Epoch [60/10000] Avg train loss: 0.862116\n",
      "Epoch [61/10000] Avg train loss: 0.861577\n",
      "Epoch [62/10000] Avg train loss: 0.861082\n",
      "Epoch [63/10000] Avg train loss: 0.857458\n",
      "Epoch [64/10000] Avg train loss: 0.857723\n",
      "Epoch [65/10000] Avg train loss: 0.855095\n",
      "Epoch [66/10000] Avg train loss: 0.851894\n",
      "Epoch [67/10000] Avg train loss: 0.848729\n",
      "Epoch [68/10000] Avg train loss: 0.847584\n",
      "Epoch [69/10000] Avg train loss: 0.847211\n",
      "Epoch [70/10000] Avg train loss: 0.843966\n",
      "Epoch [71/10000] Avg train loss: 0.841239\n",
      "Epoch [72/10000] Avg train loss: 0.837861\n",
      "Epoch [73/10000] Avg train loss: 0.843669\n",
      "Epoch [74/10000] Avg train loss: 0.840171\n",
      "Epoch [75/10000] Avg train loss: 0.836614\n",
      "Epoch [76/10000] Avg train loss: 0.833806\n",
      "Epoch [77/10000] Avg train loss: 0.831099\n",
      "Epoch [78/10000] Avg train loss: 0.827422\n",
      "Epoch [79/10000] Avg train loss: 0.824131\n",
      "Epoch [80/10000] Avg train loss: 0.820972\n",
      "Epoch [81/10000] Avg train loss: 0.818134\n",
      "Epoch [82/10000] Avg train loss: 0.816736\n",
      "Epoch [83/10000] Avg train loss: 0.814026\n",
      "Epoch [84/10000] Avg train loss: 0.810433\n",
      "Epoch [85/10000] Avg train loss: 0.809346\n",
      "Epoch [86/10000] Avg train loss: 0.806290\n",
      "Epoch [87/10000] Avg train loss: 0.802848\n",
      "Epoch [88/10000] Avg train loss: 0.800098\n",
      "Epoch [89/10000] Avg train loss: 0.797309\n",
      "Epoch [90/10000] Avg train loss: 0.793865\n",
      "Epoch [91/10000] Avg train loss: 0.791826\n",
      "Epoch [92/10000] Avg train loss: 0.789202\n",
      "Epoch [93/10000] Avg train loss: 0.786108\n",
      "Epoch [94/10000] Avg train loss: 0.782993\n",
      "Epoch [95/10000] Avg train loss: 0.779989\n",
      "Epoch [96/10000] Avg train loss: 0.776845\n",
      "Epoch [97/10000] Avg train loss: 0.773788\n",
      "Epoch [98/10000] Avg train loss: 0.777429\n",
      "Epoch [99/10000] Avg train loss: 0.776882\n",
      "Epoch [100/10000] Avg train loss: 0.774670\n",
      "Epoch [101/10000] Avg train loss: 0.771752\n",
      "Epoch [102/10000] Avg train loss: 0.768758\n",
      "Epoch [103/10000] Avg train loss: 0.765597\n",
      "Epoch [104/10000] Avg train loss: 0.763058\n",
      "Epoch [105/10000] Avg train loss: 0.760262\n",
      "Epoch [106/10000] Avg train loss: 0.757602\n",
      "Epoch [107/10000] Avg train loss: 0.754817\n",
      "Epoch [108/10000] Avg train loss: 0.754272\n",
      "Epoch [109/10000] Avg train loss: 0.757343\n",
      "Epoch [110/10000] Avg train loss: 0.754793\n",
      "Epoch [111/10000] Avg train loss: 0.752032\n",
      "Epoch [112/10000] Avg train loss: 0.749458\n",
      "Epoch [113/10000] Avg train loss: 0.746612\n",
      "Epoch [114/10000] Avg train loss: 0.743906\n",
      "Epoch [115/10000] Avg train loss: 0.741169\n",
      "Epoch [116/10000] Avg train loss: 0.738496\n",
      "Epoch [117/10000] Avg train loss: 0.736031\n",
      "Epoch [118/10000] Avg train loss: 0.733270\n",
      "Epoch [119/10000] Avg train loss: 0.730651\n",
      "Epoch [120/10000] Avg train loss: 0.728008\n",
      "Epoch [121/10000] Avg train loss: 0.725218\n",
      "Epoch [122/10000] Avg train loss: 0.722689\n",
      "Epoch [123/10000] Avg train loss: 0.720160\n",
      "Epoch [124/10000] Avg train loss: 0.717448\n",
      "Epoch [125/10000] Avg train loss: 0.714731\n",
      "Epoch [126/10000] Avg train loss: 0.712140\n",
      "Epoch [127/10000] Avg train loss: 0.709556\n",
      "Epoch [128/10000] Avg train loss: 0.706908\n",
      "Epoch [129/10000] Avg train loss: 0.704416\n",
      "Epoch [130/10000] Avg train loss: 0.701744\n",
      "Epoch [131/10000] Avg train loss: 0.699191\n",
      "Epoch [132/10000] Avg train loss: 0.697609\n",
      "Epoch [133/10000] Avg train loss: 0.695583\n",
      "Epoch [134/10000] Avg train loss: 0.693043\n",
      "Epoch [135/10000] Avg train loss: 0.690583\n",
      "Epoch [136/10000] Avg train loss: 0.688358\n",
      "Epoch [137/10000] Avg train loss: 0.686086\n",
      "Epoch [138/10000] Avg train loss: 0.684643\n",
      "Epoch [139/10000] Avg train loss: 0.682839\n",
      "Epoch [140/10000] Avg train loss: 0.680342\n",
      "Epoch [141/10000] Avg train loss: 0.678412\n",
      "Epoch [142/10000] Avg train loss: 0.676036\n",
      "Epoch [143/10000] Avg train loss: 0.673735\n",
      "Epoch [144/10000] Avg train loss: 0.671559\n",
      "Epoch [145/10000] Avg train loss: 0.669495\n",
      "Epoch [146/10000] Avg train loss: 0.667162\n",
      "Epoch [147/10000] Avg train loss: 0.664815\n",
      "Epoch [148/10000] Avg train loss: 0.662672\n",
      "Epoch [149/10000] Avg train loss: 0.660971\n",
      "Epoch [150/10000] Avg train loss: 0.658870\n",
      "Epoch [151/10000] Avg train loss: 0.656548\n",
      "Epoch [152/10000] Avg train loss: 0.654313\n",
      "Epoch [153/10000] Avg train loss: 0.652247\n",
      "Epoch [154/10000] Avg train loss: 0.650101\n",
      "Epoch [155/10000] Avg train loss: 0.647892\n",
      "Epoch [156/10000] Avg train loss: 0.646106\n",
      "Epoch [157/10000] Avg train loss: 0.644376\n",
      "Epoch [158/10000] Avg train loss: 0.642282\n",
      "Epoch [159/10000] Avg train loss: 0.640847\n",
      "Epoch [160/10000] Avg train loss: 0.639056\n",
      "Epoch [161/10000] Avg train loss: 0.636956\n",
      "Epoch [162/10000] Avg train loss: 0.635126\n",
      "Epoch [163/10000] Avg train loss: 0.633270\n",
      "Epoch [164/10000] Avg train loss: 0.631888\n",
      "Epoch [165/10000] Avg train loss: 0.630080\n",
      "Epoch [166/10000] Avg train loss: 0.628066\n",
      "Epoch [167/10000] Avg train loss: 0.626092\n",
      "Epoch [168/10000] Avg train loss: 0.624178\n",
      "Epoch [169/10000] Avg train loss: 0.622253\n",
      "Epoch [170/10000] Avg train loss: 0.620256\n",
      "Epoch [171/10000] Avg train loss: 0.618359\n",
      "Epoch [172/10000] Avg train loss: 0.616660\n",
      "Epoch [173/10000] Avg train loss: 0.615211\n",
      "Epoch [174/10000] Avg train loss: 0.613280\n",
      "Epoch [175/10000] Avg train loss: 0.611684\n",
      "Epoch [176/10000] Avg train loss: 0.609812\n",
      "Epoch [177/10000] Avg train loss: 0.607869\n",
      "Epoch [178/10000] Avg train loss: 0.605957\n",
      "Epoch [179/10000] Avg train loss: 0.604225\n",
      "Epoch [180/10000] Avg train loss: 0.602452\n",
      "Epoch [181/10000] Avg train loss: 0.603639\n",
      "Epoch [182/10000] Avg train loss: 0.601786\n",
      "Epoch [183/10000] Avg train loss: 0.599905\n",
      "Epoch [184/10000] Avg train loss: 0.598095\n",
      "Epoch [185/10000] Avg train loss: 0.596477\n",
      "Epoch [186/10000] Avg train loss: 0.594838\n",
      "Epoch [187/10000] Avg train loss: 0.593071\n",
      "Epoch [188/10000] Avg train loss: 0.591257\n",
      "Epoch [189/10000] Avg train loss: 0.589492\n",
      "Epoch [190/10000] Avg train loss: 0.587794\n",
      "Epoch [191/10000] Avg train loss: 0.589318\n",
      "Epoch [192/10000] Avg train loss: 0.587592\n",
      "Epoch [193/10000] Avg train loss: 0.585950\n",
      "Epoch [194/10000] Avg train loss: 0.584228\n",
      "Epoch [195/10000] Avg train loss: 0.582712\n",
      "Epoch [196/10000] Avg train loss: 0.580966\n",
      "Epoch [197/10000] Avg train loss: 0.579249\n",
      "Epoch [198/10000] Avg train loss: 0.577553\n",
      "Epoch [199/10000] Avg train loss: 0.576002\n",
      "Epoch [200/10000] Avg train loss: 0.574349\n",
      "Epoch [201/10000] Avg train loss: 0.572645\n",
      "Epoch [202/10000] Avg train loss: 0.571280\n",
      "Epoch [203/10000] Avg train loss: 0.569651\n",
      "Epoch [204/10000] Avg train loss: 0.567972\n",
      "Epoch [205/10000] Avg train loss: 0.566322\n",
      "Epoch [206/10000] Avg train loss: 0.564671\n",
      "Epoch [207/10000] Avg train loss: 0.565591\n",
      "Epoch [208/10000] Avg train loss: 0.564090\n",
      "Epoch [209/10000] Avg train loss: 0.562529\n",
      "Epoch [210/10000] Avg train loss: 0.560924\n",
      "Epoch [211/10000] Avg train loss: 0.559330\n",
      "Epoch [212/10000] Avg train loss: 0.557748\n",
      "Epoch [213/10000] Avg train loss: 0.556577\n",
      "Epoch [214/10000] Avg train loss: 0.555007\n",
      "Epoch [215/10000] Avg train loss: 0.553439\n",
      "Epoch [216/10000] Avg train loss: 0.552339\n",
      "Epoch [217/10000] Avg train loss: 0.551128\n",
      "Epoch [218/10000] Avg train loss: 0.549786\n",
      "Epoch [219/10000] Avg train loss: 0.548449\n",
      "Epoch [220/10000] Avg train loss: 0.546967\n",
      "Epoch [221/10000] Avg train loss: 0.545461\n",
      "Epoch [222/10000] Avg train loss: 0.544331\n",
      "Epoch [223/10000] Avg train loss: 0.545098\n",
      "Epoch [224/10000] Avg train loss: 0.543678\n",
      "Epoch [225/10000] Avg train loss: 0.543143\n",
      "Epoch [226/10000] Avg train loss: 0.541819\n",
      "Epoch [227/10000] Avg train loss: 0.540401\n",
      "Epoch [228/10000] Avg train loss: 0.539289\n",
      "Epoch [229/10000] Avg train loss: 0.537935\n",
      "Epoch [230/10000] Avg train loss: 0.536496\n",
      "Epoch [231/10000] Avg train loss: 0.535063\n",
      "Epoch [232/10000] Avg train loss: 0.533826\n",
      "Epoch [233/10000] Avg train loss: 0.532385\n",
      "Epoch [234/10000] Avg train loss: 0.531017\n",
      "Epoch [235/10000] Avg train loss: 0.529591\n",
      "Epoch [236/10000] Avg train loss: 0.528236\n",
      "Epoch [237/10000] Avg train loss: 0.526902\n",
      "Epoch [238/10000] Avg train loss: 0.525879\n",
      "Epoch [239/10000] Avg train loss: 0.524648\n",
      "Epoch [240/10000] Avg train loss: 0.523371\n",
      "Epoch [241/10000] Avg train loss: 0.522736\n",
      "Epoch [242/10000] Avg train loss: 0.521436\n",
      "Epoch [243/10000] Avg train loss: 0.520147\n",
      "Epoch [244/10000] Avg train loss: 0.518897\n",
      "Epoch [245/10000] Avg train loss: 0.517683\n",
      "Epoch [246/10000] Avg train loss: 0.516608\n",
      "Epoch [247/10000] Avg train loss: 0.515377\n",
      "Epoch [248/10000] Avg train loss: 0.514215\n",
      "Epoch [249/10000] Avg train loss: 0.513020\n",
      "Epoch [250/10000] Avg train loss: 0.511739\n",
      "Epoch [251/10000] Avg train loss: 0.512218\n",
      "Epoch [252/10000] Avg train loss: 0.510917\n",
      "Epoch [253/10000] Avg train loss: 0.509834\n",
      "Epoch [254/10000] Avg train loss: 0.510311\n",
      "Epoch [255/10000] Avg train loss: 0.509050\n",
      "Epoch [256/10000] Avg train loss: 0.507970\n",
      "Epoch [257/10000] Avg train loss: 0.506743\n",
      "Epoch [258/10000] Avg train loss: 0.505477\n",
      "Epoch [259/10000] Avg train loss: 0.504240\n",
      "Epoch [260/10000] Avg train loss: 0.503100\n",
      "Epoch [261/10000] Avg train loss: 0.501873\n",
      "Epoch [262/10000] Avg train loss: 0.500759\n",
      "Epoch [263/10000] Avg train loss: 0.499561\n",
      "Epoch [264/10000] Avg train loss: 0.498384\n",
      "Epoch [265/10000] Avg train loss: 0.497143\n",
      "Epoch [266/10000] Avg train loss: 0.496482\n",
      "Epoch [267/10000] Avg train loss: 0.495357\n",
      "Epoch [268/10000] Avg train loss: 0.494143\n",
      "Epoch [269/10000] Avg train loss: 0.492978\n",
      "Epoch [270/10000] Avg train loss: 0.491866\n",
      "Epoch [271/10000] Avg train loss: 0.490735\n",
      "Epoch [272/10000] Avg train loss: 0.489554\n",
      "Epoch [273/10000] Avg train loss: 0.488401\n",
      "Epoch [274/10000] Avg train loss: 0.487278\n",
      "Epoch [275/10000] Avg train loss: 0.486125\n",
      "Epoch [276/10000] Avg train loss: 0.484972\n",
      "Epoch [277/10000] Avg train loss: 0.483828\n",
      "Epoch [278/10000] Avg train loss: 0.482688\n",
      "Epoch [279/10000] Avg train loss: 0.481534\n",
      "Epoch [280/10000] Avg train loss: 0.480394\n",
      "Epoch [281/10000] Avg train loss: 0.479258\n",
      "Epoch [282/10000] Avg train loss: 0.478138\n",
      "Epoch [283/10000] Avg train loss: 0.477035\n",
      "Epoch [284/10000] Avg train loss: 0.475924\n",
      "Epoch [285/10000] Avg train loss: 0.474866\n",
      "Epoch [286/10000] Avg train loss: 0.473747\n",
      "Epoch [287/10000] Avg train loss: 0.472704\n",
      "Epoch [288/10000] Avg train loss: 0.471608\n",
      "Epoch [289/10000] Avg train loss: 0.470534\n",
      "Epoch [290/10000] Avg train loss: 0.469449\n",
      "Epoch [291/10000] Avg train loss: 0.468434\n",
      "Epoch [292/10000] Avg train loss: 0.467428\n",
      "Epoch [293/10000] Avg train loss: 0.466358\n",
      "Epoch [294/10000] Avg train loss: 0.465283\n",
      "Epoch [295/10000] Avg train loss: 0.464217\n",
      "Epoch [296/10000] Avg train loss: 0.463209\n",
      "Epoch [297/10000] Avg train loss: 0.462213\n",
      "Epoch [298/10000] Avg train loss: 0.461231\n",
      "Epoch [299/10000] Avg train loss: 0.460260\n",
      "Epoch [300/10000] Avg train loss: 0.459231\n",
      "Epoch [301/10000] Avg train loss: 0.458238\n",
      "Epoch [302/10000] Avg train loss: 0.457230\n",
      "Epoch [303/10000] Avg train loss: 0.456261\n",
      "Epoch [304/10000] Avg train loss: 0.455529\n",
      "Epoch [305/10000] Avg train loss: 0.454772\n",
      "Epoch [306/10000] Avg train loss: 0.453809\n",
      "Epoch [307/10000] Avg train loss: 0.452871\n",
      "Epoch [308/10000] Avg train loss: 0.451890\n",
      "Epoch [309/10000] Avg train loss: 0.450990\n",
      "Epoch [310/10000] Avg train loss: 0.449996\n",
      "Epoch [311/10000] Avg train loss: 0.449090\n",
      "Epoch [312/10000] Avg train loss: 0.448112\n",
      "Epoch [313/10000] Avg train loss: 0.447455\n",
      "Epoch [314/10000] Avg train loss: 0.446516\n",
      "Epoch [315/10000] Avg train loss: 0.445593\n",
      "Epoch [316/10000] Avg train loss: 0.444651\n",
      "Epoch [317/10000] Avg train loss: 0.443739\n",
      "Epoch [318/10000] Avg train loss: 0.442901\n",
      "Epoch [319/10000] Avg train loss: 0.442058\n",
      "Epoch [320/10000] Avg train loss: 0.441129\n",
      "Epoch [321/10000] Avg train loss: 0.440192\n",
      "Epoch [322/10000] Avg train loss: 0.439284\n",
      "Epoch [323/10000] Avg train loss: 0.438356\n",
      "Epoch [324/10000] Avg train loss: 0.437422\n",
      "Epoch [325/10000] Avg train loss: 0.436512\n",
      "Epoch [326/10000] Avg train loss: 0.435578\n",
      "Epoch [327/10000] Avg train loss: 0.434647\n",
      "Epoch [328/10000] Avg train loss: 0.433731\n",
      "Epoch [329/10000] Avg train loss: 0.432817\n",
      "Epoch [330/10000] Avg train loss: 0.432092\n",
      "Epoch [331/10000] Avg train loss: 0.431218\n",
      "Epoch [332/10000] Avg train loss: 0.430309\n",
      "Epoch [333/10000] Avg train loss: 0.429461\n",
      "Epoch [334/10000] Avg train loss: 0.428604\n",
      "Epoch [335/10000] Avg train loss: 0.427754\n",
      "Epoch [336/10000] Avg train loss: 0.427029\n",
      "Epoch [337/10000] Avg train loss: 0.426193\n",
      "Epoch [338/10000] Avg train loss: 0.425313\n",
      "Epoch [339/10000] Avg train loss: 0.424480\n",
      "Epoch [340/10000] Avg train loss: 0.423607\n",
      "Epoch [341/10000] Avg train loss: 0.422739\n",
      "Epoch [342/10000] Avg train loss: 0.421918\n",
      "Epoch [343/10000] Avg train loss: 0.421080\n",
      "Epoch [344/10000] Avg train loss: 0.420254\n",
      "Epoch [345/10000] Avg train loss: 0.419455\n",
      "Epoch [346/10000] Avg train loss: 0.418684\n",
      "Epoch [347/10000] Avg train loss: 0.418674\n",
      "Epoch [348/10000] Avg train loss: 0.417855\n",
      "Epoch [349/10000] Avg train loss: 0.417047\n",
      "Epoch [350/10000] Avg train loss: 0.416237\n",
      "Epoch [351/10000] Avg train loss: 0.415584\n",
      "Epoch [352/10000] Avg train loss: 0.414742\n",
      "Epoch [353/10000] Avg train loss: 0.413964\n",
      "Epoch [354/10000] Avg train loss: 0.413166\n",
      "Epoch [355/10000] Avg train loss: 0.412382\n",
      "Epoch [356/10000] Avg train loss: 0.412301\n",
      "Epoch [357/10000] Avg train loss: 0.411531\n",
      "Epoch [358/10000] Avg train loss: 0.410708\n",
      "Epoch [359/10000] Avg train loss: 0.409912\n",
      "Epoch [360/10000] Avg train loss: 0.409128\n",
      "Epoch [361/10000] Avg train loss: 0.408457\n",
      "Epoch [362/10000] Avg train loss: 0.407636\n",
      "Epoch [363/10000] Avg train loss: 0.406875\n",
      "Epoch [364/10000] Avg train loss: 0.406120\n",
      "Epoch [365/10000] Avg train loss: 0.405305\n",
      "Epoch [366/10000] Avg train loss: 0.404525\n",
      "Epoch [367/10000] Avg train loss: 0.403727\n",
      "Epoch [368/10000] Avg train loss: 0.402901\n",
      "Epoch [369/10000] Avg train loss: 0.402137\n",
      "Epoch [370/10000] Avg train loss: 0.401349\n",
      "Epoch [371/10000] Avg train loss: 0.400543\n",
      "Epoch [372/10000] Avg train loss: 0.399744\n",
      "Epoch [373/10000] Avg train loss: 0.398957\n",
      "Epoch [374/10000] Avg train loss: 0.398356\n",
      "Epoch [375/10000] Avg train loss: 0.397683\n",
      "Epoch [376/10000] Avg train loss: 0.396926\n",
      "Epoch [377/10000] Avg train loss: 0.396151\n",
      "Epoch [378/10000] Avg train loss: 0.395387\n",
      "Epoch [379/10000] Avg train loss: 0.394607\n",
      "Epoch [380/10000] Avg train loss: 0.394104\n",
      "Epoch [381/10000] Avg train loss: 0.393410\n",
      "Epoch [382/10000] Avg train loss: 0.393140\n",
      "Epoch [383/10000] Avg train loss: 0.392417\n",
      "Epoch [384/10000] Avg train loss: 0.391680\n",
      "Epoch [385/10000] Avg train loss: 0.390897\n",
      "Epoch [386/10000] Avg train loss: 0.390183\n",
      "Epoch [387/10000] Avg train loss: 0.389477\n",
      "Epoch [388/10000] Avg train loss: 0.388736\n",
      "Epoch [389/10000] Avg train loss: 0.387988\n",
      "Epoch [390/10000] Avg train loss: 0.387259\n",
      "Epoch [391/10000] Avg train loss: 0.386538\n",
      "Epoch [392/10000] Avg train loss: 0.385786\n",
      "Epoch [393/10000] Avg train loss: 0.385023\n",
      "Epoch [394/10000] Avg train loss: 0.384270\n",
      "Epoch [395/10000] Avg train loss: 0.383536\n",
      "Epoch [396/10000] Avg train loss: 0.382793\n",
      "Epoch [397/10000] Avg train loss: 0.382047\n",
      "Epoch [398/10000] Avg train loss: 0.381355\n",
      "Epoch [399/10000] Avg train loss: 0.380649\n",
      "Epoch [400/10000] Avg train loss: 0.379910\n",
      "Epoch [401/10000] Avg train loss: 0.379247\n",
      "Epoch [402/10000] Avg train loss: 0.378512\n",
      "Epoch [403/10000] Avg train loss: 0.377795\n",
      "Epoch [404/10000] Avg train loss: 0.377093\n",
      "Epoch [405/10000] Avg train loss: 0.376363\n",
      "Epoch [406/10000] Avg train loss: 0.375651\n",
      "Epoch [407/10000] Avg train loss: 0.374944\n",
      "Epoch [408/10000] Avg train loss: 0.374214\n",
      "Epoch [409/10000] Avg train loss: 0.373539\n",
      "Epoch [410/10000] Avg train loss: 0.372811\n",
      "Epoch [411/10000] Avg train loss: 0.372103\n",
      "Epoch [412/10000] Avg train loss: 0.371391\n",
      "Epoch [413/10000] Avg train loss: 0.370665\n",
      "Epoch [414/10000] Avg train loss: 0.369949\n",
      "Epoch [415/10000] Avg train loss: 0.369259\n",
      "Epoch [416/10000] Avg train loss: 0.368579\n",
      "Epoch [417/10000] Avg train loss: 0.367913\n",
      "Epoch [418/10000] Avg train loss: 0.367224\n",
      "Epoch [419/10000] Avg train loss: 0.366517\n",
      "Epoch [420/10000] Avg train loss: 0.365810\n",
      "Epoch [421/10000] Avg train loss: 0.365100\n",
      "Epoch [422/10000] Avg train loss: 0.364538\n",
      "Epoch [423/10000] Avg train loss: 0.363848\n",
      "Epoch [424/10000] Avg train loss: 0.363164\n",
      "Epoch [425/10000] Avg train loss: 0.362465\n",
      "Epoch [426/10000] Avg train loss: 0.361773\n",
      "Epoch [427/10000] Avg train loss: 0.361259\n",
      "Epoch [428/10000] Avg train loss: 0.360593\n",
      "Epoch [429/10000] Avg train loss: 0.359897\n",
      "Epoch [430/10000] Avg train loss: 0.359246\n",
      "Epoch [431/10000] Avg train loss: 0.358712\n",
      "Epoch [432/10000] Avg train loss: 0.358098\n",
      "Epoch [433/10000] Avg train loss: 0.357493\n",
      "Epoch [434/10000] Avg train loss: 0.356824\n",
      "Epoch [435/10000] Avg train loss: 0.356178\n",
      "Epoch [436/10000] Avg train loss: 0.355529\n",
      "Epoch [437/10000] Avg train loss: 0.354847\n",
      "Epoch [438/10000] Avg train loss: 0.354163\n",
      "Epoch [439/10000] Avg train loss: 0.353538\n",
      "Epoch [440/10000] Avg train loss: 0.352897\n",
      "Epoch [441/10000] Avg train loss: 0.352229\n",
      "Epoch [442/10000] Avg train loss: 0.351554\n",
      "Epoch [443/10000] Avg train loss: 0.350881\n",
      "Epoch [444/10000] Avg train loss: 0.350207\n",
      "Epoch [445/10000] Avg train loss: 0.349560\n",
      "Epoch [446/10000] Avg train loss: 0.348929\n",
      "Epoch [447/10000] Avg train loss: 0.348322\n",
      "Epoch [448/10000] Avg train loss: 0.347669\n",
      "Epoch [449/10000] Avg train loss: 0.347045\n",
      "Epoch [450/10000] Avg train loss: 0.346461\n",
      "Epoch [451/10000] Avg train loss: 0.345835\n",
      "Epoch [452/10000] Avg train loss: 0.345265\n",
      "Epoch [453/10000] Avg train loss: 0.344640\n",
      "Epoch [454/10000] Avg train loss: 0.344005\n",
      "Epoch [455/10000] Avg train loss: 0.343366\n",
      "Epoch [456/10000] Avg train loss: 0.342722\n",
      "Epoch [457/10000] Avg train loss: 0.342076\n",
      "Epoch [458/10000] Avg train loss: 0.341432\n",
      "Epoch [459/10000] Avg train loss: 0.340799\n",
      "Epoch [460/10000] Avg train loss: 0.340160\n",
      "Epoch [461/10000] Avg train loss: 0.339533\n",
      "Epoch [462/10000] Avg train loss: 0.338920\n",
      "Epoch [463/10000] Avg train loss: 0.338305\n",
      "Epoch [464/10000] Avg train loss: 0.337672\n",
      "Epoch [465/10000] Avg train loss: 0.337056\n",
      "Epoch [466/10000] Avg train loss: 0.336540\n",
      "Epoch [467/10000] Avg train loss: 0.335959\n",
      "Epoch [468/10000] Avg train loss: 0.335342\n",
      "Epoch [469/10000] Avg train loss: 0.334736\n",
      "Epoch [470/10000] Avg train loss: 0.334223\n",
      "Epoch [471/10000] Avg train loss: 0.333706\n",
      "Epoch [472/10000] Avg train loss: 0.333354\n",
      "Epoch [473/10000] Avg train loss: 0.332875\n",
      "Epoch [474/10000] Avg train loss: 0.332356\n",
      "Epoch [475/10000] Avg train loss: 0.331787\n",
      "Epoch [476/10000] Avg train loss: 0.331281\n",
      "Epoch [477/10000] Avg train loss: 0.330693\n",
      "Epoch [478/10000] Avg train loss: 0.330107\n",
      "Epoch [479/10000] Avg train loss: 0.329539\n",
      "Epoch [480/10000] Avg train loss: 0.328955\n",
      "Epoch [481/10000] Avg train loss: 0.328393\n",
      "Epoch [482/10000] Avg train loss: 0.327832\n",
      "Epoch [483/10000] Avg train loss: 0.327256\n",
      "Epoch [484/10000] Avg train loss: 0.326675\n",
      "Epoch [485/10000] Avg train loss: 0.326091\n",
      "Epoch [486/10000] Avg train loss: 0.325510\n",
      "Epoch [487/10000] Avg train loss: 0.324964\n",
      "Epoch [488/10000] Avg train loss: 0.324395\n",
      "Epoch [489/10000] Avg train loss: 0.323849\n",
      "Epoch [490/10000] Avg train loss: 0.323271\n",
      "Epoch [491/10000] Avg train loss: 0.322711\n",
      "Epoch [492/10000] Avg train loss: 0.322224\n",
      "Epoch [493/10000] Avg train loss: 0.321677\n",
      "Epoch [494/10000] Avg train loss: 0.321119\n",
      "Epoch [495/10000] Avg train loss: 0.320556\n",
      "Epoch [496/10000] Avg train loss: 0.319985\n",
      "Epoch [497/10000] Avg train loss: 0.319420\n",
      "Epoch [498/10000] Avg train loss: 0.318856\n",
      "Epoch [499/10000] Avg train loss: 0.318300\n",
      "Epoch [500/10000] Avg train loss: 0.317745\n",
      "Epoch [501/10000] Avg train loss: 0.317189\n",
      "Epoch [502/10000] Avg train loss: 0.316629\n",
      "Epoch [503/10000] Avg train loss: 0.316086\n",
      "Epoch [504/10000] Avg train loss: 0.315531\n",
      "Epoch [505/10000] Avg train loss: 0.314977\n",
      "Epoch [506/10000] Avg train loss: 0.314427\n",
      "Epoch [507/10000] Avg train loss: 0.313896\n",
      "Epoch [508/10000] Avg train loss: 0.313368\n",
      "Epoch [509/10000] Avg train loss: 0.312839\n",
      "Epoch [510/10000] Avg train loss: 0.312308\n",
      "Epoch [511/10000] Avg train loss: 0.311778\n",
      "Epoch [512/10000] Avg train loss: 0.311251\n",
      "Epoch [513/10000] Avg train loss: 0.310716\n",
      "Epoch [514/10000] Avg train loss: 0.310187\n",
      "Epoch [515/10000] Avg train loss: 0.309684\n",
      "Epoch [516/10000] Avg train loss: 0.309171\n",
      "Epoch [517/10000] Avg train loss: 0.308654\n",
      "Epoch [518/10000] Avg train loss: 0.308125\n",
      "Epoch [519/10000] Avg train loss: 0.307677\n",
      "Epoch [520/10000] Avg train loss: 0.307157\n",
      "Epoch [521/10000] Avg train loss: 0.306649\n",
      "Epoch [522/10000] Avg train loss: 0.306124\n",
      "Epoch [523/10000] Avg train loss: 0.305615\n",
      "Epoch [524/10000] Avg train loss: 0.305092\n",
      "Epoch [525/10000] Avg train loss: 0.304585\n",
      "Epoch [526/10000] Avg train loss: 0.304075\n",
      "Epoch [527/10000] Avg train loss: 0.303558\n",
      "Epoch [528/10000] Avg train loss: 0.303050\n",
      "Epoch [529/10000] Avg train loss: 0.302539\n",
      "Epoch [530/10000] Avg train loss: 0.302028\n",
      "Epoch [531/10000] Avg train loss: 0.301521\n",
      "Epoch [532/10000] Avg train loss: 0.301018\n",
      "Epoch [533/10000] Avg train loss: 0.300514\n",
      "Epoch [534/10000] Avg train loss: 0.300035\n",
      "Epoch [535/10000] Avg train loss: 0.299537\n",
      "Epoch [536/10000] Avg train loss: 0.299044\n",
      "Epoch [537/10000] Avg train loss: 0.298552\n",
      "Epoch [538/10000] Avg train loss: 0.298067\n",
      "Epoch [539/10000] Avg train loss: 0.297578\n",
      "Epoch [540/10000] Avg train loss: 0.297085\n",
      "Epoch [541/10000] Avg train loss: 0.296594\n",
      "Epoch [542/10000] Avg train loss: 0.296114\n",
      "Epoch [543/10000] Avg train loss: 0.295637\n",
      "Epoch [544/10000] Avg train loss: 0.295168\n",
      "Epoch [545/10000] Avg train loss: 0.294746\n",
      "Epoch [546/10000] Avg train loss: 0.294311\n",
      "Epoch [547/10000] Avg train loss: 0.293864\n",
      "Epoch [548/10000] Avg train loss: 0.293417\n",
      "Epoch [549/10000] Avg train loss: 0.293001\n",
      "Epoch [550/10000] Avg train loss: 0.292592\n",
      "Epoch [551/10000] Avg train loss: 0.292146\n",
      "Epoch [552/10000] Avg train loss: 0.291725\n",
      "Epoch [553/10000] Avg train loss: 0.291274\n",
      "Epoch [554/10000] Avg train loss: 0.290814\n",
      "Epoch [555/10000] Avg train loss: 0.290343\n",
      "Epoch [556/10000] Avg train loss: 0.289872\n",
      "Epoch [557/10000] Avg train loss: 0.289426\n",
      "Epoch [558/10000] Avg train loss: 0.288960\n",
      "Epoch [559/10000] Avg train loss: 0.288492\n",
      "Epoch [560/10000] Avg train loss: 0.288035\n",
      "Epoch [561/10000] Avg train loss: 0.287636\n",
      "Epoch [562/10000] Avg train loss: 0.287209\n",
      "Epoch [563/10000] Avg train loss: 0.286775\n",
      "Epoch [564/10000] Avg train loss: 0.286341\n",
      "Epoch [565/10000] Avg train loss: 0.285898\n",
      "Epoch [566/10000] Avg train loss: 0.285468\n",
      "Epoch [567/10000] Avg train loss: 0.285065\n",
      "Epoch [568/10000] Avg train loss: 0.284630\n",
      "Epoch [569/10000] Avg train loss: 0.284181\n",
      "Epoch [570/10000] Avg train loss: 0.283730\n",
      "Epoch [571/10000] Avg train loss: 0.283282\n",
      "Epoch [572/10000] Avg train loss: 0.282855\n",
      "Epoch [573/10000] Avg train loss: 0.282447\n",
      "Epoch [574/10000] Avg train loss: 0.282045\n",
      "Epoch [575/10000] Avg train loss: 0.281606\n",
      "Epoch [576/10000] Avg train loss: 0.281165\n",
      "Epoch [577/10000] Avg train loss: 0.280729\n",
      "Epoch [578/10000] Avg train loss: 0.280290\n",
      "Epoch [579/10000] Avg train loss: 0.279861\n",
      "Epoch [580/10000] Avg train loss: 0.279439\n",
      "Epoch [581/10000] Avg train loss: 0.279005\n",
      "Epoch [582/10000] Avg train loss: 0.278585\n",
      "Epoch [583/10000] Avg train loss: 0.278157\n",
      "Epoch [584/10000] Avg train loss: 0.277727\n",
      "Epoch [585/10000] Avg train loss: 0.277309\n",
      "Epoch [586/10000] Avg train loss: 0.276881\n",
      "Epoch [587/10000] Avg train loss: 0.276457\n",
      "Epoch [588/10000] Avg train loss: 0.276030\n",
      "Epoch [589/10000] Avg train loss: 0.275606\n",
      "Epoch [590/10000] Avg train loss: 0.275194\n",
      "Epoch [591/10000] Avg train loss: 0.274773\n",
      "Epoch [592/10000] Avg train loss: 0.274352\n",
      "Epoch [593/10000] Avg train loss: 0.273931\n",
      "Epoch [594/10000] Avg train loss: 0.273527\n",
      "Epoch [595/10000] Avg train loss: 0.273115\n",
      "Epoch [596/10000] Avg train loss: 0.272713\n",
      "Epoch [597/10000] Avg train loss: 0.272307\n",
      "Epoch [598/10000] Avg train loss: 0.271904\n",
      "Epoch [599/10000] Avg train loss: 0.271527\n",
      "Epoch [600/10000] Avg train loss: 0.271119\n",
      "Epoch [601/10000] Avg train loss: 0.270711\n",
      "Epoch [602/10000] Avg train loss: 0.270303\n",
      "Epoch [603/10000] Avg train loss: 0.269902\n",
      "Epoch [604/10000] Avg train loss: 0.269497\n",
      "Epoch [605/10000] Avg train loss: 0.269099\n",
      "Epoch [606/10000] Avg train loss: 0.268708\n",
      "Epoch [607/10000] Avg train loss: 0.268325\n",
      "Epoch [608/10000] Avg train loss: 0.267924\n",
      "Epoch [609/10000] Avg train loss: 0.267524\n",
      "Epoch [610/10000] Avg train loss: 0.267157\n",
      "Epoch [611/10000] Avg train loss: 0.266766\n",
      "Epoch [612/10000] Avg train loss: 0.266379\n",
      "Epoch [613/10000] Avg train loss: 0.265982\n",
      "Epoch [614/10000] Avg train loss: 0.265598\n",
      "Epoch [615/10000] Avg train loss: 0.265208\n",
      "Epoch [616/10000] Avg train loss: 0.264818\n",
      "Epoch [617/10000] Avg train loss: 0.264445\n",
      "Epoch [618/10000] Avg train loss: 0.264055\n",
      "Epoch [619/10000] Avg train loss: 0.263683\n",
      "Epoch [620/10000] Avg train loss: 0.263295\n",
      "Epoch [621/10000] Avg train loss: 0.262916\n",
      "Epoch [622/10000] Avg train loss: 0.262538\n",
      "Epoch [623/10000] Avg train loss: 0.262161\n",
      "Epoch [624/10000] Avg train loss: 0.261787\n",
      "Epoch [625/10000] Avg train loss: 0.261412\n",
      "Epoch [626/10000] Avg train loss: 0.261051\n",
      "Epoch [627/10000] Avg train loss: 0.260672\n",
      "Epoch [628/10000] Avg train loss: 0.260308\n",
      "Epoch [629/10000] Avg train loss: 0.259943\n",
      "Epoch [630/10000] Avg train loss: 0.259578\n",
      "Epoch [631/10000] Avg train loss: 0.259218\n",
      "Epoch [632/10000] Avg train loss: 0.258846\n",
      "Epoch [633/10000] Avg train loss: 0.258480\n",
      "Epoch [634/10000] Avg train loss: 0.258133\n",
      "Epoch [635/10000] Avg train loss: 0.257779\n",
      "Epoch [636/10000] Avg train loss: 0.257418\n",
      "Epoch [637/10000] Avg train loss: 0.257048\n",
      "Epoch [638/10000] Avg train loss: 0.256679\n",
      "Epoch [639/10000] Avg train loss: 0.256312\n",
      "Epoch [640/10000] Avg train loss: 0.255951\n",
      "Epoch [641/10000] Avg train loss: 0.255589\n",
      "Epoch [642/10000] Avg train loss: 0.255225\n",
      "Epoch [643/10000] Avg train loss: 0.254861\n",
      "Epoch [644/10000] Avg train loss: 0.254499\n",
      "Epoch [645/10000] Avg train loss: 0.254137\n",
      "Epoch [646/10000] Avg train loss: 0.253780\n",
      "Epoch [647/10000] Avg train loss: 0.253425\n",
      "Epoch [648/10000] Avg train loss: 0.253071\n",
      "Epoch [649/10000] Avg train loss: 0.252771\n",
      "Epoch [650/10000] Avg train loss: 0.252470\n",
      "Epoch [651/10000] Avg train loss: 0.252143\n",
      "Epoch [652/10000] Avg train loss: 0.251814\n",
      "Epoch [653/10000] Avg train loss: 0.251500\n",
      "Epoch [654/10000] Avg train loss: 0.251149\n",
      "Epoch [655/10000] Avg train loss: 0.250804\n",
      "Epoch [656/10000] Avg train loss: 0.250476\n",
      "Epoch [657/10000] Avg train loss: 0.250137\n",
      "Epoch [658/10000] Avg train loss: 0.249788\n",
      "Epoch [659/10000] Avg train loss: 0.249440\n",
      "Epoch [660/10000] Avg train loss: 0.249094\n",
      "Epoch [661/10000] Avg train loss: 0.248750\n",
      "Epoch [662/10000] Avg train loss: 0.248418\n",
      "Epoch [663/10000] Avg train loss: 0.248076\n",
      "Epoch [664/10000] Avg train loss: 0.247747\n",
      "Epoch [665/10000] Avg train loss: 0.247444\n",
      "Epoch [666/10000] Avg train loss: 0.247113\n",
      "Epoch [667/10000] Avg train loss: 0.246780\n",
      "Epoch [668/10000] Avg train loss: 0.246441\n",
      "Epoch [669/10000] Avg train loss: 0.246102\n",
      "Epoch [670/10000] Avg train loss: 0.245782\n",
      "Epoch [671/10000] Avg train loss: 0.245446\n",
      "Epoch [672/10000] Avg train loss: 0.245113\n",
      "Epoch [673/10000] Avg train loss: 0.244780\n",
      "Epoch [674/10000] Avg train loss: 0.244445\n",
      "Epoch [675/10000] Avg train loss: 0.244112\n",
      "Epoch [676/10000] Avg train loss: 0.243782\n",
      "Epoch [677/10000] Avg train loss: 0.243454\n",
      "Epoch [678/10000] Avg train loss: 0.243125\n",
      "Epoch [679/10000] Avg train loss: 0.242800\n",
      "Epoch [680/10000] Avg train loss: 0.242484\n",
      "Epoch [681/10000] Avg train loss: 0.242156\n",
      "Epoch [682/10000] Avg train loss: 0.241828\n",
      "Epoch [683/10000] Avg train loss: 0.241504\n",
      "Epoch [684/10000] Avg train loss: 0.241191\n",
      "Epoch [685/10000] Avg train loss: 0.240870\n",
      "Epoch [686/10000] Avg train loss: 0.240550\n",
      "Epoch [687/10000] Avg train loss: 0.240247\n",
      "Epoch [688/10000] Avg train loss: 0.239927\n",
      "Epoch [689/10000] Avg train loss: 0.239608\n",
      "Epoch [690/10000] Avg train loss: 0.239303\n",
      "Epoch [691/10000] Avg train loss: 0.238992\n",
      "Epoch [692/10000] Avg train loss: 0.238673\n",
      "Epoch [693/10000] Avg train loss: 0.238355\n",
      "Epoch [694/10000] Avg train loss: 0.238069\n",
      "Epoch [695/10000] Avg train loss: 0.237762\n",
      "Epoch [696/10000] Avg train loss: 0.237469\n",
      "Epoch [697/10000] Avg train loss: 0.237162\n",
      "Epoch [698/10000] Avg train loss: 0.236879\n",
      "Epoch [699/10000] Avg train loss: 0.236583\n",
      "Epoch [700/10000] Avg train loss: 0.236278\n",
      "Epoch [701/10000] Avg train loss: 0.235969\n",
      "Epoch [702/10000] Avg train loss: 0.235657\n",
      "Epoch [703/10000] Avg train loss: 0.235359\n",
      "Epoch [704/10000] Avg train loss: 0.235050\n",
      "Epoch [705/10000] Avg train loss: 0.234742\n",
      "Epoch [706/10000] Avg train loss: 0.234443\n",
      "Epoch [707/10000] Avg train loss: 0.234139\n",
      "Epoch [708/10000] Avg train loss: 0.233837\n",
      "Epoch [709/10000] Avg train loss: 0.233534\n",
      "Epoch [710/10000] Avg train loss: 0.233231\n",
      "Epoch [711/10000] Avg train loss: 0.232932\n",
      "Epoch [712/10000] Avg train loss: 0.232629\n",
      "Epoch [713/10000] Avg train loss: 0.232330\n",
      "Epoch [714/10000] Avg train loss: 0.232030\n",
      "Epoch [715/10000] Avg train loss: 0.231730\n",
      "Epoch [716/10000] Avg train loss: 0.231431\n",
      "Epoch [717/10000] Avg train loss: 0.231132\n",
      "Epoch [718/10000] Avg train loss: 0.230841\n",
      "Epoch [719/10000] Avg train loss: 0.230542\n",
      "Epoch [720/10000] Avg train loss: 0.230245\n",
      "Epoch [721/10000] Avg train loss: 0.229948\n",
      "Epoch [722/10000] Avg train loss: 0.229659\n",
      "Epoch [723/10000] Avg train loss: 0.229372\n",
      "Epoch [724/10000] Avg train loss: 0.229093\n",
      "Epoch [725/10000] Avg train loss: 0.228808\n",
      "Epoch [726/10000] Avg train loss: 0.228524\n",
      "Epoch [727/10000] Avg train loss: 0.228239\n",
      "Epoch [728/10000] Avg train loss: 0.227949\n",
      "Epoch [729/10000] Avg train loss: 0.227677\n",
      "Epoch [730/10000] Avg train loss: 0.227402\n",
      "Epoch [731/10000] Avg train loss: 0.227119\n",
      "Epoch [732/10000] Avg train loss: 0.226844\n",
      "Epoch [733/10000] Avg train loss: 0.226567\n",
      "Epoch [734/10000] Avg train loss: 0.226302\n",
      "Epoch [735/10000] Avg train loss: 0.226018\n",
      "Epoch [736/10000] Avg train loss: 0.225734\n",
      "Epoch [737/10000] Avg train loss: 0.225462\n",
      "Epoch [738/10000] Avg train loss: 0.225185\n",
      "Epoch [739/10000] Avg train loss: 0.224904\n",
      "Epoch [740/10000] Avg train loss: 0.224630\n",
      "Epoch [741/10000] Avg train loss: 0.224363\n",
      "Epoch [742/10000] Avg train loss: 0.224087\n",
      "Epoch [743/10000] Avg train loss: 0.223806\n",
      "Epoch [744/10000] Avg train loss: 0.223526\n",
      "Epoch [745/10000] Avg train loss: 0.223248\n",
      "Epoch [746/10000] Avg train loss: 0.222973\n",
      "Epoch [747/10000] Avg train loss: 0.222699\n",
      "Epoch [748/10000] Avg train loss: 0.222433\n",
      "Epoch [749/10000] Avg train loss: 0.222161\n",
      "Epoch [750/10000] Avg train loss: 0.221885\n",
      "Epoch [751/10000] Avg train loss: 0.221613\n",
      "Epoch [752/10000] Avg train loss: 0.221339\n",
      "Epoch [753/10000] Avg train loss: 0.221089\n",
      "Epoch [754/10000] Avg train loss: 0.220820\n",
      "Epoch [755/10000] Avg train loss: 0.220570\n",
      "Epoch [756/10000] Avg train loss: 0.220301\n",
      "Epoch [757/10000] Avg train loss: 0.220034\n",
      "Epoch [758/10000] Avg train loss: 0.219766\n",
      "Epoch [759/10000] Avg train loss: 0.219501\n",
      "Epoch [760/10000] Avg train loss: 0.219233\n",
      "Epoch [761/10000] Avg train loss: 0.218964\n",
      "Epoch [762/10000] Avg train loss: 0.218701\n",
      "Epoch [763/10000] Avg train loss: 0.218439\n",
      "Epoch [764/10000] Avg train loss: 0.218185\n",
      "Epoch [765/10000] Avg train loss: 0.217921\n",
      "Epoch [766/10000] Avg train loss: 0.217657\n",
      "Epoch [767/10000] Avg train loss: 0.217394\n",
      "Epoch [768/10000] Avg train loss: 0.217133\n",
      "Epoch [769/10000] Avg train loss: 0.216869\n",
      "Epoch [770/10000] Avg train loss: 0.216609\n",
      "Epoch [771/10000] Avg train loss: 0.216351\n",
      "Epoch [772/10000] Avg train loss: 0.216093\n",
      "Epoch [773/10000] Avg train loss: 0.215862\n",
      "Epoch [774/10000] Avg train loss: 0.215648\n",
      "Epoch [775/10000] Avg train loss: 0.215407\n",
      "Epoch [776/10000] Avg train loss: 0.215164\n",
      "Epoch [777/10000] Avg train loss: 0.214964\n",
      "Epoch [778/10000] Avg train loss: 0.214729\n",
      "Epoch [779/10000] Avg train loss: 0.214481\n",
      "Epoch [780/10000] Avg train loss: 0.214234\n",
      "Epoch [781/10000] Avg train loss: 0.213983\n",
      "Epoch [782/10000] Avg train loss: 0.213739\n",
      "Epoch [783/10000] Avg train loss: 0.213498\n",
      "Epoch [784/10000] Avg train loss: 0.213247\n",
      "Epoch [785/10000] Avg train loss: 0.213004\n",
      "Epoch [786/10000] Avg train loss: 0.212752\n",
      "Epoch [787/10000] Avg train loss: 0.212501\n",
      "Epoch [788/10000] Avg train loss: 0.212252\n",
      "Epoch [789/10000] Avg train loss: 0.212012\n",
      "Epoch [790/10000] Avg train loss: 0.211763\n",
      "Epoch [791/10000] Avg train loss: 0.211516\n",
      "Epoch [792/10000] Avg train loss: 0.211268\n",
      "Epoch [793/10000] Avg train loss: 0.211023\n",
      "Epoch [794/10000] Avg train loss: 0.210776\n",
      "Epoch [795/10000] Avg train loss: 0.210532\n",
      "Epoch [796/10000] Avg train loss: 0.210285\n",
      "Epoch [797/10000] Avg train loss: 0.210052\n",
      "Epoch [798/10000] Avg train loss: 0.209811\n",
      "Epoch [799/10000] Avg train loss: 0.209568\n",
      "Epoch [800/10000] Avg train loss: 0.209325\n",
      "Epoch [801/10000] Avg train loss: 0.209085\n",
      "Epoch [802/10000] Avg train loss: 0.208842\n",
      "Epoch [803/10000] Avg train loss: 0.208602\n",
      "Epoch [804/10000] Avg train loss: 0.208376\n",
      "Epoch [805/10000] Avg train loss: 0.208150\n",
      "Epoch [806/10000] Avg train loss: 0.207926\n",
      "Epoch [807/10000] Avg train loss: 0.207719\n",
      "Epoch [808/10000] Avg train loss: 0.207486\n",
      "Epoch [809/10000] Avg train loss: 0.207256\n",
      "Epoch [810/10000] Avg train loss: 0.207019\n",
      "Epoch [811/10000] Avg train loss: 0.206788\n",
      "Epoch [812/10000] Avg train loss: 0.206555\n",
      "Epoch [813/10000] Avg train loss: 0.206324\n",
      "Epoch [814/10000] Avg train loss: 0.206087\n",
      "Epoch [815/10000] Avg train loss: 0.205853\n",
      "Epoch [816/10000] Avg train loss: 0.205620\n",
      "Epoch [817/10000] Avg train loss: 0.205384\n",
      "Epoch [818/10000] Avg train loss: 0.205152\n",
      "Epoch [819/10000] Avg train loss: 0.204921\n",
      "Epoch [820/10000] Avg train loss: 0.204697\n",
      "Epoch [821/10000] Avg train loss: 0.204466\n",
      "Epoch [822/10000] Avg train loss: 0.204233\n",
      "Epoch [823/10000] Avg train loss: 0.204004\n",
      "Epoch [824/10000] Avg train loss: 0.203777\n",
      "Epoch [825/10000] Avg train loss: 0.203549\n",
      "Epoch [826/10000] Avg train loss: 0.203319\n",
      "Epoch [827/10000] Avg train loss: 0.203088\n",
      "Epoch [828/10000] Avg train loss: 0.202870\n",
      "Epoch [829/10000] Avg train loss: 0.202641\n",
      "Epoch [830/10000] Avg train loss: 0.202414\n",
      "Epoch [831/10000] Avg train loss: 0.202188\n",
      "Epoch [832/10000] Avg train loss: 0.201973\n",
      "Epoch [833/10000] Avg train loss: 0.201748\n",
      "Epoch [834/10000] Avg train loss: 0.201520\n",
      "Epoch [835/10000] Avg train loss: 0.201295\n",
      "Epoch [836/10000] Avg train loss: 0.201069\n",
      "Epoch [837/10000] Avg train loss: 0.200848\n",
      "Epoch [838/10000] Avg train loss: 0.200623\n",
      "Epoch [839/10000] Avg train loss: 0.200398\n",
      "Epoch [840/10000] Avg train loss: 0.200187\n",
      "Epoch [841/10000] Avg train loss: 0.199978\n",
      "Epoch [842/10000] Avg train loss: 0.199761\n",
      "Epoch [843/10000] Avg train loss: 0.199541\n",
      "Epoch [844/10000] Avg train loss: 0.199325\n",
      "Epoch [845/10000] Avg train loss: 0.199108\n",
      "Epoch [846/10000] Avg train loss: 0.198896\n",
      "Epoch [847/10000] Avg train loss: 0.198693\n",
      "Epoch [848/10000] Avg train loss: 0.198486\n",
      "Epoch [849/10000] Avg train loss: 0.198266\n",
      "Epoch [850/10000] Avg train loss: 0.198059\n",
      "Epoch [851/10000] Avg train loss: 0.197844\n",
      "Epoch [852/10000] Avg train loss: 0.197626\n",
      "Epoch [853/10000] Avg train loss: 0.197407\n",
      "Epoch [854/10000] Avg train loss: 0.197189\n",
      "Epoch [855/10000] Avg train loss: 0.196972\n",
      "Epoch [856/10000] Avg train loss: 0.196758\n",
      "Epoch [857/10000] Avg train loss: 0.196542\n",
      "Epoch [858/10000] Avg train loss: 0.196326\n",
      "Epoch [859/10000] Avg train loss: 0.196111\n",
      "Epoch [860/10000] Avg train loss: 0.195898\n",
      "Epoch [861/10000] Avg train loss: 0.195683\n",
      "Epoch [862/10000] Avg train loss: 0.195469\n",
      "Epoch [863/10000] Avg train loss: 0.195257\n",
      "Epoch [864/10000] Avg train loss: 0.195045\n",
      "Epoch [865/10000] Avg train loss: 0.194835\n",
      "Epoch [866/10000] Avg train loss: 0.194625\n",
      "Epoch [867/10000] Avg train loss: 0.194414\n",
      "Epoch [868/10000] Avg train loss: 0.194206\n",
      "Epoch [869/10000] Avg train loss: 0.193995\n",
      "Epoch [870/10000] Avg train loss: 0.193786\n",
      "Epoch [871/10000] Avg train loss: 0.193576\n",
      "Epoch [872/10000] Avg train loss: 0.193368\n",
      "Epoch [873/10000] Avg train loss: 0.193159\n",
      "Epoch [874/10000] Avg train loss: 0.192952\n",
      "Epoch [875/10000] Avg train loss: 0.192745\n",
      "Epoch [876/10000] Avg train loss: 0.192539\n",
      "Epoch [877/10000] Avg train loss: 0.192334\n",
      "Epoch [878/10000] Avg train loss: 0.192127\n",
      "Epoch [879/10000] Avg train loss: 0.191919\n",
      "Epoch [880/10000] Avg train loss: 0.191718\n",
      "Epoch [881/10000] Avg train loss: 0.191513\n",
      "Epoch [882/10000] Avg train loss: 0.191307\n",
      "Epoch [883/10000] Avg train loss: 0.191106\n",
      "Epoch [884/10000] Avg train loss: 0.190904\n",
      "Epoch [885/10000] Avg train loss: 0.190706\n",
      "Epoch [886/10000] Avg train loss: 0.190504\n",
      "Epoch [887/10000] Avg train loss: 0.190304\n",
      "Epoch [888/10000] Avg train loss: 0.190104\n",
      "Epoch [889/10000] Avg train loss: 0.189903\n",
      "Epoch [890/10000] Avg train loss: 0.189707\n",
      "Epoch [891/10000] Avg train loss: 0.189508\n",
      "Epoch [892/10000] Avg train loss: 0.189310\n",
      "Epoch [893/10000] Avg train loss: 0.189109\n",
      "Epoch [894/10000] Avg train loss: 0.188910\n",
      "Epoch [895/10000] Avg train loss: 0.188719\n",
      "Epoch [896/10000] Avg train loss: 0.188522\n",
      "Epoch [897/10000] Avg train loss: 0.188326\n",
      "Epoch [898/10000] Avg train loss: 0.188135\n",
      "Epoch [899/10000] Avg train loss: 0.187947\n",
      "Epoch [900/10000] Avg train loss: 0.187763\n",
      "Epoch [901/10000] Avg train loss: 0.187571\n",
      "Epoch [902/10000] Avg train loss: 0.187376\n",
      "Epoch [903/10000] Avg train loss: 0.187186\n",
      "Epoch [904/10000] Avg train loss: 0.186991\n",
      "Epoch [905/10000] Avg train loss: 0.186795\n",
      "Epoch [906/10000] Avg train loss: 0.186608\n",
      "Epoch [907/10000] Avg train loss: 0.186416\n",
      "Epoch [908/10000] Avg train loss: 0.186222\n",
      "Epoch [909/10000] Avg train loss: 0.186028\n",
      "Epoch [910/10000] Avg train loss: 0.185837\n",
      "Epoch [911/10000] Avg train loss: 0.185645\n",
      "Epoch [912/10000] Avg train loss: 0.185451\n",
      "Epoch [913/10000] Avg train loss: 0.185259\n",
      "Epoch [914/10000] Avg train loss: 0.185066\n",
      "Epoch [915/10000] Avg train loss: 0.184874\n",
      "Epoch [916/10000] Avg train loss: 0.184682\n",
      "Epoch [917/10000] Avg train loss: 0.184492\n",
      "Epoch [918/10000] Avg train loss: 0.184301\n",
      "Epoch [919/10000] Avg train loss: 0.184110\n",
      "Epoch [920/10000] Avg train loss: 0.183919\n",
      "Epoch [921/10000] Avg train loss: 0.183734\n",
      "Epoch [922/10000] Avg train loss: 0.183552\n",
      "Epoch [923/10000] Avg train loss: 0.183365\n",
      "Epoch [924/10000] Avg train loss: 0.183178\n",
      "Epoch [925/10000] Avg train loss: 0.182991\n",
      "Epoch [926/10000] Avg train loss: 0.182804\n",
      "Epoch [927/10000] Avg train loss: 0.182625\n",
      "Epoch [928/10000] Avg train loss: 0.182440\n",
      "Epoch [929/10000] Avg train loss: 0.182257\n",
      "Epoch [930/10000] Avg train loss: 0.182075\n",
      "Epoch [931/10000] Avg train loss: 0.181896\n",
      "Epoch [932/10000] Avg train loss: 0.181712\n",
      "Epoch [933/10000] Avg train loss: 0.181527\n",
      "Epoch [934/10000] Avg train loss: 0.181342\n",
      "Epoch [935/10000] Avg train loss: 0.181157\n",
      "Epoch [936/10000] Avg train loss: 0.180972\n",
      "Epoch [937/10000] Avg train loss: 0.180790\n",
      "Epoch [938/10000] Avg train loss: 0.180607\n",
      "Epoch [939/10000] Avg train loss: 0.180425\n",
      "Epoch [940/10000] Avg train loss: 0.180247\n",
      "Epoch [941/10000] Avg train loss: 0.180067\n",
      "Epoch [942/10000] Avg train loss: 0.179887\n",
      "Epoch [943/10000] Avg train loss: 0.179712\n",
      "Epoch [944/10000] Avg train loss: 0.179534\n",
      "Epoch [945/10000] Avg train loss: 0.179354\n",
      "Epoch [946/10000] Avg train loss: 0.179175\n",
      "Epoch [947/10000] Avg train loss: 0.178994\n",
      "Epoch [948/10000] Avg train loss: 0.178815\n",
      "Epoch [949/10000] Avg train loss: 0.178636\n",
      "Epoch [950/10000] Avg train loss: 0.178458\n",
      "Epoch [951/10000] Avg train loss: 0.178280\n",
      "Epoch [952/10000] Avg train loss: 0.178102\n",
      "Epoch [953/10000] Avg train loss: 0.177929\n",
      "Epoch [954/10000] Avg train loss: 0.177759\n",
      "Epoch [955/10000] Avg train loss: 0.177589\n",
      "Epoch [956/10000] Avg train loss: 0.177412\n",
      "Epoch [957/10000] Avg train loss: 0.177236\n",
      "Epoch [958/10000] Avg train loss: 0.177061\n",
      "Epoch [959/10000] Avg train loss: 0.176886\n",
      "Epoch [960/10000] Avg train loss: 0.176711\n",
      "Epoch [961/10000] Avg train loss: 0.176537\n",
      "Epoch [962/10000] Avg train loss: 0.176362\n",
      "Epoch [963/10000] Avg train loss: 0.176187\n",
      "Epoch [964/10000] Avg train loss: 0.176019\n",
      "Epoch [965/10000] Avg train loss: 0.175863\n",
      "Epoch [966/10000] Avg train loss: 0.175695\n",
      "Epoch [967/10000] Avg train loss: 0.175532\n",
      "Epoch [968/10000] Avg train loss: 0.175361\n",
      "Epoch [969/10000] Avg train loss: 0.175189\n",
      "Epoch [970/10000] Avg train loss: 0.175017\n",
      "Epoch [971/10000] Avg train loss: 0.174845\n",
      "Epoch [972/10000] Avg train loss: 0.174674\n",
      "Epoch [973/10000] Avg train loss: 0.174508\n",
      "Epoch [974/10000] Avg train loss: 0.174337\n",
      "Epoch [975/10000] Avg train loss: 0.174171\n",
      "Epoch [976/10000] Avg train loss: 0.174010\n",
      "Epoch [977/10000] Avg train loss: 0.173847\n",
      "Epoch [978/10000] Avg train loss: 0.173679\n",
      "Epoch [979/10000] Avg train loss: 0.173516\n",
      "Epoch [980/10000] Avg train loss: 0.173349\n",
      "Epoch [981/10000] Avg train loss: 0.173182\n",
      "Epoch [982/10000] Avg train loss: 0.173014\n",
      "Epoch [983/10000] Avg train loss: 0.172849\n",
      "Epoch [984/10000] Avg train loss: 0.172681\n",
      "Epoch [985/10000] Avg train loss: 0.172519\n",
      "Epoch [986/10000] Avg train loss: 0.172353\n",
      "Epoch [987/10000] Avg train loss: 0.172186\n",
      "Epoch [988/10000] Avg train loss: 0.172020\n",
      "Epoch [989/10000] Avg train loss: 0.171853\n",
      "Epoch [990/10000] Avg train loss: 0.171692\n",
      "Epoch [991/10000] Avg train loss: 0.171534\n",
      "Epoch [992/10000] Avg train loss: 0.171375\n",
      "Epoch [993/10000] Avg train loss: 0.171218\n",
      "Epoch [994/10000] Avg train loss: 0.171057\n",
      "Epoch [995/10000] Avg train loss: 0.170894\n",
      "Epoch [996/10000] Avg train loss: 0.170732\n",
      "Epoch [997/10000] Avg train loss: 0.170568\n",
      "Epoch [998/10000] Avg train loss: 0.170407\n",
      "Epoch [999/10000] Avg train loss: 0.170248\n",
      "Epoch [1000/10000] Avg train loss: 0.170087\n",
      "Epoch [1001/10000] Avg train loss: 0.169926\n",
      "Epoch [1002/10000] Avg train loss: 0.169763\n",
      "Epoch [1003/10000] Avg train loss: 0.169601\n",
      "Epoch [1004/10000] Avg train loss: 0.169442\n",
      "Epoch [1005/10000] Avg train loss: 0.169284\n",
      "Epoch [1006/10000] Avg train loss: 0.169126\n",
      "Epoch [1007/10000] Avg train loss: 0.168966\n",
      "Epoch [1008/10000] Avg train loss: 0.168806\n",
      "Epoch [1009/10000] Avg train loss: 0.168645\n",
      "Epoch [1010/10000] Avg train loss: 0.168484\n",
      "Epoch [1011/10000] Avg train loss: 0.168328\n",
      "Epoch [1012/10000] Avg train loss: 0.168169\n",
      "Epoch [1013/10000] Avg train loss: 0.168010\n",
      "Epoch [1014/10000] Avg train loss: 0.167851\n",
      "Epoch [1015/10000] Avg train loss: 0.167693\n",
      "Epoch [1016/10000] Avg train loss: 0.167535\n",
      "Epoch [1017/10000] Avg train loss: 0.167378\n",
      "Epoch [1018/10000] Avg train loss: 0.167220\n",
      "Epoch [1019/10000] Avg train loss: 0.167062\n",
      "Epoch [1020/10000] Avg train loss: 0.166906\n",
      "Epoch [1021/10000] Avg train loss: 0.166749\n",
      "Epoch [1022/10000] Avg train loss: 0.166595\n",
      "Epoch [1023/10000] Avg train loss: 0.166440\n",
      "Epoch [1024/10000] Avg train loss: 0.166285\n",
      "Epoch [1025/10000] Avg train loss: 0.166131\n",
      "Epoch [1026/10000] Avg train loss: 0.165976\n",
      "Epoch [1027/10000] Avg train loss: 0.165820\n",
      "Epoch [1028/10000] Avg train loss: 0.165665\n",
      "Epoch [1029/10000] Avg train loss: 0.165511\n",
      "Epoch [1030/10000] Avg train loss: 0.165356\n",
      "Epoch [1031/10000] Avg train loss: 0.165202\n",
      "Epoch [1032/10000] Avg train loss: 0.165052\n",
      "Epoch [1033/10000] Avg train loss: 0.164899\n",
      "Epoch [1034/10000] Avg train loss: 0.164745\n",
      "Epoch [1035/10000] Avg train loss: 0.164598\n",
      "Epoch [1036/10000] Avg train loss: 0.164452\n",
      "Epoch [1037/10000] Avg train loss: 0.164309\n",
      "Epoch [1038/10000] Avg train loss: 0.164159\n",
      "Epoch [1039/10000] Avg train loss: 0.164008\n",
      "Epoch [1040/10000] Avg train loss: 0.163860\n",
      "Epoch [1041/10000] Avg train loss: 0.163709\n",
      "Epoch [1042/10000] Avg train loss: 0.163560\n",
      "Epoch [1043/10000] Avg train loss: 0.163413\n",
      "Epoch [1044/10000] Avg train loss: 0.163268\n",
      "Epoch [1045/10000] Avg train loss: 0.163118\n",
      "Epoch [1046/10000] Avg train loss: 0.162968\n",
      "Epoch [1047/10000] Avg train loss: 0.162819\n",
      "Epoch [1048/10000] Avg train loss: 0.162670\n",
      "Epoch [1049/10000] Avg train loss: 0.162522\n",
      "Epoch [1050/10000] Avg train loss: 0.162375\n",
      "Epoch [1051/10000] Avg train loss: 0.162233\n",
      "Epoch [1052/10000] Avg train loss: 0.162085\n",
      "Epoch [1053/10000] Avg train loss: 0.161937\n",
      "Epoch [1054/10000] Avg train loss: 0.161788\n",
      "Epoch [1055/10000] Avg train loss: 0.161643\n",
      "Epoch [1056/10000] Avg train loss: 0.161500\n",
      "Epoch [1057/10000] Avg train loss: 0.161353\n",
      "Epoch [1058/10000] Avg train loss: 0.161206\n",
      "Epoch [1059/10000] Avg train loss: 0.161061\n",
      "Epoch [1060/10000] Avg train loss: 0.160917\n",
      "Epoch [1061/10000] Avg train loss: 0.160777\n",
      "Epoch [1062/10000] Avg train loss: 0.160634\n",
      "Epoch [1063/10000] Avg train loss: 0.160489\n",
      "Epoch [1064/10000] Avg train loss: 0.160343\n",
      "Epoch [1065/10000] Avg train loss: 0.160199\n",
      "Epoch [1066/10000] Avg train loss: 0.160054\n",
      "Epoch [1067/10000] Avg train loss: 0.159909\n",
      "Epoch [1068/10000] Avg train loss: 0.159766\n",
      "Epoch [1069/10000] Avg train loss: 0.159623\n",
      "Epoch [1070/10000] Avg train loss: 0.159479\n",
      "Epoch [1071/10000] Avg train loss: 0.159335\n",
      "Epoch [1072/10000] Avg train loss: 0.159192\n",
      "Epoch [1073/10000] Avg train loss: 0.159048\n",
      "Epoch [1074/10000] Avg train loss: 0.158905\n",
      "Epoch [1075/10000] Avg train loss: 0.158763\n",
      "Epoch [1076/10000] Avg train loss: 0.158620\n",
      "Epoch [1077/10000] Avg train loss: 0.158481\n",
      "Epoch [1078/10000] Avg train loss: 0.158348\n",
      "Epoch [1079/10000] Avg train loss: 0.158208\n",
      "Epoch [1080/10000] Avg train loss: 0.158067\n",
      "Epoch [1081/10000] Avg train loss: 0.157929\n",
      "Epoch [1082/10000] Avg train loss: 0.157788\n",
      "Epoch [1083/10000] Avg train loss: 0.157650\n",
      "Epoch [1084/10000] Avg train loss: 0.157511\n",
      "Epoch [1085/10000] Avg train loss: 0.157373\n",
      "Epoch [1086/10000] Avg train loss: 0.157233\n",
      "Epoch [1087/10000] Avg train loss: 0.157095\n",
      "Epoch [1088/10000] Avg train loss: 0.156955\n",
      "Epoch [1089/10000] Avg train loss: 0.156816\n",
      "Epoch [1090/10000] Avg train loss: 0.156677\n",
      "Epoch [1091/10000] Avg train loss: 0.156538\n",
      "Epoch [1092/10000] Avg train loss: 0.156399\n",
      "Epoch [1093/10000] Avg train loss: 0.156263\n",
      "Epoch [1094/10000] Avg train loss: 0.156125\n",
      "Epoch [1095/10000] Avg train loss: 0.155998\n",
      "Epoch [1096/10000] Avg train loss: 0.155861\n",
      "Epoch [1097/10000] Avg train loss: 0.155724\n",
      "Epoch [1098/10000] Avg train loss: 0.155587\n",
      "Epoch [1099/10000] Avg train loss: 0.155450\n",
      "Epoch [1100/10000] Avg train loss: 0.155314\n",
      "Epoch [1101/10000] Avg train loss: 0.155178\n",
      "Epoch [1102/10000] Avg train loss: 0.155042\n",
      "Epoch [1103/10000] Avg train loss: 0.154906\n",
      "Epoch [1104/10000] Avg train loss: 0.154774\n",
      "Epoch [1105/10000] Avg train loss: 0.154638\n",
      "Epoch [1106/10000] Avg train loss: 0.154504\n",
      "Epoch [1107/10000] Avg train loss: 0.154370\n",
      "Epoch [1108/10000] Avg train loss: 0.154235\n",
      "Epoch [1109/10000] Avg train loss: 0.154100\n",
      "Epoch [1110/10000] Avg train loss: 0.153969\n",
      "Epoch [1111/10000] Avg train loss: 0.153835\n",
      "Epoch [1112/10000] Avg train loss: 0.153700\n",
      "Epoch [1113/10000] Avg train loss: 0.153568\n",
      "Epoch [1114/10000] Avg train loss: 0.153437\n",
      "Epoch [1115/10000] Avg train loss: 0.153306\n",
      "Epoch [1116/10000] Avg train loss: 0.153173\n",
      "Epoch [1117/10000] Avg train loss: 0.153042\n",
      "Epoch [1118/10000] Avg train loss: 0.152912\n",
      "Epoch [1119/10000] Avg train loss: 0.152780\n",
      "Epoch [1120/10000] Avg train loss: 0.152649\n",
      "Epoch [1121/10000] Avg train loss: 0.152520\n",
      "Epoch [1122/10000] Avg train loss: 0.152391\n",
      "Epoch [1123/10000] Avg train loss: 0.152263\n",
      "Epoch [1124/10000] Avg train loss: 0.152132\n",
      "Epoch [1125/10000] Avg train loss: 0.152006\n",
      "Epoch [1126/10000] Avg train loss: 0.151876\n",
      "Epoch [1127/10000] Avg train loss: 0.151745\n",
      "Epoch [1128/10000] Avg train loss: 0.151616\n",
      "Epoch [1129/10000] Avg train loss: 0.151504\n",
      "Epoch [1130/10000] Avg train loss: 0.151379\n",
      "Epoch [1131/10000] Avg train loss: 0.151250\n",
      "Epoch [1132/10000] Avg train loss: 0.151122\n",
      "Epoch [1133/10000] Avg train loss: 0.150993\n",
      "Epoch [1134/10000] Avg train loss: 0.150865\n",
      "Epoch [1135/10000] Avg train loss: 0.150737\n",
      "Epoch [1136/10000] Avg train loss: 0.150608\n",
      "Epoch [1137/10000] Avg train loss: 0.150480\n",
      "Epoch [1138/10000] Avg train loss: 0.150352\n",
      "Epoch [1139/10000] Avg train loss: 0.150224\n",
      "Epoch [1140/10000] Avg train loss: 0.150096\n",
      "Epoch [1141/10000] Avg train loss: 0.149969\n",
      "Epoch [1142/10000] Avg train loss: 0.149846\n",
      "Epoch [1143/10000] Avg train loss: 0.149721\n",
      "Epoch [1144/10000] Avg train loss: 0.149595\n",
      "Epoch [1145/10000] Avg train loss: 0.149468\n",
      "Epoch [1146/10000] Avg train loss: 0.149341\n",
      "Epoch [1147/10000] Avg train loss: 0.149218\n",
      "Epoch [1148/10000] Avg train loss: 0.149092\n",
      "Epoch [1149/10000] Avg train loss: 0.148966\n",
      "Epoch [1150/10000] Avg train loss: 0.148841\n",
      "Epoch [1151/10000] Avg train loss: 0.148715\n",
      "Epoch [1152/10000] Avg train loss: 0.148590\n",
      "Epoch [1153/10000] Avg train loss: 0.148465\n",
      "Epoch [1154/10000] Avg train loss: 0.148341\n",
      "Epoch [1155/10000] Avg train loss: 0.148218\n",
      "Epoch [1156/10000] Avg train loss: 0.148094\n",
      "Epoch [1157/10000] Avg train loss: 0.147972\n",
      "Epoch [1158/10000] Avg train loss: 0.147851\n",
      "Epoch [1159/10000] Avg train loss: 0.147728\n",
      "Epoch [1160/10000] Avg train loss: 0.147605\n",
      "Epoch [1161/10000] Avg train loss: 0.147491\n",
      "Epoch [1162/10000] Avg train loss: 0.147369\n",
      "Epoch [1163/10000] Avg train loss: 0.147246\n",
      "Epoch [1164/10000] Avg train loss: 0.147123\n",
      "Epoch [1165/10000] Avg train loss: 0.147001\n",
      "Epoch [1166/10000] Avg train loss: 0.146884\n",
      "Epoch [1167/10000] Avg train loss: 0.146763\n",
      "Epoch [1168/10000] Avg train loss: 0.146642\n",
      "Epoch [1169/10000] Avg train loss: 0.146521\n",
      "Epoch [1170/10000] Avg train loss: 0.146402\n",
      "Epoch [1171/10000] Avg train loss: 0.146282\n",
      "Epoch [1172/10000] Avg train loss: 0.146162\n",
      "Epoch [1173/10000] Avg train loss: 0.146041\n",
      "Epoch [1174/10000] Avg train loss: 0.145923\n",
      "Epoch [1175/10000] Avg train loss: 0.145802\n",
      "Epoch [1176/10000] Avg train loss: 0.145682\n",
      "Epoch [1177/10000] Avg train loss: 0.145562\n",
      "Epoch [1178/10000] Avg train loss: 0.145441\n",
      "Epoch [1179/10000] Avg train loss: 0.145321\n",
      "Epoch [1180/10000] Avg train loss: 0.145202\n",
      "Epoch [1181/10000] Avg train loss: 0.145092\n",
      "Epoch [1182/10000] Avg train loss: 0.144974\n",
      "Epoch [1183/10000] Avg train loss: 0.144855\n",
      "Epoch [1184/10000] Avg train loss: 0.144736\n",
      "Epoch [1185/10000] Avg train loss: 0.144618\n",
      "Epoch [1186/10000] Avg train loss: 0.144499\n",
      "Epoch [1187/10000] Avg train loss: 0.144381\n",
      "Epoch [1188/10000] Avg train loss: 0.144262\n",
      "Epoch [1189/10000] Avg train loss: 0.144144\n",
      "Epoch [1190/10000] Avg train loss: 0.144026\n",
      "Epoch [1191/10000] Avg train loss: 0.143908\n",
      "Epoch [1192/10000] Avg train loss: 0.143790\n",
      "Epoch [1193/10000] Avg train loss: 0.143673\n",
      "Epoch [1194/10000] Avg train loss: 0.143556\n",
      "Epoch [1195/10000] Avg train loss: 0.143439\n",
      "Epoch [1196/10000] Avg train loss: 0.143322\n",
      "Epoch [1197/10000] Avg train loss: 0.143205\n",
      "Epoch [1198/10000] Avg train loss: 0.143089\n",
      "Epoch [1199/10000] Avg train loss: 0.142975\n",
      "Epoch [1200/10000] Avg train loss: 0.142861\n",
      "Epoch [1201/10000] Avg train loss: 0.142744\n",
      "Epoch [1202/10000] Avg train loss: 0.142629\n",
      "Epoch [1203/10000] Avg train loss: 0.142515\n",
      "Epoch [1204/10000] Avg train loss: 0.142399\n",
      "Epoch [1205/10000] Avg train loss: 0.142286\n",
      "Epoch [1206/10000] Avg train loss: 0.142174\n",
      "Epoch [1207/10000] Avg train loss: 0.142063\n",
      "Epoch [1208/10000] Avg train loss: 0.141950\n",
      "Epoch [1209/10000] Avg train loss: 0.141836\n",
      "Epoch [1210/10000] Avg train loss: 0.141722\n",
      "Epoch [1211/10000] Avg train loss: 0.141607\n",
      "Epoch [1212/10000] Avg train loss: 0.141493\n",
      "Epoch [1213/10000] Avg train loss: 0.141380\n",
      "Epoch [1214/10000] Avg train loss: 0.141268\n",
      "Epoch [1215/10000] Avg train loss: 0.141155\n",
      "Epoch [1216/10000] Avg train loss: 0.141042\n",
      "Epoch [1217/10000] Avg train loss: 0.140930\n",
      "Epoch [1218/10000] Avg train loss: 0.140817\n",
      "Epoch [1219/10000] Avg train loss: 0.140705\n",
      "Epoch [1220/10000] Avg train loss: 0.140592\n",
      "Epoch [1221/10000] Avg train loss: 0.140479\n",
      "Epoch [1222/10000] Avg train loss: 0.140369\n",
      "Epoch [1223/10000] Avg train loss: 0.140257\n",
      "Epoch [1224/10000] Avg train loss: 0.140145\n",
      "Epoch [1225/10000] Avg train loss: 0.140034\n",
      "Epoch [1226/10000] Avg train loss: 0.139922\n",
      "Epoch [1227/10000] Avg train loss: 0.139811\n",
      "Epoch [1228/10000] Avg train loss: 0.139699\n",
      "Epoch [1229/10000] Avg train loss: 0.139589\n",
      "Epoch [1230/10000] Avg train loss: 0.139479\n",
      "Epoch [1231/10000] Avg train loss: 0.139369\n",
      "Epoch [1232/10000] Avg train loss: 0.139258\n",
      "Epoch [1233/10000] Avg train loss: 0.139148\n",
      "Epoch [1234/10000] Avg train loss: 0.139038\n",
      "Epoch [1235/10000] Avg train loss: 0.138928\n",
      "Epoch [1236/10000] Avg train loss: 0.138818\n",
      "Epoch [1237/10000] Avg train loss: 0.138710\n",
      "Epoch [1238/10000] Avg train loss: 0.138600\n",
      "Epoch [1239/10000] Avg train loss: 0.138491\n",
      "Epoch [1240/10000] Avg train loss: 0.138382\n",
      "Epoch [1241/10000] Avg train loss: 0.138275\n",
      "Epoch [1242/10000] Avg train loss: 0.138178\n",
      "Epoch [1243/10000] Avg train loss: 0.138072\n",
      "Epoch [1244/10000] Avg train loss: 0.137964\n",
      "Epoch [1245/10000] Avg train loss: 0.137860\n",
      "Epoch [1246/10000] Avg train loss: 0.137754\n",
      "Epoch [1247/10000] Avg train loss: 0.137649\n",
      "Epoch [1248/10000] Avg train loss: 0.137542\n",
      "Epoch [1249/10000] Avg train loss: 0.137436\n",
      "Epoch [1250/10000] Avg train loss: 0.137333\n",
      "Epoch [1251/10000] Avg train loss: 0.137252\n",
      "Epoch [1252/10000] Avg train loss: 0.137154\n",
      "Epoch [1253/10000] Avg train loss: 0.137050\n",
      "Epoch [1254/10000] Avg train loss: 0.136947\n",
      "Epoch [1255/10000] Avg train loss: 0.136842\n",
      "Epoch [1256/10000] Avg train loss: 0.136735\n",
      "Epoch [1257/10000] Avg train loss: 0.136631\n",
      "Epoch [1258/10000] Avg train loss: 0.136525\n",
      "Epoch [1259/10000] Avg train loss: 0.136419\n",
      "Epoch [1260/10000] Avg train loss: 0.136315\n",
      "Epoch [1261/10000] Avg train loss: 0.136210\n",
      "Epoch [1262/10000] Avg train loss: 0.136105\n",
      "Epoch [1263/10000] Avg train loss: 0.135999\n",
      "Epoch [1264/10000] Avg train loss: 0.135894\n",
      "Epoch [1265/10000] Avg train loss: 0.135789\n",
      "Epoch [1266/10000] Avg train loss: 0.135686\n",
      "Epoch [1267/10000] Avg train loss: 0.135582\n",
      "Epoch [1268/10000] Avg train loss: 0.135483\n",
      "Epoch [1269/10000] Avg train loss: 0.135379\n",
      "Epoch [1270/10000] Avg train loss: 0.135276\n",
      "Epoch [1271/10000] Avg train loss: 0.135173\n",
      "Epoch [1272/10000] Avg train loss: 0.135070\n",
      "Epoch [1273/10000] Avg train loss: 0.134967\n",
      "Epoch [1274/10000] Avg train loss: 0.134863\n",
      "Epoch [1275/10000] Avg train loss: 0.134761\n",
      "Epoch [1276/10000] Avg train loss: 0.134661\n",
      "Epoch [1277/10000] Avg train loss: 0.134563\n",
      "Epoch [1278/10000] Avg train loss: 0.134462\n",
      "Epoch [1279/10000] Avg train loss: 0.134360\n",
      "Epoch [1280/10000] Avg train loss: 0.134257\n",
      "Epoch [1281/10000] Avg train loss: 0.134155\n",
      "Epoch [1282/10000] Avg train loss: 0.134053\n",
      "Epoch [1283/10000] Avg train loss: 0.133951\n",
      "Epoch [1284/10000] Avg train loss: 0.133851\n",
      "Epoch [1285/10000] Avg train loss: 0.133749\n",
      "Epoch [1286/10000] Avg train loss: 0.133647\n",
      "Epoch [1287/10000] Avg train loss: 0.133546\n",
      "Epoch [1288/10000] Avg train loss: 0.133445\n",
      "Epoch [1289/10000] Avg train loss: 0.133344\n",
      "Epoch [1290/10000] Avg train loss: 0.133244\n",
      "Epoch [1291/10000] Avg train loss: 0.133143\n",
      "Epoch [1292/10000] Avg train loss: 0.133044\n",
      "Epoch [1293/10000] Avg train loss: 0.132944\n",
      "Epoch [1294/10000] Avg train loss: 0.132843\n",
      "Epoch [1295/10000] Avg train loss: 0.132743\n",
      "Epoch [1296/10000] Avg train loss: 0.132644\n",
      "Epoch [1297/10000] Avg train loss: 0.132544\n",
      "Epoch [1298/10000] Avg train loss: 0.132444\n",
      "Epoch [1299/10000] Avg train loss: 0.132353\n",
      "Epoch [1300/10000] Avg train loss: 0.132295\n",
      "Epoch [1301/10000] Avg train loss: 0.132221\n",
      "Epoch [1302/10000] Avg train loss: 0.132136\n",
      "Epoch [1303/10000] Avg train loss: 0.132040\n",
      "Epoch [1304/10000] Avg train loss: 0.131942\n",
      "Epoch [1305/10000] Avg train loss: 0.131844\n",
      "Epoch [1306/10000] Avg train loss: 0.131749\n",
      "Epoch [1307/10000] Avg train loss: 0.131654\n",
      "Epoch [1308/10000] Avg train loss: 0.131610\n",
      "Epoch [1309/10000] Avg train loss: 0.131516\n",
      "Epoch [1310/10000] Avg train loss: 0.131419\n",
      "Epoch [1311/10000] Avg train loss: 0.131324\n",
      "Epoch [1312/10000] Avg train loss: 0.131228\n",
      "Epoch [1313/10000] Avg train loss: 0.131131\n",
      "Epoch [1314/10000] Avg train loss: 0.131035\n",
      "Epoch [1315/10000] Avg train loss: 0.130938\n",
      "Epoch [1316/10000] Avg train loss: 0.130841\n",
      "Epoch [1317/10000] Avg train loss: 0.130744\n",
      "Epoch [1318/10000] Avg train loss: 0.130648\n",
      "Epoch [1319/10000] Avg train loss: 0.130552\n",
      "Epoch [1320/10000] Avg train loss: 0.130458\n",
      "Epoch [1321/10000] Avg train loss: 0.130362\n",
      "Epoch [1322/10000] Avg train loss: 0.130266\n",
      "Epoch [1323/10000] Avg train loss: 0.130171\n",
      "Epoch [1324/10000] Avg train loss: 0.130076\n",
      "Epoch [1325/10000] Avg train loss: 0.129980\n",
      "Epoch [1326/10000] Avg train loss: 0.129883\n",
      "Epoch [1327/10000] Avg train loss: 0.129788\n",
      "Epoch [1328/10000] Avg train loss: 0.129692\n",
      "Epoch [1329/10000] Avg train loss: 0.129596\n",
      "Epoch [1330/10000] Avg train loss: 0.129501\n",
      "Epoch [1331/10000] Avg train loss: 0.129406\n",
      "Epoch [1332/10000] Avg train loss: 0.129313\n",
      "Epoch [1333/10000] Avg train loss: 0.129220\n",
      "Epoch [1334/10000] Avg train loss: 0.129125\n",
      "Epoch [1335/10000] Avg train loss: 0.129030\n",
      "Epoch [1336/10000] Avg train loss: 0.128935\n",
      "Epoch [1337/10000] Avg train loss: 0.128840\n",
      "Epoch [1338/10000] Avg train loss: 0.128746\n",
      "Epoch [1339/10000] Avg train loss: 0.128653\n",
      "Epoch [1340/10000] Avg train loss: 0.128560\n",
      "Epoch [1341/10000] Avg train loss: 0.128467\n",
      "Epoch [1342/10000] Avg train loss: 0.128374\n",
      "Epoch [1343/10000] Avg train loss: 0.128281\n",
      "Epoch [1344/10000] Avg train loss: 0.128187\n",
      "Epoch [1345/10000] Avg train loss: 0.128094\n",
      "Epoch [1346/10000] Avg train loss: 0.128002\n",
      "Epoch [1347/10000] Avg train loss: 0.127908\n",
      "Epoch [1348/10000] Avg train loss: 0.127816\n",
      "Epoch [1349/10000] Avg train loss: 0.127723\n",
      "Epoch [1350/10000] Avg train loss: 0.127630\n",
      "Epoch [1351/10000] Avg train loss: 0.127538\n",
      "Epoch [1352/10000] Avg train loss: 0.127445\n",
      "Epoch [1353/10000] Avg train loss: 0.127355\n",
      "Epoch [1354/10000] Avg train loss: 0.127263\n",
      "Epoch [1355/10000] Avg train loss: 0.127176\n",
      "Epoch [1356/10000] Avg train loss: 0.127085\n",
      "Epoch [1357/10000] Avg train loss: 0.126993\n",
      "Epoch [1358/10000] Avg train loss: 0.126902\n",
      "Epoch [1359/10000] Avg train loss: 0.126811\n",
      "Epoch [1360/10000] Avg train loss: 0.126719\n",
      "Epoch [1361/10000] Avg train loss: 0.126628\n",
      "Epoch [1362/10000] Avg train loss: 0.126536\n",
      "Epoch [1363/10000] Avg train loss: 0.126445\n",
      "Epoch [1364/10000] Avg train loss: 0.126354\n",
      "Epoch [1365/10000] Avg train loss: 0.126263\n",
      "Epoch [1366/10000] Avg train loss: 0.126172\n",
      "Epoch [1367/10000] Avg train loss: 0.126081\n",
      "Epoch [1368/10000] Avg train loss: 0.125991\n",
      "Epoch [1369/10000] Avg train loss: 0.125900\n",
      "Epoch [1370/10000] Avg train loss: 0.125811\n",
      "Epoch [1371/10000] Avg train loss: 0.125721\n",
      "Epoch [1372/10000] Avg train loss: 0.125631\n",
      "Epoch [1373/10000] Avg train loss: 0.125542\n",
      "Epoch [1374/10000] Avg train loss: 0.125452\n",
      "Epoch [1375/10000] Avg train loss: 0.125362\n",
      "Epoch [1376/10000] Avg train loss: 0.125272\n",
      "Epoch [1377/10000] Avg train loss: 0.125183\n",
      "Epoch [1378/10000] Avg train loss: 0.125093\n",
      "Epoch [1379/10000] Avg train loss: 0.125004\n",
      "Epoch [1380/10000] Avg train loss: 0.124916\n",
      "Epoch [1381/10000] Avg train loss: 0.124827\n",
      "Epoch [1382/10000] Avg train loss: 0.124738\n",
      "Epoch [1383/10000] Avg train loss: 0.124649\n",
      "Epoch [1384/10000] Avg train loss: 0.124560\n",
      "Epoch [1385/10000] Avg train loss: 0.124471\n",
      "Epoch [1386/10000] Avg train loss: 0.124383\n",
      "Epoch [1387/10000] Avg train loss: 0.124296\n",
      "Epoch [1388/10000] Avg train loss: 0.124208\n",
      "Epoch [1389/10000] Avg train loss: 0.124120\n",
      "Epoch [1390/10000] Avg train loss: 0.124032\n",
      "Epoch [1391/10000] Avg train loss: 0.123944\n",
      "Epoch [1392/10000] Avg train loss: 0.123856\n",
      "Epoch [1393/10000] Avg train loss: 0.123769\n",
      "Epoch [1394/10000] Avg train loss: 0.123681\n",
      "Epoch [1395/10000] Avg train loss: 0.123593\n",
      "Epoch [1396/10000] Avg train loss: 0.123506\n",
      "Epoch [1397/10000] Avg train loss: 0.123419\n",
      "Epoch [1398/10000] Avg train loss: 0.123332\n",
      "Epoch [1399/10000] Avg train loss: 0.123246\n",
      "Epoch [1400/10000] Avg train loss: 0.123160\n",
      "Epoch [1401/10000] Avg train loss: 0.123073\n",
      "Epoch [1402/10000] Avg train loss: 0.122987\n",
      "Epoch [1403/10000] Avg train loss: 0.122900\n",
      "Epoch [1404/10000] Avg train loss: 0.122814\n",
      "Epoch [1405/10000] Avg train loss: 0.122728\n",
      "Epoch [1406/10000] Avg train loss: 0.122642\n",
      "Epoch [1407/10000] Avg train loss: 0.122556\n",
      "Epoch [1408/10000] Avg train loss: 0.122471\n",
      "Epoch [1409/10000] Avg train loss: 0.122385\n",
      "Epoch [1410/10000] Avg train loss: 0.122300\n",
      "Epoch [1411/10000] Avg train loss: 0.122214\n",
      "Epoch [1412/10000] Avg train loss: 0.122129\n",
      "Epoch [1413/10000] Avg train loss: 0.122044\n",
      "Epoch [1414/10000] Avg train loss: 0.121959\n",
      "Epoch [1415/10000] Avg train loss: 0.121874\n",
      "Epoch [1416/10000] Avg train loss: 0.121789\n",
      "Epoch [1417/10000] Avg train loss: 0.121705\n",
      "Epoch [1418/10000] Avg train loss: 0.121625\n",
      "Epoch [1419/10000] Avg train loss: 0.121541\n",
      "Epoch [1420/10000] Avg train loss: 0.121458\n",
      "Epoch [1421/10000] Avg train loss: 0.121374\n",
      "Epoch [1422/10000] Avg train loss: 0.121292\n",
      "Epoch [1423/10000] Avg train loss: 0.121208\n",
      "Epoch [1424/10000] Avg train loss: 0.121125\n",
      "Epoch [1425/10000] Avg train loss: 0.121041\n",
      "Epoch [1426/10000] Avg train loss: 0.120958\n",
      "Epoch [1427/10000] Avg train loss: 0.120874\n",
      "Epoch [1428/10000] Avg train loss: 0.120791\n",
      "Epoch [1429/10000] Avg train loss: 0.120711\n",
      "Epoch [1430/10000] Avg train loss: 0.120630\n",
      "Epoch [1431/10000] Avg train loss: 0.120548\n",
      "Epoch [1432/10000] Avg train loss: 0.120465\n",
      "Epoch [1433/10000] Avg train loss: 0.120382\n",
      "Epoch [1434/10000] Avg train loss: 0.120299\n",
      "Epoch [1435/10000] Avg train loss: 0.120217\n",
      "Epoch [1436/10000] Avg train loss: 0.120135\n",
      "Epoch [1437/10000] Avg train loss: 0.120053\n",
      "Epoch [1438/10000] Avg train loss: 0.119971\n",
      "Epoch [1439/10000] Avg train loss: 0.119889\n",
      "Epoch [1440/10000] Avg train loss: 0.119808\n",
      "Epoch [1441/10000] Avg train loss: 0.119726\n",
      "Epoch [1442/10000] Avg train loss: 0.119645\n",
      "Epoch [1443/10000] Avg train loss: 0.119563\n",
      "Epoch [1444/10000] Avg train loss: 0.119482\n",
      "Epoch [1445/10000] Avg train loss: 0.119401\n",
      "Epoch [1446/10000] Avg train loss: 0.119319\n",
      "Epoch [1447/10000] Avg train loss: 0.119238\n",
      "Epoch [1448/10000] Avg train loss: 0.119156\n",
      "Epoch [1449/10000] Avg train loss: 0.119078\n",
      "Epoch [1450/10000] Avg train loss: 0.118998\n",
      "Epoch [1451/10000] Avg train loss: 0.118918\n",
      "Epoch [1452/10000] Avg train loss: 0.118838\n",
      "Epoch [1453/10000] Avg train loss: 0.118758\n",
      "Epoch [1454/10000] Avg train loss: 0.118678\n",
      "Epoch [1455/10000] Avg train loss: 0.118600\n",
      "Epoch [1456/10000] Avg train loss: 0.118520\n",
      "Epoch [1457/10000] Avg train loss: 0.118439\n",
      "Epoch [1458/10000] Avg train loss: 0.118359\n",
      "Epoch [1459/10000] Avg train loss: 0.118280\n",
      "Epoch [1460/10000] Avg train loss: 0.118200\n",
      "Epoch [1461/10000] Avg train loss: 0.118121\n",
      "Epoch [1462/10000] Avg train loss: 0.118042\n",
      "Epoch [1463/10000] Avg train loss: 0.117962\n",
      "Epoch [1464/10000] Avg train loss: 0.117883\n",
      "Epoch [1465/10000] Avg train loss: 0.117803\n",
      "Epoch [1466/10000] Avg train loss: 0.117724\n",
      "Epoch [1467/10000] Avg train loss: 0.117645\n",
      "Epoch [1468/10000] Avg train loss: 0.117566\n",
      "Epoch [1469/10000] Avg train loss: 0.117487\n",
      "Epoch [1470/10000] Avg train loss: 0.117408\n",
      "Epoch [1471/10000] Avg train loss: 0.117329\n",
      "Epoch [1472/10000] Avg train loss: 0.117250\n",
      "Epoch [1473/10000] Avg train loss: 0.117172\n",
      "Epoch [1474/10000] Avg train loss: 0.117093\n",
      "Epoch [1475/10000] Avg train loss: 0.117015\n",
      "Epoch [1476/10000] Avg train loss: 0.116937\n",
      "Epoch [1477/10000] Avg train loss: 0.116858\n",
      "Epoch [1478/10000] Avg train loss: 0.116780\n",
      "Epoch [1479/10000] Avg train loss: 0.116702\n",
      "Epoch [1480/10000] Avg train loss: 0.116624\n",
      "Epoch [1481/10000] Avg train loss: 0.116546\n",
      "Epoch [1482/10000] Avg train loss: 0.116469\n",
      "Epoch [1483/10000] Avg train loss: 0.116391\n",
      "Epoch [1484/10000] Avg train loss: 0.116313\n",
      "Epoch [1485/10000] Avg train loss: 0.116236\n",
      "Epoch [1486/10000] Avg train loss: 0.116159\n",
      "Epoch [1487/10000] Avg train loss: 0.116082\n",
      "Epoch [1488/10000] Avg train loss: 0.116005\n",
      "Epoch [1489/10000] Avg train loss: 0.115928\n",
      "Epoch [1490/10000] Avg train loss: 0.115851\n",
      "Epoch [1491/10000] Avg train loss: 0.115774\n",
      "Epoch [1492/10000] Avg train loss: 0.115698\n",
      "Epoch [1493/10000] Avg train loss: 0.115621\n",
      "Epoch [1494/10000] Avg train loss: 0.115545\n",
      "Epoch [1495/10000] Avg train loss: 0.115469\n",
      "Epoch [1496/10000] Avg train loss: 0.115394\n",
      "Epoch [1497/10000] Avg train loss: 0.115319\n",
      "Epoch [1498/10000] Avg train loss: 0.115244\n",
      "Epoch [1499/10000] Avg train loss: 0.115169\n",
      "Epoch [1500/10000] Avg train loss: 0.115094\n",
      "Epoch [1501/10000] Avg train loss: 0.115018\n",
      "Epoch [1502/10000] Avg train loss: 0.114943\n",
      "Epoch [1503/10000] Avg train loss: 0.114868\n",
      "Epoch [1504/10000] Avg train loss: 0.114792\n",
      "Epoch [1505/10000] Avg train loss: 0.114717\n",
      "Epoch [1506/10000] Avg train loss: 0.114642\n",
      "Epoch [1507/10000] Avg train loss: 0.114567\n",
      "Epoch [1508/10000] Avg train loss: 0.114492\n",
      "Epoch [1509/10000] Avg train loss: 0.114417\n",
      "Epoch [1510/10000] Avg train loss: 0.114344\n",
      "Epoch [1511/10000] Avg train loss: 0.114271\n",
      "Epoch [1512/10000] Avg train loss: 0.114196\n",
      "Epoch [1513/10000] Avg train loss: 0.114121\n",
      "Epoch [1514/10000] Avg train loss: 0.114047\n",
      "Epoch [1515/10000] Avg train loss: 0.113972\n",
      "Epoch [1516/10000] Avg train loss: 0.113898\n",
      "Epoch [1517/10000] Avg train loss: 0.113824\n",
      "Epoch [1518/10000] Avg train loss: 0.113751\n",
      "Epoch [1519/10000] Avg train loss: 0.113677\n",
      "Epoch [1520/10000] Avg train loss: 0.113604\n",
      "Epoch [1521/10000] Avg train loss: 0.113530\n",
      "Epoch [1522/10000] Avg train loss: 0.113456\n",
      "Epoch [1523/10000] Avg train loss: 0.113382\n",
      "Epoch [1524/10000] Avg train loss: 0.113309\n",
      "Epoch [1525/10000] Avg train loss: 0.113235\n",
      "Epoch [1526/10000] Avg train loss: 0.113162\n",
      "Epoch [1527/10000] Avg train loss: 0.113088\n",
      "Epoch [1528/10000] Avg train loss: 0.113015\n",
      "Epoch [1529/10000] Avg train loss: 0.112942\n",
      "Epoch [1530/10000] Avg train loss: 0.112869\n",
      "Epoch [1531/10000] Avg train loss: 0.112799\n",
      "Epoch [1532/10000] Avg train loss: 0.112727\n",
      "Epoch [1533/10000] Avg train loss: 0.112654\n",
      "Epoch [1534/10000] Avg train loss: 0.112582\n",
      "Epoch [1535/10000] Avg train loss: 0.112509\n",
      "Epoch [1536/10000] Avg train loss: 0.112437\n",
      "Epoch [1537/10000] Avg train loss: 0.112365\n",
      "Epoch [1538/10000] Avg train loss: 0.112293\n",
      "Epoch [1539/10000] Avg train loss: 0.112221\n",
      "Epoch [1540/10000] Avg train loss: 0.112149\n",
      "Epoch [1541/10000] Avg train loss: 0.112077\n",
      "Epoch [1542/10000] Avg train loss: 0.112005\n",
      "Epoch [1543/10000] Avg train loss: 0.111933\n",
      "Epoch [1544/10000] Avg train loss: 0.111863\n",
      "Epoch [1545/10000] Avg train loss: 0.111792\n",
      "Epoch [1546/10000] Avg train loss: 0.111720\n",
      "Epoch [1547/10000] Avg train loss: 0.111649\n",
      "Epoch [1548/10000] Avg train loss: 0.111578\n",
      "Epoch [1549/10000] Avg train loss: 0.111507\n",
      "Epoch [1550/10000] Avg train loss: 0.111437\n",
      "Epoch [1551/10000] Avg train loss: 0.111366\n",
      "Epoch [1552/10000] Avg train loss: 0.111296\n",
      "Epoch [1553/10000] Avg train loss: 0.111227\n",
      "Epoch [1554/10000] Avg train loss: 0.111156\n",
      "Epoch [1555/10000] Avg train loss: 0.111088\n",
      "Epoch [1556/10000] Avg train loss: 0.111018\n",
      "Epoch [1557/10000] Avg train loss: 0.110948\n",
      "Epoch [1558/10000] Avg train loss: 0.110878\n",
      "Epoch [1559/10000] Avg train loss: 0.110808\n",
      "Epoch [1560/10000] Avg train loss: 0.110738\n",
      "Epoch [1561/10000] Avg train loss: 0.110667\n",
      "Epoch [1562/10000] Avg train loss: 0.110597\n",
      "Epoch [1563/10000] Avg train loss: 0.110527\n",
      "Epoch [1564/10000] Avg train loss: 0.110457\n",
      "Epoch [1565/10000] Avg train loss: 0.110387\n",
      "Epoch [1566/10000] Avg train loss: 0.110317\n",
      "Epoch [1567/10000] Avg train loss: 0.110247\n",
      "Epoch [1568/10000] Avg train loss: 0.110178\n",
      "Epoch [1569/10000] Avg train loss: 0.110109\n",
      "Epoch [1570/10000] Avg train loss: 0.110040\n",
      "Epoch [1571/10000] Avg train loss: 0.109970\n",
      "Epoch [1572/10000] Avg train loss: 0.109901\n",
      "Epoch [1573/10000] Avg train loss: 0.109832\n",
      "Epoch [1574/10000] Avg train loss: 0.109763\n",
      "Epoch [1575/10000] Avg train loss: 0.109696\n",
      "Epoch [1576/10000] Avg train loss: 0.109628\n",
      "Epoch [1577/10000] Avg train loss: 0.109562\n",
      "Epoch [1578/10000] Avg train loss: 0.109495\n",
      "Epoch [1579/10000] Avg train loss: 0.109427\n",
      "Epoch [1580/10000] Avg train loss: 0.109358\n",
      "Epoch [1581/10000] Avg train loss: 0.109290\n",
      "Epoch [1582/10000] Avg train loss: 0.109221\n",
      "Epoch [1583/10000] Avg train loss: 0.109153\n",
      "Epoch [1584/10000] Avg train loss: 0.109084\n",
      "Epoch [1585/10000] Avg train loss: 0.109016\n",
      "Epoch [1586/10000] Avg train loss: 0.108948\n",
      "Epoch [1587/10000] Avg train loss: 0.108880\n",
      "Epoch [1588/10000] Avg train loss: 0.108813\n",
      "Epoch [1589/10000] Avg train loss: 0.108746\n",
      "Epoch [1590/10000] Avg train loss: 0.108678\n",
      "Epoch [1591/10000] Avg train loss: 0.108611\n",
      "Epoch [1592/10000] Avg train loss: 0.108543\n",
      "Epoch [1593/10000] Avg train loss: 0.108476\n",
      "Epoch [1594/10000] Avg train loss: 0.108409\n",
      "Epoch [1595/10000] Avg train loss: 0.108341\n",
      "Epoch [1596/10000] Avg train loss: 0.108274\n",
      "Epoch [1597/10000] Avg train loss: 0.108207\n",
      "Epoch [1598/10000] Avg train loss: 0.108143\n",
      "Epoch [1599/10000] Avg train loss: 0.108077\n",
      "Epoch [1600/10000] Avg train loss: 0.108011\n",
      "Epoch [1601/10000] Avg train loss: 0.107944\n",
      "Epoch [1602/10000] Avg train loss: 0.107878\n",
      "Epoch [1603/10000] Avg train loss: 0.107811\n",
      "Epoch [1604/10000] Avg train loss: 0.107744\n",
      "Epoch [1605/10000] Avg train loss: 0.107678\n",
      "Epoch [1606/10000] Avg train loss: 0.107611\n",
      "Epoch [1607/10000] Avg train loss: 0.107545\n",
      "Epoch [1608/10000] Avg train loss: 0.107479\n",
      "Epoch [1609/10000] Avg train loss: 0.107413\n",
      "Epoch [1610/10000] Avg train loss: 0.107347\n",
      "Epoch [1611/10000] Avg train loss: 0.107281\n",
      "Epoch [1612/10000] Avg train loss: 0.107216\n",
      "Epoch [1613/10000] Avg train loss: 0.107150\n",
      "Epoch [1614/10000] Avg train loss: 0.107084\n",
      "Epoch [1615/10000] Avg train loss: 0.107019\n",
      "Epoch [1616/10000] Avg train loss: 0.106954\n",
      "Epoch [1617/10000] Avg train loss: 0.106890\n",
      "Epoch [1618/10000] Avg train loss: 0.106825\n",
      "Epoch [1619/10000] Avg train loss: 0.106760\n",
      "Epoch [1620/10000] Avg train loss: 0.106695\n",
      "Epoch [1621/10000] Avg train loss: 0.106630\n",
      "Epoch [1622/10000] Avg train loss: 0.106566\n",
      "Epoch [1623/10000] Avg train loss: 0.106501\n",
      "Epoch [1624/10000] Avg train loss: 0.106436\n",
      "Epoch [1625/10000] Avg train loss: 0.106371\n",
      "Epoch [1626/10000] Avg train loss: 0.106306\n",
      "Epoch [1627/10000] Avg train loss: 0.106241\n",
      "Epoch [1628/10000] Avg train loss: 0.106176\n",
      "Epoch [1629/10000] Avg train loss: 0.106111\n",
      "Epoch [1630/10000] Avg train loss: 0.106047\n",
      "Epoch [1631/10000] Avg train loss: 0.105982\n",
      "Epoch [1632/10000] Avg train loss: 0.105918\n",
      "Epoch [1633/10000] Avg train loss: 0.105854\n",
      "Epoch [1634/10000] Avg train loss: 0.105790\n",
      "Epoch [1635/10000] Avg train loss: 0.105726\n",
      "Epoch [1636/10000] Avg train loss: 0.105662\n",
      "Epoch [1637/10000] Avg train loss: 0.105598\n",
      "Epoch [1638/10000] Avg train loss: 0.105534\n",
      "Epoch [1639/10000] Avg train loss: 0.105470\n",
      "Epoch [1640/10000] Avg train loss: 0.105406\n",
      "Epoch [1641/10000] Avg train loss: 0.105342\n",
      "Epoch [1642/10000] Avg train loss: 0.105279\n",
      "Epoch [1643/10000] Avg train loss: 0.105216\n",
      "Epoch [1644/10000] Avg train loss: 0.105152\n",
      "Epoch [1645/10000] Avg train loss: 0.105089\n",
      "Epoch [1646/10000] Avg train loss: 0.105025\n",
      "Epoch [1647/10000] Avg train loss: 0.104962\n",
      "Epoch [1648/10000] Avg train loss: 0.104899\n",
      "Epoch [1649/10000] Avg train loss: 0.104836\n",
      "Epoch [1650/10000] Avg train loss: 0.104773\n",
      "Epoch [1651/10000] Avg train loss: 0.104711\n",
      "Epoch [1652/10000] Avg train loss: 0.104648\n",
      "Epoch [1653/10000] Avg train loss: 0.104586\n",
      "Epoch [1654/10000] Avg train loss: 0.104523\n",
      "Epoch [1655/10000] Avg train loss: 0.104461\n",
      "Epoch [1656/10000] Avg train loss: 0.104399\n",
      "Epoch [1657/10000] Avg train loss: 0.104336\n",
      "Epoch [1658/10000] Avg train loss: 0.104274\n",
      "Epoch [1659/10000] Avg train loss: 0.104212\n",
      "Epoch [1660/10000] Avg train loss: 0.104150\n",
      "Epoch [1661/10000] Avg train loss: 0.104091\n",
      "Epoch [1662/10000] Avg train loss: 0.104030\n",
      "Epoch [1663/10000] Avg train loss: 0.103969\n",
      "Epoch [1664/10000] Avg train loss: 0.103908\n",
      "Epoch [1665/10000] Avg train loss: 0.103846\n",
      "Epoch [1666/10000] Avg train loss: 0.103785\n",
      "Epoch [1667/10000] Avg train loss: 0.103723\n",
      "Epoch [1668/10000] Avg train loss: 0.103661\n",
      "Epoch [1669/10000] Avg train loss: 0.103600\n",
      "Epoch [1670/10000] Avg train loss: 0.103538\n",
      "Epoch [1671/10000] Avg train loss: 0.103477\n",
      "Epoch [1672/10000] Avg train loss: 0.103416\n",
      "Epoch [1673/10000] Avg train loss: 0.103356\n",
      "Epoch [1674/10000] Avg train loss: 0.103295\n",
      "Epoch [1675/10000] Avg train loss: 0.103234\n",
      "Epoch [1676/10000] Avg train loss: 0.103174\n",
      "Epoch [1677/10000] Avg train loss: 0.103114\n",
      "Epoch [1678/10000] Avg train loss: 0.103054\n",
      "Epoch [1679/10000] Avg train loss: 0.102993\n",
      "Epoch [1680/10000] Avg train loss: 0.102932\n",
      "Epoch [1681/10000] Avg train loss: 0.102871\n",
      "Epoch [1682/10000] Avg train loss: 0.102810\n",
      "Epoch [1683/10000] Avg train loss: 0.102750\n",
      "Epoch [1684/10000] Avg train loss: 0.102689\n",
      "Epoch [1685/10000] Avg train loss: 0.102629\n",
      "Epoch [1686/10000] Avg train loss: 0.102568\n",
      "Epoch [1687/10000] Avg train loss: 0.102508\n",
      "Epoch [1688/10000] Avg train loss: 0.102447\n",
      "Epoch [1689/10000] Avg train loss: 0.102387\n",
      "Epoch [1690/10000] Avg train loss: 0.102327\n",
      "Epoch [1691/10000] Avg train loss: 0.102267\n",
      "Epoch [1692/10000] Avg train loss: 0.102206\n",
      "Epoch [1693/10000] Avg train loss: 0.102147\n",
      "Epoch [1694/10000] Avg train loss: 0.102087\n",
      "Epoch [1695/10000] Avg train loss: 0.102027\n",
      "Epoch [1696/10000] Avg train loss: 0.101967\n",
      "Epoch [1697/10000] Avg train loss: 0.101908\n",
      "Epoch [1698/10000] Avg train loss: 0.101848\n",
      "Epoch [1699/10000] Avg train loss: 0.101788\n",
      "Epoch [1700/10000] Avg train loss: 0.101729\n",
      "Epoch [1701/10000] Avg train loss: 0.101670\n",
      "Epoch [1702/10000] Avg train loss: 0.101610\n",
      "Epoch [1703/10000] Avg train loss: 0.101551\n",
      "Epoch [1704/10000] Avg train loss: 0.101492\n",
      "Epoch [1705/10000] Avg train loss: 0.101433\n",
      "Epoch [1706/10000] Avg train loss: 0.101374\n",
      "Epoch [1707/10000] Avg train loss: 0.101316\n",
      "Epoch [1708/10000] Avg train loss: 0.101257\n",
      "Epoch [1709/10000] Avg train loss: 0.101199\n",
      "Epoch [1710/10000] Avg train loss: 0.101140\n",
      "Epoch [1711/10000] Avg train loss: 0.101081\n",
      "Epoch [1712/10000] Avg train loss: 0.101023\n",
      "Epoch [1713/10000] Avg train loss: 0.100965\n",
      "Epoch [1714/10000] Avg train loss: 0.100906\n",
      "Epoch [1715/10000] Avg train loss: 0.100848\n",
      "Epoch [1716/10000] Avg train loss: 0.100789\n",
      "Epoch [1717/10000] Avg train loss: 0.100731\n",
      "Epoch [1718/10000] Avg train loss: 0.100673\n",
      "Epoch [1719/10000] Avg train loss: 0.100615\n",
      "Epoch [1720/10000] Avg train loss: 0.100556\n",
      "Epoch [1721/10000] Avg train loss: 0.100498\n",
      "Epoch [1722/10000] Avg train loss: 0.100440\n",
      "Epoch [1723/10000] Avg train loss: 0.100382\n",
      "Epoch [1724/10000] Avg train loss: 0.100324\n",
      "Epoch [1725/10000] Avg train loss: 0.100266\n",
      "Epoch [1726/10000] Avg train loss: 0.100209\n",
      "Epoch [1727/10000] Avg train loss: 0.100151\n",
      "Epoch [1728/10000] Avg train loss: 0.100093\n",
      "Epoch [1729/10000] Avg train loss: 0.100036\n",
      "Epoch [1730/10000] Avg train loss: 0.099978\n",
      "Epoch [1731/10000] Avg train loss: 0.099921\n",
      "Epoch [1732/10000] Avg train loss: 0.099864\n",
      "Epoch [1733/10000] Avg train loss: 0.099807\n",
      "Epoch [1734/10000] Avg train loss: 0.099749\n",
      "Epoch [1735/10000] Avg train loss: 0.099692\n",
      "Epoch [1736/10000] Avg train loss: 0.099635\n",
      "Epoch [1737/10000] Avg train loss: 0.099578\n",
      "Epoch [1738/10000] Avg train loss: 0.099522\n",
      "Epoch [1739/10000] Avg train loss: 0.099465\n",
      "Epoch [1740/10000] Avg train loss: 0.099408\n",
      "Epoch [1741/10000] Avg train loss: 0.099351\n",
      "Epoch [1742/10000] Avg train loss: 0.099295\n",
      "Epoch [1743/10000] Avg train loss: 0.099238\n",
      "Epoch [1744/10000] Avg train loss: 0.099182\n",
      "Epoch [1745/10000] Avg train loss: 0.099125\n",
      "Epoch [1746/10000] Avg train loss: 0.099069\n",
      "Epoch [1747/10000] Avg train loss: 0.099012\n",
      "Epoch [1748/10000] Avg train loss: 0.098956\n",
      "Epoch [1749/10000] Avg train loss: 0.098900\n",
      "Epoch [1750/10000] Avg train loss: 0.098844\n",
      "Epoch [1751/10000] Avg train loss: 0.098788\n",
      "Epoch [1752/10000] Avg train loss: 0.098732\n",
      "Epoch [1753/10000] Avg train loss: 0.098676\n",
      "Epoch [1754/10000] Avg train loss: 0.098620\n",
      "Epoch [1755/10000] Avg train loss: 0.098564\n",
      "Epoch [1756/10000] Avg train loss: 0.098508\n",
      "Epoch [1757/10000] Avg train loss: 0.098452\n",
      "Epoch [1758/10000] Avg train loss: 0.098397\n",
      "Epoch [1759/10000] Avg train loss: 0.098341\n",
      "Epoch [1760/10000] Avg train loss: 0.098286\n",
      "Epoch [1761/10000] Avg train loss: 0.098230\n",
      "Epoch [1762/10000] Avg train loss: 0.098175\n",
      "Epoch [1763/10000] Avg train loss: 0.098119\n",
      "Epoch [1764/10000] Avg train loss: 0.098064\n",
      "Epoch [1765/10000] Avg train loss: 0.098008\n",
      "Epoch [1766/10000] Avg train loss: 0.097953\n",
      "Epoch [1767/10000] Avg train loss: 0.097898\n",
      "Epoch [1768/10000] Avg train loss: 0.097843\n",
      "Epoch [1769/10000] Avg train loss: 0.097788\n",
      "Epoch [1770/10000] Avg train loss: 0.097733\n",
      "Epoch [1771/10000] Avg train loss: 0.097678\n",
      "Epoch [1772/10000] Avg train loss: 0.097623\n",
      "Epoch [1773/10000] Avg train loss: 0.097569\n",
      "Epoch [1774/10000] Avg train loss: 0.097515\n",
      "Epoch [1775/10000] Avg train loss: 0.097460\n",
      "Epoch [1776/10000] Avg train loss: 0.097406\n",
      "Epoch [1777/10000] Avg train loss: 0.097352\n",
      "Epoch [1778/10000] Avg train loss: 0.097297\n",
      "Epoch [1779/10000] Avg train loss: 0.097243\n",
      "Epoch [1780/10000] Avg train loss: 0.097188\n",
      "Epoch [1781/10000] Avg train loss: 0.097134\n",
      "Epoch [1782/10000] Avg train loss: 0.097080\n",
      "Epoch [1783/10000] Avg train loss: 0.097026\n",
      "Epoch [1784/10000] Avg train loss: 0.096972\n",
      "Epoch [1785/10000] Avg train loss: 0.096919\n",
      "Epoch [1786/10000] Avg train loss: 0.096871\n",
      "Epoch [1787/10000] Avg train loss: 0.096818\n",
      "Epoch [1788/10000] Avg train loss: 0.096765\n",
      "Epoch [1789/10000] Avg train loss: 0.096713\n",
      "Epoch [1790/10000] Avg train loss: 0.096661\n",
      "Epoch [1791/10000] Avg train loss: 0.096609\n",
      "Epoch [1792/10000] Avg train loss: 0.096555\n",
      "Epoch [1793/10000] Avg train loss: 0.096502\n",
      "Epoch [1794/10000] Avg train loss: 0.096448\n",
      "Epoch [1795/10000] Avg train loss: 0.096395\n",
      "Epoch [1796/10000] Avg train loss: 0.096341\n",
      "Epoch [1797/10000] Avg train loss: 0.096288\n",
      "Epoch [1798/10000] Avg train loss: 0.096235\n",
      "Epoch [1799/10000] Avg train loss: 0.096182\n",
      "Epoch [1800/10000] Avg train loss: 0.096129\n",
      "Epoch [1801/10000] Avg train loss: 0.096076\n",
      "Epoch [1802/10000] Avg train loss: 0.096023\n",
      "Epoch [1803/10000] Avg train loss: 0.095971\n",
      "Epoch [1804/10000] Avg train loss: 0.095918\n",
      "Epoch [1805/10000] Avg train loss: 0.095866\n",
      "Epoch [1806/10000] Avg train loss: 0.095813\n",
      "Epoch [1807/10000] Avg train loss: 0.095761\n",
      "Epoch [1808/10000] Avg train loss: 0.095708\n",
      "Epoch [1809/10000] Avg train loss: 0.095656\n",
      "Epoch [1810/10000] Avg train loss: 0.095603\n",
      "Epoch [1811/10000] Avg train loss: 0.095551\n",
      "Epoch [1812/10000] Avg train loss: 0.095499\n",
      "Epoch [1813/10000] Avg train loss: 0.095466\n",
      "Epoch [1814/10000] Avg train loss: 0.095435\n",
      "Epoch [1815/10000] Avg train loss: 0.095386\n",
      "Epoch [1816/10000] Avg train loss: 0.095339\n",
      "Epoch [1817/10000] Avg train loss: 0.095294\n",
      "Epoch [1818/10000] Avg train loss: 0.095246\n",
      "Epoch [1819/10000] Avg train loss: 0.095196\n",
      "Epoch [1820/10000] Avg train loss: 0.095144\n",
      "Epoch [1821/10000] Avg train loss: 0.095093\n",
      "Epoch [1822/10000] Avg train loss: 0.095042\n",
      "Epoch [1823/10000] Avg train loss: 0.094990\n",
      "Epoch [1824/10000] Avg train loss: 0.094938\n",
      "Epoch [1825/10000] Avg train loss: 0.094887\n",
      "Epoch [1826/10000] Avg train loss: 0.094835\n",
      "Epoch [1827/10000] Avg train loss: 0.094784\n",
      "Epoch [1828/10000] Avg train loss: 0.094732\n",
      "Epoch [1829/10000] Avg train loss: 0.094681\n",
      "Epoch [1830/10000] Avg train loss: 0.094630\n",
      "Epoch [1831/10000] Avg train loss: 0.094578\n",
      "Epoch [1832/10000] Avg train loss: 0.094527\n",
      "Epoch [1833/10000] Avg train loss: 0.094476\n",
      "Epoch [1834/10000] Avg train loss: 0.094425\n",
      "Epoch [1835/10000] Avg train loss: 0.094373\n",
      "Epoch [1836/10000] Avg train loss: 0.094323\n",
      "Epoch [1837/10000] Avg train loss: 0.094272\n",
      "Epoch [1838/10000] Avg train loss: 0.094221\n",
      "Epoch [1839/10000] Avg train loss: 0.094170\n",
      "Epoch [1840/10000] Avg train loss: 0.094119\n",
      "Epoch [1841/10000] Avg train loss: 0.094068\n",
      "Epoch [1842/10000] Avg train loss: 0.094017\n",
      "Epoch [1843/10000] Avg train loss: 0.093966\n",
      "Epoch [1844/10000] Avg train loss: 0.093916\n",
      "Epoch [1845/10000] Avg train loss: 0.093865\n",
      "Epoch [1846/10000] Avg train loss: 0.093814\n",
      "Epoch [1847/10000] Avg train loss: 0.093764\n",
      "Epoch [1848/10000] Avg train loss: 0.093713\n",
      "Epoch [1849/10000] Avg train loss: 0.093663\n",
      "Epoch [1850/10000] Avg train loss: 0.093613\n",
      "Epoch [1851/10000] Avg train loss: 0.093563\n",
      "Epoch [1852/10000] Avg train loss: 0.093513\n",
      "Epoch [1853/10000] Avg train loss: 0.093463\n",
      "Epoch [1854/10000] Avg train loss: 0.093417\n",
      "Epoch [1855/10000] Avg train loss: 0.093368\n",
      "Epoch [1856/10000] Avg train loss: 0.093319\n",
      "Epoch [1857/10000] Avg train loss: 0.093270\n",
      "Epoch [1858/10000] Avg train loss: 0.093221\n",
      "Epoch [1859/10000] Avg train loss: 0.093172\n",
      "Epoch [1860/10000] Avg train loss: 0.093136\n",
      "Epoch [1861/10000] Avg train loss: 0.093091\n",
      "Epoch [1862/10000] Avg train loss: 0.093043\n",
      "Epoch [1863/10000] Avg train loss: 0.092999\n",
      "Epoch [1864/10000] Avg train loss: 0.093003\n",
      "Epoch [1865/10000] Avg train loss: 0.092959\n",
      "Epoch [1866/10000] Avg train loss: 0.092916\n",
      "Epoch [1867/10000] Avg train loss: 0.092888\n",
      "Epoch [1868/10000] Avg train loss: 0.092849\n",
      "Epoch [1869/10000] Avg train loss: 0.092801\n",
      "Epoch [1870/10000] Avg train loss: 0.092753\n",
      "Epoch [1871/10000] Avg train loss: 0.092706\n",
      "Epoch [1872/10000] Avg train loss: 0.092658\n",
      "Epoch [1873/10000] Avg train loss: 0.092609\n",
      "Epoch [1874/10000] Avg train loss: 0.092561\n",
      "Epoch [1875/10000] Avg train loss: 0.092512\n",
      "Epoch [1876/10000] Avg train loss: 0.092463\n",
      "Epoch [1877/10000] Avg train loss: 0.092414\n",
      "Epoch [1878/10000] Avg train loss: 0.092366\n",
      "Epoch [1879/10000] Avg train loss: 0.092317\n",
      "Epoch [1880/10000] Avg train loss: 0.092268\n",
      "Epoch [1881/10000] Avg train loss: 0.092219\n",
      "Epoch [1882/10000] Avg train loss: 0.092171\n",
      "Epoch [1883/10000] Avg train loss: 0.092122\n",
      "Epoch [1884/10000] Avg train loss: 0.092073\n",
      "Epoch [1885/10000] Avg train loss: 0.092025\n",
      "Epoch [1886/10000] Avg train loss: 0.091976\n",
      "Epoch [1887/10000] Avg train loss: 0.091927\n",
      "Epoch [1888/10000] Avg train loss: 0.091879\n",
      "Epoch [1889/10000] Avg train loss: 0.091831\n",
      "Epoch [1890/10000] Avg train loss: 0.091782\n",
      "Epoch [1891/10000] Avg train loss: 0.091734\n",
      "Epoch [1892/10000] Avg train loss: 0.091686\n",
      "Epoch [1893/10000] Avg train loss: 0.091638\n",
      "Epoch [1894/10000] Avg train loss: 0.091589\n",
      "Epoch [1895/10000] Avg train loss: 0.091541\n",
      "Epoch [1896/10000] Avg train loss: 0.091493\n",
      "Epoch [1897/10000] Avg train loss: 0.091445\n",
      "Epoch [1898/10000] Avg train loss: 0.091397\n",
      "Epoch [1899/10000] Avg train loss: 0.091349\n",
      "Epoch [1900/10000] Avg train loss: 0.091302\n",
      "Epoch [1901/10000] Avg train loss: 0.091254\n",
      "Epoch [1902/10000] Avg train loss: 0.091206\n",
      "Epoch [1903/10000] Avg train loss: 0.091158\n",
      "Epoch [1904/10000] Avg train loss: 0.091110\n",
      "Epoch [1905/10000] Avg train loss: 0.091063\n",
      "Epoch [1906/10000] Avg train loss: 0.091015\n",
      "Epoch [1907/10000] Avg train loss: 0.090968\n",
      "Epoch [1908/10000] Avg train loss: 0.090921\n",
      "Epoch [1909/10000] Avg train loss: 0.090873\n",
      "Epoch [1910/10000] Avg train loss: 0.090826\n",
      "Epoch [1911/10000] Avg train loss: 0.090778\n",
      "Epoch [1912/10000] Avg train loss: 0.090731\n",
      "Epoch [1913/10000] Avg train loss: 0.090684\n",
      "Epoch [1914/10000] Avg train loss: 0.090637\n",
      "Epoch [1915/10000] Avg train loss: 0.090590\n",
      "Epoch [1916/10000] Avg train loss: 0.090542\n",
      "Epoch [1917/10000] Avg train loss: 0.090495\n",
      "Epoch [1918/10000] Avg train loss: 0.090448\n",
      "Epoch [1919/10000] Avg train loss: 0.090401\n",
      "Epoch [1920/10000] Avg train loss: 0.090354\n",
      "Epoch [1921/10000] Avg train loss: 0.090308\n",
      "Epoch [1922/10000] Avg train loss: 0.090261\n",
      "Epoch [1923/10000] Avg train loss: 0.090214\n",
      "Epoch [1924/10000] Avg train loss: 0.090167\n",
      "Epoch [1925/10000] Avg train loss: 0.090121\n",
      "Epoch [1926/10000] Avg train loss: 0.090074\n",
      "Epoch [1927/10000] Avg train loss: 0.090027\n",
      "Epoch [1928/10000] Avg train loss: 0.089981\n",
      "Epoch [1929/10000] Avg train loss: 0.089934\n",
      "Epoch [1930/10000] Avg train loss: 0.089888\n",
      "Epoch [1931/10000] Avg train loss: 0.089842\n",
      "Epoch [1932/10000] Avg train loss: 0.089796\n",
      "Epoch [1933/10000] Avg train loss: 0.089749\n",
      "Epoch [1934/10000] Avg train loss: 0.089703\n",
      "Epoch [1935/10000] Avg train loss: 0.089657\n",
      "Epoch [1936/10000] Avg train loss: 0.089611\n",
      "Epoch [1937/10000] Avg train loss: 0.089565\n",
      "Epoch [1938/10000] Avg train loss: 0.089519\n",
      "Epoch [1939/10000] Avg train loss: 0.089473\n",
      "Epoch [1940/10000] Avg train loss: 0.089427\n",
      "Epoch [1941/10000] Avg train loss: 0.089381\n",
      "Epoch [1942/10000] Avg train loss: 0.089335\n",
      "Epoch [1943/10000] Avg train loss: 0.089289\n",
      "Epoch [1944/10000] Avg train loss: 0.089243\n",
      "Epoch [1945/10000] Avg train loss: 0.089197\n",
      "Epoch [1946/10000] Avg train loss: 0.089152\n",
      "Epoch [1947/10000] Avg train loss: 0.089106\n",
      "Epoch [1948/10000] Avg train loss: 0.089060\n",
      "Epoch [1949/10000] Avg train loss: 0.089015\n",
      "Epoch [1950/10000] Avg train loss: 0.088969\n",
      "Epoch [1951/10000] Avg train loss: 0.088924\n",
      "Epoch [1952/10000] Avg train loss: 0.088878\n",
      "Epoch [1953/10000] Avg train loss: 0.088833\n",
      "Epoch [1954/10000] Avg train loss: 0.088788\n",
      "Epoch [1955/10000] Avg train loss: 0.088743\n",
      "Epoch [1956/10000] Avg train loss: 0.088697\n",
      "Epoch [1957/10000] Avg train loss: 0.088652\n",
      "Epoch [1958/10000] Avg train loss: 0.088607\n",
      "Epoch [1959/10000] Avg train loss: 0.088562\n",
      "Epoch [1960/10000] Avg train loss: 0.088517\n",
      "Epoch [1961/10000] Avg train loss: 0.088472\n",
      "Epoch [1962/10000] Avg train loss: 0.088427\n",
      "Epoch [1963/10000] Avg train loss: 0.088382\n",
      "Epoch [1964/10000] Avg train loss: 0.088337\n",
      "Epoch [1965/10000] Avg train loss: 0.088292\n",
      "Epoch [1966/10000] Avg train loss: 0.088248\n",
      "Epoch [1967/10000] Avg train loss: 0.088203\n",
      "Epoch [1968/10000] Avg train loss: 0.088158\n",
      "Epoch [1969/10000] Avg train loss: 0.088114\n",
      "Epoch [1970/10000] Avg train loss: 0.088069\n",
      "Epoch [1971/10000] Avg train loss: 0.088024\n",
      "Epoch [1972/10000] Avg train loss: 0.087980\n",
      "Epoch [1973/10000] Avg train loss: 0.087935\n",
      "Epoch [1974/10000] Avg train loss: 0.087891\n",
      "Epoch [1975/10000] Avg train loss: 0.087846\n",
      "Epoch [1976/10000] Avg train loss: 0.087802\n",
      "Epoch [1977/10000] Avg train loss: 0.087758\n",
      "Epoch [1978/10000] Avg train loss: 0.087714\n",
      "Epoch [1979/10000] Avg train loss: 0.087669\n",
      "Epoch [1980/10000] Avg train loss: 0.087625\n",
      "Epoch [1981/10000] Avg train loss: 0.087581\n",
      "Epoch [1982/10000] Avg train loss: 0.087537\n",
      "Epoch [1983/10000] Avg train loss: 0.087493\n",
      "Epoch [1984/10000] Avg train loss: 0.087449\n",
      "Epoch [1985/10000] Avg train loss: 0.087405\n",
      "Epoch [1986/10000] Avg train loss: 0.087361\n",
      "Epoch [1987/10000] Avg train loss: 0.087318\n",
      "Epoch [1988/10000] Avg train loss: 0.087274\n",
      "Epoch [1989/10000] Avg train loss: 0.087230\n",
      "Epoch [1990/10000] Avg train loss: 0.087186\n",
      "Epoch [1991/10000] Avg train loss: 0.087143\n",
      "Epoch [1992/10000] Avg train loss: 0.087099\n",
      "Epoch [1993/10000] Avg train loss: 0.087055\n",
      "Epoch [1994/10000] Avg train loss: 0.087012\n",
      "Epoch [1995/10000] Avg train loss: 0.086968\n",
      "Epoch [1996/10000] Avg train loss: 0.086925\n",
      "Epoch [1997/10000] Avg train loss: 0.086882\n",
      "Epoch [1998/10000] Avg train loss: 0.086838\n",
      "Epoch [1999/10000] Avg train loss: 0.086795\n",
      "Epoch [2000/10000] Avg train loss: 0.086752\n",
      "Epoch [2001/10000] Avg train loss: 0.086708\n",
      "Epoch [2002/10000] Avg train loss: 0.086665\n",
      "Epoch [2003/10000] Avg train loss: 0.086622\n",
      "Epoch [2004/10000] Avg train loss: 0.086579\n",
      "Epoch [2005/10000] Avg train loss: 0.086536\n",
      "Epoch [2006/10000] Avg train loss: 0.086493\n",
      "Epoch [2007/10000] Avg train loss: 0.086450\n",
      "Epoch [2008/10000] Avg train loss: 0.086407\n",
      "Epoch [2009/10000] Avg train loss: 0.086364\n",
      "Epoch [2010/10000] Avg train loss: 0.086321\n",
      "Epoch [2011/10000] Avg train loss: 0.086278\n",
      "Epoch [2012/10000] Avg train loss: 0.086236\n",
      "Epoch [2013/10000] Avg train loss: 0.086193\n",
      "Epoch [2014/10000] Avg train loss: 0.086151\n",
      "Epoch [2015/10000] Avg train loss: 0.086109\n",
      "Epoch [2016/10000] Avg train loss: 0.086066\n",
      "Epoch [2017/10000] Avg train loss: 0.086024\n",
      "Epoch [2018/10000] Avg train loss: 0.085981\n",
      "Epoch [2019/10000] Avg train loss: 0.085939\n",
      "Epoch [2020/10000] Avg train loss: 0.085896\n",
      "Epoch [2021/10000] Avg train loss: 0.085854\n",
      "Epoch [2022/10000] Avg train loss: 0.085812\n",
      "Epoch [2023/10000] Avg train loss: 0.085769\n",
      "Epoch [2024/10000] Avg train loss: 0.085727\n",
      "Epoch [2025/10000] Avg train loss: 0.085685\n",
      "Epoch [2026/10000] Avg train loss: 0.085643\n",
      "Epoch [2027/10000] Avg train loss: 0.085601\n",
      "Epoch [2028/10000] Avg train loss: 0.085558\n",
      "Epoch [2029/10000] Avg train loss: 0.085516\n",
      "Epoch [2030/10000] Avg train loss: 0.085474\n",
      "Epoch [2031/10000] Avg train loss: 0.085432\n",
      "Epoch [2032/10000] Avg train loss: 0.085390\n",
      "Epoch [2033/10000] Avg train loss: 0.085348\n",
      "Epoch [2034/10000] Avg train loss: 0.085307\n",
      "Epoch [2035/10000] Avg train loss: 0.085265\n",
      "Epoch [2036/10000] Avg train loss: 0.085223\n",
      "Epoch [2037/10000] Avg train loss: 0.085181\n",
      "Epoch [2038/10000] Avg train loss: 0.085139\n",
      "Epoch [2039/10000] Avg train loss: 0.085098\n",
      "Epoch [2040/10000] Avg train loss: 0.085056\n",
      "Epoch [2041/10000] Avg train loss: 0.085015\n",
      "Epoch [2042/10000] Avg train loss: 0.084973\n",
      "Epoch [2043/10000] Avg train loss: 0.084932\n",
      "Epoch [2044/10000] Avg train loss: 0.084890\n",
      "Epoch [2045/10000] Avg train loss: 0.084849\n",
      "Epoch [2046/10000] Avg train loss: 0.084807\n",
      "Epoch [2047/10000] Avg train loss: 0.084766\n",
      "Epoch [2048/10000] Avg train loss: 0.084725\n",
      "Epoch [2049/10000] Avg train loss: 0.084683\n",
      "Epoch [2050/10000] Avg train loss: 0.084642\n",
      "Epoch [2051/10000] Avg train loss: 0.084601\n",
      "Epoch [2052/10000] Avg train loss: 0.084560\n",
      "Epoch [2053/10000] Avg train loss: 0.084519\n",
      "Epoch [2054/10000] Avg train loss: 0.084478\n",
      "Epoch [2055/10000] Avg train loss: 0.084437\n",
      "Epoch [2056/10000] Avg train loss: 0.084396\n",
      "Epoch [2057/10000] Avg train loss: 0.084355\n",
      "Epoch [2058/10000] Avg train loss: 0.084314\n",
      "Epoch [2059/10000] Avg train loss: 0.084273\n",
      "Epoch [2060/10000] Avg train loss: 0.084232\n",
      "Epoch [2061/10000] Avg train loss: 0.084192\n",
      "Epoch [2062/10000] Avg train loss: 0.084151\n",
      "Epoch [2063/10000] Avg train loss: 0.084110\n",
      "Epoch [2064/10000] Avg train loss: 0.084070\n",
      "Epoch [2065/10000] Avg train loss: 0.084029\n",
      "Epoch [2066/10000] Avg train loss: 0.083988\n",
      "Epoch [2067/10000] Avg train loss: 0.083948\n",
      "Epoch [2068/10000] Avg train loss: 0.083908\n",
      "Epoch [2069/10000] Avg train loss: 0.083867\n",
      "Epoch [2070/10000] Avg train loss: 0.083827\n",
      "Epoch [2071/10000] Avg train loss: 0.083787\n",
      "Epoch [2072/10000] Avg train loss: 0.083746\n",
      "Epoch [2073/10000] Avg train loss: 0.083706\n",
      "Epoch [2074/10000] Avg train loss: 0.083666\n",
      "Epoch [2075/10000] Avg train loss: 0.083625\n",
      "Epoch [2076/10000] Avg train loss: 0.083586\n",
      "Epoch [2077/10000] Avg train loss: 0.083547\n",
      "Epoch [2078/10000] Avg train loss: 0.083507\n",
      "Epoch [2079/10000] Avg train loss: 0.083467\n",
      "Epoch [2080/10000] Avg train loss: 0.083427\n",
      "Epoch [2081/10000] Avg train loss: 0.083387\n",
      "Epoch [2082/10000] Avg train loss: 0.083347\n",
      "Epoch [2083/10000] Avg train loss: 0.083307\n",
      "Epoch [2084/10000] Avg train loss: 0.083267\n",
      "Epoch [2085/10000] Avg train loss: 0.083228\n",
      "Epoch [2086/10000] Avg train loss: 0.083188\n",
      "Epoch [2087/10000] Avg train loss: 0.083148\n",
      "Epoch [2088/10000] Avg train loss: 0.083108\n",
      "Epoch [2089/10000] Avg train loss: 0.083069\n",
      "Epoch [2090/10000] Avg train loss: 0.083029\n",
      "Epoch [2091/10000] Avg train loss: 0.082989\n",
      "Epoch [2092/10000] Avg train loss: 0.082950\n",
      "Epoch [2093/10000] Avg train loss: 0.082910\n",
      "Epoch [2094/10000] Avg train loss: 0.082871\n",
      "Epoch [2095/10000] Avg train loss: 0.082831\n",
      "Epoch [2096/10000] Avg train loss: 0.082792\n",
      "Epoch [2097/10000] Avg train loss: 0.082753\n",
      "Epoch [2098/10000] Avg train loss: 0.082713\n",
      "Epoch [2099/10000] Avg train loss: 0.082674\n",
      "Epoch [2100/10000] Avg train loss: 0.082635\n",
      "Epoch [2101/10000] Avg train loss: 0.082596\n",
      "Epoch [2102/10000] Avg train loss: 0.082556\n",
      "Epoch [2103/10000] Avg train loss: 0.082517\n",
      "Epoch [2104/10000] Avg train loss: 0.082478\n",
      "Epoch [2105/10000] Avg train loss: 0.082439\n",
      "Epoch [2106/10000] Avg train loss: 0.082400\n",
      "Epoch [2107/10000] Avg train loss: 0.082361\n",
      "Epoch [2108/10000] Avg train loss: 0.082322\n",
      "Epoch [2109/10000] Avg train loss: 0.082283\n",
      "Epoch [2110/10000] Avg train loss: 0.082244\n",
      "Epoch [2111/10000] Avg train loss: 0.082205\n",
      "Epoch [2112/10000] Avg train loss: 0.082166\n",
      "Epoch [2113/10000] Avg train loss: 0.082127\n",
      "Epoch [2114/10000] Avg train loss: 0.082089\n",
      "Epoch [2115/10000] Avg train loss: 0.082050\n",
      "Epoch [2116/10000] Avg train loss: 0.082012\n",
      "Epoch [2117/10000] Avg train loss: 0.081973\n",
      "Epoch [2118/10000] Avg train loss: 0.081934\n",
      "Epoch [2119/10000] Avg train loss: 0.081896\n",
      "Epoch [2120/10000] Avg train loss: 0.081857\n",
      "Epoch [2121/10000] Avg train loss: 0.081819\n",
      "Epoch [2122/10000] Avg train loss: 0.081780\n",
      "Epoch [2123/10000] Avg train loss: 0.081742\n",
      "Epoch [2124/10000] Avg train loss: 0.081703\n",
      "Epoch [2125/10000] Avg train loss: 0.081665\n",
      "Epoch [2126/10000] Avg train loss: 0.081627\n",
      "Epoch [2127/10000] Avg train loss: 0.081588\n",
      "Epoch [2128/10000] Avg train loss: 0.081550\n",
      "Epoch [2129/10000] Avg train loss: 0.081512\n",
      "Epoch [2130/10000] Avg train loss: 0.081473\n",
      "Epoch [2131/10000] Avg train loss: 0.081435\n",
      "Epoch [2132/10000] Avg train loss: 0.081397\n",
      "Epoch [2133/10000] Avg train loss: 0.081359\n",
      "Epoch [2134/10000] Avg train loss: 0.081321\n",
      "Epoch [2135/10000] Avg train loss: 0.081283\n",
      "Epoch [2136/10000] Avg train loss: 0.081245\n",
      "Epoch [2137/10000] Avg train loss: 0.081207\n",
      "Epoch [2138/10000] Avg train loss: 0.081169\n",
      "Epoch [2139/10000] Avg train loss: 0.081131\n",
      "Epoch [2140/10000] Avg train loss: 0.081093\n",
      "Epoch [2141/10000] Avg train loss: 0.081056\n",
      "Epoch [2142/10000] Avg train loss: 0.081018\n",
      "Epoch [2143/10000] Avg train loss: 0.080980\n",
      "Epoch [2144/10000] Avg train loss: 0.080942\n",
      "Epoch [2145/10000] Avg train loss: 0.080905\n",
      "Epoch [2146/10000] Avg train loss: 0.080867\n",
      "Epoch [2147/10000] Avg train loss: 0.080829\n",
      "Epoch [2148/10000] Avg train loss: 0.080792\n",
      "Epoch [2149/10000] Avg train loss: 0.080754\n",
      "Epoch [2150/10000] Avg train loss: 0.080717\n",
      "Epoch [2151/10000] Avg train loss: 0.080679\n",
      "Epoch [2152/10000] Avg train loss: 0.080642\n",
      "Epoch [2153/10000] Avg train loss: 0.080605\n",
      "Epoch [2154/10000] Avg train loss: 0.080567\n",
      "Epoch [2155/10000] Avg train loss: 0.080530\n",
      "Epoch [2156/10000] Avg train loss: 0.080493\n",
      "Epoch [2157/10000] Avg train loss: 0.080455\n",
      "Epoch [2158/10000] Avg train loss: 0.080418\n",
      "Epoch [2159/10000] Avg train loss: 0.080381\n",
      "Epoch [2160/10000] Avg train loss: 0.080344\n",
      "Epoch [2161/10000] Avg train loss: 0.080307\n",
      "Epoch [2162/10000] Avg train loss: 0.080270\n",
      "Epoch [2163/10000] Avg train loss: 0.080233\n",
      "Epoch [2164/10000] Avg train loss: 0.080196\n",
      "Epoch [2165/10000] Avg train loss: 0.080159\n",
      "Epoch [2166/10000] Avg train loss: 0.080122\n",
      "Epoch [2167/10000] Avg train loss: 0.080085\n",
      "Epoch [2168/10000] Avg train loss: 0.080048\n",
      "Epoch [2169/10000] Avg train loss: 0.080011\n",
      "Epoch [2170/10000] Avg train loss: 0.079974\n",
      "Epoch [2171/10000] Avg train loss: 0.079937\n",
      "Epoch [2172/10000] Avg train loss: 0.079901\n",
      "Epoch [2173/10000] Avg train loss: 0.079864\n",
      "Epoch [2174/10000] Avg train loss: 0.079827\n",
      "Epoch [2175/10000] Avg train loss: 0.079791\n",
      "Epoch [2176/10000] Avg train loss: 0.079754\n",
      "Epoch [2177/10000] Avg train loss: 0.079717\n",
      "Epoch [2178/10000] Avg train loss: 0.079681\n",
      "Epoch [2179/10000] Avg train loss: 0.079645\n",
      "Epoch [2180/10000] Avg train loss: 0.079608\n",
      "Epoch [2181/10000] Avg train loss: 0.079572\n",
      "Epoch [2182/10000] Avg train loss: 0.079535\n",
      "Epoch [2183/10000] Avg train loss: 0.079499\n",
      "Epoch [2184/10000] Avg train loss: 0.079463\n",
      "Epoch [2185/10000] Avg train loss: 0.079426\n",
      "Epoch [2186/10000] Avg train loss: 0.079390\n",
      "Epoch [2187/10000] Avg train loss: 0.079354\n",
      "Epoch [2188/10000] Avg train loss: 0.079318\n",
      "Epoch [2189/10000] Avg train loss: 0.079281\n",
      "Epoch [2190/10000] Avg train loss: 0.079245\n",
      "Epoch [2191/10000] Avg train loss: 0.079209\n",
      "Epoch [2192/10000] Avg train loss: 0.079173\n",
      "Epoch [2193/10000] Avg train loss: 0.079137\n",
      "Epoch [2194/10000] Avg train loss: 0.079101\n",
      "Epoch [2195/10000] Avg train loss: 0.079065\n",
      "Epoch [2196/10000] Avg train loss: 0.079029\n",
      "Epoch [2197/10000] Avg train loss: 0.078993\n",
      "Epoch [2198/10000] Avg train loss: 0.078957\n",
      "Epoch [2199/10000] Avg train loss: 0.078921\n",
      "Epoch [2200/10000] Avg train loss: 0.078886\n",
      "Epoch [2201/10000] Avg train loss: 0.078850\n",
      "Epoch [2202/10000] Avg train loss: 0.078814\n",
      "Epoch [2203/10000] Avg train loss: 0.078778\n",
      "Epoch [2204/10000] Avg train loss: 0.078743\n",
      "Epoch [2205/10000] Avg train loss: 0.078707\n",
      "Epoch [2206/10000] Avg train loss: 0.078671\n",
      "Epoch [2207/10000] Avg train loss: 0.078636\n",
      "Epoch [2208/10000] Avg train loss: 0.078600\n",
      "Epoch [2209/10000] Avg train loss: 0.078565\n",
      "Epoch [2210/10000] Avg train loss: 0.078529\n",
      "Epoch [2211/10000] Avg train loss: 0.078494\n",
      "Epoch [2212/10000] Avg train loss: 0.078458\n",
      "Epoch [2213/10000] Avg train loss: 0.078423\n",
      "Epoch [2214/10000] Avg train loss: 0.078388\n",
      "Epoch [2215/10000] Avg train loss: 0.078352\n",
      "Epoch [2216/10000] Avg train loss: 0.078317\n",
      "Epoch [2217/10000] Avg train loss: 0.078282\n",
      "Epoch [2218/10000] Avg train loss: 0.078247\n",
      "Epoch [2219/10000] Avg train loss: 0.078211\n",
      "Epoch [2220/10000] Avg train loss: 0.078176\n",
      "Epoch [2221/10000] Avg train loss: 0.078141\n",
      "Epoch [2222/10000] Avg train loss: 0.078106\n",
      "Epoch [2223/10000] Avg train loss: 0.078071\n",
      "Epoch [2224/10000] Avg train loss: 0.078036\n",
      "Epoch [2225/10000] Avg train loss: 0.078001\n",
      "Epoch [2226/10000] Avg train loss: 0.077966\n",
      "Epoch [2227/10000] Avg train loss: 0.077931\n",
      "Epoch [2228/10000] Avg train loss: 0.077896\n",
      "Epoch [2229/10000] Avg train loss: 0.077861\n",
      "Epoch [2230/10000] Avg train loss: 0.077826\n",
      "Epoch [2231/10000] Avg train loss: 0.077791\n",
      "Epoch [2232/10000] Avg train loss: 0.077757\n",
      "Epoch [2233/10000] Avg train loss: 0.077722\n",
      "Epoch [2234/10000] Avg train loss: 0.077687\n",
      "Epoch [2235/10000] Avg train loss: 0.077652\n",
      "Epoch [2236/10000] Avg train loss: 0.077618\n",
      "Epoch [2237/10000] Avg train loss: 0.077583\n",
      "Epoch [2238/10000] Avg train loss: 0.077548\n",
      "Epoch [2239/10000] Avg train loss: 0.077514\n",
      "Epoch [2240/10000] Avg train loss: 0.077479\n",
      "Epoch [2241/10000] Avg train loss: 0.077445\n",
      "Epoch [2242/10000] Avg train loss: 0.077410\n",
      "Epoch [2243/10000] Avg train loss: 0.077376\n",
      "Epoch [2244/10000] Avg train loss: 0.077341\n",
      "Epoch [2245/10000] Avg train loss: 0.077307\n",
      "Epoch [2246/10000] Avg train loss: 0.077273\n",
      "Epoch [2247/10000] Avg train loss: 0.077238\n",
      "Epoch [2248/10000] Avg train loss: 0.077204\n",
      "Epoch [2249/10000] Avg train loss: 0.077170\n",
      "Epoch [2250/10000] Avg train loss: 0.077135\n",
      "Epoch [2251/10000] Avg train loss: 0.077101\n",
      "Epoch [2252/10000] Avg train loss: 0.077067\n",
      "Epoch [2253/10000] Avg train loss: 0.077033\n",
      "Epoch [2254/10000] Avg train loss: 0.076999\n",
      "Epoch [2255/10000] Avg train loss: 0.076965\n",
      "Epoch [2256/10000] Avg train loss: 0.076931\n",
      "Epoch [2257/10000] Avg train loss: 0.076896\n",
      "Epoch [2258/10000] Avg train loss: 0.076863\n",
      "Epoch [2259/10000] Avg train loss: 0.076829\n",
      "Epoch [2260/10000] Avg train loss: 0.076795\n",
      "Epoch [2261/10000] Avg train loss: 0.076762\n",
      "Epoch [2262/10000] Avg train loss: 0.076728\n",
      "Epoch [2263/10000] Avg train loss: 0.076695\n",
      "Epoch [2264/10000] Avg train loss: 0.076661\n",
      "Epoch [2265/10000] Avg train loss: 0.076627\n",
      "Epoch [2266/10000] Avg train loss: 0.076594\n",
      "Epoch [2267/10000] Avg train loss: 0.076561\n",
      "Epoch [2268/10000] Avg train loss: 0.076527\n",
      "Epoch [2269/10000] Avg train loss: 0.076494\n",
      "Epoch [2270/10000] Avg train loss: 0.076460\n",
      "Epoch [2271/10000] Avg train loss: 0.076427\n",
      "Epoch [2272/10000] Avg train loss: 0.076393\n",
      "Epoch [2273/10000] Avg train loss: 0.076359\n",
      "Epoch [2274/10000] Avg train loss: 0.076326\n",
      "Epoch [2275/10000] Avg train loss: 0.076292\n",
      "Epoch [2276/10000] Avg train loss: 0.076259\n",
      "Epoch [2277/10000] Avg train loss: 0.076226\n",
      "Epoch [2278/10000] Avg train loss: 0.076192\n",
      "Epoch [2279/10000] Avg train loss: 0.076159\n",
      "Epoch [2280/10000] Avg train loss: 0.076125\n",
      "Epoch [2281/10000] Avg train loss: 0.076092\n",
      "Epoch [2282/10000] Avg train loss: 0.076059\n",
      "Epoch [2283/10000] Avg train loss: 0.076025\n",
      "Epoch [2284/10000] Avg train loss: 0.075992\n",
      "Epoch [2285/10000] Avg train loss: 0.075959\n",
      "Epoch [2286/10000] Avg train loss: 0.075926\n",
      "Epoch [2287/10000] Avg train loss: 0.075893\n",
      "Epoch [2288/10000] Avg train loss: 0.075860\n",
      "Epoch [2289/10000] Avg train loss: 0.075826\n",
      "Epoch [2290/10000] Avg train loss: 0.075793\n",
      "Epoch [2291/10000] Avg train loss: 0.075760\n",
      "Epoch [2292/10000] Avg train loss: 0.075727\n",
      "Epoch [2293/10000] Avg train loss: 0.075694\n",
      "Epoch [2294/10000] Avg train loss: 0.075661\n",
      "Epoch [2295/10000] Avg train loss: 0.075628\n",
      "Epoch [2296/10000] Avg train loss: 0.075596\n",
      "Epoch [2297/10000] Avg train loss: 0.075563\n",
      "Epoch [2298/10000] Avg train loss: 0.075530\n",
      "Epoch [2299/10000] Avg train loss: 0.075497\n",
      "Epoch [2300/10000] Avg train loss: 0.075464\n",
      "Epoch [2301/10000] Avg train loss: 0.075432\n",
      "Epoch [2302/10000] Avg train loss: 0.075399\n",
      "Epoch [2303/10000] Avg train loss: 0.075366\n",
      "Epoch [2304/10000] Avg train loss: 0.075333\n",
      "Epoch [2305/10000] Avg train loss: 0.075301\n",
      "Epoch [2306/10000] Avg train loss: 0.075268\n",
      "Epoch [2307/10000] Avg train loss: 0.075236\n",
      "Epoch [2308/10000] Avg train loss: 0.075203\n",
      "Epoch [2309/10000] Avg train loss: 0.075171\n",
      "Epoch [2310/10000] Avg train loss: 0.075138\n",
      "Epoch [2311/10000] Avg train loss: 0.075106\n",
      "Epoch [2312/10000] Avg train loss: 0.075073\n",
      "Epoch [2313/10000] Avg train loss: 0.075041\n",
      "Epoch [2314/10000] Avg train loss: 0.075008\n",
      "Epoch [2315/10000] Avg train loss: 0.074976\n",
      "Epoch [2316/10000] Avg train loss: 0.074944\n",
      "Epoch [2317/10000] Avg train loss: 0.074911\n",
      "Epoch [2318/10000] Avg train loss: 0.074879\n",
      "Epoch [2319/10000] Avg train loss: 0.074847\n",
      "Epoch [2320/10000] Avg train loss: 0.074815\n",
      "Epoch [2321/10000] Avg train loss: 0.074782\n",
      "Epoch [2322/10000] Avg train loss: 0.074750\n",
      "Epoch [2323/10000] Avg train loss: 0.074718\n",
      "Epoch [2324/10000] Avg train loss: 0.074686\n",
      "Epoch [2325/10000] Avg train loss: 0.074654\n",
      "Epoch [2326/10000] Avg train loss: 0.074622\n",
      "Epoch [2327/10000] Avg train loss: 0.074590\n",
      "Epoch [2328/10000] Avg train loss: 0.074558\n",
      "Epoch [2329/10000] Avg train loss: 0.074526\n",
      "Epoch [2330/10000] Avg train loss: 0.074494\n",
      "Epoch [2331/10000] Avg train loss: 0.074462\n",
      "Epoch [2332/10000] Avg train loss: 0.074430\n",
      "Epoch [2333/10000] Avg train loss: 0.074398\n",
      "Epoch [2334/10000] Avg train loss: 0.074366\n",
      "Epoch [2335/10000] Avg train loss: 0.074334\n",
      "Epoch [2336/10000] Avg train loss: 0.074303\n",
      "Epoch [2337/10000] Avg train loss: 0.074271\n",
      "Epoch [2338/10000] Avg train loss: 0.074239\n",
      "Epoch [2339/10000] Avg train loss: 0.074208\n",
      "Epoch [2340/10000] Avg train loss: 0.074176\n",
      "Epoch [2341/10000] Avg train loss: 0.074144\n",
      "Epoch [2342/10000] Avg train loss: 0.074113\n",
      "Epoch [2343/10000] Avg train loss: 0.074081\n",
      "Epoch [2344/10000] Avg train loss: 0.074050\n",
      "Epoch [2345/10000] Avg train loss: 0.074018\n",
      "Epoch [2346/10000] Avg train loss: 0.073986\n",
      "Epoch [2347/10000] Avg train loss: 0.073955\n",
      "Epoch [2348/10000] Avg train loss: 0.073924\n",
      "Epoch [2349/10000] Avg train loss: 0.073892\n",
      "Epoch [2350/10000] Avg train loss: 0.073861\n",
      "Epoch [2351/10000] Avg train loss: 0.073829\n",
      "Epoch [2352/10000] Avg train loss: 0.073798\n",
      "Epoch [2353/10000] Avg train loss: 0.073767\n",
      "Epoch [2354/10000] Avg train loss: 0.073735\n",
      "Epoch [2355/10000] Avg train loss: 0.073704\n",
      "Epoch [2356/10000] Avg train loss: 0.073673\n",
      "Epoch [2357/10000] Avg train loss: 0.073642\n",
      "Epoch [2358/10000] Avg train loss: 0.073610\n",
      "Epoch [2359/10000] Avg train loss: 0.073579\n",
      "Epoch [2360/10000] Avg train loss: 0.073548\n",
      "Epoch [2361/10000] Avg train loss: 0.073517\n",
      "Epoch [2362/10000] Avg train loss: 0.073486\n",
      "Epoch [2363/10000] Avg train loss: 0.073455\n",
      "Epoch [2364/10000] Avg train loss: 0.073424\n",
      "Epoch [2365/10000] Avg train loss: 0.073393\n",
      "Epoch [2366/10000] Avg train loss: 0.073362\n",
      "Epoch [2367/10000] Avg train loss: 0.073331\n",
      "Epoch [2368/10000] Avg train loss: 0.073300\n",
      "Epoch [2369/10000] Avg train loss: 0.073269\n",
      "Epoch [2370/10000] Avg train loss: 0.073238\n",
      "Epoch [2371/10000] Avg train loss: 0.073208\n",
      "Epoch [2372/10000] Avg train loss: 0.073177\n",
      "Epoch [2373/10000] Avg train loss: 0.073146\n",
      "Epoch [2374/10000] Avg train loss: 0.073115\n",
      "Epoch [2375/10000] Avg train loss: 0.073084\n",
      "Epoch [2376/10000] Avg train loss: 0.073054\n",
      "Epoch [2377/10000] Avg train loss: 0.073023\n",
      "Epoch [2378/10000] Avg train loss: 0.072992\n",
      "Epoch [2379/10000] Avg train loss: 0.072962\n",
      "Epoch [2380/10000] Avg train loss: 0.072931\n",
      "Epoch [2381/10000] Avg train loss: 0.072900\n",
      "Epoch [2382/10000] Avg train loss: 0.072870\n",
      "Epoch [2383/10000] Avg train loss: 0.072839\n",
      "Epoch [2384/10000] Avg train loss: 0.072809\n",
      "Epoch [2385/10000] Avg train loss: 0.072778\n",
      "Epoch [2386/10000] Avg train loss: 0.072748\n",
      "Epoch [2387/10000] Avg train loss: 0.072717\n",
      "Epoch [2388/10000] Avg train loss: 0.072687\n",
      "Epoch [2389/10000] Avg train loss: 0.072657\n",
      "Epoch [2390/10000] Avg train loss: 0.072626\n",
      "Epoch [2391/10000] Avg train loss: 0.072596\n",
      "Epoch [2392/10000] Avg train loss: 0.072566\n",
      "Epoch [2393/10000] Avg train loss: 0.072536\n",
      "Epoch [2394/10000] Avg train loss: 0.072505\n",
      "Epoch [2395/10000] Avg train loss: 0.072475\n",
      "Epoch [2396/10000] Avg train loss: 0.072445\n",
      "Epoch [2397/10000] Avg train loss: 0.072415\n",
      "Epoch [2398/10000] Avg train loss: 0.072385\n",
      "Epoch [2399/10000] Avg train loss: 0.072354\n",
      "Epoch [2400/10000] Avg train loss: 0.072324\n",
      "Epoch [2401/10000] Avg train loss: 0.072294\n",
      "Epoch [2402/10000] Avg train loss: 0.072264\n",
      "Epoch [2403/10000] Avg train loss: 0.072234\n",
      "Epoch [2404/10000] Avg train loss: 0.072204\n",
      "Epoch [2405/10000] Avg train loss: 0.072174\n",
      "Epoch [2406/10000] Avg train loss: 0.072144\n",
      "Epoch [2407/10000] Avg train loss: 0.072114\n",
      "Epoch [2408/10000] Avg train loss: 0.072085\n",
      "Epoch [2409/10000] Avg train loss: 0.072055\n",
      "Epoch [2410/10000] Avg train loss: 0.072025\n",
      "Epoch [2411/10000] Avg train loss: 0.071995\n",
      "Epoch [2412/10000] Avg train loss: 0.071965\n",
      "Epoch [2413/10000] Avg train loss: 0.071935\n",
      "Epoch [2414/10000] Avg train loss: 0.071906\n",
      "Epoch [2415/10000] Avg train loss: 0.071876\n",
      "Epoch [2416/10000] Avg train loss: 0.071846\n",
      "Epoch [2417/10000] Avg train loss: 0.071816\n",
      "Epoch [2418/10000] Avg train loss: 0.071787\n",
      "Epoch [2419/10000] Avg train loss: 0.071757\n",
      "Epoch [2420/10000] Avg train loss: 0.071727\n",
      "Epoch [2421/10000] Avg train loss: 0.071698\n",
      "Epoch [2422/10000] Avg train loss: 0.071668\n",
      "Epoch [2423/10000] Avg train loss: 0.071639\n",
      "Epoch [2424/10000] Avg train loss: 0.071609\n",
      "Epoch [2425/10000] Avg train loss: 0.071580\n",
      "Epoch [2426/10000] Avg train loss: 0.071550\n",
      "Epoch [2427/10000] Avg train loss: 0.071521\n",
      "Epoch [2428/10000] Avg train loss: 0.071491\n",
      "Epoch [2429/10000] Avg train loss: 0.071462\n",
      "Epoch [2430/10000] Avg train loss: 0.071433\n",
      "Epoch [2431/10000] Avg train loss: 0.071403\n",
      "Epoch [2432/10000] Avg train loss: 0.071374\n",
      "Epoch [2433/10000] Avg train loss: 0.071345\n",
      "Epoch [2434/10000] Avg train loss: 0.071315\n",
      "Epoch [2435/10000] Avg train loss: 0.071286\n",
      "Epoch [2436/10000] Avg train loss: 0.071257\n",
      "Epoch [2437/10000] Avg train loss: 0.071228\n",
      "Epoch [2438/10000] Avg train loss: 0.071198\n",
      "Epoch [2439/10000] Avg train loss: 0.071169\n",
      "Epoch [2440/10000] Avg train loss: 0.071140\n",
      "Epoch [2441/10000] Avg train loss: 0.071111\n",
      "Epoch [2442/10000] Avg train loss: 0.071082\n",
      "Epoch [2443/10000] Avg train loss: 0.071053\n",
      "Epoch [2444/10000] Avg train loss: 0.071024\n",
      "Epoch [2445/10000] Avg train loss: 0.070995\n",
      "Epoch [2446/10000] Avg train loss: 0.070966\n",
      "Epoch [2447/10000] Avg train loss: 0.070937\n",
      "Epoch [2448/10000] Avg train loss: 0.070908\n",
      "Epoch [2449/10000] Avg train loss: 0.070879\n",
      "Epoch [2450/10000] Avg train loss: 0.070850\n",
      "Epoch [2451/10000] Avg train loss: 0.070821\n",
      "Epoch [2452/10000] Avg train loss: 0.070792\n",
      "Epoch [2453/10000] Avg train loss: 0.070763\n",
      "Epoch [2454/10000] Avg train loss: 0.070735\n",
      "Epoch [2455/10000] Avg train loss: 0.070706\n",
      "Epoch [2456/10000] Avg train loss: 0.070677\n",
      "Epoch [2457/10000] Avg train loss: 0.070648\n",
      "Epoch [2458/10000] Avg train loss: 0.070620\n",
      "Epoch [2459/10000] Avg train loss: 0.070591\n",
      "Epoch [2460/10000] Avg train loss: 0.070563\n",
      "Epoch [2461/10000] Avg train loss: 0.070534\n",
      "Epoch [2462/10000] Avg train loss: 0.070505\n",
      "Epoch [2463/10000] Avg train loss: 0.070477\n",
      "Epoch [2464/10000] Avg train loss: 0.070448\n",
      "Epoch [2465/10000] Avg train loss: 0.070420\n",
      "Epoch [2466/10000] Avg train loss: 0.070391\n",
      "Epoch [2467/10000] Avg train loss: 0.070363\n",
      "Epoch [2468/10000] Avg train loss: 0.070334\n",
      "Epoch [2469/10000] Avg train loss: 0.070306\n",
      "Epoch [2470/10000] Avg train loss: 0.070277\n",
      "Epoch [2471/10000] Avg train loss: 0.070249\n",
      "Epoch [2472/10000] Avg train loss: 0.070220\n",
      "Epoch [2473/10000] Avg train loss: 0.070192\n",
      "Epoch [2474/10000] Avg train loss: 0.070164\n",
      "Epoch [2475/10000] Avg train loss: 0.070135\n",
      "Epoch [2476/10000] Avg train loss: 0.070107\n",
      "Epoch [2477/10000] Avg train loss: 0.070079\n",
      "Epoch [2478/10000] Avg train loss: 0.070051\n",
      "Epoch [2479/10000] Avg train loss: 0.070022\n",
      "Epoch [2480/10000] Avg train loss: 0.069994\n",
      "Epoch [2481/10000] Avg train loss: 0.069966\n",
      "Epoch [2482/10000] Avg train loss: 0.069938\n",
      "Epoch [2483/10000] Avg train loss: 0.069910\n",
      "Epoch [2484/10000] Avg train loss: 0.069882\n",
      "Epoch [2485/10000] Avg train loss: 0.069854\n",
      "Epoch [2486/10000] Avg train loss: 0.069826\n",
      "Epoch [2487/10000] Avg train loss: 0.069798\n",
      "Epoch [2488/10000] Avg train loss: 0.069770\n",
      "Epoch [2489/10000] Avg train loss: 0.069742\n",
      "Epoch [2490/10000] Avg train loss: 0.069714\n",
      "Epoch [2491/10000] Avg train loss: 0.069686\n",
      "Epoch [2492/10000] Avg train loss: 0.069658\n",
      "Epoch [2493/10000] Avg train loss: 0.069630\n",
      "Epoch [2494/10000] Avg train loss: 0.069602\n",
      "Epoch [2495/10000] Avg train loss: 0.069574\n",
      "Epoch [2496/10000] Avg train loss: 0.069546\n",
      "Epoch [2497/10000] Avg train loss: 0.069519\n",
      "Epoch [2498/10000] Avg train loss: 0.069491\n",
      "Epoch [2499/10000] Avg train loss: 0.069463\n",
      "Epoch [2500/10000] Avg train loss: 0.069435\n",
      "Epoch [2501/10000] Avg train loss: 0.069408\n",
      "Epoch [2502/10000] Avg train loss: 0.069380\n",
      "Epoch [2503/10000] Avg train loss: 0.069352\n",
      "Epoch [2504/10000] Avg train loss: 0.069324\n",
      "Epoch [2505/10000] Avg train loss: 0.069297\n",
      "Epoch [2506/10000] Avg train loss: 0.069269\n",
      "Epoch [2507/10000] Avg train loss: 0.069241\n",
      "Epoch [2508/10000] Avg train loss: 0.069214\n",
      "Epoch [2509/10000] Avg train loss: 0.069186\n",
      "Epoch [2510/10000] Avg train loss: 0.069159\n",
      "Epoch [2511/10000] Avg train loss: 0.069131\n",
      "Epoch [2512/10000] Avg train loss: 0.069104\n",
      "Epoch [2513/10000] Avg train loss: 0.069076\n",
      "Epoch [2514/10000] Avg train loss: 0.069049\n",
      "Epoch [2515/10000] Avg train loss: 0.069021\n",
      "Epoch [2516/10000] Avg train loss: 0.068994\n",
      "Epoch [2517/10000] Avg train loss: 0.068967\n",
      "Epoch [2518/10000] Avg train loss: 0.068939\n",
      "Epoch [2519/10000] Avg train loss: 0.068912\n",
      "Epoch [2520/10000] Avg train loss: 0.068885\n",
      "Epoch [2521/10000] Avg train loss: 0.068857\n",
      "Epoch [2522/10000] Avg train loss: 0.068830\n",
      "Epoch [2523/10000] Avg train loss: 0.068803\n",
      "Epoch [2524/10000] Avg train loss: 0.068775\n",
      "Epoch [2525/10000] Avg train loss: 0.068748\n",
      "Epoch [2526/10000] Avg train loss: 0.068721\n",
      "Epoch [2527/10000] Avg train loss: 0.068694\n",
      "Epoch [2528/10000] Avg train loss: 0.068667\n",
      "Epoch [2529/10000] Avg train loss: 0.068640\n",
      "Epoch [2530/10000] Avg train loss: 0.068613\n",
      "Epoch [2531/10000] Avg train loss: 0.068585\n",
      "Epoch [2532/10000] Avg train loss: 0.068558\n",
      "Epoch [2533/10000] Avg train loss: 0.068531\n",
      "Epoch [2534/10000] Avg train loss: 0.068504\n",
      "Epoch [2535/10000] Avg train loss: 0.068477\n",
      "Epoch [2536/10000] Avg train loss: 0.068450\n",
      "Epoch [2537/10000] Avg train loss: 0.068423\n",
      "Epoch [2538/10000] Avg train loss: 0.068396\n",
      "Epoch [2539/10000] Avg train loss: 0.068370\n",
      "Epoch [2540/10000] Avg train loss: 0.068343\n",
      "Epoch [2541/10000] Avg train loss: 0.068316\n",
      "Epoch [2542/10000] Avg train loss: 0.068289\n",
      "Epoch [2543/10000] Avg train loss: 0.068262\n",
      "Epoch [2544/10000] Avg train loss: 0.068235\n",
      "Epoch [2545/10000] Avg train loss: 0.068208\n",
      "Epoch [2546/10000] Avg train loss: 0.068182\n",
      "Epoch [2547/10000] Avg train loss: 0.068155\n",
      "Epoch [2548/10000] Avg train loss: 0.068128\n",
      "Epoch [2549/10000] Avg train loss: 0.068102\n",
      "Epoch [2550/10000] Avg train loss: 0.068075\n",
      "Epoch [2551/10000] Avg train loss: 0.068048\n",
      "Epoch [2552/10000] Avg train loss: 0.068022\n",
      "Epoch [2553/10000] Avg train loss: 0.067995\n",
      "Epoch [2554/10000] Avg train loss: 0.067968\n",
      "Epoch [2555/10000] Avg train loss: 0.067942\n",
      "Epoch [2556/10000] Avg train loss: 0.067915\n",
      "Epoch [2557/10000] Avg train loss: 0.067889\n",
      "Epoch [2558/10000] Avg train loss: 0.067862\n",
      "Epoch [2559/10000] Avg train loss: 0.067836\n",
      "Epoch [2560/10000] Avg train loss: 0.067809\n",
      "Epoch [2561/10000] Avg train loss: 0.067783\n",
      "Epoch [2562/10000] Avg train loss: 0.067756\n",
      "Epoch [2563/10000] Avg train loss: 0.067730\n",
      "Epoch [2564/10000] Avg train loss: 0.067703\n",
      "Epoch [2565/10000] Avg train loss: 0.067677\n",
      "Epoch [2566/10000] Avg train loss: 0.067651\n",
      "Epoch [2567/10000] Avg train loss: 0.067624\n",
      "Epoch [2568/10000] Avg train loss: 0.067598\n",
      "Epoch [2569/10000] Avg train loss: 0.067572\n",
      "Epoch [2570/10000] Avg train loss: 0.067545\n",
      "Epoch [2571/10000] Avg train loss: 0.067519\n",
      "Epoch [2572/10000] Avg train loss: 0.067493\n",
      "Epoch [2573/10000] Avg train loss: 0.067467\n",
      "Epoch [2574/10000] Avg train loss: 0.067440\n",
      "Epoch [2575/10000] Avg train loss: 0.067414\n",
      "Epoch [2576/10000] Avg train loss: 0.067388\n",
      "Epoch [2577/10000] Avg train loss: 0.067362\n",
      "Epoch [2578/10000] Avg train loss: 0.067336\n",
      "Epoch [2579/10000] Avg train loss: 0.067310\n",
      "Epoch [2580/10000] Avg train loss: 0.067284\n",
      "Epoch [2581/10000] Avg train loss: 0.067258\n",
      "Epoch [2582/10000] Avg train loss: 0.067232\n",
      "Epoch [2583/10000] Avg train loss: 0.067206\n",
      "Epoch [2584/10000] Avg train loss: 0.067180\n",
      "Epoch [2585/10000] Avg train loss: 0.067154\n",
      "Epoch [2586/10000] Avg train loss: 0.067128\n",
      "Epoch [2587/10000] Avg train loss: 0.067102\n",
      "Epoch [2588/10000] Avg train loss: 0.067076\n",
      "Epoch [2589/10000] Avg train loss: 0.067050\n",
      "Epoch [2590/10000] Avg train loss: 0.067024\n",
      "Epoch [2591/10000] Avg train loss: 0.066998\n",
      "Epoch [2592/10000] Avg train loss: 0.066972\n",
      "Epoch [2593/10000] Avg train loss: 0.066947\n",
      "Epoch [2594/10000] Avg train loss: 0.066921\n",
      "Epoch [2595/10000] Avg train loss: 0.066895\n",
      "Epoch [2596/10000] Avg train loss: 0.066869\n",
      "Epoch [2597/10000] Avg train loss: 0.066844\n",
      "Epoch [2598/10000] Avg train loss: 0.066818\n",
      "Epoch [2599/10000] Avg train loss: 0.066792\n",
      "Epoch [2600/10000] Avg train loss: 0.066766\n",
      "Epoch [2601/10000] Avg train loss: 0.066741\n",
      "Epoch [2602/10000] Avg train loss: 0.066715\n",
      "Epoch [2603/10000] Avg train loss: 0.066690\n",
      "Epoch [2604/10000] Avg train loss: 0.066664\n",
      "Epoch [2605/10000] Avg train loss: 0.066638\n",
      "Epoch [2606/10000] Avg train loss: 0.066613\n",
      "Epoch [2607/10000] Avg train loss: 0.066587\n",
      "Epoch [2608/10000] Avg train loss: 0.066562\n",
      "Epoch [2609/10000] Avg train loss: 0.066536\n",
      "Epoch [2610/10000] Avg train loss: 0.066511\n",
      "Epoch [2611/10000] Avg train loss: 0.066485\n",
      "Epoch [2612/10000] Avg train loss: 0.066460\n",
      "Epoch [2613/10000] Avg train loss: 0.066435\n",
      "Epoch [2614/10000] Avg train loss: 0.066409\n",
      "Epoch [2615/10000] Avg train loss: 0.066384\n",
      "Epoch [2616/10000] Avg train loss: 0.066358\n",
      "Epoch [2617/10000] Avg train loss: 0.066333\n",
      "Epoch [2618/10000] Avg train loss: 0.066308\n",
      "Epoch [2619/10000] Avg train loss: 0.066282\n",
      "Epoch [2620/10000] Avg train loss: 0.066257\n",
      "Epoch [2621/10000] Avg train loss: 0.066232\n",
      "Epoch [2622/10000] Avg train loss: 0.066207\n",
      "Epoch [2623/10000] Avg train loss: 0.066181\n",
      "Epoch [2624/10000] Avg train loss: 0.066156\n",
      "Epoch [2625/10000] Avg train loss: 0.066131\n",
      "Epoch [2626/10000] Avg train loss: 0.066106\n",
      "Epoch [2627/10000] Avg train loss: 0.066081\n",
      "Epoch [2628/10000] Avg train loss: 0.066056\n",
      "Epoch [2629/10000] Avg train loss: 0.066030\n",
      "Epoch [2630/10000] Avg train loss: 0.066005\n",
      "Epoch [2631/10000] Avg train loss: 0.065980\n",
      "Epoch [2632/10000] Avg train loss: 0.065955\n",
      "Epoch [2633/10000] Avg train loss: 0.065930\n",
      "Epoch [2634/10000] Avg train loss: 0.065905\n",
      "Epoch [2635/10000] Avg train loss: 0.065880\n",
      "Epoch [2636/10000] Avg train loss: 0.065855\n",
      "Epoch [2637/10000] Avg train loss: 0.065830\n",
      "Epoch [2638/10000] Avg train loss: 0.065805\n",
      "Epoch [2639/10000] Avg train loss: 0.065780\n",
      "Epoch [2640/10000] Avg train loss: 0.065756\n",
      "Epoch [2641/10000] Avg train loss: 0.065731\n",
      "Epoch [2642/10000] Avg train loss: 0.065706\n",
      "Epoch [2643/10000] Avg train loss: 0.065681\n",
      "Epoch [2644/10000] Avg train loss: 0.065656\n",
      "Epoch [2645/10000] Avg train loss: 0.065631\n",
      "Epoch [2646/10000] Avg train loss: 0.065607\n",
      "Epoch [2647/10000] Avg train loss: 0.065582\n",
      "Epoch [2648/10000] Avg train loss: 0.065557\n",
      "Epoch [2649/10000] Avg train loss: 0.065532\n",
      "Epoch [2650/10000] Avg train loss: 0.065508\n",
      "Epoch [2651/10000] Avg train loss: 0.065483\n",
      "Epoch [2652/10000] Avg train loss: 0.065458\n",
      "Epoch [2653/10000] Avg train loss: 0.065434\n",
      "Epoch [2654/10000] Avg train loss: 0.065409\n",
      "Epoch [2655/10000] Avg train loss: 0.065384\n",
      "Epoch [2656/10000] Avg train loss: 0.065360\n",
      "Epoch [2657/10000] Avg train loss: 0.065335\n",
      "Epoch [2658/10000] Avg train loss: 0.065311\n",
      "Epoch [2659/10000] Avg train loss: 0.065286\n",
      "Epoch [2660/10000] Avg train loss: 0.065261\n",
      "Epoch [2661/10000] Avg train loss: 0.065237\n",
      "Epoch [2662/10000] Avg train loss: 0.065212\n",
      "Epoch [2663/10000] Avg train loss: 0.065188\n",
      "Epoch [2664/10000] Avg train loss: 0.065164\n",
      "Epoch [2665/10000] Avg train loss: 0.065139\n",
      "Epoch [2666/10000] Avg train loss: 0.065115\n",
      "Epoch [2667/10000] Avg train loss: 0.065090\n",
      "Epoch [2668/10000] Avg train loss: 0.065066\n",
      "Epoch [2669/10000] Avg train loss: 0.065042\n",
      "Epoch [2670/10000] Avg train loss: 0.065017\n",
      "Epoch [2671/10000] Avg train loss: 0.064993\n",
      "Epoch [2672/10000] Avg train loss: 0.064969\n",
      "Epoch [2673/10000] Avg train loss: 0.064944\n",
      "Epoch [2674/10000] Avg train loss: 0.064920\n",
      "Epoch [2675/10000] Avg train loss: 0.064896\n",
      "Epoch [2676/10000] Avg train loss: 0.064872\n",
      "Epoch [2677/10000] Avg train loss: 0.064847\n",
      "Epoch [2678/10000] Avg train loss: 0.064823\n",
      "Epoch [2679/10000] Avg train loss: 0.064799\n",
      "Epoch [2680/10000] Avg train loss: 0.064775\n",
      "Epoch [2681/10000] Avg train loss: 0.064751\n",
      "Epoch [2682/10000] Avg train loss: 0.064727\n",
      "Epoch [2683/10000] Avg train loss: 0.064702\n",
      "Epoch [2684/10000] Avg train loss: 0.064678\n",
      "Epoch [2685/10000] Avg train loss: 0.064654\n",
      "Epoch [2686/10000] Avg train loss: 0.064630\n",
      "Epoch [2687/10000] Avg train loss: 0.064606\n",
      "Epoch [2688/10000] Avg train loss: 0.064582\n",
      "Epoch [2689/10000] Avg train loss: 0.064558\n",
      "Epoch [2690/10000] Avg train loss: 0.064534\n",
      "Epoch [2691/10000] Avg train loss: 0.064510\n",
      "Epoch [2692/10000] Avg train loss: 0.064486\n",
      "Epoch [2693/10000] Avg train loss: 0.064462\n",
      "Epoch [2694/10000] Avg train loss: 0.064438\n",
      "Epoch [2695/10000] Avg train loss: 0.064415\n",
      "Epoch [2696/10000] Avg train loss: 0.064391\n",
      "Epoch [2697/10000] Avg train loss: 0.064367\n",
      "Epoch [2698/10000] Avg train loss: 0.064343\n",
      "Epoch [2699/10000] Avg train loss: 0.064319\n",
      "Epoch [2700/10000] Avg train loss: 0.064295\n",
      "Epoch [2701/10000] Avg train loss: 0.064272\n",
      "Epoch [2702/10000] Avg train loss: 0.064248\n",
      "Epoch [2703/10000] Avg train loss: 0.064224\n",
      "Epoch [2704/10000] Avg train loss: 0.064200\n",
      "Epoch [2705/10000] Avg train loss: 0.064177\n",
      "Epoch [2706/10000] Avg train loss: 0.064153\n",
      "Epoch [2707/10000] Avg train loss: 0.064129\n",
      "Epoch [2708/10000] Avg train loss: 0.064105\n",
      "Epoch [2709/10000] Avg train loss: 0.064082\n",
      "Epoch [2710/10000] Avg train loss: 0.064058\n",
      "Epoch [2711/10000] Avg train loss: 0.064035\n",
      "Epoch [2712/10000] Avg train loss: 0.064011\n",
      "Epoch [2713/10000] Avg train loss: 0.063987\n",
      "Epoch [2714/10000] Avg train loss: 0.063964\n",
      "Epoch [2715/10000] Avg train loss: 0.063940\n",
      "Epoch [2716/10000] Avg train loss: 0.063917\n",
      "Epoch [2717/10000] Avg train loss: 0.063893\n",
      "Epoch [2718/10000] Avg train loss: 0.063870\n",
      "Epoch [2719/10000] Avg train loss: 0.063846\n",
      "Epoch [2720/10000] Avg train loss: 0.063823\n",
      "Epoch [2721/10000] Avg train loss: 0.063799\n",
      "Epoch [2722/10000] Avg train loss: 0.063776\n",
      "Epoch [2723/10000] Avg train loss: 0.063753\n",
      "Epoch [2724/10000] Avg train loss: 0.063729\n",
      "Epoch [2725/10000] Avg train loss: 0.063706\n",
      "Epoch [2726/10000] Avg train loss: 0.063682\n",
      "Epoch [2727/10000] Avg train loss: 0.063659\n",
      "Epoch [2728/10000] Avg train loss: 0.063636\n",
      "Epoch [2729/10000] Avg train loss: 0.063612\n",
      "Epoch [2730/10000] Avg train loss: 0.063589\n",
      "Epoch [2731/10000] Avg train loss: 0.063566\n",
      "Epoch [2732/10000] Avg train loss: 0.063543\n",
      "Epoch [2733/10000] Avg train loss: 0.063519\n",
      "Epoch [2734/10000] Avg train loss: 0.063496\n",
      "Epoch [2735/10000] Avg train loss: 0.063473\n",
      "Epoch [2736/10000] Avg train loss: 0.063450\n",
      "Epoch [2737/10000] Avg train loss: 0.063427\n",
      "Epoch [2738/10000] Avg train loss: 0.063403\n",
      "Epoch [2739/10000] Avg train loss: 0.063380\n",
      "Epoch [2740/10000] Avg train loss: 0.063357\n",
      "Epoch [2741/10000] Avg train loss: 0.063334\n",
      "Epoch [2742/10000] Avg train loss: 0.063311\n",
      "Epoch [2743/10000] Avg train loss: 0.063288\n",
      "Epoch [2744/10000] Avg train loss: 0.063265\n",
      "Epoch [2745/10000] Avg train loss: 0.063242\n",
      "Epoch [2746/10000] Avg train loss: 0.063219\n",
      "Epoch [2747/10000] Avg train loss: 0.063196\n",
      "Epoch [2748/10000] Avg train loss: 0.063173\n",
      "Epoch [2749/10000] Avg train loss: 0.063150\n",
      "Epoch [2750/10000] Avg train loss: 0.063127\n",
      "Epoch [2751/10000] Avg train loss: 0.063104\n",
      "Epoch [2752/10000] Avg train loss: 0.063081\n",
      "Epoch [2753/10000] Avg train loss: 0.063058\n",
      "Epoch [2754/10000] Avg train loss: 0.063035\n",
      "Epoch [2755/10000] Avg train loss: 0.063013\n",
      "Epoch [2756/10000] Avg train loss: 0.062990\n",
      "Epoch [2757/10000] Avg train loss: 0.062967\n",
      "Epoch [2758/10000] Avg train loss: 0.062944\n",
      "Epoch [2759/10000] Avg train loss: 0.062921\n",
      "Epoch [2760/10000] Avg train loss: 0.062899\n",
      "Epoch [2761/10000] Avg train loss: 0.062876\n",
      "Epoch [2762/10000] Avg train loss: 0.062853\n",
      "Epoch [2763/10000] Avg train loss: 0.062830\n",
      "Epoch [2764/10000] Avg train loss: 0.062808\n",
      "Epoch [2765/10000] Avg train loss: 0.062785\n",
      "Epoch [2766/10000] Avg train loss: 0.062762\n",
      "Epoch [2767/10000] Avg train loss: 0.062739\n",
      "Epoch [2768/10000] Avg train loss: 0.062717\n",
      "Epoch [2769/10000] Avg train loss: 0.062694\n",
      "Epoch [2770/10000] Avg train loss: 0.062672\n",
      "Epoch [2771/10000] Avg train loss: 0.062649\n",
      "Epoch [2772/10000] Avg train loss: 0.062626\n",
      "Epoch [2773/10000] Avg train loss: 0.062604\n",
      "Epoch [2774/10000] Avg train loss: 0.062581\n",
      "Epoch [2775/10000] Avg train loss: 0.062559\n",
      "Epoch [2776/10000] Avg train loss: 0.062536\n",
      "Epoch [2777/10000] Avg train loss: 0.062514\n",
      "Epoch [2778/10000] Avg train loss: 0.062491\n",
      "Epoch [2779/10000] Avg train loss: 0.062469\n",
      "Epoch [2780/10000] Avg train loss: 0.062446\n",
      "Epoch [2781/10000] Avg train loss: 0.062424\n",
      "Epoch [2782/10000] Avg train loss: 0.062401\n",
      "Epoch [2783/10000] Avg train loss: 0.062379\n",
      "Epoch [2784/10000] Avg train loss: 0.062357\n",
      "Epoch [2785/10000] Avg train loss: 0.062334\n",
      "Epoch [2786/10000] Avg train loss: 0.062312\n",
      "Epoch [2787/10000] Avg train loss: 0.062290\n",
      "Epoch [2788/10000] Avg train loss: 0.062267\n",
      "Epoch [2789/10000] Avg train loss: 0.062245\n",
      "Epoch [2790/10000] Avg train loss: 0.062223\n",
      "Epoch [2791/10000] Avg train loss: 0.062200\n",
      "Epoch [2792/10000] Avg train loss: 0.062178\n",
      "Epoch [2793/10000] Avg train loss: 0.062156\n",
      "Epoch [2794/10000] Avg train loss: 0.062134\n",
      "Epoch [2795/10000] Avg train loss: 0.062111\n",
      "Epoch [2796/10000] Avg train loss: 0.062089\n",
      "Epoch [2797/10000] Avg train loss: 0.062067\n",
      "Epoch [2798/10000] Avg train loss: 0.062045\n",
      "Epoch [2799/10000] Avg train loss: 0.062023\n",
      "Epoch [2800/10000] Avg train loss: 0.062000\n",
      "Epoch [2801/10000] Avg train loss: 0.061978\n",
      "Epoch [2802/10000] Avg train loss: 0.061956\n",
      "Epoch [2803/10000] Avg train loss: 0.061934\n",
      "Epoch [2804/10000] Avg train loss: 0.061912\n",
      "Epoch [2805/10000] Avg train loss: 0.061890\n",
      "Epoch [2806/10000] Avg train loss: 0.061868\n",
      "Epoch [2807/10000] Avg train loss: 0.061846\n",
      "Epoch [2808/10000] Avg train loss: 0.061824\n",
      "Epoch [2809/10000] Avg train loss: 0.061802\n",
      "Epoch [2810/10000] Avg train loss: 0.061780\n",
      "Epoch [2811/10000] Avg train loss: 0.061758\n",
      "Epoch [2812/10000] Avg train loss: 0.061736\n",
      "Epoch [2813/10000] Avg train loss: 0.061714\n",
      "Epoch [2814/10000] Avg train loss: 0.061692\n",
      "Epoch [2815/10000] Avg train loss: 0.061670\n",
      "Epoch [2816/10000] Avg train loss: 0.061648\n",
      "Epoch [2817/10000] Avg train loss: 0.061626\n",
      "Epoch [2818/10000] Avg train loss: 0.061605\n",
      "Epoch [2819/10000] Avg train loss: 0.061583\n",
      "Epoch [2820/10000] Avg train loss: 0.061561\n",
      "Epoch [2821/10000] Avg train loss: 0.061539\n",
      "Epoch [2822/10000] Avg train loss: 0.061517\n",
      "Epoch [2823/10000] Avg train loss: 0.061496\n",
      "Epoch [2824/10000] Avg train loss: 0.061474\n",
      "Epoch [2825/10000] Avg train loss: 0.061452\n",
      "Epoch [2826/10000] Avg train loss: 0.061430\n",
      "Epoch [2827/10000] Avg train loss: 0.061409\n",
      "Epoch [2828/10000] Avg train loss: 0.061387\n",
      "Epoch [2829/10000] Avg train loss: 0.061365\n",
      "Epoch [2830/10000] Avg train loss: 0.061343\n",
      "Epoch [2831/10000] Avg train loss: 0.061322\n",
      "Epoch [2832/10000] Avg train loss: 0.061300\n",
      "Epoch [2833/10000] Avg train loss: 0.061279\n",
      "Epoch [2834/10000] Avg train loss: 0.061257\n",
      "Epoch [2835/10000] Avg train loss: 0.061235\n",
      "Epoch [2836/10000] Avg train loss: 0.061214\n",
      "Epoch [2837/10000] Avg train loss: 0.061192\n",
      "Epoch [2838/10000] Avg train loss: 0.061171\n",
      "Epoch [2839/10000] Avg train loss: 0.061149\n",
      "Epoch [2840/10000] Avg train loss: 0.061128\n",
      "Epoch [2841/10000] Avg train loss: 0.061106\n",
      "Epoch [2842/10000] Avg train loss: 0.061085\n",
      "Epoch [2843/10000] Avg train loss: 0.061063\n",
      "Epoch [2844/10000] Avg train loss: 0.061042\n",
      "Epoch [2845/10000] Avg train loss: 0.061020\n",
      "Epoch [2846/10000] Avg train loss: 0.060999\n",
      "Epoch [2847/10000] Avg train loss: 0.060977\n",
      "Epoch [2848/10000] Avg train loss: 0.060956\n",
      "Epoch [2849/10000] Avg train loss: 0.060935\n",
      "Epoch [2850/10000] Avg train loss: 0.060913\n",
      "Epoch [2851/10000] Avg train loss: 0.060892\n",
      "Epoch [2852/10000] Avg train loss: 0.060870\n",
      "Epoch [2853/10000] Avg train loss: 0.060849\n",
      "Epoch [2854/10000] Avg train loss: 0.060828\n",
      "Epoch [2855/10000] Avg train loss: 0.060807\n",
      "Epoch [2856/10000] Avg train loss: 0.060785\n",
      "Epoch [2857/10000] Avg train loss: 0.060764\n",
      "Epoch [2858/10000] Avg train loss: 0.060743\n",
      "Epoch [2859/10000] Avg train loss: 0.060721\n",
      "Epoch [2860/10000] Avg train loss: 0.060700\n",
      "Epoch [2861/10000] Avg train loss: 0.060679\n",
      "Epoch [2862/10000] Avg train loss: 0.060658\n",
      "Epoch [2863/10000] Avg train loss: 0.060637\n",
      "Epoch [2864/10000] Avg train loss: 0.060616\n",
      "Epoch [2865/10000] Avg train loss: 0.060594\n",
      "Epoch [2866/10000] Avg train loss: 0.060573\n",
      "Epoch [2867/10000] Avg train loss: 0.060552\n",
      "Epoch [2868/10000] Avg train loss: 0.060531\n",
      "Epoch [2869/10000] Avg train loss: 0.060510\n",
      "Epoch [2870/10000] Avg train loss: 0.060489\n",
      "Epoch [2871/10000] Avg train loss: 0.060468\n",
      "Epoch [2872/10000] Avg train loss: 0.060447\n",
      "Epoch [2873/10000] Avg train loss: 0.060426\n",
      "Epoch [2874/10000] Avg train loss: 0.060405\n",
      "Epoch [2875/10000] Avg train loss: 0.060384\n",
      "Epoch [2876/10000] Avg train loss: 0.060363\n",
      "Epoch [2877/10000] Avg train loss: 0.060342\n",
      "Epoch [2878/10000] Avg train loss: 0.060321\n",
      "Epoch [2879/10000] Avg train loss: 0.060300\n",
      "Epoch [2880/10000] Avg train loss: 0.060279\n",
      "Epoch [2881/10000] Avg train loss: 0.060258\n",
      "Epoch [2882/10000] Avg train loss: 0.060237\n",
      "Epoch [2883/10000] Avg train loss: 0.060216\n",
      "Epoch [2884/10000] Avg train loss: 0.060195\n",
      "Epoch [2885/10000] Avg train loss: 0.060175\n",
      "Epoch [2886/10000] Avg train loss: 0.060154\n",
      "Epoch [2887/10000] Avg train loss: 0.060133\n",
      "Epoch [2888/10000] Avg train loss: 0.060112\n",
      "Epoch [2889/10000] Avg train loss: 0.060091\n",
      "Epoch [2890/10000] Avg train loss: 0.060070\n",
      "Epoch [2891/10000] Avg train loss: 0.060050\n",
      "Epoch [2892/10000] Avg train loss: 0.060029\n",
      "Epoch [2893/10000] Avg train loss: 0.060008\n",
      "Epoch [2894/10000] Avg train loss: 0.059987\n",
      "Epoch [2895/10000] Avg train loss: 0.059967\n",
      "Epoch [2896/10000] Avg train loss: 0.059946\n",
      "Epoch [2897/10000] Avg train loss: 0.059925\n",
      "Epoch [2898/10000] Avg train loss: 0.059905\n",
      "Epoch [2899/10000] Avg train loss: 0.059884\n",
      "Epoch [2900/10000] Avg train loss: 0.059863\n",
      "Epoch [2901/10000] Avg train loss: 0.059843\n",
      "Epoch [2902/10000] Avg train loss: 0.059822\n",
      "Epoch [2903/10000] Avg train loss: 0.059802\n",
      "Epoch [2904/10000] Avg train loss: 0.059781\n",
      "Epoch [2905/10000] Avg train loss: 0.059760\n",
      "Epoch [2906/10000] Avg train loss: 0.059740\n",
      "Epoch [2907/10000] Avg train loss: 0.059719\n",
      "Epoch [2908/10000] Avg train loss: 0.059699\n",
      "Epoch [2909/10000] Avg train loss: 0.059678\n",
      "Epoch [2910/10000] Avg train loss: 0.059658\n",
      "Epoch [2911/10000] Avg train loss: 0.059637\n",
      "Epoch [2912/10000] Avg train loss: 0.059617\n",
      "Epoch [2913/10000] Avg train loss: 0.059596\n",
      "Epoch [2914/10000] Avg train loss: 0.059576\n",
      "Epoch [2915/10000] Avg train loss: 0.059555\n",
      "Epoch [2916/10000] Avg train loss: 0.059535\n",
      "Epoch [2917/10000] Avg train loss: 0.059515\n",
      "Epoch [2918/10000] Avg train loss: 0.059494\n",
      "Epoch [2919/10000] Avg train loss: 0.059474\n",
      "Epoch [2920/10000] Avg train loss: 0.059453\n",
      "Epoch [2921/10000] Avg train loss: 0.059433\n",
      "Epoch [2922/10000] Avg train loss: 0.059413\n",
      "Epoch [2923/10000] Avg train loss: 0.059392\n",
      "Epoch [2924/10000] Avg train loss: 0.059372\n",
      "Epoch [2925/10000] Avg train loss: 0.059352\n",
      "Epoch [2926/10000] Avg train loss: 0.059332\n",
      "Epoch [2927/10000] Avg train loss: 0.059311\n",
      "Epoch [2928/10000] Avg train loss: 0.059291\n",
      "Epoch [2929/10000] Avg train loss: 0.059271\n",
      "Epoch [2930/10000] Avg train loss: 0.059251\n",
      "Epoch [2931/10000] Avg train loss: 0.059230\n",
      "Epoch [2932/10000] Avg train loss: 0.059210\n",
      "Epoch [2933/10000] Avg train loss: 0.059190\n",
      "Epoch [2934/10000] Avg train loss: 0.059170\n",
      "Epoch [2935/10000] Avg train loss: 0.059150\n",
      "Epoch [2936/10000] Avg train loss: 0.059130\n",
      "Epoch [2937/10000] Avg train loss: 0.059109\n",
      "Epoch [2938/10000] Avg train loss: 0.059089\n",
      "Epoch [2939/10000] Avg train loss: 0.059069\n",
      "Epoch [2940/10000] Avg train loss: 0.059049\n",
      "Epoch [2941/10000] Avg train loss: 0.059029\n",
      "Epoch [2942/10000] Avg train loss: 0.059009\n",
      "Epoch [2943/10000] Avg train loss: 0.058989\n",
      "Epoch [2944/10000] Avg train loss: 0.058969\n",
      "Epoch [2945/10000] Avg train loss: 0.058949\n",
      "Epoch [2946/10000] Avg train loss: 0.058929\n",
      "Epoch [2947/10000] Avg train loss: 0.058909\n",
      "Epoch [2948/10000] Avg train loss: 0.058889\n",
      "Epoch [2949/10000] Avg train loss: 0.058869\n",
      "Epoch [2950/10000] Avg train loss: 0.058849\n",
      "Epoch [2951/10000] Avg train loss: 0.058829\n",
      "Epoch [2952/10000] Avg train loss: 0.058809\n",
      "Epoch [2953/10000] Avg train loss: 0.058789\n",
      "Epoch [2954/10000] Avg train loss: 0.058769\n",
      "Epoch [2955/10000] Avg train loss: 0.058749\n",
      "Epoch [2956/10000] Avg train loss: 0.058730\n",
      "Epoch [2957/10000] Avg train loss: 0.058710\n",
      "Epoch [2958/10000] Avg train loss: 0.058690\n",
      "Epoch [2959/10000] Avg train loss: 0.058670\n",
      "Epoch [2960/10000] Avg train loss: 0.058650\n",
      "Epoch [2961/10000] Avg train loss: 0.058630\n",
      "Epoch [2962/10000] Avg train loss: 0.058611\n",
      "Epoch [2963/10000] Avg train loss: 0.058591\n",
      "Epoch [2964/10000] Avg train loss: 0.058571\n",
      "Epoch [2965/10000] Avg train loss: 0.058551\n",
      "Epoch [2966/10000] Avg train loss: 0.058532\n",
      "Epoch [2967/10000] Avg train loss: 0.058512\n",
      "Epoch [2968/10000] Avg train loss: 0.058492\n",
      "Epoch [2969/10000] Avg train loss: 0.058472\n",
      "Epoch [2970/10000] Avg train loss: 0.058453\n",
      "Epoch [2971/10000] Avg train loss: 0.058433\n",
      "Epoch [2972/10000] Avg train loss: 0.058413\n",
      "Epoch [2973/10000] Avg train loss: 0.058394\n",
      "Epoch [2974/10000] Avg train loss: 0.058374\n",
      "Epoch [2975/10000] Avg train loss: 0.058355\n",
      "Epoch [2976/10000] Avg train loss: 0.058335\n",
      "Epoch [2977/10000] Avg train loss: 0.058315\n",
      "Epoch [2978/10000] Avg train loss: 0.058296\n",
      "Epoch [2979/10000] Avg train loss: 0.058276\n",
      "Epoch [2980/10000] Avg train loss: 0.058257\n",
      "Epoch [2981/10000] Avg train loss: 0.058237\n",
      "Epoch [2982/10000] Avg train loss: 0.058218\n",
      "Epoch [2983/10000] Avg train loss: 0.058198\n",
      "Epoch [2984/10000] Avg train loss: 0.058179\n",
      "Epoch [2985/10000] Avg train loss: 0.058159\n",
      "Epoch [2986/10000] Avg train loss: 0.058140\n",
      "Epoch [2987/10000] Avg train loss: 0.058120\n",
      "Epoch [2988/10000] Avg train loss: 0.058101\n",
      "Epoch [2989/10000] Avg train loss: 0.058081\n",
      "Epoch [2990/10000] Avg train loss: 0.058062\n",
      "Epoch [2991/10000] Avg train loss: 0.058043\n",
      "Epoch [2992/10000] Avg train loss: 0.058023\n",
      "Epoch [2993/10000] Avg train loss: 0.058004\n",
      "Epoch [2994/10000] Avg train loss: 0.057984\n",
      "Epoch [2995/10000] Avg train loss: 0.057965\n",
      "Epoch [2996/10000] Avg train loss: 0.057946\n",
      "Epoch [2997/10000] Avg train loss: 0.057926\n",
      "Epoch [2998/10000] Avg train loss: 0.057907\n",
      "Epoch [2999/10000] Avg train loss: 0.057888\n",
      "Epoch [3000/10000] Avg train loss: 0.057868\n",
      "Epoch [3001/10000] Avg train loss: 0.057849\n",
      "Epoch [3002/10000] Avg train loss: 0.057830\n",
      "Epoch [3003/10000] Avg train loss: 0.057811\n",
      "Epoch [3004/10000] Avg train loss: 0.057791\n",
      "Epoch [3005/10000] Avg train loss: 0.057772\n",
      "Epoch [3006/10000] Avg train loss: 0.057753\n",
      "Epoch [3007/10000] Avg train loss: 0.057734\n",
      "Epoch [3008/10000] Avg train loss: 0.057715\n",
      "Epoch [3009/10000] Avg train loss: 0.057695\n",
      "Epoch [3010/10000] Avg train loss: 0.057676\n",
      "Epoch [3011/10000] Avg train loss: 0.057657\n",
      "Epoch [3012/10000] Avg train loss: 0.057638\n",
      "Epoch [3013/10000] Avg train loss: 0.057619\n",
      "Epoch [3014/10000] Avg train loss: 0.057600\n",
      "Epoch [3015/10000] Avg train loss: 0.057581\n",
      "Epoch [3016/10000] Avg train loss: 0.057562\n",
      "Epoch [3017/10000] Avg train loss: 0.057542\n",
      "Epoch [3018/10000] Avg train loss: 0.057523\n",
      "Epoch [3019/10000] Avg train loss: 0.057504\n",
      "Epoch [3020/10000] Avg train loss: 0.057485\n",
      "Epoch [3021/10000] Avg train loss: 0.057466\n",
      "Epoch [3022/10000] Avg train loss: 0.057447\n",
      "Epoch [3023/10000] Avg train loss: 0.057428\n",
      "Epoch [3024/10000] Avg train loss: 0.057409\n",
      "Epoch [3025/10000] Avg train loss: 0.057390\n",
      "Epoch [3026/10000] Avg train loss: 0.057371\n",
      "Epoch [3027/10000] Avg train loss: 0.057352\n",
      "Epoch [3028/10000] Avg train loss: 0.057333\n",
      "Epoch [3029/10000] Avg train loss: 0.057315\n",
      "Epoch [3030/10000] Avg train loss: 0.057296\n",
      "Epoch [3031/10000] Avg train loss: 0.057277\n",
      "Epoch [3032/10000] Avg train loss: 0.057258\n",
      "Epoch [3033/10000] Avg train loss: 0.057239\n",
      "Epoch [3034/10000] Avg train loss: 0.057220\n",
      "Epoch [3035/10000] Avg train loss: 0.057201\n",
      "Epoch [3036/10000] Avg train loss: 0.057182\n",
      "Epoch [3037/10000] Avg train loss: 0.057164\n",
      "Epoch [3038/10000] Avg train loss: 0.057145\n",
      "Epoch [3039/10000] Avg train loss: 0.057126\n",
      "Epoch [3040/10000] Avg train loss: 0.057107\n",
      "Epoch [3041/10000] Avg train loss: 0.057088\n",
      "Epoch [3042/10000] Avg train loss: 0.057070\n",
      "Epoch [3043/10000] Avg train loss: 0.057051\n",
      "Epoch [3044/10000] Avg train loss: 0.057032\n",
      "Epoch [3045/10000] Avg train loss: 0.057013\n",
      "Epoch [3046/10000] Avg train loss: 0.056995\n",
      "Epoch [3047/10000] Avg train loss: 0.056976\n",
      "Epoch [3048/10000] Avg train loss: 0.056957\n",
      "Epoch [3049/10000] Avg train loss: 0.056939\n",
      "Epoch [3050/10000] Avg train loss: 0.056920\n",
      "Epoch [3051/10000] Avg train loss: 0.056901\n",
      "Epoch [3052/10000] Avg train loss: 0.056883\n",
      "Epoch [3053/10000] Avg train loss: 0.056864\n",
      "Epoch [3054/10000] Avg train loss: 0.056845\n",
      "Epoch [3055/10000] Avg train loss: 0.056827\n",
      "Epoch [3056/10000] Avg train loss: 0.056808\n",
      "Epoch [3057/10000] Avg train loss: 0.056790\n",
      "Epoch [3058/10000] Avg train loss: 0.056771\n",
      "Epoch [3059/10000] Avg train loss: 0.056753\n",
      "Epoch [3060/10000] Avg train loss: 0.056734\n",
      "Epoch [3061/10000] Avg train loss: 0.056716\n",
      "Epoch [3062/10000] Avg train loss: 0.056697\n",
      "Epoch [3063/10000] Avg train loss: 0.056678\n",
      "Epoch [3064/10000] Avg train loss: 0.056660\n",
      "Epoch [3065/10000] Avg train loss: 0.056642\n",
      "Epoch [3066/10000] Avg train loss: 0.056623\n",
      "Epoch [3067/10000] Avg train loss: 0.056605\n",
      "Epoch [3068/10000] Avg train loss: 0.056586\n",
      "Epoch [3069/10000] Avg train loss: 0.056568\n",
      "Epoch [3070/10000] Avg train loss: 0.056549\n",
      "Epoch [3071/10000] Avg train loss: 0.056531\n",
      "Epoch [3072/10000] Avg train loss: 0.056512\n",
      "Epoch [3073/10000] Avg train loss: 0.056494\n",
      "Epoch [3074/10000] Avg train loss: 0.056476\n",
      "Epoch [3075/10000] Avg train loss: 0.056457\n",
      "Epoch [3076/10000] Avg train loss: 0.056439\n",
      "Epoch [3077/10000] Avg train loss: 0.056421\n",
      "Epoch [3078/10000] Avg train loss: 0.056402\n",
      "Epoch [3079/10000] Avg train loss: 0.056384\n",
      "Epoch [3080/10000] Avg train loss: 0.056366\n",
      "Epoch [3081/10000] Avg train loss: 0.056347\n",
      "Epoch [3082/10000] Avg train loss: 0.056329\n",
      "Epoch [3083/10000] Avg train loss: 0.056311\n",
      "Epoch [3084/10000] Avg train loss: 0.056293\n",
      "Epoch [3085/10000] Avg train loss: 0.056274\n",
      "Epoch [3086/10000] Avg train loss: 0.056256\n",
      "Epoch [3087/10000] Avg train loss: 0.056238\n",
      "Epoch [3088/10000] Avg train loss: 0.056220\n",
      "Epoch [3089/10000] Avg train loss: 0.056202\n",
      "Epoch [3090/10000] Avg train loss: 0.056183\n",
      "Epoch [3091/10000] Avg train loss: 0.056165\n",
      "Epoch [3092/10000] Avg train loss: 0.056147\n",
      "Epoch [3093/10000] Avg train loss: 0.056129\n",
      "Epoch [3094/10000] Avg train loss: 0.056111\n",
      "Epoch [3095/10000] Avg train loss: 0.056093\n",
      "Epoch [3096/10000] Avg train loss: 0.056074\n",
      "Epoch [3097/10000] Avg train loss: 0.056056\n",
      "Epoch [3098/10000] Avg train loss: 0.056038\n",
      "Epoch [3099/10000] Avg train loss: 0.056020\n",
      "Epoch [3100/10000] Avg train loss: 0.056002\n",
      "Epoch [3101/10000] Avg train loss: 0.055984\n",
      "Epoch [3102/10000] Avg train loss: 0.055966\n",
      "Epoch [3103/10000] Avg train loss: 0.055948\n",
      "Epoch [3104/10000] Avg train loss: 0.055930\n",
      "Epoch [3105/10000] Avg train loss: 0.055912\n",
      "Epoch [3106/10000] Avg train loss: 0.055894\n",
      "Epoch [3107/10000] Avg train loss: 0.055876\n",
      "Epoch [3108/10000] Avg train loss: 0.055858\n",
      "Epoch [3109/10000] Avg train loss: 0.055840\n",
      "Epoch [3110/10000] Avg train loss: 0.055822\n",
      "Epoch [3111/10000] Avg train loss: 0.055804\n",
      "Epoch [3112/10000] Avg train loss: 0.055786\n",
      "Epoch [3113/10000] Avg train loss: 0.055768\n",
      "Epoch [3114/10000] Avg train loss: 0.055750\n",
      "Epoch [3115/10000] Avg train loss: 0.055732\n",
      "Epoch [3116/10000] Avg train loss: 0.055715\n",
      "Epoch [3117/10000] Avg train loss: 0.055697\n",
      "Epoch [3118/10000] Avg train loss: 0.055679\n",
      "Epoch [3119/10000] Avg train loss: 0.055661\n",
      "Epoch [3120/10000] Avg train loss: 0.055643\n",
      "Epoch [3121/10000] Avg train loss: 0.055625\n",
      "Epoch [3122/10000] Avg train loss: 0.055608\n",
      "Epoch [3123/10000] Avg train loss: 0.055590\n",
      "Epoch [3124/10000] Avg train loss: 0.055572\n",
      "Epoch [3125/10000] Avg train loss: 0.055554\n",
      "Epoch [3126/10000] Avg train loss: 0.055536\n",
      "Epoch [3127/10000] Avg train loss: 0.055519\n",
      "Epoch [3128/10000] Avg train loss: 0.055501\n",
      "Epoch [3129/10000] Avg train loss: 0.055483\n",
      "Epoch [3130/10000] Avg train loss: 0.055465\n",
      "Epoch [3131/10000] Avg train loss: 0.055448\n",
      "Epoch [3132/10000] Avg train loss: 0.055430\n",
      "Epoch [3133/10000] Avg train loss: 0.055412\n",
      "Epoch [3134/10000] Avg train loss: 0.055395\n",
      "Epoch [3135/10000] Avg train loss: 0.055377\n",
      "Epoch [3136/10000] Avg train loss: 0.055359\n",
      "Epoch [3137/10000] Avg train loss: 0.055342\n",
      "Epoch [3138/10000] Avg train loss: 0.055324\n",
      "Epoch [3139/10000] Avg train loss: 0.055306\n",
      "Epoch [3140/10000] Avg train loss: 0.055289\n",
      "Epoch [3141/10000] Avg train loss: 0.055271\n",
      "Epoch [3142/10000] Avg train loss: 0.055254\n",
      "Epoch [3143/10000] Avg train loss: 0.055236\n",
      "Epoch [3144/10000] Avg train loss: 0.055218\n",
      "Epoch [3145/10000] Avg train loss: 0.055201\n",
      "Epoch [3146/10000] Avg train loss: 0.055183\n",
      "Epoch [3147/10000] Avg train loss: 0.055166\n",
      "Epoch [3148/10000] Avg train loss: 0.055148\n",
      "Epoch [3149/10000] Avg train loss: 0.055131\n",
      "Epoch [3150/10000] Avg train loss: 0.055113\n",
      "Epoch [3151/10000] Avg train loss: 0.055096\n",
      "Epoch [3152/10000] Avg train loss: 0.055078\n",
      "Epoch [3153/10000] Avg train loss: 0.055061\n",
      "Epoch [3154/10000] Avg train loss: 0.055043\n",
      "Epoch [3155/10000] Avg train loss: 0.055026\n",
      "Epoch [3156/10000] Avg train loss: 0.055009\n",
      "Epoch [3157/10000] Avg train loss: 0.054991\n",
      "Epoch [3158/10000] Avg train loss: 0.054974\n",
      "Epoch [3159/10000] Avg train loss: 0.054956\n",
      "Epoch [3160/10000] Avg train loss: 0.054939\n",
      "Epoch [3161/10000] Avg train loss: 0.054922\n",
      "Epoch [3162/10000] Avg train loss: 0.054904\n",
      "Epoch [3163/10000] Avg train loss: 0.054887\n",
      "Epoch [3164/10000] Avg train loss: 0.054870\n",
      "Epoch [3165/10000] Avg train loss: 0.054852\n",
      "Epoch [3166/10000] Avg train loss: 0.054835\n",
      "Epoch [3167/10000] Avg train loss: 0.054818\n",
      "Epoch [3168/10000] Avg train loss: 0.054800\n",
      "Epoch [3169/10000] Avg train loss: 0.054783\n",
      "Epoch [3170/10000] Avg train loss: 0.054766\n",
      "Epoch [3171/10000] Avg train loss: 0.054748\n",
      "Epoch [3172/10000] Avg train loss: 0.054731\n",
      "Epoch [3173/10000] Avg train loss: 0.054714\n",
      "Epoch [3174/10000] Avg train loss: 0.054697\n",
      "Epoch [3175/10000] Avg train loss: 0.054679\n",
      "Epoch [3176/10000] Avg train loss: 0.054662\n",
      "Epoch [3177/10000] Avg train loss: 0.054645\n",
      "Epoch [3178/10000] Avg train loss: 0.054628\n",
      "Epoch [3179/10000] Avg train loss: 0.054611\n",
      "Epoch [3180/10000] Avg train loss: 0.054593\n",
      "Epoch [3181/10000] Avg train loss: 0.054576\n",
      "Epoch [3182/10000] Avg train loss: 0.054559\n",
      "Epoch [3183/10000] Avg train loss: 0.054542\n",
      "Epoch [3184/10000] Avg train loss: 0.054525\n",
      "Epoch [3185/10000] Avg train loss: 0.054508\n",
      "Epoch [3186/10000] Avg train loss: 0.054491\n",
      "Epoch [3187/10000] Avg train loss: 0.054474\n",
      "Epoch [3188/10000] Avg train loss: 0.054456\n",
      "Epoch [3189/10000] Avg train loss: 0.054439\n",
      "Epoch [3190/10000] Avg train loss: 0.054422\n",
      "Epoch [3191/10000] Avg train loss: 0.054405\n",
      "Epoch [3192/10000] Avg train loss: 0.054388\n",
      "Epoch [3193/10000] Avg train loss: 0.054371\n",
      "Epoch [3194/10000] Avg train loss: 0.054354\n",
      "Epoch [3195/10000] Avg train loss: 0.054337\n",
      "Epoch [3196/10000] Avg train loss: 0.054320\n",
      "Epoch [3197/10000] Avg train loss: 0.054303\n",
      "Epoch [3198/10000] Avg train loss: 0.054286\n",
      "Epoch [3199/10000] Avg train loss: 0.054269\n",
      "Epoch [3200/10000] Avg train loss: 0.054252\n",
      "Epoch [3201/10000] Avg train loss: 0.054235\n",
      "Epoch [3202/10000] Avg train loss: 0.054218\n",
      "Epoch [3203/10000] Avg train loss: 0.054201\n",
      "Epoch [3204/10000] Avg train loss: 0.054185\n",
      "Epoch [3205/10000] Avg train loss: 0.054168\n",
      "Epoch [3206/10000] Avg train loss: 0.054151\n",
      "Epoch [3207/10000] Avg train loss: 0.054134\n",
      "Epoch [3208/10000] Avg train loss: 0.054117\n",
      "Epoch [3209/10000] Avg train loss: 0.054100\n",
      "Epoch [3210/10000] Avg train loss: 0.054083\n",
      "Epoch [3211/10000] Avg train loss: 0.054066\n",
      "Epoch [3212/10000] Avg train loss: 0.054050\n",
      "Epoch [3213/10000] Avg train loss: 0.054033\n",
      "Epoch [3214/10000] Avg train loss: 0.054016\n",
      "Epoch [3215/10000] Avg train loss: 0.053999\n",
      "Epoch [3216/10000] Avg train loss: 0.053982\n",
      "Epoch [3217/10000] Avg train loss: 0.053966\n",
      "Epoch [3218/10000] Avg train loss: 0.053949\n",
      "Epoch [3219/10000] Avg train loss: 0.053932\n",
      "Epoch [3220/10000] Avg train loss: 0.053915\n",
      "Epoch [3221/10000] Avg train loss: 0.053899\n",
      "Epoch [3222/10000] Avg train loss: 0.053882\n",
      "Epoch [3223/10000] Avg train loss: 0.053865\n",
      "Epoch [3224/10000] Avg train loss: 0.053848\n",
      "Epoch [3225/10000] Avg train loss: 0.053832\n",
      "Epoch [3226/10000] Avg train loss: 0.053815\n",
      "Epoch [3227/10000] Avg train loss: 0.053798\n",
      "Epoch [3228/10000] Avg train loss: 0.053782\n",
      "Epoch [3229/10000] Avg train loss: 0.053765\n",
      "Epoch [3230/10000] Avg train loss: 0.053748\n",
      "Epoch [3231/10000] Avg train loss: 0.053732\n",
      "Epoch [3232/10000] Avg train loss: 0.053715\n",
      "Epoch [3233/10000] Avg train loss: 0.053699\n",
      "Epoch [3234/10000] Avg train loss: 0.053682\n",
      "Epoch [3235/10000] Avg train loss: 0.053665\n",
      "Epoch [3236/10000] Avg train loss: 0.053649\n",
      "Epoch [3237/10000] Avg train loss: 0.053632\n",
      "Epoch [3238/10000] Avg train loss: 0.053616\n",
      "Epoch [3239/10000] Avg train loss: 0.053599\n",
      "Epoch [3240/10000] Avg train loss: 0.053583\n",
      "Epoch [3241/10000] Avg train loss: 0.053566\n",
      "Epoch [3242/10000] Avg train loss: 0.053550\n",
      "Epoch [3243/10000] Avg train loss: 0.053533\n",
      "Epoch [3244/10000] Avg train loss: 0.053517\n",
      "Epoch [3245/10000] Avg train loss: 0.053500\n",
      "Epoch [3246/10000] Avg train loss: 0.053484\n",
      "Epoch [3247/10000] Avg train loss: 0.053467\n",
      "Epoch [3248/10000] Avg train loss: 0.053451\n",
      "Epoch [3249/10000] Avg train loss: 0.053434\n",
      "Epoch [3250/10000] Avg train loss: 0.053418\n",
      "Epoch [3251/10000] Avg train loss: 0.053401\n",
      "Epoch [3252/10000] Avg train loss: 0.053385\n",
      "Epoch [3253/10000] Avg train loss: 0.053369\n",
      "Epoch [3254/10000] Avg train loss: 0.053352\n",
      "Epoch [3255/10000] Avg train loss: 0.053336\n",
      "Epoch [3256/10000] Avg train loss: 0.053319\n",
      "Epoch [3257/10000] Avg train loss: 0.053303\n",
      "Epoch [3258/10000] Avg train loss: 0.053287\n",
      "Epoch [3259/10000] Avg train loss: 0.053270\n",
      "Epoch [3260/10000] Avg train loss: 0.053254\n",
      "Epoch [3261/10000] Avg train loss: 0.053238\n",
      "Epoch [3262/10000] Avg train loss: 0.053221\n",
      "Epoch [3263/10000] Avg train loss: 0.053205\n",
      "Epoch [3264/10000] Avg train loss: 0.053189\n",
      "Epoch [3265/10000] Avg train loss: 0.053172\n",
      "Epoch [3266/10000] Avg train loss: 0.053156\n",
      "Epoch [3267/10000] Avg train loss: 0.053140\n",
      "Epoch [3268/10000] Avg train loss: 0.053124\n",
      "Epoch [3269/10000] Avg train loss: 0.053107\n",
      "Epoch [3270/10000] Avg train loss: 0.053091\n",
      "Epoch [3271/10000] Avg train loss: 0.053075\n",
      "Epoch [3272/10000] Avg train loss: 0.053059\n",
      "Epoch [3273/10000] Avg train loss: 0.053042\n",
      "Epoch [3274/10000] Avg train loss: 0.053026\n",
      "Epoch [3275/10000] Avg train loss: 0.053010\n",
      "Epoch [3276/10000] Avg train loss: 0.052994\n",
      "Epoch [3277/10000] Avg train loss: 0.052978\n",
      "Epoch [3278/10000] Avg train loss: 0.052962\n",
      "Epoch [3279/10000] Avg train loss: 0.052945\n",
      "Epoch [3280/10000] Avg train loss: 0.052929\n",
      "Epoch [3281/10000] Avg train loss: 0.052913\n",
      "Epoch [3282/10000] Avg train loss: 0.052897\n",
      "Epoch [3283/10000] Avg train loss: 0.052881\n",
      "Epoch [3284/10000] Avg train loss: 0.052865\n",
      "Epoch [3285/10000] Avg train loss: 0.052849\n",
      "Epoch [3286/10000] Avg train loss: 0.052833\n",
      "Epoch [3287/10000] Avg train loss: 0.052817\n",
      "Epoch [3288/10000] Avg train loss: 0.052800\n",
      "Epoch [3289/10000] Avg train loss: 0.052784\n",
      "Epoch [3290/10000] Avg train loss: 0.052768\n",
      "Epoch [3291/10000] Avg train loss: 0.052752\n",
      "Epoch [3292/10000] Avg train loss: 0.052736\n",
      "Epoch [3293/10000] Avg train loss: 0.052720\n",
      "Epoch [3294/10000] Avg train loss: 0.052704\n",
      "Epoch [3295/10000] Avg train loss: 0.052688\n",
      "Epoch [3296/10000] Avg train loss: 0.052672\n",
      "Epoch [3297/10000] Avg train loss: 0.052656\n",
      "Epoch [3298/10000] Avg train loss: 0.052640\n",
      "Epoch [3299/10000] Avg train loss: 0.052624\n",
      "Epoch [3300/10000] Avg train loss: 0.052609\n",
      "Epoch [3301/10000] Avg train loss: 0.052593\n",
      "Epoch [3302/10000] Avg train loss: 0.052577\n",
      "Epoch [3303/10000] Avg train loss: 0.052561\n",
      "Epoch [3304/10000] Avg train loss: 0.052545\n",
      "Epoch [3305/10000] Avg train loss: 0.052529\n",
      "Epoch [3306/10000] Avg train loss: 0.052513\n",
      "Epoch [3307/10000] Avg train loss: 0.052497\n",
      "Epoch [3308/10000] Avg train loss: 0.052481\n",
      "Epoch [3309/10000] Avg train loss: 0.052465\n",
      "Epoch [3310/10000] Avg train loss: 0.052450\n",
      "Epoch [3311/10000] Avg train loss: 0.052434\n",
      "Epoch [3312/10000] Avg train loss: 0.052418\n",
      "Epoch [3313/10000] Avg train loss: 0.052402\n",
      "Epoch [3314/10000] Avg train loss: 0.052386\n",
      "Epoch [3315/10000] Avg train loss: 0.052370\n",
      "Epoch [3316/10000] Avg train loss: 0.052355\n",
      "Epoch [3317/10000] Avg train loss: 0.052339\n",
      "Epoch [3318/10000] Avg train loss: 0.052323\n",
      "Epoch [3319/10000] Avg train loss: 0.052307\n",
      "Epoch [3320/10000] Avg train loss: 0.052292\n",
      "Epoch [3321/10000] Avg train loss: 0.052276\n",
      "Epoch [3322/10000] Avg train loss: 0.052260\n",
      "Epoch [3323/10000] Avg train loss: 0.052244\n",
      "Epoch [3324/10000] Avg train loss: 0.052229\n",
      "Epoch [3325/10000] Avg train loss: 0.052213\n",
      "Epoch [3326/10000] Avg train loss: 0.052197\n",
      "Epoch [3327/10000] Avg train loss: 0.052182\n",
      "Epoch [3328/10000] Avg train loss: 0.052166\n",
      "Epoch [3329/10000] Avg train loss: 0.052150\n",
      "Epoch [3330/10000] Avg train loss: 0.052135\n",
      "Epoch [3331/10000] Avg train loss: 0.052119\n",
      "Epoch [3332/10000] Avg train loss: 0.052103\n",
      "Epoch [3333/10000] Avg train loss: 0.052088\n",
      "Epoch [3334/10000] Avg train loss: 0.052072\n",
      "Epoch [3335/10000] Avg train loss: 0.052056\n",
      "Epoch [3336/10000] Avg train loss: 0.052041\n",
      "Epoch [3337/10000] Avg train loss: 0.052025\n",
      "Epoch [3338/10000] Avg train loss: 0.052010\n",
      "Epoch [3339/10000] Avg train loss: 0.051994\n",
      "Epoch [3340/10000] Avg train loss: 0.051979\n",
      "Epoch [3341/10000] Avg train loss: 0.051963\n",
      "Epoch [3342/10000] Avg train loss: 0.051947\n",
      "Epoch [3343/10000] Avg train loss: 0.051932\n",
      "Epoch [3344/10000] Avg train loss: 0.051916\n",
      "Epoch [3345/10000] Avg train loss: 0.051901\n",
      "Epoch [3346/10000] Avg train loss: 0.051885\n",
      "Epoch [3347/10000] Avg train loss: 0.051870\n",
      "Epoch [3348/10000] Avg train loss: 0.051854\n",
      "Epoch [3349/10000] Avg train loss: 0.051839\n",
      "Epoch [3350/10000] Avg train loss: 0.051823\n",
      "Epoch [3351/10000] Avg train loss: 0.051808\n",
      "Epoch [3352/10000] Avg train loss: 0.051792\n",
      "Epoch [3353/10000] Avg train loss: 0.051777\n",
      "Epoch [3354/10000] Avg train loss: 0.051762\n",
      "Epoch [3355/10000] Avg train loss: 0.051746\n",
      "Epoch [3356/10000] Avg train loss: 0.051731\n",
      "Epoch [3357/10000] Avg train loss: 0.051715\n",
      "Epoch [3358/10000] Avg train loss: 0.051700\n",
      "Epoch [3359/10000] Avg train loss: 0.051685\n",
      "Epoch [3360/10000] Avg train loss: 0.051669\n",
      "Epoch [3361/10000] Avg train loss: 0.051654\n",
      "Epoch [3362/10000] Avg train loss: 0.051638\n",
      "Epoch [3363/10000] Avg train loss: 0.051623\n",
      "Epoch [3364/10000] Avg train loss: 0.051608\n",
      "Epoch [3365/10000] Avg train loss: 0.051592\n",
      "Epoch [3366/10000] Avg train loss: 0.051577\n",
      "Epoch [3367/10000] Avg train loss: 0.051562\n",
      "Epoch [3368/10000] Avg train loss: 0.051546\n",
      "Epoch [3369/10000] Avg train loss: 0.051531\n",
      "Epoch [3370/10000] Avg train loss: 0.051516\n",
      "Epoch [3371/10000] Avg train loss: 0.051501\n",
      "Epoch [3372/10000] Avg train loss: 0.051485\n",
      "Epoch [3373/10000] Avg train loss: 0.051470\n",
      "Epoch [3374/10000] Avg train loss: 0.051455\n",
      "Epoch [3375/10000] Avg train loss: 0.051440\n",
      "Epoch [3376/10000] Avg train loss: 0.051424\n",
      "Epoch [3377/10000] Avg train loss: 0.051409\n",
      "Epoch [3378/10000] Avg train loss: 0.051394\n",
      "Epoch [3379/10000] Avg train loss: 0.051379\n",
      "Epoch [3380/10000] Avg train loss: 0.051363\n",
      "Epoch [3381/10000] Avg train loss: 0.051348\n",
      "Epoch [3382/10000] Avg train loss: 0.051333\n",
      "Epoch [3383/10000] Avg train loss: 0.051318\n",
      "Epoch [3384/10000] Avg train loss: 0.051303\n",
      "Epoch [3385/10000] Avg train loss: 0.051288\n",
      "Epoch [3386/10000] Avg train loss: 0.051272\n",
      "Epoch [3387/10000] Avg train loss: 0.051257\n",
      "Epoch [3388/10000] Avg train loss: 0.051242\n",
      "Epoch [3389/10000] Avg train loss: 0.051227\n",
      "Epoch [3390/10000] Avg train loss: 0.051212\n",
      "Epoch [3391/10000] Avg train loss: 0.051197\n",
      "Epoch [3392/10000] Avg train loss: 0.051182\n",
      "Epoch [3393/10000] Avg train loss: 0.051167\n",
      "Epoch [3394/10000] Avg train loss: 0.051152\n",
      "Epoch [3395/10000] Avg train loss: 0.051137\n",
      "Epoch [3396/10000] Avg train loss: 0.051122\n",
      "Epoch [3397/10000] Avg train loss: 0.051106\n",
      "Epoch [3398/10000] Avg train loss: 0.051091\n",
      "Epoch [3399/10000] Avg train loss: 0.051076\n",
      "Epoch [3400/10000] Avg train loss: 0.051061\n",
      "Epoch [3401/10000] Avg train loss: 0.051046\n",
      "Epoch [3402/10000] Avg train loss: 0.051031\n",
      "Epoch [3403/10000] Avg train loss: 0.051016\n",
      "Epoch [3404/10000] Avg train loss: 0.051001\n",
      "Epoch [3405/10000] Avg train loss: 0.050986\n",
      "Epoch [3406/10000] Avg train loss: 0.050971\n",
      "Epoch [3407/10000] Avg train loss: 0.050957\n",
      "Epoch [3408/10000] Avg train loss: 0.050942\n",
      "Epoch [3409/10000] Avg train loss: 0.050927\n",
      "Epoch [3410/10000] Avg train loss: 0.050912\n",
      "Epoch [3411/10000] Avg train loss: 0.050897\n",
      "Epoch [3412/10000] Avg train loss: 0.050882\n",
      "Epoch [3413/10000] Avg train loss: 0.050867\n",
      "Epoch [3414/10000] Avg train loss: 0.050852\n",
      "Epoch [3415/10000] Avg train loss: 0.050837\n",
      "Epoch [3416/10000] Avg train loss: 0.050822\n",
      "Epoch [3417/10000] Avg train loss: 0.050807\n",
      "Epoch [3418/10000] Avg train loss: 0.050793\n",
      "Epoch [3419/10000] Avg train loss: 0.050778\n",
      "Epoch [3420/10000] Avg train loss: 0.050763\n",
      "Epoch [3421/10000] Avg train loss: 0.050748\n",
      "Epoch [3422/10000] Avg train loss: 0.050733\n",
      "Epoch [3423/10000] Avg train loss: 0.050718\n",
      "Epoch [3424/10000] Avg train loss: 0.050704\n",
      "Epoch [3425/10000] Avg train loss: 0.050689\n",
      "Epoch [3426/10000] Avg train loss: 0.050674\n",
      "Epoch [3427/10000] Avg train loss: 0.050659\n",
      "Epoch [3428/10000] Avg train loss: 0.050644\n",
      "Epoch [3429/10000] Avg train loss: 0.050630\n",
      "Epoch [3430/10000] Avg train loss: 0.050615\n",
      "Epoch [3431/10000] Avg train loss: 0.050600\n",
      "Epoch [3432/10000] Avg train loss: 0.050585\n",
      "Epoch [3433/10000] Avg train loss: 0.050571\n",
      "Epoch [3434/10000] Avg train loss: 0.050556\n",
      "Epoch [3435/10000] Avg train loss: 0.050541\n",
      "Epoch [3436/10000] Avg train loss: 0.050526\n",
      "Epoch [3437/10000] Avg train loss: 0.050512\n",
      "Epoch [3438/10000] Avg train loss: 0.050497\n",
      "Epoch [3439/10000] Avg train loss: 0.050482\n",
      "Epoch [3440/10000] Avg train loss: 0.050468\n",
      "Epoch [3441/10000] Avg train loss: 0.050453\n",
      "Epoch [3442/10000] Avg train loss: 0.050438\n",
      "Epoch [3443/10000] Avg train loss: 0.050424\n",
      "Epoch [3444/10000] Avg train loss: 0.050409\n",
      "Epoch [3445/10000] Avg train loss: 0.050394\n",
      "Epoch [3446/10000] Avg train loss: 0.050380\n",
      "Epoch [3447/10000] Avg train loss: 0.050365\n",
      "Epoch [3448/10000] Avg train loss: 0.050351\n",
      "Epoch [3449/10000] Avg train loss: 0.050336\n",
      "Epoch [3450/10000] Avg train loss: 0.050321\n",
      "Epoch [3451/10000] Avg train loss: 0.050307\n",
      "Epoch [3452/10000] Avg train loss: 0.050292\n",
      "Epoch [3453/10000] Avg train loss: 0.050278\n",
      "Epoch [3454/10000] Avg train loss: 0.050263\n",
      "Epoch [3455/10000] Avg train loss: 0.050249\n",
      "Epoch [3456/10000] Avg train loss: 0.050234\n",
      "Epoch [3457/10000] Avg train loss: 0.050220\n",
      "Epoch [3458/10000] Avg train loss: 0.050205\n",
      "Epoch [3459/10000] Avg train loss: 0.050191\n",
      "Epoch [3460/10000] Avg train loss: 0.050176\n",
      "Epoch [3461/10000] Avg train loss: 0.050162\n",
      "Epoch [3462/10000] Avg train loss: 0.050147\n",
      "Epoch [3463/10000] Avg train loss: 0.050133\n",
      "Epoch [3464/10000] Avg train loss: 0.050118\n",
      "Epoch [3465/10000] Avg train loss: 0.050104\n",
      "Epoch [3466/10000] Avg train loss: 0.050089\n",
      "Epoch [3467/10000] Avg train loss: 0.050075\n",
      "Epoch [3468/10000] Avg train loss: 0.050060\n",
      "Epoch [3469/10000] Avg train loss: 0.050046\n",
      "Epoch [3470/10000] Avg train loss: 0.050031\n",
      "Epoch [3471/10000] Avg train loss: 0.050017\n",
      "Epoch [3472/10000] Avg train loss: 0.050003\n",
      "Epoch [3473/10000] Avg train loss: 0.049988\n",
      "Epoch [3474/10000] Avg train loss: 0.049974\n",
      "Epoch [3475/10000] Avg train loss: 0.049959\n",
      "Epoch [3476/10000] Avg train loss: 0.049945\n",
      "Epoch [3477/10000] Avg train loss: 0.049931\n",
      "Epoch [3478/10000] Avg train loss: 0.049916\n",
      "Epoch [3479/10000] Avg train loss: 0.049902\n",
      "Epoch [3480/10000] Avg train loss: 0.049888\n",
      "Epoch [3481/10000] Avg train loss: 0.049873\n",
      "Epoch [3482/10000] Avg train loss: 0.049859\n",
      "Epoch [3483/10000] Avg train loss: 0.049845\n",
      "Epoch [3484/10000] Avg train loss: 0.049830\n",
      "Epoch [3485/10000] Avg train loss: 0.049816\n",
      "Epoch [3486/10000] Avg train loss: 0.049802\n",
      "Epoch [3487/10000] Avg train loss: 0.049788\n",
      "Epoch [3488/10000] Avg train loss: 0.049773\n",
      "Epoch [3489/10000] Avg train loss: 0.049759\n",
      "Epoch [3490/10000] Avg train loss: 0.049745\n",
      "Epoch [3491/10000] Avg train loss: 0.049731\n",
      "Epoch [3492/10000] Avg train loss: 0.049716\n",
      "Epoch [3493/10000] Avg train loss: 0.049702\n",
      "Epoch [3494/10000] Avg train loss: 0.049688\n",
      "Epoch [3495/10000] Avg train loss: 0.049674\n",
      "Epoch [3496/10000] Avg train loss: 0.049659\n",
      "Epoch [3497/10000] Avg train loss: 0.049645\n",
      "Epoch [3498/10000] Avg train loss: 0.049631\n",
      "Epoch [3499/10000] Avg train loss: 0.049617\n",
      "Epoch [3500/10000] Avg train loss: 0.049603\n",
      "Epoch [3501/10000] Avg train loss: 0.049588\n",
      "Epoch [3502/10000] Avg train loss: 0.049574\n",
      "Epoch [3503/10000] Avg train loss: 0.049560\n",
      "Epoch [3504/10000] Avg train loss: 0.049546\n",
      "Epoch [3505/10000] Avg train loss: 0.049532\n",
      "Epoch [3506/10000] Avg train loss: 0.049518\n",
      "Epoch [3507/10000] Avg train loss: 0.049504\n",
      "Epoch [3508/10000] Avg train loss: 0.049490\n",
      "Epoch [3509/10000] Avg train loss: 0.049475\n",
      "Epoch [3510/10000] Avg train loss: 0.049461\n",
      "Epoch [3511/10000] Avg train loss: 0.049447\n",
      "Epoch [3512/10000] Avg train loss: 0.049433\n",
      "Epoch [3513/10000] Avg train loss: 0.049419\n",
      "Epoch [3514/10000] Avg train loss: 0.049405\n",
      "Epoch [3515/10000] Avg train loss: 0.049391\n",
      "Epoch [3516/10000] Avg train loss: 0.049377\n",
      "Epoch [3517/10000] Avg train loss: 0.049363\n",
      "Epoch [3518/10000] Avg train loss: 0.049349\n",
      "Epoch [3519/10000] Avg train loss: 0.049335\n",
      "Epoch [3520/10000] Avg train loss: 0.049321\n",
      "Epoch [3521/10000] Avg train loss: 0.049307\n",
      "Epoch [3522/10000] Avg train loss: 0.049293\n",
      "Epoch [3523/10000] Avg train loss: 0.049279\n",
      "Epoch [3524/10000] Avg train loss: 0.049265\n",
      "Epoch [3525/10000] Avg train loss: 0.049251\n",
      "Epoch [3526/10000] Avg train loss: 0.049237\n",
      "Epoch [3527/10000] Avg train loss: 0.049223\n",
      "Epoch [3528/10000] Avg train loss: 0.049209\n",
      "Epoch [3529/10000] Avg train loss: 0.049195\n",
      "Epoch [3530/10000] Avg train loss: 0.049181\n",
      "Epoch [3531/10000] Avg train loss: 0.049167\n",
      "Epoch [3532/10000] Avg train loss: 0.049153\n",
      "Epoch [3533/10000] Avg train loss: 0.049139\n",
      "Epoch [3534/10000] Avg train loss: 0.049125\n",
      "Epoch [3535/10000] Avg train loss: 0.049112\n",
      "Epoch [3536/10000] Avg train loss: 0.049098\n",
      "Epoch [3537/10000] Avg train loss: 0.049084\n",
      "Epoch [3538/10000] Avg train loss: 0.049070\n",
      "Epoch [3539/10000] Avg train loss: 0.049056\n",
      "Epoch [3540/10000] Avg train loss: 0.049042\n",
      "Epoch [3541/10000] Avg train loss: 0.049028\n",
      "Epoch [3542/10000] Avg train loss: 0.049015\n",
      "Epoch [3543/10000] Avg train loss: 0.049001\n",
      "Epoch [3544/10000] Avg train loss: 0.048987\n",
      "Epoch [3545/10000] Avg train loss: 0.048973\n",
      "Epoch [3546/10000] Avg train loss: 0.048959\n",
      "Epoch [3547/10000] Avg train loss: 0.048945\n",
      "Epoch [3548/10000] Avg train loss: 0.048932\n",
      "Epoch [3549/10000] Avg train loss: 0.048918\n",
      "Epoch [3550/10000] Avg train loss: 0.048904\n",
      "Epoch [3551/10000] Avg train loss: 0.048890\n",
      "Epoch [3552/10000] Avg train loss: 0.048877\n",
      "Epoch [3553/10000] Avg train loss: 0.048863\n",
      "Epoch [3554/10000] Avg train loss: 0.048849\n",
      "Epoch [3555/10000] Avg train loss: 0.048835\n",
      "Epoch [3556/10000] Avg train loss: 0.048822\n",
      "Epoch [3557/10000] Avg train loss: 0.048808\n",
      "Epoch [3558/10000] Avg train loss: 0.048794\n",
      "Epoch [3559/10000] Avg train loss: 0.048780\n",
      "Epoch [3560/10000] Avg train loss: 0.048767\n",
      "Epoch [3561/10000] Avg train loss: 0.048753\n",
      "Epoch [3562/10000] Avg train loss: 0.048739\n",
      "Epoch [3563/10000] Avg train loss: 0.048726\n",
      "Epoch [3564/10000] Avg train loss: 0.048712\n",
      "Epoch [3565/10000] Avg train loss: 0.048698\n",
      "Epoch [3566/10000] Avg train loss: 0.048685\n",
      "Epoch [3567/10000] Avg train loss: 0.048671\n",
      "Epoch [3568/10000] Avg train loss: 0.048657\n",
      "Epoch [3569/10000] Avg train loss: 0.048644\n",
      "Epoch [3570/10000] Avg train loss: 0.048630\n",
      "Epoch [3571/10000] Avg train loss: 0.048617\n",
      "Epoch [3572/10000] Avg train loss: 0.048603\n",
      "Epoch [3573/10000] Avg train loss: 0.048589\n",
      "Epoch [3574/10000] Avg train loss: 0.048576\n",
      "Epoch [3575/10000] Avg train loss: 0.048562\n",
      "Epoch [3576/10000] Avg train loss: 0.048549\n",
      "Epoch [3577/10000] Avg train loss: 0.048535\n",
      "Epoch [3578/10000] Avg train loss: 0.048521\n",
      "Epoch [3579/10000] Avg train loss: 0.048508\n",
      "Epoch [3580/10000] Avg train loss: 0.048494\n",
      "Epoch [3581/10000] Avg train loss: 0.048481\n",
      "Epoch [3582/10000] Avg train loss: 0.048467\n",
      "Epoch [3583/10000] Avg train loss: 0.048454\n",
      "Epoch [3584/10000] Avg train loss: 0.048440\n",
      "Epoch [3585/10000] Avg train loss: 0.048427\n",
      "Epoch [3586/10000] Avg train loss: 0.048413\n",
      "Epoch [3587/10000] Avg train loss: 0.048400\n",
      "Epoch [3588/10000] Avg train loss: 0.048386\n",
      "Epoch [3589/10000] Avg train loss: 0.048373\n",
      "Epoch [3590/10000] Avg train loss: 0.048359\n",
      "Epoch [3591/10000] Avg train loss: 0.048346\n",
      "Epoch [3592/10000] Avg train loss: 0.048332\n",
      "Epoch [3593/10000] Avg train loss: 0.048319\n",
      "Epoch [3594/10000] Avg train loss: 0.048305\n",
      "Epoch [3595/10000] Avg train loss: 0.048292\n",
      "Epoch [3596/10000] Avg train loss: 0.048279\n",
      "Epoch [3597/10000] Avg train loss: 0.048265\n",
      "Epoch [3598/10000] Avg train loss: 0.048252\n",
      "Epoch [3599/10000] Avg train loss: 0.048238\n",
      "Epoch [3600/10000] Avg train loss: 0.048225\n",
      "Epoch [3601/10000] Avg train loss: 0.048212\n",
      "Epoch [3602/10000] Avg train loss: 0.048198\n",
      "Epoch [3603/10000] Avg train loss: 0.048185\n",
      "Epoch [3604/10000] Avg train loss: 0.048171\n",
      "Epoch [3605/10000] Avg train loss: 0.048158\n",
      "Epoch [3606/10000] Avg train loss: 0.048145\n",
      "Epoch [3607/10000] Avg train loss: 0.048131\n",
      "Epoch [3608/10000] Avg train loss: 0.048118\n",
      "Epoch [3609/10000] Avg train loss: 0.048105\n",
      "Epoch [3610/10000] Avg train loss: 0.048091\n",
      "Epoch [3611/10000] Avg train loss: 0.048078\n",
      "Epoch [3612/10000] Avg train loss: 0.048065\n",
      "Epoch [3613/10000] Avg train loss: 0.048051\n",
      "Epoch [3614/10000] Avg train loss: 0.048038\n",
      "Epoch [3615/10000] Avg train loss: 0.048025\n",
      "Epoch [3616/10000] Avg train loss: 0.048012\n",
      "Epoch [3617/10000] Avg train loss: 0.047998\n",
      "Epoch [3618/10000] Avg train loss: 0.047985\n",
      "Epoch [3619/10000] Avg train loss: 0.047972\n",
      "Epoch [3620/10000] Avg train loss: 0.047958\n",
      "Epoch [3621/10000] Avg train loss: 0.047945\n",
      "Epoch [3622/10000] Avg train loss: 0.047932\n",
      "Epoch [3623/10000] Avg train loss: 0.047919\n",
      "Epoch [3624/10000] Avg train loss: 0.047906\n",
      "Epoch [3625/10000] Avg train loss: 0.047892\n",
      "Epoch [3626/10000] Avg train loss: 0.047879\n",
      "Epoch [3627/10000] Avg train loss: 0.047866\n",
      "Epoch [3628/10000] Avg train loss: 0.047853\n",
      "Epoch [3629/10000] Avg train loss: 0.047840\n",
      "Epoch [3630/10000] Avg train loss: 0.047826\n",
      "Epoch [3631/10000] Avg train loss: 0.047813\n",
      "Epoch [3632/10000] Avg train loss: 0.047800\n",
      "Epoch [3633/10000] Avg train loss: 0.047787\n",
      "Epoch [3634/10000] Avg train loss: 0.047774\n",
      "Epoch [3635/10000] Avg train loss: 0.047761\n",
      "Epoch [3636/10000] Avg train loss: 0.047747\n",
      "Epoch [3637/10000] Avg train loss: 0.047734\n",
      "Epoch [3638/10000] Avg train loss: 0.047721\n",
      "Epoch [3639/10000] Avg train loss: 0.047708\n",
      "Epoch [3640/10000] Avg train loss: 0.047695\n",
      "Epoch [3641/10000] Avg train loss: 0.047682\n",
      "Epoch [3642/10000] Avg train loss: 0.047669\n",
      "Epoch [3643/10000] Avg train loss: 0.047656\n",
      "Epoch [3644/10000] Avg train loss: 0.047643\n",
      "Epoch [3645/10000] Avg train loss: 0.047630\n",
      "Epoch [3646/10000] Avg train loss: 0.047617\n",
      "Epoch [3647/10000] Avg train loss: 0.047603\n",
      "Epoch [3648/10000] Avg train loss: 0.047590\n",
      "Epoch [3649/10000] Avg train loss: 0.047577\n",
      "Epoch [3650/10000] Avg train loss: 0.047564\n",
      "Epoch [3651/10000] Avg train loss: 0.047551\n",
      "Epoch [3652/10000] Avg train loss: 0.047538\n",
      "Epoch [3653/10000] Avg train loss: 0.047525\n",
      "Epoch [3654/10000] Avg train loss: 0.047512\n",
      "Epoch [3655/10000] Avg train loss: 0.047499\n",
      "Epoch [3656/10000] Avg train loss: 0.047486\n",
      "Epoch [3657/10000] Avg train loss: 0.047473\n",
      "Epoch [3658/10000] Avg train loss: 0.047460\n",
      "Epoch [3659/10000] Avg train loss: 0.047447\n",
      "Epoch [3660/10000] Avg train loss: 0.047434\n",
      "Epoch [3661/10000] Avg train loss: 0.047421\n",
      "Epoch [3662/10000] Avg train loss: 0.047408\n",
      "Epoch [3663/10000] Avg train loss: 0.047396\n",
      "Epoch [3664/10000] Avg train loss: 0.047383\n",
      "Epoch [3665/10000] Avg train loss: 0.047370\n",
      "Epoch [3666/10000] Avg train loss: 0.047357\n",
      "Epoch [3667/10000] Avg train loss: 0.047344\n",
      "Epoch [3668/10000] Avg train loss: 0.047331\n",
      "Epoch [3669/10000] Avg train loss: 0.047318\n",
      "Epoch [3670/10000] Avg train loss: 0.047305\n",
      "Epoch [3671/10000] Avg train loss: 0.047292\n",
      "Epoch [3672/10000] Avg train loss: 0.047279\n",
      "Epoch [3673/10000] Avg train loss: 0.047267\n",
      "Epoch [3674/10000] Avg train loss: 0.047254\n",
      "Epoch [3675/10000] Avg train loss: 0.047241\n",
      "Epoch [3676/10000] Avg train loss: 0.047228\n",
      "Epoch [3677/10000] Avg train loss: 0.047215\n",
      "Epoch [3678/10000] Avg train loss: 0.047202\n",
      "Epoch [3679/10000] Avg train loss: 0.047189\n",
      "Epoch [3680/10000] Avg train loss: 0.047177\n",
      "Epoch [3681/10000] Avg train loss: 0.047164\n",
      "Epoch [3682/10000] Avg train loss: 0.047151\n",
      "Epoch [3683/10000] Avg train loss: 0.047138\n",
      "Epoch [3684/10000] Avg train loss: 0.047125\n",
      "Epoch [3685/10000] Avg train loss: 0.047113\n",
      "Epoch [3686/10000] Avg train loss: 0.047100\n",
      "Epoch [3687/10000] Avg train loss: 0.047087\n",
      "Epoch [3688/10000] Avg train loss: 0.047074\n",
      "Epoch [3689/10000] Avg train loss: 0.047062\n",
      "Epoch [3690/10000] Avg train loss: 0.047049\n",
      "Epoch [3691/10000] Avg train loss: 0.047036\n",
      "Epoch [3692/10000] Avg train loss: 0.047023\n",
      "Epoch [3693/10000] Avg train loss: 0.047011\n",
      "Epoch [3694/10000] Avg train loss: 0.046998\n",
      "Epoch [3695/10000] Avg train loss: 0.046985\n",
      "Epoch [3696/10000] Avg train loss: 0.046972\n",
      "Epoch [3697/10000] Avg train loss: 0.046960\n",
      "Epoch [3698/10000] Avg train loss: 0.046947\n",
      "Epoch [3699/10000] Avg train loss: 0.046934\n",
      "Epoch [3700/10000] Avg train loss: 0.046922\n",
      "Epoch [3701/10000] Avg train loss: 0.046909\n",
      "Epoch [3702/10000] Avg train loss: 0.046896\n",
      "Epoch [3703/10000] Avg train loss: 0.046884\n",
      "Epoch [3704/10000] Avg train loss: 0.046871\n",
      "Epoch [3705/10000] Avg train loss: 0.046858\n",
      "Epoch [3706/10000] Avg train loss: 0.046846\n",
      "Epoch [3707/10000] Avg train loss: 0.046833\n",
      "Epoch [3708/10000] Avg train loss: 0.046820\n",
      "Epoch [3709/10000] Avg train loss: 0.046808\n",
      "Epoch [3710/10000] Avg train loss: 0.046795\n",
      "Epoch [3711/10000] Avg train loss: 0.046783\n",
      "Epoch [3712/10000] Avg train loss: 0.046770\n",
      "Epoch [3713/10000] Avg train loss: 0.046757\n",
      "Epoch [3714/10000] Avg train loss: 0.046745\n",
      "Epoch [3715/10000] Avg train loss: 0.046732\n",
      "Epoch [3716/10000] Avg train loss: 0.046720\n",
      "Epoch [3717/10000] Avg train loss: 0.046707\n",
      "Epoch [3718/10000] Avg train loss: 0.046694\n",
      "Epoch [3719/10000] Avg train loss: 0.046682\n",
      "Epoch [3720/10000] Avg train loss: 0.046669\n",
      "Epoch [3721/10000] Avg train loss: 0.046657\n",
      "Epoch [3722/10000] Avg train loss: 0.046644\n",
      "Epoch [3723/10000] Avg train loss: 0.046632\n",
      "Epoch [3724/10000] Avg train loss: 0.046619\n",
      "Epoch [3725/10000] Avg train loss: 0.046607\n",
      "Epoch [3726/10000] Avg train loss: 0.046594\n",
      "Epoch [3727/10000] Avg train loss: 0.046582\n",
      "Epoch [3728/10000] Avg train loss: 0.046569\n",
      "Epoch [3729/10000] Avg train loss: 0.046557\n",
      "Epoch [3730/10000] Avg train loss: 0.046544\n",
      "Epoch [3731/10000] Avg train loss: 0.046532\n",
      "Epoch [3732/10000] Avg train loss: 0.046519\n",
      "Epoch [3733/10000] Avg train loss: 0.046507\n",
      "Epoch [3734/10000] Avg train loss: 0.046494\n",
      "Epoch [3735/10000] Avg train loss: 0.046482\n",
      "Epoch [3736/10000] Avg train loss: 0.046469\n",
      "Epoch [3737/10000] Avg train loss: 0.046457\n",
      "Epoch [3738/10000] Avg train loss: 0.046445\n",
      "Epoch [3739/10000] Avg train loss: 0.046432\n",
      "Epoch [3740/10000] Avg train loss: 0.046420\n",
      "Epoch [3741/10000] Avg train loss: 0.046407\n",
      "Epoch [3742/10000] Avg train loss: 0.046395\n",
      "Epoch [3743/10000] Avg train loss: 0.046383\n",
      "Epoch [3744/10000] Avg train loss: 0.046370\n",
      "Epoch [3745/10000] Avg train loss: 0.046358\n",
      "Epoch [3746/10000] Avg train loss: 0.046345\n",
      "Epoch [3747/10000] Avg train loss: 0.046333\n",
      "Epoch [3748/10000] Avg train loss: 0.046321\n",
      "Epoch [3749/10000] Avg train loss: 0.046308\n",
      "Epoch [3750/10000] Avg train loss: 0.046296\n",
      "Epoch [3751/10000] Avg train loss: 0.046284\n",
      "Epoch [3752/10000] Avg train loss: 0.046271\n",
      "Epoch [3753/10000] Avg train loss: 0.046259\n",
      "Epoch [3754/10000] Avg train loss: 0.046247\n",
      "Epoch [3755/10000] Avg train loss: 0.046234\n",
      "Epoch [3756/10000] Avg train loss: 0.046222\n",
      "Epoch [3757/10000] Avg train loss: 0.046210\n",
      "Epoch [3758/10000] Avg train loss: 0.046197\n",
      "Epoch [3759/10000] Avg train loss: 0.046185\n",
      "Epoch [3760/10000] Avg train loss: 0.046173\n",
      "Epoch [3761/10000] Avg train loss: 0.046161\n",
      "Epoch [3762/10000] Avg train loss: 0.046148\n",
      "Epoch [3763/10000] Avg train loss: 0.046136\n",
      "Epoch [3764/10000] Avg train loss: 0.046124\n",
      "Epoch [3765/10000] Avg train loss: 0.046112\n",
      "Epoch [3766/10000] Avg train loss: 0.046099\n",
      "Epoch [3767/10000] Avg train loss: 0.046087\n",
      "Epoch [3768/10000] Avg train loss: 0.046075\n",
      "Epoch [3769/10000] Avg train loss: 0.046063\n",
      "Epoch [3770/10000] Avg train loss: 0.046050\n",
      "Epoch [3771/10000] Avg train loss: 0.046038\n",
      "Epoch [3772/10000] Avg train loss: 0.046026\n",
      "Epoch [3773/10000] Avg train loss: 0.046014\n",
      "Epoch [3774/10000] Avg train loss: 0.046002\n",
      "Epoch [3775/10000] Avg train loss: 0.045989\n",
      "Epoch [3776/10000] Avg train loss: 0.045977\n",
      "Epoch [3777/10000] Avg train loss: 0.045965\n",
      "Epoch [3778/10000] Avg train loss: 0.045953\n",
      "Epoch [3779/10000] Avg train loss: 0.045941\n",
      "Epoch [3780/10000] Avg train loss: 0.045929\n",
      "Epoch [3781/10000] Avg train loss: 0.045916\n",
      "Epoch [3782/10000] Avg train loss: 0.045904\n",
      "Epoch [3783/10000] Avg train loss: 0.045892\n",
      "Epoch [3784/10000] Avg train loss: 0.045880\n",
      "Epoch [3785/10000] Avg train loss: 0.045868\n",
      "Epoch [3786/10000] Avg train loss: 0.045856\n",
      "Epoch [3787/10000] Avg train loss: 0.045844\n",
      "Epoch [3788/10000] Avg train loss: 0.045832\n",
      "Epoch [3789/10000] Avg train loss: 0.045820\n",
      "Epoch [3790/10000] Avg train loss: 0.045807\n",
      "Epoch [3791/10000] Avg train loss: 0.045795\n",
      "Epoch [3792/10000] Avg train loss: 0.045783\n",
      "Epoch [3793/10000] Avg train loss: 0.045771\n",
      "Epoch [3794/10000] Avg train loss: 0.045759\n",
      "Epoch [3795/10000] Avg train loss: 0.045747\n",
      "Epoch [3796/10000] Avg train loss: 0.045735\n",
      "Epoch [3797/10000] Avg train loss: 0.045723\n",
      "Epoch [3798/10000] Avg train loss: 0.045711\n",
      "Epoch [3799/10000] Avg train loss: 0.045699\n",
      "Epoch [3800/10000] Avg train loss: 0.045687\n",
      "Epoch [3801/10000] Avg train loss: 0.045675\n",
      "Epoch [3802/10000] Avg train loss: 0.045663\n",
      "Epoch [3803/10000] Avg train loss: 0.045651\n",
      "Epoch [3804/10000] Avg train loss: 0.045639\n",
      "Epoch [3805/10000] Avg train loss: 0.045627\n",
      "Epoch [3806/10000] Avg train loss: 0.045615\n",
      "Epoch [3807/10000] Avg train loss: 0.045603\n",
      "Epoch [3808/10000] Avg train loss: 0.045591\n",
      "Epoch [3809/10000] Avg train loss: 0.045579\n",
      "Epoch [3810/10000] Avg train loss: 0.045567\n",
      "Epoch [3811/10000] Avg train loss: 0.045555\n",
      "Epoch [3812/10000] Avg train loss: 0.045543\n",
      "Epoch [3813/10000] Avg train loss: 0.045531\n",
      "Epoch [3814/10000] Avg train loss: 0.045519\n",
      "Epoch [3815/10000] Avg train loss: 0.045507\n",
      "Epoch [3816/10000] Avg train loss: 0.045495\n",
      "Epoch [3817/10000] Avg train loss: 0.045483\n",
      "Epoch [3818/10000] Avg train loss: 0.045472\n",
      "Epoch [3819/10000] Avg train loss: 0.045460\n",
      "Epoch [3820/10000] Avg train loss: 0.045448\n",
      "Epoch [3821/10000] Avg train loss: 0.045436\n",
      "Epoch [3822/10000] Avg train loss: 0.045424\n",
      "Epoch [3823/10000] Avg train loss: 0.045412\n",
      "Epoch [3824/10000] Avg train loss: 0.045400\n",
      "Epoch [3825/10000] Avg train loss: 0.045388\n",
      "Epoch [3826/10000] Avg train loss: 0.045376\n",
      "Epoch [3827/10000] Avg train loss: 0.045365\n",
      "Epoch [3828/10000] Avg train loss: 0.045353\n",
      "Epoch [3829/10000] Avg train loss: 0.045341\n",
      "Epoch [3830/10000] Avg train loss: 0.045329\n",
      "Epoch [3831/10000] Avg train loss: 0.045317\n",
      "Epoch [3832/10000] Avg train loss: 0.045305\n",
      "Epoch [3833/10000] Avg train loss: 0.045294\n",
      "Epoch [3834/10000] Avg train loss: 0.045282\n",
      "Epoch [3835/10000] Avg train loss: 0.045270\n",
      "Epoch [3836/10000] Avg train loss: 0.045258\n",
      "Epoch [3837/10000] Avg train loss: 0.045246\n",
      "Epoch [3838/10000] Avg train loss: 0.045235\n",
      "Epoch [3839/10000] Avg train loss: 0.045223\n",
      "Epoch [3840/10000] Avg train loss: 0.045211\n",
      "Epoch [3841/10000] Avg train loss: 0.045199\n",
      "Epoch [3842/10000] Avg train loss: 0.045187\n",
      "Epoch [3843/10000] Avg train loss: 0.045176\n",
      "Epoch [3844/10000] Avg train loss: 0.045164\n",
      "Epoch [3845/10000] Avg train loss: 0.045152\n",
      "Epoch [3846/10000] Avg train loss: 0.045141\n",
      "Epoch [3847/10000] Avg train loss: 0.045129\n",
      "Epoch [3848/10000] Avg train loss: 0.045117\n",
      "Epoch [3849/10000] Avg train loss: 0.045105\n",
      "Epoch [3850/10000] Avg train loss: 0.045094\n",
      "Epoch [3851/10000] Avg train loss: 0.045082\n",
      "Epoch [3852/10000] Avg train loss: 0.045070\n",
      "Epoch [3853/10000] Avg train loss: 0.045058\n",
      "Epoch [3854/10000] Avg train loss: 0.045047\n",
      "Epoch [3855/10000] Avg train loss: 0.045035\n",
      "Epoch [3856/10000] Avg train loss: 0.045023\n",
      "Epoch [3857/10000] Avg train loss: 0.045012\n",
      "Epoch [3858/10000] Avg train loss: 0.045000\n",
      "Epoch [3859/10000] Avg train loss: 0.044988\n",
      "Epoch [3860/10000] Avg train loss: 0.044977\n",
      "Epoch [3861/10000] Avg train loss: 0.044965\n",
      "Epoch [3862/10000] Avg train loss: 0.044953\n",
      "Epoch [3863/10000] Avg train loss: 0.044942\n",
      "Epoch [3864/10000] Avg train loss: 0.044930\n",
      "Epoch [3865/10000] Avg train loss: 0.044919\n",
      "Epoch [3866/10000] Avg train loss: 0.044907\n",
      "Epoch [3867/10000] Avg train loss: 0.044895\n",
      "Epoch [3868/10000] Avg train loss: 0.044884\n",
      "Epoch [3869/10000] Avg train loss: 0.044872\n",
      "Epoch [3870/10000] Avg train loss: 0.044861\n",
      "Epoch [3871/10000] Avg train loss: 0.044849\n",
      "Epoch [3872/10000] Avg train loss: 0.044838\n",
      "Epoch [3873/10000] Avg train loss: 0.044826\n",
      "Epoch [3874/10000] Avg train loss: 0.044814\n",
      "Epoch [3875/10000] Avg train loss: 0.044803\n",
      "Epoch [3876/10000] Avg train loss: 0.044791\n",
      "Epoch [3877/10000] Avg train loss: 0.044780\n",
      "Epoch [3878/10000] Avg train loss: 0.044768\n",
      "Epoch [3879/10000] Avg train loss: 0.044757\n",
      "Epoch [3880/10000] Avg train loss: 0.044745\n",
      "Epoch [3881/10000] Avg train loss: 0.044734\n",
      "Epoch [3882/10000] Avg train loss: 0.044722\n",
      "Epoch [3883/10000] Avg train loss: 0.044711\n",
      "Epoch [3884/10000] Avg train loss: 0.044699\n",
      "Epoch [3885/10000] Avg train loss: 0.044688\n",
      "Epoch [3886/10000] Avg train loss: 0.044676\n",
      "Epoch [3887/10000] Avg train loss: 0.044665\n",
      "Epoch [3888/10000] Avg train loss: 0.044653\n",
      "Epoch [3889/10000] Avg train loss: 0.044642\n",
      "Epoch [3890/10000] Avg train loss: 0.044630\n",
      "Epoch [3891/10000] Avg train loss: 0.044619\n",
      "Epoch [3892/10000] Avg train loss: 0.044607\n",
      "Epoch [3893/10000] Avg train loss: 0.044596\n",
      "Epoch [3894/10000] Avg train loss: 0.044584\n",
      "Epoch [3895/10000] Avg train loss: 0.044573\n",
      "Epoch [3896/10000] Avg train loss: 0.044562\n",
      "Epoch [3897/10000] Avg train loss: 0.044550\n",
      "Epoch [3898/10000] Avg train loss: 0.044539\n",
      "Epoch [3899/10000] Avg train loss: 0.044527\n",
      "Epoch [3900/10000] Avg train loss: 0.044516\n",
      "Epoch [3901/10000] Avg train loss: 0.044504\n",
      "Epoch [3902/10000] Avg train loss: 0.044493\n",
      "Epoch [3903/10000] Avg train loss: 0.044482\n",
      "Epoch [3904/10000] Avg train loss: 0.044470\n",
      "Epoch [3905/10000] Avg train loss: 0.044459\n",
      "Epoch [3906/10000] Avg train loss: 0.044447\n",
      "Epoch [3907/10000] Avg train loss: 0.044436\n",
      "Epoch [3908/10000] Avg train loss: 0.044425\n",
      "Epoch [3909/10000] Avg train loss: 0.044413\n",
      "Epoch [3910/10000] Avg train loss: 0.044402\n",
      "Epoch [3911/10000] Avg train loss: 0.044391\n",
      "Epoch [3912/10000] Avg train loss: 0.044379\n",
      "Epoch [3913/10000] Avg train loss: 0.044368\n",
      "Epoch [3914/10000] Avg train loss: 0.044357\n",
      "Epoch [3915/10000] Avg train loss: 0.044345\n",
      "Epoch [3916/10000] Avg train loss: 0.044334\n",
      "Epoch [3917/10000] Avg train loss: 0.044323\n",
      "Epoch [3918/10000] Avg train loss: 0.044311\n",
      "Epoch [3919/10000] Avg train loss: 0.044300\n",
      "Epoch [3920/10000] Avg train loss: 0.044289\n",
      "Epoch [3921/10000] Avg train loss: 0.044278\n",
      "Epoch [3922/10000] Avg train loss: 0.044266\n",
      "Epoch [3923/10000] Avg train loss: 0.044255\n",
      "Epoch [3924/10000] Avg train loss: 0.044244\n",
      "Epoch [3925/10000] Avg train loss: 0.044233\n",
      "Epoch [3926/10000] Avg train loss: 0.044221\n",
      "Epoch [3927/10000] Avg train loss: 0.044210\n",
      "Epoch [3928/10000] Avg train loss: 0.044199\n",
      "Epoch [3929/10000] Avg train loss: 0.044188\n",
      "Epoch [3930/10000] Avg train loss: 0.044176\n",
      "Epoch [3931/10000] Avg train loss: 0.044165\n",
      "Epoch [3932/10000] Avg train loss: 0.044154\n",
      "Epoch [3933/10000] Avg train loss: 0.044143\n",
      "Epoch [3934/10000] Avg train loss: 0.044131\n",
      "Epoch [3935/10000] Avg train loss: 0.044120\n",
      "Epoch [3936/10000] Avg train loss: 0.044109\n",
      "Epoch [3937/10000] Avg train loss: 0.044098\n",
      "Epoch [3938/10000] Avg train loss: 0.044087\n",
      "Epoch [3939/10000] Avg train loss: 0.044075\n",
      "Epoch [3940/10000] Avg train loss: 0.044064\n",
      "Epoch [3941/10000] Avg train loss: 0.044053\n",
      "Epoch [3942/10000] Avg train loss: 0.044042\n",
      "Epoch [3943/10000] Avg train loss: 0.044031\n",
      "Epoch [3944/10000] Avg train loss: 0.044019\n",
      "Epoch [3945/10000] Avg train loss: 0.044008\n",
      "Epoch [3946/10000] Avg train loss: 0.043997\n",
      "Epoch [3947/10000] Avg train loss: 0.043986\n",
      "Epoch [3948/10000] Avg train loss: 0.043975\n",
      "Epoch [3949/10000] Avg train loss: 0.043964\n",
      "Epoch [3950/10000] Avg train loss: 0.043953\n",
      "Epoch [3951/10000] Avg train loss: 0.043941\n",
      "Epoch [3952/10000] Avg train loss: 0.043930\n",
      "Epoch [3953/10000] Avg train loss: 0.043919\n",
      "Epoch [3954/10000] Avg train loss: 0.043908\n",
      "Epoch [3955/10000] Avg train loss: 0.043897\n",
      "Epoch [3956/10000] Avg train loss: 0.043886\n",
      "Epoch [3957/10000] Avg train loss: 0.043875\n",
      "Epoch [3958/10000] Avg train loss: 0.043864\n",
      "Epoch [3959/10000] Avg train loss: 0.043853\n",
      "Epoch [3960/10000] Avg train loss: 0.043842\n",
      "Epoch [3961/10000] Avg train loss: 0.043831\n",
      "Epoch [3962/10000] Avg train loss: 0.043820\n",
      "Epoch [3963/10000] Avg train loss: 0.043808\n",
      "Epoch [3964/10000] Avg train loss: 0.043797\n",
      "Epoch [3965/10000] Avg train loss: 0.043786\n",
      "Epoch [3966/10000] Avg train loss: 0.043775\n",
      "Epoch [3967/10000] Avg train loss: 0.043764\n",
      "Epoch [3968/10000] Avg train loss: 0.043753\n",
      "Epoch [3969/10000] Avg train loss: 0.043742\n",
      "Epoch [3970/10000] Avg train loss: 0.043731\n",
      "Epoch [3971/10000] Avg train loss: 0.043720\n",
      "Epoch [3972/10000] Avg train loss: 0.043709\n",
      "Epoch [3973/10000] Avg train loss: 0.043698\n",
      "Epoch [3974/10000] Avg train loss: 0.043687\n",
      "Epoch [3975/10000] Avg train loss: 0.043676\n",
      "Epoch [3976/10000] Avg train loss: 0.043665\n",
      "Epoch [3977/10000] Avg train loss: 0.043654\n",
      "Epoch [3978/10000] Avg train loss: 0.043643\n",
      "Epoch [3979/10000] Avg train loss: 0.043632\n",
      "Epoch [3980/10000] Avg train loss: 0.043621\n",
      "Epoch [3981/10000] Avg train loss: 0.043610\n",
      "Epoch [3982/10000] Avg train loss: 0.043599\n",
      "Epoch [3983/10000] Avg train loss: 0.043588\n",
      "Epoch [3984/10000] Avg train loss: 0.043578\n",
      "Epoch [3985/10000] Avg train loss: 0.043567\n",
      "Epoch [3986/10000] Avg train loss: 0.043556\n",
      "Epoch [3987/10000] Avg train loss: 0.043545\n",
      "Epoch [3988/10000] Avg train loss: 0.043534\n",
      "Epoch [3989/10000] Avg train loss: 0.043523\n",
      "Epoch [3990/10000] Avg train loss: 0.043512\n",
      "Epoch [3991/10000] Avg train loss: 0.043501\n",
      "Epoch [3992/10000] Avg train loss: 0.043490\n",
      "Epoch [3993/10000] Avg train loss: 0.043479\n",
      "Epoch [3994/10000] Avg train loss: 0.043468\n",
      "Epoch [3995/10000] Avg train loss: 0.043458\n",
      "Epoch [3996/10000] Avg train loss: 0.043447\n",
      "Epoch [3997/10000] Avg train loss: 0.043436\n",
      "Epoch [3998/10000] Avg train loss: 0.043425\n",
      "Epoch [3999/10000] Avg train loss: 0.043414\n",
      "Epoch [4000/10000] Avg train loss: 0.043403\n",
      "Epoch [4001/10000] Avg train loss: 0.043392\n",
      "Epoch [4002/10000] Avg train loss: 0.043382\n",
      "Epoch [4003/10000] Avg train loss: 0.043371\n",
      "Epoch [4004/10000] Avg train loss: 0.043360\n",
      "Epoch [4005/10000] Avg train loss: 0.043349\n",
      "Epoch [4006/10000] Avg train loss: 0.043338\n",
      "Epoch [4007/10000] Avg train loss: 0.043327\n",
      "Epoch [4008/10000] Avg train loss: 0.043317\n",
      "Epoch [4009/10000] Avg train loss: 0.043306\n",
      "Epoch [4010/10000] Avg train loss: 0.043295\n",
      "Epoch [4011/10000] Avg train loss: 0.043284\n",
      "Epoch [4012/10000] Avg train loss: 0.043273\n",
      "Epoch [4013/10000] Avg train loss: 0.043263\n",
      "Epoch [4014/10000] Avg train loss: 0.043252\n",
      "Epoch [4015/10000] Avg train loss: 0.043241\n",
      "Epoch [4016/10000] Avg train loss: 0.043230\n",
      "Epoch [4017/10000] Avg train loss: 0.043220\n",
      "Epoch [4018/10000] Avg train loss: 0.043209\n",
      "Epoch [4019/10000] Avg train loss: 0.043198\n",
      "Epoch [4020/10000] Avg train loss: 0.043187\n",
      "Epoch [4021/10000] Avg train loss: 0.043177\n",
      "Epoch [4022/10000] Avg train loss: 0.043166\n",
      "Epoch [4023/10000] Avg train loss: 0.043155\n",
      "Epoch [4024/10000] Avg train loss: 0.043144\n",
      "Epoch [4025/10000] Avg train loss: 0.043134\n",
      "Epoch [4026/10000] Avg train loss: 0.043123\n",
      "Epoch [4027/10000] Avg train loss: 0.043112\n",
      "Epoch [4028/10000] Avg train loss: 0.043102\n",
      "Epoch [4029/10000] Avg train loss: 0.043091\n",
      "Epoch [4030/10000] Avg train loss: 0.043080\n",
      "Epoch [4031/10000] Avg train loss: 0.043069\n",
      "Epoch [4032/10000] Avg train loss: 0.043059\n",
      "Epoch [4033/10000] Avg train loss: 0.043048\n",
      "Epoch [4034/10000] Avg train loss: 0.043037\n",
      "Epoch [4035/10000] Avg train loss: 0.043027\n",
      "Epoch [4036/10000] Avg train loss: 0.043016\n",
      "Epoch [4037/10000] Avg train loss: 0.043005\n",
      "Epoch [4038/10000] Avg train loss: 0.042995\n",
      "Epoch [4039/10000] Avg train loss: 0.042984\n",
      "Epoch [4040/10000] Avg train loss: 0.042974\n",
      "Epoch [4041/10000] Avg train loss: 0.042963\n",
      "Epoch [4042/10000] Avg train loss: 0.042952\n",
      "Epoch [4043/10000] Avg train loss: 0.042942\n",
      "Epoch [4044/10000] Avg train loss: 0.042931\n",
      "Epoch [4045/10000] Avg train loss: 0.042920\n",
      "Epoch [4046/10000] Avg train loss: 0.042910\n",
      "Epoch [4047/10000] Avg train loss: 0.042899\n",
      "Epoch [4048/10000] Avg train loss: 0.042889\n",
      "Epoch [4049/10000] Avg train loss: 0.042878\n",
      "Epoch [4050/10000] Avg train loss: 0.042867\n",
      "Epoch [4051/10000] Avg train loss: 0.042857\n",
      "Epoch [4052/10000] Avg train loss: 0.042846\n",
      "Epoch [4053/10000] Avg train loss: 0.042836\n",
      "Epoch [4054/10000] Avg train loss: 0.042825\n",
      "Epoch [4055/10000] Avg train loss: 0.042815\n",
      "Epoch [4056/10000] Avg train loss: 0.042804\n",
      "Epoch [4057/10000] Avg train loss: 0.042793\n",
      "Epoch [4058/10000] Avg train loss: 0.042783\n",
      "Epoch [4059/10000] Avg train loss: 0.042772\n",
      "Epoch [4060/10000] Avg train loss: 0.042762\n",
      "Epoch [4061/10000] Avg train loss: 0.042751\n",
      "Epoch [4062/10000] Avg train loss: 0.042741\n",
      "Epoch [4063/10000] Avg train loss: 0.042730\n",
      "Epoch [4064/10000] Avg train loss: 0.042720\n",
      "Epoch [4065/10000] Avg train loss: 0.042709\n",
      "Epoch [4066/10000] Avg train loss: 0.042699\n",
      "Epoch [4067/10000] Avg train loss: 0.042688\n",
      "Epoch [4068/10000] Avg train loss: 0.042678\n",
      "Epoch [4069/10000] Avg train loss: 0.042667\n",
      "Epoch [4070/10000] Avg train loss: 0.042657\n",
      "Epoch [4071/10000] Avg train loss: 0.042646\n",
      "Epoch [4072/10000] Avg train loss: 0.042636\n",
      "Epoch [4073/10000] Avg train loss: 0.042625\n",
      "Epoch [4074/10000] Avg train loss: 0.042615\n",
      "Epoch [4075/10000] Avg train loss: 0.042604\n",
      "Epoch [4076/10000] Avg train loss: 0.042594\n",
      "Epoch [4077/10000] Avg train loss: 0.042584\n",
      "Epoch [4078/10000] Avg train loss: 0.042573\n",
      "Epoch [4079/10000] Avg train loss: 0.042563\n",
      "Epoch [4080/10000] Avg train loss: 0.042552\n",
      "Epoch [4081/10000] Avg train loss: 0.042542\n",
      "Epoch [4082/10000] Avg train loss: 0.042531\n",
      "Epoch [4083/10000] Avg train loss: 0.042521\n",
      "Epoch [4084/10000] Avg train loss: 0.042511\n",
      "Epoch [4085/10000] Avg train loss: 0.042500\n",
      "Epoch [4086/10000] Avg train loss: 0.042490\n",
      "Epoch [4087/10000] Avg train loss: 0.042479\n",
      "Epoch [4088/10000] Avg train loss: 0.042469\n",
      "Epoch [4089/10000] Avg train loss: 0.042459\n",
      "Epoch [4090/10000] Avg train loss: 0.042448\n",
      "Epoch [4091/10000] Avg train loss: 0.042438\n",
      "Epoch [4092/10000] Avg train loss: 0.042427\n",
      "Epoch [4093/10000] Avg train loss: 0.042417\n",
      "Epoch [4094/10000] Avg train loss: 0.042407\n",
      "Epoch [4095/10000] Avg train loss: 0.042396\n",
      "Epoch [4096/10000] Avg train loss: 0.042386\n",
      "Epoch [4097/10000] Avg train loss: 0.042376\n",
      "Epoch [4098/10000] Avg train loss: 0.042365\n",
      "Epoch [4099/10000] Avg train loss: 0.042355\n",
      "Epoch [4100/10000] Avg train loss: 0.042345\n",
      "Epoch [4101/10000] Avg train loss: 0.042334\n",
      "Epoch [4102/10000] Avg train loss: 0.042324\n",
      "Epoch [4103/10000] Avg train loss: 0.042314\n",
      "Epoch [4104/10000] Avg train loss: 0.042303\n",
      "Epoch [4105/10000] Avg train loss: 0.042293\n",
      "Epoch [4106/10000] Avg train loss: 0.042283\n",
      "Epoch [4107/10000] Avg train loss: 0.042272\n",
      "Epoch [4108/10000] Avg train loss: 0.042262\n",
      "Epoch [4109/10000] Avg train loss: 0.042252\n",
      "Epoch [4110/10000] Avg train loss: 0.042242\n",
      "Epoch [4111/10000] Avg train loss: 0.042231\n",
      "Epoch [4112/10000] Avg train loss: 0.042221\n",
      "Epoch [4113/10000] Avg train loss: 0.042211\n",
      "Epoch [4114/10000] Avg train loss: 0.042201\n",
      "Epoch [4115/10000] Avg train loss: 0.042190\n",
      "Epoch [4116/10000] Avg train loss: 0.042180\n",
      "Epoch [4117/10000] Avg train loss: 0.042170\n",
      "Epoch [4118/10000] Avg train loss: 0.042160\n",
      "Epoch [4119/10000] Avg train loss: 0.042149\n",
      "Epoch [4120/10000] Avg train loss: 0.042139\n",
      "Epoch [4121/10000] Avg train loss: 0.042129\n",
      "Epoch [4122/10000] Avg train loss: 0.042119\n",
      "Epoch [4123/10000] Avg train loss: 0.042108\n",
      "Epoch [4124/10000] Avg train loss: 0.042098\n",
      "Epoch [4125/10000] Avg train loss: 0.042088\n",
      "Epoch [4126/10000] Avg train loss: 0.042078\n",
      "Epoch [4127/10000] Avg train loss: 0.042068\n",
      "Epoch [4128/10000] Avg train loss: 0.042057\n",
      "Epoch [4129/10000] Avg train loss: 0.042047\n",
      "Epoch [4130/10000] Avg train loss: 0.042037\n",
      "Epoch [4131/10000] Avg train loss: 0.042027\n",
      "Epoch [4132/10000] Avg train loss: 0.042017\n",
      "Epoch [4133/10000] Avg train loss: 0.042007\n",
      "Epoch [4134/10000] Avg train loss: 0.041996\n",
      "Epoch [4135/10000] Avg train loss: 0.041986\n",
      "Epoch [4136/10000] Avg train loss: 0.041976\n",
      "Epoch [4137/10000] Avg train loss: 0.041966\n",
      "Epoch [4138/10000] Avg train loss: 0.041956\n",
      "Epoch [4139/10000] Avg train loss: 0.041946\n",
      "Epoch [4140/10000] Avg train loss: 0.041936\n",
      "Epoch [4141/10000] Avg train loss: 0.041925\n",
      "Epoch [4142/10000] Avg train loss: 0.041915\n",
      "Epoch [4143/10000] Avg train loss: 0.041905\n",
      "Epoch [4144/10000] Avg train loss: 0.041895\n",
      "Epoch [4145/10000] Avg train loss: 0.041885\n",
      "Epoch [4146/10000] Avg train loss: 0.041875\n",
      "Epoch [4147/10000] Avg train loss: 0.041865\n",
      "Epoch [4148/10000] Avg train loss: 0.041855\n",
      "Epoch [4149/10000] Avg train loss: 0.041845\n",
      "Epoch [4150/10000] Avg train loss: 0.041834\n",
      "Epoch [4151/10000] Avg train loss: 0.041824\n",
      "Epoch [4152/10000] Avg train loss: 0.041814\n",
      "Epoch [4153/10000] Avg train loss: 0.041804\n",
      "Epoch [4154/10000] Avg train loss: 0.041794\n",
      "Epoch [4155/10000] Avg train loss: 0.041784\n",
      "Epoch [4156/10000] Avg train loss: 0.041774\n",
      "Epoch [4157/10000] Avg train loss: 0.041764\n",
      "Epoch [4158/10000] Avg train loss: 0.041754\n",
      "Epoch [4159/10000] Avg train loss: 0.041744\n",
      "Epoch [4160/10000] Avg train loss: 0.041734\n",
      "Epoch [4161/10000] Avg train loss: 0.041724\n",
      "Epoch [4162/10000] Avg train loss: 0.041714\n",
      "Epoch [4163/10000] Avg train loss: 0.041704\n",
      "Epoch [4164/10000] Avg train loss: 0.041694\n",
      "Epoch [4165/10000] Avg train loss: 0.041684\n",
      "Epoch [4166/10000] Avg train loss: 0.041674\n",
      "Epoch [4167/10000] Avg train loss: 0.041664\n",
      "Epoch [4168/10000] Avg train loss: 0.041654\n",
      "Epoch [4169/10000] Avg train loss: 0.041644\n",
      "Epoch [4170/10000] Avg train loss: 0.041634\n",
      "Epoch [4171/10000] Avg train loss: 0.041624\n",
      "Epoch [4172/10000] Avg train loss: 0.041614\n",
      "Epoch [4173/10000] Avg train loss: 0.041604\n",
      "Epoch [4174/10000] Avg train loss: 0.041594\n",
      "Epoch [4175/10000] Avg train loss: 0.041584\n",
      "Epoch [4176/10000] Avg train loss: 0.041574\n",
      "Epoch [4177/10000] Avg train loss: 0.041564\n",
      "Epoch [4178/10000] Avg train loss: 0.041554\n",
      "Epoch [4179/10000] Avg train loss: 0.041544\n",
      "Epoch [4180/10000] Avg train loss: 0.041534\n",
      "Epoch [4181/10000] Avg train loss: 0.041524\n",
      "Epoch [4182/10000] Avg train loss: 0.041514\n",
      "Epoch [4183/10000] Avg train loss: 0.041504\n",
      "Epoch [4184/10000] Avg train loss: 0.041495\n",
      "Epoch [4185/10000] Avg train loss: 0.041485\n",
      "Epoch [4186/10000] Avg train loss: 0.041475\n",
      "Epoch [4187/10000] Avg train loss: 0.041465\n",
      "Epoch [4188/10000] Avg train loss: 0.041455\n",
      "Epoch [4189/10000] Avg train loss: 0.041445\n",
      "Epoch [4190/10000] Avg train loss: 0.041435\n",
      "Epoch [4191/10000] Avg train loss: 0.041425\n",
      "Epoch [4192/10000] Avg train loss: 0.041415\n",
      "Epoch [4193/10000] Avg train loss: 0.041405\n",
      "Epoch [4194/10000] Avg train loss: 0.041396\n",
      "Epoch [4195/10000] Avg train loss: 0.041386\n",
      "Epoch [4196/10000] Avg train loss: 0.041376\n",
      "Epoch [4197/10000] Avg train loss: 0.041366\n",
      "Epoch [4198/10000] Avg train loss: 0.041356\n",
      "Epoch [4199/10000] Avg train loss: 0.041346\n",
      "Epoch [4200/10000] Avg train loss: 0.041336\n",
      "Epoch [4201/10000] Avg train loss: 0.041327\n",
      "Epoch [4202/10000] Avg train loss: 0.041317\n",
      "Epoch [4203/10000] Avg train loss: 0.041307\n",
      "Epoch [4204/10000] Avg train loss: 0.041297\n",
      "Epoch [4205/10000] Avg train loss: 0.041287\n",
      "Epoch [4206/10000] Avg train loss: 0.041278\n",
      "Epoch [4207/10000] Avg train loss: 0.041268\n",
      "Epoch [4208/10000] Avg train loss: 0.041258\n",
      "Epoch [4209/10000] Avg train loss: 0.041248\n",
      "Epoch [4210/10000] Avg train loss: 0.041238\n",
      "Epoch [4211/10000] Avg train loss: 0.041228\n",
      "Epoch [4212/10000] Avg train loss: 0.041219\n",
      "Epoch [4213/10000] Avg train loss: 0.041209\n",
      "Epoch [4214/10000] Avg train loss: 0.041199\n",
      "Epoch [4215/10000] Avg train loss: 0.041189\n",
      "Epoch [4216/10000] Avg train loss: 0.041180\n",
      "Epoch [4217/10000] Avg train loss: 0.041170\n",
      "Epoch [4218/10000] Avg train loss: 0.041160\n",
      "Epoch [4219/10000] Avg train loss: 0.041150\n",
      "Epoch [4220/10000] Avg train loss: 0.041141\n",
      "Epoch [4221/10000] Avg train loss: 0.041131\n",
      "Epoch [4222/10000] Avg train loss: 0.041121\n",
      "Epoch [4223/10000] Avg train loss: 0.041111\n",
      "Epoch [4224/10000] Avg train loss: 0.041102\n",
      "Epoch [4225/10000] Avg train loss: 0.041092\n",
      "Epoch [4226/10000] Avg train loss: 0.041082\n",
      "Epoch [4227/10000] Avg train loss: 0.041072\n",
      "Epoch [4228/10000] Avg train loss: 0.041063\n",
      "Epoch [4229/10000] Avg train loss: 0.041053\n",
      "Epoch [4230/10000] Avg train loss: 0.041043\n",
      "Epoch [4231/10000] Avg train loss: 0.041034\n",
      "Epoch [4232/10000] Avg train loss: 0.041024\n",
      "Epoch [4233/10000] Avg train loss: 0.041014\n",
      "Epoch [4234/10000] Avg train loss: 0.041005\n",
      "Epoch [4235/10000] Avg train loss: 0.040995\n",
      "Epoch [4236/10000] Avg train loss: 0.040985\n",
      "Epoch [4237/10000] Avg train loss: 0.040976\n",
      "Epoch [4238/10000] Avg train loss: 0.040966\n",
      "Epoch [4239/10000] Avg train loss: 0.040956\n",
      "Epoch [4240/10000] Avg train loss: 0.040947\n",
      "Epoch [4241/10000] Avg train loss: 0.040937\n",
      "Epoch [4242/10000] Avg train loss: 0.040927\n",
      "Epoch [4243/10000] Avg train loss: 0.040918\n",
      "Epoch [4244/10000] Avg train loss: 0.040908\n",
      "Epoch [4245/10000] Avg train loss: 0.040898\n",
      "Epoch [4246/10000] Avg train loss: 0.040889\n",
      "Epoch [4247/10000] Avg train loss: 0.040879\n",
      "Epoch [4248/10000] Avg train loss: 0.040869\n",
      "Epoch [4249/10000] Avg train loss: 0.040860\n",
      "Epoch [4250/10000] Avg train loss: 0.040850\n",
      "Epoch [4251/10000] Avg train loss: 0.040841\n",
      "Epoch [4252/10000] Avg train loss: 0.040831\n",
      "Epoch [4253/10000] Avg train loss: 0.040821\n",
      "Epoch [4254/10000] Avg train loss: 0.040812\n",
      "Epoch [4255/10000] Avg train loss: 0.040802\n",
      "Epoch [4256/10000] Avg train loss: 0.040793\n",
      "Epoch [4257/10000] Avg train loss: 0.040783\n",
      "Epoch [4258/10000] Avg train loss: 0.040773\n",
      "Epoch [4259/10000] Avg train loss: 0.040764\n",
      "Epoch [4260/10000] Avg train loss: 0.040754\n",
      "Epoch [4261/10000] Avg train loss: 0.040745\n",
      "Epoch [4262/10000] Avg train loss: 0.040735\n",
      "Epoch [4263/10000] Avg train loss: 0.040726\n",
      "Epoch [4264/10000] Avg train loss: 0.040716\n",
      "Epoch [4265/10000] Avg train loss: 0.040707\n",
      "Epoch [4266/10000] Avg train loss: 0.040697\n",
      "Epoch [4267/10000] Avg train loss: 0.040687\n",
      "Epoch [4268/10000] Avg train loss: 0.040678\n",
      "Epoch [4269/10000] Avg train loss: 0.040668\n",
      "Epoch [4270/10000] Avg train loss: 0.040659\n",
      "Epoch [4271/10000] Avg train loss: 0.040649\n",
      "Epoch [4272/10000] Avg train loss: 0.040640\n",
      "Epoch [4273/10000] Avg train loss: 0.040630\n",
      "Epoch [4274/10000] Avg train loss: 0.040621\n",
      "Epoch [4275/10000] Avg train loss: 0.040611\n",
      "Epoch [4276/10000] Avg train loss: 0.040602\n",
      "Epoch [4277/10000] Avg train loss: 0.040592\n",
      "Epoch [4278/10000] Avg train loss: 0.040583\n",
      "Epoch [4279/10000] Avg train loss: 0.040573\n",
      "Epoch [4280/10000] Avg train loss: 0.040564\n",
      "Epoch [4281/10000] Avg train loss: 0.040554\n",
      "Epoch [4282/10000] Avg train loss: 0.040545\n",
      "Epoch [4283/10000] Avg train loss: 0.040535\n",
      "Epoch [4284/10000] Avg train loss: 0.040526\n",
      "Epoch [4285/10000] Avg train loss: 0.040517\n",
      "Epoch [4286/10000] Avg train loss: 0.040507\n",
      "Epoch [4287/10000] Avg train loss: 0.040498\n",
      "Epoch [4288/10000] Avg train loss: 0.040488\n",
      "Epoch [4289/10000] Avg train loss: 0.040479\n",
      "Epoch [4290/10000] Avg train loss: 0.040469\n",
      "Epoch [4291/10000] Avg train loss: 0.040460\n",
      "Epoch [4292/10000] Avg train loss: 0.040450\n",
      "Epoch [4293/10000] Avg train loss: 0.040441\n",
      "Epoch [4294/10000] Avg train loss: 0.040432\n",
      "Epoch [4295/10000] Avg train loss: 0.040422\n",
      "Epoch [4296/10000] Avg train loss: 0.040413\n",
      "Epoch [4297/10000] Avg train loss: 0.040403\n",
      "Epoch [4298/10000] Avg train loss: 0.040394\n",
      "Epoch [4299/10000] Avg train loss: 0.040385\n",
      "Epoch [4300/10000] Avg train loss: 0.040375\n",
      "Epoch [4301/10000] Avg train loss: 0.040366\n",
      "Epoch [4302/10000] Avg train loss: 0.040356\n",
      "Epoch [4303/10000] Avg train loss: 0.040347\n",
      "Epoch [4304/10000] Avg train loss: 0.040338\n",
      "Epoch [4305/10000] Avg train loss: 0.040328\n",
      "Epoch [4306/10000] Avg train loss: 0.040319\n",
      "Epoch [4307/10000] Avg train loss: 0.040310\n",
      "Epoch [4308/10000] Avg train loss: 0.040300\n",
      "Epoch [4309/10000] Avg train loss: 0.040291\n",
      "Epoch [4310/10000] Avg train loss: 0.040281\n",
      "Epoch [4311/10000] Avg train loss: 0.040272\n",
      "Epoch [4312/10000] Avg train loss: 0.040263\n",
      "Epoch [4313/10000] Avg train loss: 0.040253\n",
      "Epoch [4314/10000] Avg train loss: 0.040244\n",
      "Epoch [4315/10000] Avg train loss: 0.040235\n",
      "Epoch [4316/10000] Avg train loss: 0.040225\n",
      "Epoch [4317/10000] Avg train loss: 0.040216\n",
      "Epoch [4318/10000] Avg train loss: 0.040207\n",
      "Epoch [4319/10000] Avg train loss: 0.040198\n",
      "Epoch [4320/10000] Avg train loss: 0.040188\n",
      "Epoch [4321/10000] Avg train loss: 0.040179\n",
      "Epoch [4322/10000] Avg train loss: 0.040170\n",
      "Epoch [4323/10000] Avg train loss: 0.040160\n",
      "Epoch [4324/10000] Avg train loss: 0.040151\n",
      "Epoch [4325/10000] Avg train loss: 0.040142\n",
      "Epoch [4326/10000] Avg train loss: 0.040133\n",
      "Epoch [4327/10000] Avg train loss: 0.040123\n",
      "Epoch [4328/10000] Avg train loss: 0.040114\n",
      "Epoch [4329/10000] Avg train loss: 0.040105\n",
      "Epoch [4330/10000] Avg train loss: 0.040095\n",
      "Epoch [4331/10000] Avg train loss: 0.040086\n",
      "Epoch [4332/10000] Avg train loss: 0.040077\n",
      "Epoch [4333/10000] Avg train loss: 0.040068\n",
      "Epoch [4334/10000] Avg train loss: 0.040058\n",
      "Epoch [4335/10000] Avg train loss: 0.040049\n",
      "Epoch [4336/10000] Avg train loss: 0.040040\n",
      "Epoch [4337/10000] Avg train loss: 0.040031\n",
      "Epoch [4338/10000] Avg train loss: 0.040022\n",
      "Epoch [4339/10000] Avg train loss: 0.040012\n",
      "Epoch [4340/10000] Avg train loss: 0.040003\n",
      "Epoch [4341/10000] Avg train loss: 0.039994\n",
      "Epoch [4342/10000] Avg train loss: 0.039985\n",
      "Epoch [4343/10000] Avg train loss: 0.039975\n",
      "Epoch [4344/10000] Avg train loss: 0.039966\n",
      "Epoch [4345/10000] Avg train loss: 0.039957\n",
      "Epoch [4346/10000] Avg train loss: 0.039948\n",
      "Epoch [4347/10000] Avg train loss: 0.039939\n",
      "Epoch [4348/10000] Avg train loss: 0.039929\n",
      "Epoch [4349/10000] Avg train loss: 0.039920\n",
      "Epoch [4350/10000] Avg train loss: 0.039911\n",
      "Epoch [4351/10000] Avg train loss: 0.039902\n",
      "Epoch [4352/10000] Avg train loss: 0.039893\n",
      "Epoch [4353/10000] Avg train loss: 0.039884\n",
      "Epoch [4354/10000] Avg train loss: 0.039874\n",
      "Epoch [4355/10000] Avg train loss: 0.039865\n",
      "Epoch [4356/10000] Avg train loss: 0.039856\n",
      "Epoch [4357/10000] Avg train loss: 0.039847\n",
      "Epoch [4358/10000] Avg train loss: 0.039838\n",
      "Epoch [4359/10000] Avg train loss: 0.039829\n",
      "Epoch [4360/10000] Avg train loss: 0.039820\n",
      "Epoch [4361/10000] Avg train loss: 0.039810\n",
      "Epoch [4362/10000] Avg train loss: 0.039801\n",
      "Epoch [4363/10000] Avg train loss: 0.039792\n",
      "Epoch [4364/10000] Avg train loss: 0.039783\n",
      "Epoch [4365/10000] Avg train loss: 0.039774\n",
      "Epoch [4366/10000] Avg train loss: 0.039765\n",
      "Epoch [4367/10000] Avg train loss: 0.039756\n",
      "Epoch [4368/10000] Avg train loss: 0.039747\n",
      "Epoch [4369/10000] Avg train loss: 0.039738\n",
      "Epoch [4370/10000] Avg train loss: 0.039728\n",
      "Epoch [4371/10000] Avg train loss: 0.039719\n",
      "Epoch [4372/10000] Avg train loss: 0.039710\n",
      "Epoch [4373/10000] Avg train loss: 0.039701\n",
      "Epoch [4374/10000] Avg train loss: 0.039692\n",
      "Epoch [4375/10000] Avg train loss: 0.039683\n",
      "Epoch [4376/10000] Avg train loss: 0.039674\n",
      "Epoch [4377/10000] Avg train loss: 0.039665\n",
      "Epoch [4378/10000] Avg train loss: 0.039656\n",
      "Epoch [4379/10000] Avg train loss: 0.039647\n",
      "Epoch [4380/10000] Avg train loss: 0.039638\n",
      "Epoch [4381/10000] Avg train loss: 0.039629\n",
      "Epoch [4382/10000] Avg train loss: 0.039620\n",
      "Epoch [4383/10000] Avg train loss: 0.039611\n",
      "Epoch [4384/10000] Avg train loss: 0.039602\n",
      "Epoch [4385/10000] Avg train loss: 0.039593\n",
      "Epoch [4386/10000] Avg train loss: 0.039584\n",
      "Epoch [4387/10000] Avg train loss: 0.039574\n",
      "Epoch [4388/10000] Avg train loss: 0.039565\n",
      "Epoch [4389/10000] Avg train loss: 0.039556\n",
      "Epoch [4390/10000] Avg train loss: 0.039547\n",
      "Epoch [4391/10000] Avg train loss: 0.039538\n",
      "Epoch [4392/10000] Avg train loss: 0.039529\n",
      "Epoch [4393/10000] Avg train loss: 0.039520\n",
      "Epoch [4394/10000] Avg train loss: 0.039511\n",
      "Epoch [4395/10000] Avg train loss: 0.039502\n",
      "Epoch [4396/10000] Avg train loss: 0.039493\n",
      "Epoch [4397/10000] Avg train loss: 0.039484\n",
      "Epoch [4398/10000] Avg train loss: 0.039476\n",
      "Epoch [4399/10000] Avg train loss: 0.039467\n",
      "Epoch [4400/10000] Avg train loss: 0.039458\n",
      "Epoch [4401/10000] Avg train loss: 0.039449\n",
      "Epoch [4402/10000] Avg train loss: 0.039440\n",
      "Epoch [4403/10000] Avg train loss: 0.039431\n",
      "Epoch [4404/10000] Avg train loss: 0.039422\n",
      "Epoch [4405/10000] Avg train loss: 0.039413\n",
      "Epoch [4406/10000] Avg train loss: 0.039404\n",
      "Epoch [4407/10000] Avg train loss: 0.039395\n",
      "Epoch [4408/10000] Avg train loss: 0.039386\n",
      "Epoch [4409/10000] Avg train loss: 0.039377\n",
      "Epoch [4410/10000] Avg train loss: 0.039368\n",
      "Epoch [4411/10000] Avg train loss: 0.039359\n",
      "Epoch [4412/10000] Avg train loss: 0.039350\n",
      "Epoch [4413/10000] Avg train loss: 0.039341\n",
      "Epoch [4414/10000] Avg train loss: 0.039332\n",
      "Epoch [4415/10000] Avg train loss: 0.039324\n",
      "Epoch [4416/10000] Avg train loss: 0.039315\n",
      "Epoch [4417/10000] Avg train loss: 0.039306\n",
      "Epoch [4418/10000] Avg train loss: 0.039297\n",
      "Epoch [4419/10000] Avg train loss: 0.039288\n",
      "Epoch [4420/10000] Avg train loss: 0.039279\n",
      "Epoch [4421/10000] Avg train loss: 0.039270\n",
      "Epoch [4422/10000] Avg train loss: 0.039261\n",
      "Epoch [4423/10000] Avg train loss: 0.039252\n",
      "Epoch [4424/10000] Avg train loss: 0.039244\n",
      "Epoch [4425/10000] Avg train loss: 0.039235\n",
      "Epoch [4426/10000] Avg train loss: 0.039226\n",
      "Epoch [4427/10000] Avg train loss: 0.039217\n",
      "Epoch [4428/10000] Avg train loss: 0.039208\n",
      "Epoch [4429/10000] Avg train loss: 0.039199\n",
      "Epoch [4430/10000] Avg train loss: 0.039190\n",
      "Epoch [4431/10000] Avg train loss: 0.039182\n",
      "Epoch [4432/10000] Avg train loss: 0.039173\n",
      "Epoch [4433/10000] Avg train loss: 0.039164\n",
      "Epoch [4434/10000] Avg train loss: 0.039155\n",
      "Epoch [4435/10000] Avg train loss: 0.039146\n",
      "Epoch [4436/10000] Avg train loss: 0.039137\n",
      "Epoch [4437/10000] Avg train loss: 0.039129\n",
      "Epoch [4438/10000] Avg train loss: 0.039120\n",
      "Epoch [4439/10000] Avg train loss: 0.039111\n",
      "Epoch [4440/10000] Avg train loss: 0.039102\n",
      "Epoch [4441/10000] Avg train loss: 0.039093\n",
      "Epoch [4442/10000] Avg train loss: 0.039084\n",
      "Epoch [4443/10000] Avg train loss: 0.039076\n",
      "Epoch [4444/10000] Avg train loss: 0.039067\n",
      "Epoch [4445/10000] Avg train loss: 0.039058\n",
      "Epoch [4446/10000] Avg train loss: 0.039049\n",
      "Epoch [4447/10000] Avg train loss: 0.039041\n",
      "Epoch [4448/10000] Avg train loss: 0.039032\n",
      "Epoch [4449/10000] Avg train loss: 0.039023\n",
      "Epoch [4450/10000] Avg train loss: 0.039014\n",
      "Epoch [4451/10000] Avg train loss: 0.039005\n",
      "Epoch [4452/10000] Avg train loss: 0.038997\n",
      "Epoch [4453/10000] Avg train loss: 0.038988\n",
      "Epoch [4454/10000] Avg train loss: 0.038979\n",
      "Epoch [4455/10000] Avg train loss: 0.038970\n",
      "Epoch [4456/10000] Avg train loss: 0.038962\n",
      "Epoch [4457/10000] Avg train loss: 0.038953\n",
      "Epoch [4458/10000] Avg train loss: 0.038944\n",
      "Epoch [4459/10000] Avg train loss: 0.038935\n",
      "Epoch [4460/10000] Avg train loss: 0.038927\n",
      "Epoch [4461/10000] Avg train loss: 0.038918\n",
      "Epoch [4462/10000] Avg train loss: 0.038909\n",
      "Epoch [4463/10000] Avg train loss: 0.038901\n",
      "Epoch [4464/10000] Avg train loss: 0.038892\n",
      "Epoch [4465/10000] Avg train loss: 0.038883\n",
      "Epoch [4466/10000] Avg train loss: 0.038874\n",
      "Epoch [4467/10000] Avg train loss: 0.038866\n",
      "Epoch [4468/10000] Avg train loss: 0.038857\n",
      "Epoch [4469/10000] Avg train loss: 0.038848\n",
      "Epoch [4470/10000] Avg train loss: 0.038840\n",
      "Epoch [4471/10000] Avg train loss: 0.038831\n",
      "Epoch [4472/10000] Avg train loss: 0.038822\n",
      "Epoch [4473/10000] Avg train loss: 0.038814\n",
      "Epoch [4474/10000] Avg train loss: 0.038805\n",
      "Epoch [4475/10000] Avg train loss: 0.038796\n",
      "Epoch [4476/10000] Avg train loss: 0.038788\n",
      "Epoch [4477/10000] Avg train loss: 0.038779\n",
      "Epoch [4478/10000] Avg train loss: 0.038770\n",
      "Epoch [4479/10000] Avg train loss: 0.038762\n",
      "Epoch [4480/10000] Avg train loss: 0.038753\n",
      "Epoch [4481/10000] Avg train loss: 0.038744\n",
      "Epoch [4482/10000] Avg train loss: 0.038736\n",
      "Epoch [4483/10000] Avg train loss: 0.038727\n",
      "Epoch [4484/10000] Avg train loss: 0.038718\n",
      "Epoch [4485/10000] Avg train loss: 0.038710\n",
      "Epoch [4486/10000] Avg train loss: 0.038701\n",
      "Epoch [4487/10000] Avg train loss: 0.038693\n",
      "Epoch [4488/10000] Avg train loss: 0.038684\n",
      "Epoch [4489/10000] Avg train loss: 0.038675\n",
      "Epoch [4490/10000] Avg train loss: 0.038667\n",
      "Epoch [4491/10000] Avg train loss: 0.038658\n",
      "Epoch [4492/10000] Avg train loss: 0.038649\n",
      "Epoch [4493/10000] Avg train loss: 0.038641\n",
      "Epoch [4494/10000] Avg train loss: 0.038632\n",
      "Epoch [4495/10000] Avg train loss: 0.038624\n",
      "Epoch [4496/10000] Avg train loss: 0.038615\n",
      "Epoch [4497/10000] Avg train loss: 0.038606\n",
      "Epoch [4498/10000] Avg train loss: 0.038598\n",
      "Epoch [4499/10000] Avg train loss: 0.038589\n",
      "Epoch [4500/10000] Avg train loss: 0.038581\n",
      "Epoch [4501/10000] Avg train loss: 0.038572\n",
      "Epoch [4502/10000] Avg train loss: 0.038564\n",
      "Epoch [4503/10000] Avg train loss: 0.038555\n",
      "Epoch [4504/10000] Avg train loss: 0.038546\n",
      "Epoch [4505/10000] Avg train loss: 0.038538\n",
      "Epoch [4506/10000] Avg train loss: 0.038529\n",
      "Epoch [4507/10000] Avg train loss: 0.038521\n",
      "Epoch [4508/10000] Avg train loss: 0.038512\n",
      "Epoch [4509/10000] Avg train loss: 0.038504\n",
      "Epoch [4510/10000] Avg train loss: 0.038495\n",
      "Epoch [4511/10000] Avg train loss: 0.038487\n",
      "Epoch [4512/10000] Avg train loss: 0.038478\n",
      "Epoch [4513/10000] Avg train loss: 0.038470\n",
      "Epoch [4514/10000] Avg train loss: 0.038461\n",
      "Epoch [4515/10000] Avg train loss: 0.038453\n",
      "Epoch [4516/10000] Avg train loss: 0.038444\n",
      "Epoch [4517/10000] Avg train loss: 0.038436\n",
      "Epoch [4518/10000] Avg train loss: 0.038427\n",
      "Epoch [4519/10000] Avg train loss: 0.038419\n",
      "Epoch [4520/10000] Avg train loss: 0.038410\n",
      "Epoch [4521/10000] Avg train loss: 0.038402\n",
      "Epoch [4522/10000] Avg train loss: 0.038393\n",
      "Epoch [4523/10000] Avg train loss: 0.038385\n",
      "Epoch [4524/10000] Avg train loss: 0.038376\n",
      "Epoch [4525/10000] Avg train loss: 0.038368\n",
      "Epoch [4526/10000] Avg train loss: 0.038359\n",
      "Epoch [4527/10000] Avg train loss: 0.038351\n",
      "Epoch [4528/10000] Avg train loss: 0.038342\n",
      "Epoch [4529/10000] Avg train loss: 0.038334\n",
      "Epoch [4530/10000] Avg train loss: 0.038325\n",
      "Epoch [4531/10000] Avg train loss: 0.038317\n",
      "Epoch [4532/10000] Avg train loss: 0.038308\n",
      "Epoch [4533/10000] Avg train loss: 0.038300\n",
      "Epoch [4534/10000] Avg train loss: 0.038291\n",
      "Epoch [4535/10000] Avg train loss: 0.038283\n",
      "Epoch [4536/10000] Avg train loss: 0.038275\n",
      "Epoch [4537/10000] Avg train loss: 0.038266\n",
      "Epoch [4538/10000] Avg train loss: 0.038258\n",
      "Epoch [4539/10000] Avg train loss: 0.038249\n",
      "Epoch [4540/10000] Avg train loss: 0.038241\n",
      "Epoch [4541/10000] Avg train loss: 0.038232\n",
      "Epoch [4542/10000] Avg train loss: 0.038224\n",
      "Epoch [4543/10000] Avg train loss: 0.038216\n",
      "Epoch [4544/10000] Avg train loss: 0.038207\n",
      "Epoch [4545/10000] Avg train loss: 0.038199\n",
      "Epoch [4546/10000] Avg train loss: 0.038190\n",
      "Epoch [4547/10000] Avg train loss: 0.038182\n",
      "Epoch [4548/10000] Avg train loss: 0.038174\n",
      "Epoch [4549/10000] Avg train loss: 0.038165\n",
      "Epoch [4550/10000] Avg train loss: 0.038157\n",
      "Epoch [4551/10000] Avg train loss: 0.038148\n",
      "Epoch [4552/10000] Avg train loss: 0.038140\n",
      "Epoch [4553/10000] Avg train loss: 0.038132\n",
      "Epoch [4554/10000] Avg train loss: 0.038123\n",
      "Epoch [4555/10000] Avg train loss: 0.038115\n",
      "Epoch [4556/10000] Avg train loss: 0.038107\n",
      "Epoch [4557/10000] Avg train loss: 0.038098\n",
      "Epoch [4558/10000] Avg train loss: 0.038090\n",
      "Epoch [4559/10000] Avg train loss: 0.038081\n",
      "Epoch [4560/10000] Avg train loss: 0.038073\n",
      "Epoch [4561/10000] Avg train loss: 0.038065\n",
      "Epoch [4562/10000] Avg train loss: 0.038056\n",
      "Epoch [4563/10000] Avg train loss: 0.038048\n",
      "Epoch [4564/10000] Avg train loss: 0.038040\n",
      "Epoch [4565/10000] Avg train loss: 0.038031\n",
      "Epoch [4566/10000] Avg train loss: 0.038023\n",
      "Epoch [4567/10000] Avg train loss: 0.038015\n",
      "Epoch [4568/10000] Avg train loss: 0.038006\n",
      "Epoch [4569/10000] Avg train loss: 0.037998\n",
      "Epoch [4570/10000] Avg train loss: 0.037990\n",
      "Epoch [4571/10000] Avg train loss: 0.037981\n",
      "Epoch [4572/10000] Avg train loss: 0.037973\n",
      "Epoch [4573/10000] Avg train loss: 0.037965\n",
      "Epoch [4574/10000] Avg train loss: 0.037957\n",
      "Epoch [4575/10000] Avg train loss: 0.037948\n",
      "Epoch [4576/10000] Avg train loss: 0.037940\n",
      "Epoch [4577/10000] Avg train loss: 0.037932\n",
      "Epoch [4578/10000] Avg train loss: 0.037923\n",
      "Epoch [4579/10000] Avg train loss: 0.037915\n",
      "Epoch [4580/10000] Avg train loss: 0.037907\n",
      "Epoch [4581/10000] Avg train loss: 0.037899\n",
      "Epoch [4582/10000] Avg train loss: 0.037890\n",
      "Epoch [4583/10000] Avg train loss: 0.037882\n",
      "Epoch [4584/10000] Avg train loss: 0.037874\n",
      "Epoch [4585/10000] Avg train loss: 0.037866\n",
      "Epoch [4586/10000] Avg train loss: 0.037857\n",
      "Epoch [4587/10000] Avg train loss: 0.037849\n",
      "Epoch [4588/10000] Avg train loss: 0.037841\n",
      "Epoch [4589/10000] Avg train loss: 0.037832\n",
      "Epoch [4590/10000] Avg train loss: 0.037824\n",
      "Epoch [4591/10000] Avg train loss: 0.037816\n",
      "Epoch [4592/10000] Avg train loss: 0.037808\n",
      "Epoch [4593/10000] Avg train loss: 0.037800\n",
      "Epoch [4594/10000] Avg train loss: 0.037791\n",
      "Epoch [4595/10000] Avg train loss: 0.037783\n",
      "Epoch [4596/10000] Avg train loss: 0.037775\n",
      "Epoch [4597/10000] Avg train loss: 0.037767\n",
      "Epoch [4598/10000] Avg train loss: 0.037758\n",
      "Epoch [4599/10000] Avg train loss: 0.037750\n",
      "Epoch [4600/10000] Avg train loss: 0.037742\n",
      "Epoch [4601/10000] Avg train loss: 0.037734\n",
      "Epoch [4602/10000] Avg train loss: 0.037726\n",
      "Epoch [4603/10000] Avg train loss: 0.037717\n",
      "Epoch [4604/10000] Avg train loss: 0.037709\n",
      "Epoch [4605/10000] Avg train loss: 0.037701\n",
      "Epoch [4606/10000] Avg train loss: 0.037693\n",
      "Epoch [4607/10000] Avg train loss: 0.037685\n",
      "Epoch [4608/10000] Avg train loss: 0.037677\n",
      "Epoch [4609/10000] Avg train loss: 0.037668\n",
      "Epoch [4610/10000] Avg train loss: 0.037660\n",
      "Epoch [4611/10000] Avg train loss: 0.037652\n",
      "Epoch [4612/10000] Avg train loss: 0.037644\n",
      "Epoch [4613/10000] Avg train loss: 0.037636\n",
      "Epoch [4614/10000] Avg train loss: 0.037628\n",
      "Epoch [4615/10000] Avg train loss: 0.037619\n",
      "Epoch [4616/10000] Avg train loss: 0.037611\n",
      "Epoch [4617/10000] Avg train loss: 0.037603\n",
      "Epoch [4618/10000] Avg train loss: 0.037595\n",
      "Epoch [4619/10000] Avg train loss: 0.037587\n",
      "Epoch [4620/10000] Avg train loss: 0.037579\n",
      "Epoch [4621/10000] Avg train loss: 0.037571\n",
      "Epoch [4622/10000] Avg train loss: 0.037562\n",
      "Epoch [4623/10000] Avg train loss: 0.037554\n",
      "Epoch [4624/10000] Avg train loss: 0.037546\n",
      "Epoch [4625/10000] Avg train loss: 0.037538\n",
      "Epoch [4626/10000] Avg train loss: 0.037530\n",
      "Epoch [4627/10000] Avg train loss: 0.037522\n",
      "Epoch [4628/10000] Avg train loss: 0.037514\n",
      "Epoch [4629/10000] Avg train loss: 0.037506\n",
      "Epoch [4630/10000] Avg train loss: 0.037497\n",
      "Epoch [4631/10000] Avg train loss: 0.037489\n",
      "Epoch [4632/10000] Avg train loss: 0.037481\n",
      "Epoch [4633/10000] Avg train loss: 0.037473\n",
      "Epoch [4634/10000] Avg train loss: 0.037465\n",
      "Epoch [4635/10000] Avg train loss: 0.037457\n",
      "Epoch [4636/10000] Avg train loss: 0.037449\n",
      "Epoch [4637/10000] Avg train loss: 0.037441\n",
      "Epoch [4638/10000] Avg train loss: 0.037433\n",
      "Epoch [4639/10000] Avg train loss: 0.037425\n",
      "Epoch [4640/10000] Avg train loss: 0.037417\n",
      "Epoch [4641/10000] Avg train loss: 0.037409\n",
      "Epoch [4642/10000] Avg train loss: 0.037401\n",
      "Epoch [4643/10000] Avg train loss: 0.037392\n",
      "Epoch [4644/10000] Avg train loss: 0.037384\n",
      "Epoch [4645/10000] Avg train loss: 0.037376\n",
      "Epoch [4646/10000] Avg train loss: 0.037368\n",
      "Epoch [4647/10000] Avg train loss: 0.037360\n",
      "Epoch [4648/10000] Avg train loss: 0.037352\n",
      "Epoch [4649/10000] Avg train loss: 0.037344\n",
      "Epoch [4650/10000] Avg train loss: 0.037336\n",
      "Epoch [4651/10000] Avg train loss: 0.037328\n",
      "Epoch [4652/10000] Avg train loss: 0.037320\n",
      "Epoch [4653/10000] Avg train loss: 0.037312\n",
      "Epoch [4654/10000] Avg train loss: 0.037304\n",
      "Epoch [4655/10000] Avg train loss: 0.037296\n",
      "Epoch [4656/10000] Avg train loss: 0.037288\n",
      "Epoch [4657/10000] Avg train loss: 0.037280\n",
      "Epoch [4658/10000] Avg train loss: 0.037272\n",
      "Epoch [4659/10000] Avg train loss: 0.037264\n",
      "Epoch [4660/10000] Avg train loss: 0.037256\n",
      "Epoch [4661/10000] Avg train loss: 0.037248\n",
      "Epoch [4662/10000] Avg train loss: 0.037240\n",
      "Epoch [4663/10000] Avg train loss: 0.037232\n",
      "Epoch [4664/10000] Avg train loss: 0.037224\n",
      "Epoch [4665/10000] Avg train loss: 0.037216\n",
      "Epoch [4666/10000] Avg train loss: 0.037208\n",
      "Epoch [4667/10000] Avg train loss: 0.037200\n",
      "Epoch [4668/10000] Avg train loss: 0.037192\n",
      "Epoch [4669/10000] Avg train loss: 0.037184\n",
      "Epoch [4670/10000] Avg train loss: 0.037176\n",
      "Epoch [4671/10000] Avg train loss: 0.037168\n",
      "Epoch [4672/10000] Avg train loss: 0.037160\n",
      "Epoch [4673/10000] Avg train loss: 0.037152\n",
      "Epoch [4674/10000] Avg train loss: 0.037144\n",
      "Epoch [4675/10000] Avg train loss: 0.037137\n",
      "Epoch [4676/10000] Avg train loss: 0.037129\n",
      "Epoch [4677/10000] Avg train loss: 0.037121\n",
      "Epoch [4678/10000] Avg train loss: 0.037113\n",
      "Epoch [4679/10000] Avg train loss: 0.037105\n",
      "Epoch [4680/10000] Avg train loss: 0.037097\n",
      "Epoch [4681/10000] Avg train loss: 0.037089\n",
      "Epoch [4682/10000] Avg train loss: 0.037081\n",
      "Epoch [4683/10000] Avg train loss: 0.037073\n",
      "Epoch [4684/10000] Avg train loss: 0.037065\n",
      "Epoch [4685/10000] Avg train loss: 0.037057\n",
      "Epoch [4686/10000] Avg train loss: 0.037049\n",
      "Epoch [4687/10000] Avg train loss: 0.037041\n",
      "Epoch [4688/10000] Avg train loss: 0.037034\n",
      "Epoch [4689/10000] Avg train loss: 0.037026\n",
      "Epoch [4690/10000] Avg train loss: 0.037018\n",
      "Epoch [4691/10000] Avg train loss: 0.037010\n",
      "Epoch [4692/10000] Avg train loss: 0.037002\n",
      "Epoch [4693/10000] Avg train loss: 0.036994\n",
      "Epoch [4694/10000] Avg train loss: 0.036986\n",
      "Epoch [4695/10000] Avg train loss: 0.036978\n",
      "Epoch [4696/10000] Avg train loss: 0.036970\n",
      "Epoch [4697/10000] Avg train loss: 0.036963\n",
      "Epoch [4698/10000] Avg train loss: 0.036955\n",
      "Epoch [4699/10000] Avg train loss: 0.036947\n",
      "Epoch [4700/10000] Avg train loss: 0.036939\n",
      "Epoch [4701/10000] Avg train loss: 0.036931\n",
      "Epoch [4702/10000] Avg train loss: 0.036923\n",
      "Epoch [4703/10000] Avg train loss: 0.036915\n",
      "Epoch [4704/10000] Avg train loss: 0.036908\n",
      "Epoch [4705/10000] Avg train loss: 0.036900\n",
      "Epoch [4706/10000] Avg train loss: 0.036892\n",
      "Epoch [4707/10000] Avg train loss: 0.036884\n",
      "Epoch [4708/10000] Avg train loss: 0.036876\n",
      "Epoch [4709/10000] Avg train loss: 0.036868\n",
      "Epoch [4710/10000] Avg train loss: 0.036861\n",
      "Epoch [4711/10000] Avg train loss: 0.036853\n",
      "Epoch [4712/10000] Avg train loss: 0.036845\n",
      "Epoch [4713/10000] Avg train loss: 0.036837\n",
      "Epoch [4714/10000] Avg train loss: 0.036829\n",
      "Epoch [4715/10000] Avg train loss: 0.036821\n",
      "Epoch [4716/10000] Avg train loss: 0.036814\n",
      "Epoch [4717/10000] Avg train loss: 0.036806\n",
      "Epoch [4718/10000] Avg train loss: 0.036798\n",
      "Epoch [4719/10000] Avg train loss: 0.036790\n",
      "Epoch [4720/10000] Avg train loss: 0.036782\n",
      "Epoch [4721/10000] Avg train loss: 0.036775\n",
      "Epoch [4722/10000] Avg train loss: 0.036767\n",
      "Epoch [4723/10000] Avg train loss: 0.036759\n",
      "Epoch [4724/10000] Avg train loss: 0.036751\n",
      "Epoch [4725/10000] Avg train loss: 0.036744\n",
      "Epoch [4726/10000] Avg train loss: 0.036736\n",
      "Epoch [4727/10000] Avg train loss: 0.036728\n",
      "Epoch [4728/10000] Avg train loss: 0.036720\n",
      "Epoch [4729/10000] Avg train loss: 0.036712\n",
      "Epoch [4730/10000] Avg train loss: 0.036705\n",
      "Epoch [4731/10000] Avg train loss: 0.036697\n",
      "Epoch [4732/10000] Avg train loss: 0.036689\n",
      "Epoch [4733/10000] Avg train loss: 0.036681\n",
      "Epoch [4734/10000] Avg train loss: 0.036674\n",
      "Epoch [4735/10000] Avg train loss: 0.036666\n",
      "Epoch [4736/10000] Avg train loss: 0.036658\n",
      "Epoch [4737/10000] Avg train loss: 0.036650\n",
      "Epoch [4738/10000] Avg train loss: 0.036643\n",
      "Epoch [4739/10000] Avg train loss: 0.036635\n",
      "Epoch [4740/10000] Avg train loss: 0.036627\n",
      "Epoch [4741/10000] Avg train loss: 0.036620\n",
      "Epoch [4742/10000] Avg train loss: 0.036612\n",
      "Epoch [4743/10000] Avg train loss: 0.036604\n",
      "Epoch [4744/10000] Avg train loss: 0.036596\n",
      "Epoch [4745/10000] Avg train loss: 0.036589\n",
      "Epoch [4746/10000] Avg train loss: 0.036581\n",
      "Epoch [4747/10000] Avg train loss: 0.036573\n",
      "Epoch [4748/10000] Avg train loss: 0.036566\n",
      "Epoch [4749/10000] Avg train loss: 0.036558\n",
      "Epoch [4750/10000] Avg train loss: 0.036550\n",
      "Epoch [4751/10000] Avg train loss: 0.036542\n",
      "Epoch [4752/10000] Avg train loss: 0.036535\n",
      "Epoch [4753/10000] Avg train loss: 0.036527\n",
      "Epoch [4754/10000] Avg train loss: 0.036519\n",
      "Epoch [4755/10000] Avg train loss: 0.036512\n",
      "Epoch [4756/10000] Avg train loss: 0.036504\n",
      "Epoch [4757/10000] Avg train loss: 0.036496\n",
      "Epoch [4758/10000] Avg train loss: 0.036489\n",
      "Epoch [4759/10000] Avg train loss: 0.036481\n",
      "Epoch [4760/10000] Avg train loss: 0.036473\n",
      "Epoch [4761/10000] Avg train loss: 0.036466\n",
      "Epoch [4762/10000] Avg train loss: 0.036458\n",
      "Epoch [4763/10000] Avg train loss: 0.036450\n",
      "Epoch [4764/10000] Avg train loss: 0.036443\n",
      "Epoch [4765/10000] Avg train loss: 0.036435\n",
      "Epoch [4766/10000] Avg train loss: 0.036427\n",
      "Epoch [4767/10000] Avg train loss: 0.036420\n",
      "Epoch [4768/10000] Avg train loss: 0.036412\n",
      "Epoch [4769/10000] Avg train loss: 0.036405\n",
      "Epoch [4770/10000] Avg train loss: 0.036397\n",
      "Epoch [4771/10000] Avg train loss: 0.036389\n",
      "Epoch [4772/10000] Avg train loss: 0.036382\n",
      "Epoch [4773/10000] Avg train loss: 0.036374\n",
      "Epoch [4774/10000] Avg train loss: 0.036366\n",
      "Epoch [4775/10000] Avg train loss: 0.036359\n",
      "Epoch [4776/10000] Avg train loss: 0.036351\n",
      "Epoch [4777/10000] Avg train loss: 0.036344\n",
      "Epoch [4778/10000] Avg train loss: 0.036336\n",
      "Epoch [4779/10000] Avg train loss: 0.036328\n",
      "Epoch [4780/10000] Avg train loss: 0.036321\n",
      "Epoch [4781/10000] Avg train loss: 0.036313\n",
      "Epoch [4782/10000] Avg train loss: 0.036306\n",
      "Epoch [4783/10000] Avg train loss: 0.036298\n",
      "Epoch [4784/10000] Avg train loss: 0.036290\n",
      "Epoch [4785/10000] Avg train loss: 0.036283\n",
      "Epoch [4786/10000] Avg train loss: 0.036275\n",
      "Epoch [4787/10000] Avg train loss: 0.036268\n",
      "Epoch [4788/10000] Avg train loss: 0.036260\n",
      "Epoch [4789/10000] Avg train loss: 0.036253\n",
      "Epoch [4790/10000] Avg train loss: 0.036245\n",
      "Epoch [4791/10000] Avg train loss: 0.036237\n",
      "Epoch [4792/10000] Avg train loss: 0.036230\n",
      "Epoch [4793/10000] Avg train loss: 0.036222\n",
      "Epoch [4794/10000] Avg train loss: 0.036215\n",
      "Epoch [4795/10000] Avg train loss: 0.036207\n",
      "Epoch [4796/10000] Avg train loss: 0.036200\n",
      "Epoch [4797/10000] Avg train loss: 0.036192\n",
      "Epoch [4798/10000] Avg train loss: 0.036185\n",
      "Epoch [4799/10000] Avg train loss: 0.036177\n",
      "Epoch [4800/10000] Avg train loss: 0.036169\n",
      "Epoch [4801/10000] Avg train loss: 0.036162\n",
      "Epoch [4802/10000] Avg train loss: 0.036154\n",
      "Epoch [4803/10000] Avg train loss: 0.036147\n",
      "Epoch [4804/10000] Avg train loss: 0.036139\n",
      "Epoch [4805/10000] Avg train loss: 0.036132\n",
      "Epoch [4806/10000] Avg train loss: 0.036124\n",
      "Epoch [4807/10000] Avg train loss: 0.036117\n",
      "Epoch [4808/10000] Avg train loss: 0.036109\n",
      "Epoch [4809/10000] Avg train loss: 0.036102\n",
      "Epoch [4810/10000] Avg train loss: 0.036094\n",
      "Epoch [4811/10000] Avg train loss: 0.036087\n",
      "Epoch [4812/10000] Avg train loss: 0.036079\n",
      "Epoch [4813/10000] Avg train loss: 0.036072\n",
      "Epoch [4814/10000] Avg train loss: 0.036064\n",
      "Epoch [4815/10000] Avg train loss: 0.036057\n",
      "Epoch [4816/10000] Avg train loss: 0.036049\n",
      "Epoch [4817/10000] Avg train loss: 0.036042\n",
      "Epoch [4818/10000] Avg train loss: 0.036034\n",
      "Epoch [4819/10000] Avg train loss: 0.036027\n",
      "Epoch [4820/10000] Avg train loss: 0.036019\n",
      "Epoch [4821/10000] Avg train loss: 0.036012\n",
      "Epoch [4822/10000] Avg train loss: 0.036004\n",
      "Epoch [4823/10000] Avg train loss: 0.035997\n",
      "Epoch [4824/10000] Avg train loss: 0.035990\n",
      "Epoch [4825/10000] Avg train loss: 0.035982\n",
      "Epoch [4826/10000] Avg train loss: 0.035975\n",
      "Epoch [4827/10000] Avg train loss: 0.035967\n",
      "Epoch [4828/10000] Avg train loss: 0.035960\n",
      "Epoch [4829/10000] Avg train loss: 0.035952\n",
      "Epoch [4830/10000] Avg train loss: 0.035945\n",
      "Epoch [4831/10000] Avg train loss: 0.035937\n",
      "Epoch [4832/10000] Avg train loss: 0.035930\n",
      "Epoch [4833/10000] Avg train loss: 0.035922\n",
      "Epoch [4834/10000] Avg train loss: 0.035915\n",
      "Epoch [4835/10000] Avg train loss: 0.035908\n",
      "Epoch [4836/10000] Avg train loss: 0.035900\n",
      "Epoch [4837/10000] Avg train loss: 0.035893\n",
      "Epoch [4838/10000] Avg train loss: 0.035885\n",
      "Epoch [4839/10000] Avg train loss: 0.035878\n",
      "Epoch [4840/10000] Avg train loss: 0.035871\n",
      "Epoch [4841/10000] Avg train loss: 0.035863\n",
      "Epoch [4842/10000] Avg train loss: 0.035856\n",
      "Epoch [4843/10000] Avg train loss: 0.035848\n",
      "Epoch [4844/10000] Avg train loss: 0.035841\n",
      "Epoch [4845/10000] Avg train loss: 0.035834\n",
      "Epoch [4846/10000] Avg train loss: 0.035826\n",
      "Epoch [4847/10000] Avg train loss: 0.035819\n",
      "Epoch [4848/10000] Avg train loss: 0.035811\n",
      "Epoch [4849/10000] Avg train loss: 0.035804\n",
      "Epoch [4850/10000] Avg train loss: 0.035797\n",
      "Epoch [4851/10000] Avg train loss: 0.035789\n",
      "Epoch [4852/10000] Avg train loss: 0.035782\n",
      "Epoch [4853/10000] Avg train loss: 0.035774\n",
      "Epoch [4854/10000] Avg train loss: 0.035767\n",
      "Epoch [4855/10000] Avg train loss: 0.035760\n",
      "Epoch [4856/10000] Avg train loss: 0.035752\n",
      "Epoch [4857/10000] Avg train loss: 0.035745\n",
      "Epoch [4858/10000] Avg train loss: 0.035738\n",
      "Epoch [4859/10000] Avg train loss: 0.035730\n",
      "Epoch [4860/10000] Avg train loss: 0.035723\n",
      "Epoch [4861/10000] Avg train loss: 0.035716\n",
      "Epoch [4862/10000] Avg train loss: 0.035708\n",
      "Epoch [4863/10000] Avg train loss: 0.035701\n",
      "Epoch [4864/10000] Avg train loss: 0.035694\n",
      "Epoch [4865/10000] Avg train loss: 0.035686\n",
      "Epoch [4866/10000] Avg train loss: 0.035679\n",
      "Epoch [4867/10000] Avg train loss: 0.035672\n",
      "Epoch [4868/10000] Avg train loss: 0.035664\n",
      "Epoch [4869/10000] Avg train loss: 0.035657\n",
      "Epoch [4870/10000] Avg train loss: 0.035650\n",
      "Epoch [4871/10000] Avg train loss: 0.035642\n",
      "Epoch [4872/10000] Avg train loss: 0.035635\n",
      "Epoch [4873/10000] Avg train loss: 0.035628\n",
      "Epoch [4874/10000] Avg train loss: 0.035620\n",
      "Epoch [4875/10000] Avg train loss: 0.035613\n",
      "Epoch [4876/10000] Avg train loss: 0.035606\n",
      "Epoch [4877/10000] Avg train loss: 0.035598\n",
      "Epoch [4878/10000] Avg train loss: 0.035591\n",
      "Epoch [4879/10000] Avg train loss: 0.035584\n",
      "Epoch [4880/10000] Avg train loss: 0.035577\n",
      "Epoch [4881/10000] Avg train loss: 0.035569\n",
      "Epoch [4882/10000] Avg train loss: 0.035562\n",
      "Epoch [4883/10000] Avg train loss: 0.035555\n",
      "Epoch [4884/10000] Avg train loss: 0.035547\n",
      "Epoch [4885/10000] Avg train loss: 0.035540\n",
      "Epoch [4886/10000] Avg train loss: 0.035533\n",
      "Epoch [4887/10000] Avg train loss: 0.035526\n",
      "Epoch [4888/10000] Avg train loss: 0.035518\n",
      "Epoch [4889/10000] Avg train loss: 0.035511\n",
      "Epoch [4890/10000] Avg train loss: 0.035504\n",
      "Epoch [4891/10000] Avg train loss: 0.035497\n",
      "Epoch [4892/10000] Avg train loss: 0.035489\n",
      "Epoch [4893/10000] Avg train loss: 0.035482\n",
      "Epoch [4894/10000] Avg train loss: 0.035475\n",
      "Epoch [4895/10000] Avg train loss: 0.035467\n",
      "Epoch [4896/10000] Avg train loss: 0.035460\n",
      "Epoch [4897/10000] Avg train loss: 0.035453\n",
      "Epoch [4898/10000] Avg train loss: 0.035446\n",
      "Epoch [4899/10000] Avg train loss: 0.035439\n",
      "Epoch [4900/10000] Avg train loss: 0.035431\n",
      "Epoch [4901/10000] Avg train loss: 0.035424\n",
      "Epoch [4902/10000] Avg train loss: 0.035417\n",
      "Epoch [4903/10000] Avg train loss: 0.035410\n",
      "Epoch [4904/10000] Avg train loss: 0.035402\n",
      "Epoch [4905/10000] Avg train loss: 0.035395\n",
      "Epoch [4906/10000] Avg train loss: 0.035388\n",
      "Epoch [4907/10000] Avg train loss: 0.035381\n",
      "Epoch [4908/10000] Avg train loss: 0.035374\n",
      "Epoch [4909/10000] Avg train loss: 0.035366\n",
      "Epoch [4910/10000] Avg train loss: 0.035359\n",
      "Epoch [4911/10000] Avg train loss: 0.035352\n",
      "Epoch [4912/10000] Avg train loss: 0.035345\n",
      "Epoch [4913/10000] Avg train loss: 0.035338\n",
      "Epoch [4914/10000] Avg train loss: 0.035330\n",
      "Epoch [4915/10000] Avg train loss: 0.035323\n",
      "Epoch [4916/10000] Avg train loss: 0.035316\n",
      "Epoch [4917/10000] Avg train loss: 0.035309\n",
      "Epoch [4918/10000] Avg train loss: 0.035302\n",
      "Epoch [4919/10000] Avg train loss: 0.035294\n",
      "Epoch [4920/10000] Avg train loss: 0.035287\n",
      "Epoch [4921/10000] Avg train loss: 0.035280\n",
      "Epoch [4922/10000] Avg train loss: 0.035273\n",
      "Epoch [4923/10000] Avg train loss: 0.035266\n",
      "Epoch [4924/10000] Avg train loss: 0.035259\n",
      "Epoch [4925/10000] Avg train loss: 0.035251\n",
      "Epoch [4926/10000] Avg train loss: 0.035244\n",
      "Epoch [4927/10000] Avg train loss: 0.035237\n",
      "Epoch [4928/10000] Avg train loss: 0.035230\n",
      "Epoch [4929/10000] Avg train loss: 0.035223\n",
      "Epoch [4930/10000] Avg train loss: 0.035216\n",
      "Epoch [4931/10000] Avg train loss: 0.035209\n",
      "Epoch [4932/10000] Avg train loss: 0.035201\n",
      "Epoch [4933/10000] Avg train loss: 0.035194\n",
      "Epoch [4934/10000] Avg train loss: 0.035187\n",
      "Epoch [4935/10000] Avg train loss: 0.035180\n",
      "Epoch [4936/10000] Avg train loss: 0.035173\n",
      "Epoch [4937/10000] Avg train loss: 0.035166\n",
      "Epoch [4938/10000] Avg train loss: 0.035159\n",
      "Epoch [4939/10000] Avg train loss: 0.035152\n",
      "Epoch [4940/10000] Avg train loss: 0.035144\n",
      "Epoch [4941/10000] Avg train loss: 0.035137\n",
      "Epoch [4942/10000] Avg train loss: 0.035130\n",
      "Epoch [4943/10000] Avg train loss: 0.035123\n",
      "Epoch [4944/10000] Avg train loss: 0.035116\n",
      "Epoch [4945/10000] Avg train loss: 0.035109\n",
      "Epoch [4946/10000] Avg train loss: 0.035102\n",
      "Epoch [4947/10000] Avg train loss: 0.035095\n",
      "Epoch [4948/10000] Avg train loss: 0.035088\n",
      "Epoch [4949/10000] Avg train loss: 0.035080\n",
      "Epoch [4950/10000] Avg train loss: 0.035073\n",
      "Epoch [4951/10000] Avg train loss: 0.035066\n",
      "Epoch [4952/10000] Avg train loss: 0.035059\n",
      "Epoch [4953/10000] Avg train loss: 0.035052\n",
      "Epoch [4954/10000] Avg train loss: 0.035045\n",
      "Epoch [4955/10000] Avg train loss: 0.035038\n",
      "Epoch [4956/10000] Avg train loss: 0.035031\n",
      "Epoch [4957/10000] Avg train loss: 0.035024\n",
      "Epoch [4958/10000] Avg train loss: 0.035017\n",
      "Epoch [4959/10000] Avg train loss: 0.035010\n",
      "Epoch [4960/10000] Avg train loss: 0.035003\n",
      "Epoch [4961/10000] Avg train loss: 0.034996\n",
      "Epoch [4962/10000] Avg train loss: 0.034989\n",
      "Epoch [4963/10000] Avg train loss: 0.034982\n",
      "Epoch [4964/10000] Avg train loss: 0.034974\n",
      "Epoch [4965/10000] Avg train loss: 0.034967\n",
      "Epoch [4966/10000] Avg train loss: 0.034960\n",
      "Epoch [4967/10000] Avg train loss: 0.034953\n",
      "Epoch [4968/10000] Avg train loss: 0.034946\n",
      "Epoch [4969/10000] Avg train loss: 0.034939\n",
      "Epoch [4970/10000] Avg train loss: 0.034932\n",
      "Epoch [4971/10000] Avg train loss: 0.034925\n",
      "Epoch [4972/10000] Avg train loss: 0.034918\n",
      "Epoch [4973/10000] Avg train loss: 0.034911\n",
      "Epoch [4974/10000] Avg train loss: 0.034904\n",
      "Epoch [4975/10000] Avg train loss: 0.034897\n",
      "Epoch [4976/10000] Avg train loss: 0.034890\n",
      "Epoch [4977/10000] Avg train loss: 0.034883\n",
      "Epoch [4978/10000] Avg train loss: 0.034876\n",
      "Epoch [4979/10000] Avg train loss: 0.034869\n",
      "Epoch [4980/10000] Avg train loss: 0.034862\n",
      "Epoch [4981/10000] Avg train loss: 0.034855\n",
      "Epoch [4982/10000] Avg train loss: 0.034848\n",
      "Epoch [4983/10000] Avg train loss: 0.034841\n",
      "Epoch [4984/10000] Avg train loss: 0.034834\n",
      "Epoch [4985/10000] Avg train loss: 0.034827\n",
      "Epoch [4986/10000] Avg train loss: 0.034820\n",
      "Epoch [4987/10000] Avg train loss: 0.034813\n",
      "Epoch [4988/10000] Avg train loss: 0.034806\n",
      "Epoch [4989/10000] Avg train loss: 0.034799\n",
      "Epoch [4990/10000] Avg train loss: 0.034792\n",
      "Epoch [4991/10000] Avg train loss: 0.034785\n",
      "Epoch [4992/10000] Avg train loss: 0.034778\n",
      "Epoch [4993/10000] Avg train loss: 0.034771\n",
      "Epoch [4994/10000] Avg train loss: 0.034764\n",
      "Epoch [4995/10000] Avg train loss: 0.034757\n",
      "Epoch [4996/10000] Avg train loss: 0.034750\n",
      "Epoch [4997/10000] Avg train loss: 0.034744\n",
      "Epoch [4998/10000] Avg train loss: 0.034737\n",
      "Epoch [4999/10000] Avg train loss: 0.034730\n",
      "Epoch [5000/10000] Avg train loss: 0.034723\n",
      "Epoch [5001/10000] Avg train loss: 0.034716\n",
      "Epoch [5002/10000] Avg train loss: 0.034709\n",
      "Epoch [5003/10000] Avg train loss: 0.034702\n",
      "Epoch [5004/10000] Avg train loss: 0.034695\n",
      "Epoch [5005/10000] Avg train loss: 0.034688\n",
      "Epoch [5006/10000] Avg train loss: 0.034681\n",
      "Epoch [5007/10000] Avg train loss: 0.034674\n",
      "Epoch [5008/10000] Avg train loss: 0.034667\n",
      "Epoch [5009/10000] Avg train loss: 0.034660\n",
      "Epoch [5010/10000] Avg train loss: 0.034653\n",
      "Epoch [5011/10000] Avg train loss: 0.034646\n",
      "Epoch [5012/10000] Avg train loss: 0.034640\n",
      "Epoch [5013/10000] Avg train loss: 0.034633\n",
      "Epoch [5014/10000] Avg train loss: 0.034626\n",
      "Epoch [5015/10000] Avg train loss: 0.034619\n",
      "Epoch [5016/10000] Avg train loss: 0.034612\n",
      "Epoch [5017/10000] Avg train loss: 0.034605\n",
      "Epoch [5018/10000] Avg train loss: 0.034598\n",
      "Epoch [5019/10000] Avg train loss: 0.034591\n",
      "Epoch [5020/10000] Avg train loss: 0.034584\n",
      "Epoch [5021/10000] Avg train loss: 0.034577\n",
      "Epoch [5022/10000] Avg train loss: 0.034571\n",
      "Epoch [5023/10000] Avg train loss: 0.034564\n",
      "Epoch [5024/10000] Avg train loss: 0.034557\n",
      "Epoch [5025/10000] Avg train loss: 0.034550\n",
      "Epoch [5026/10000] Avg train loss: 0.034543\n",
      "Epoch [5027/10000] Avg train loss: 0.034536\n",
      "Epoch [5028/10000] Avg train loss: 0.034529\n",
      "Epoch [5029/10000] Avg train loss: 0.034522\n",
      "Epoch [5030/10000] Avg train loss: 0.034516\n",
      "Epoch [5031/10000] Avg train loss: 0.034509\n",
      "Epoch [5032/10000] Avg train loss: 0.034502\n",
      "Epoch [5033/10000] Avg train loss: 0.034495\n",
      "Epoch [5034/10000] Avg train loss: 0.034488\n",
      "Epoch [5035/10000] Avg train loss: 0.034481\n",
      "Epoch [5036/10000] Avg train loss: 0.034474\n",
      "Epoch [5037/10000] Avg train loss: 0.034468\n",
      "Epoch [5038/10000] Avg train loss: 0.034461\n",
      "Epoch [5039/10000] Avg train loss: 0.034454\n",
      "Epoch [5040/10000] Avg train loss: 0.034447\n",
      "Epoch [5041/10000] Avg train loss: 0.034440\n",
      "Epoch [5042/10000] Avg train loss: 0.034433\n",
      "Epoch [5043/10000] Avg train loss: 0.034427\n",
      "Epoch [5044/10000] Avg train loss: 0.034420\n",
      "Epoch [5045/10000] Avg train loss: 0.034413\n",
      "Epoch [5046/10000] Avg train loss: 0.034406\n",
      "Epoch [5047/10000] Avg train loss: 0.034399\n",
      "Epoch [5048/10000] Avg train loss: 0.034393\n",
      "Epoch [5049/10000] Avg train loss: 0.034386\n",
      "Epoch [5050/10000] Avg train loss: 0.034379\n",
      "Epoch [5051/10000] Avg train loss: 0.034372\n",
      "Epoch [5052/10000] Avg train loss: 0.034365\n",
      "Epoch [5053/10000] Avg train loss: 0.034358\n",
      "Epoch [5054/10000] Avg train loss: 0.034352\n",
      "Epoch [5055/10000] Avg train loss: 0.034345\n",
      "Epoch [5056/10000] Avg train loss: 0.034338\n",
      "Epoch [5057/10000] Avg train loss: 0.034331\n",
      "Epoch [5058/10000] Avg train loss: 0.034325\n",
      "Epoch [5059/10000] Avg train loss: 0.034318\n",
      "Epoch [5060/10000] Avg train loss: 0.034311\n",
      "Epoch [5061/10000] Avg train loss: 0.034304\n",
      "Epoch [5062/10000] Avg train loss: 0.034297\n",
      "Epoch [5063/10000] Avg train loss: 0.034291\n",
      "Epoch [5064/10000] Avg train loss: 0.034284\n",
      "Epoch [5065/10000] Avg train loss: 0.034277\n",
      "Epoch [5066/10000] Avg train loss: 0.034270\n",
      "Epoch [5067/10000] Avg train loss: 0.034264\n",
      "Epoch [5068/10000] Avg train loss: 0.034257\n",
      "Epoch [5069/10000] Avg train loss: 0.034250\n",
      "Epoch [5070/10000] Avg train loss: 0.034243\n",
      "Epoch [5071/10000] Avg train loss: 0.034237\n",
      "Epoch [5072/10000] Avg train loss: 0.034230\n",
      "Epoch [5073/10000] Avg train loss: 0.034223\n",
      "Epoch [5074/10000] Avg train loss: 0.034216\n",
      "Epoch [5075/10000] Avg train loss: 0.034210\n",
      "Epoch [5076/10000] Avg train loss: 0.034203\n",
      "Epoch [5077/10000] Avg train loss: 0.034196\n",
      "Epoch [5078/10000] Avg train loss: 0.034189\n",
      "Epoch [5079/10000] Avg train loss: 0.034183\n",
      "Epoch [5080/10000] Avg train loss: 0.034176\n",
      "Epoch [5081/10000] Avg train loss: 0.034169\n",
      "Epoch [5082/10000] Avg train loss: 0.034162\n",
      "Epoch [5083/10000] Avg train loss: 0.034156\n",
      "Epoch [5084/10000] Avg train loss: 0.034149\n",
      "Epoch [5085/10000] Avg train loss: 0.034142\n",
      "Epoch [5086/10000] Avg train loss: 0.034136\n",
      "Epoch [5087/10000] Avg train loss: 0.034129\n",
      "Epoch [5088/10000] Avg train loss: 0.034122\n",
      "Epoch [5089/10000] Avg train loss: 0.034115\n",
      "Epoch [5090/10000] Avg train loss: 0.034109\n",
      "Epoch [5091/10000] Avg train loss: 0.034102\n",
      "Epoch [5092/10000] Avg train loss: 0.034095\n",
      "Epoch [5093/10000] Avg train loss: 0.034089\n",
      "Epoch [5094/10000] Avg train loss: 0.034082\n",
      "Epoch [5095/10000] Avg train loss: 0.034075\n",
      "Epoch [5096/10000] Avg train loss: 0.034069\n",
      "Epoch [5097/10000] Avg train loss: 0.034062\n",
      "Epoch [5098/10000] Avg train loss: 0.034055\n",
      "Epoch [5099/10000] Avg train loss: 0.034049\n",
      "Epoch [5100/10000] Avg train loss: 0.034042\n",
      "Epoch [5101/10000] Avg train loss: 0.034035\n",
      "Epoch [5102/10000] Avg train loss: 0.034028\n",
      "Epoch [5103/10000] Avg train loss: 0.034022\n",
      "Epoch [5104/10000] Avg train loss: 0.034015\n",
      "Epoch [5105/10000] Avg train loss: 0.034009\n",
      "Epoch [5106/10000] Avg train loss: 0.034002\n",
      "Epoch [5107/10000] Avg train loss: 0.033995\n",
      "Epoch [5108/10000] Avg train loss: 0.033989\n",
      "Epoch [5109/10000] Avg train loss: 0.033982\n",
      "Epoch [5110/10000] Avg train loss: 0.033975\n",
      "Epoch [5111/10000] Avg train loss: 0.033969\n",
      "Epoch [5112/10000] Avg train loss: 0.033962\n",
      "Epoch [5113/10000] Avg train loss: 0.033955\n",
      "Epoch [5114/10000] Avg train loss: 0.033949\n",
      "Epoch [5115/10000] Avg train loss: 0.033942\n",
      "Epoch [5116/10000] Avg train loss: 0.033935\n",
      "Epoch [5117/10000] Avg train loss: 0.033929\n",
      "Epoch [5118/10000] Avg train loss: 0.033922\n",
      "Epoch [5119/10000] Avg train loss: 0.033915\n",
      "Epoch [5120/10000] Avg train loss: 0.033909\n",
      "Epoch [5121/10000] Avg train loss: 0.033902\n",
      "Epoch [5122/10000] Avg train loss: 0.033896\n",
      "Epoch [5123/10000] Avg train loss: 0.033889\n",
      "Epoch [5124/10000] Avg train loss: 0.033882\n",
      "Epoch [5125/10000] Avg train loss: 0.033876\n",
      "Epoch [5126/10000] Avg train loss: 0.033869\n",
      "Epoch [5127/10000] Avg train loss: 0.033863\n",
      "Epoch [5128/10000] Avg train loss: 0.033856\n",
      "Epoch [5129/10000] Avg train loss: 0.033849\n",
      "Epoch [5130/10000] Avg train loss: 0.033843\n",
      "Epoch [5131/10000] Avg train loss: 0.033836\n",
      "Epoch [5132/10000] Avg train loss: 0.033830\n",
      "Epoch [5133/10000] Avg train loss: 0.033823\n",
      "Epoch [5134/10000] Avg train loss: 0.033816\n",
      "Epoch [5135/10000] Avg train loss: 0.033810\n",
      "Epoch [5136/10000] Avg train loss: 0.033803\n",
      "Epoch [5137/10000] Avg train loss: 0.033797\n",
      "Epoch [5138/10000] Avg train loss: 0.033790\n",
      "Epoch [5139/10000] Avg train loss: 0.033783\n",
      "Epoch [5140/10000] Avg train loss: 0.033777\n",
      "Epoch [5141/10000] Avg train loss: 0.033770\n",
      "Epoch [5142/10000] Avg train loss: 0.033764\n",
      "Epoch [5143/10000] Avg train loss: 0.033757\n",
      "Epoch [5144/10000] Avg train loss: 0.033751\n",
      "Epoch [5145/10000] Avg train loss: 0.033744\n",
      "Epoch [5146/10000] Avg train loss: 0.033738\n",
      "Epoch [5147/10000] Avg train loss: 0.033731\n",
      "Epoch [5148/10000] Avg train loss: 0.033724\n",
      "Epoch [5149/10000] Avg train loss: 0.033718\n",
      "Epoch [5150/10000] Avg train loss: 0.033711\n",
      "Epoch [5151/10000] Avg train loss: 0.033705\n",
      "Epoch [5152/10000] Avg train loss: 0.033698\n",
      "Epoch [5153/10000] Avg train loss: 0.033692\n",
      "Epoch [5154/10000] Avg train loss: 0.033685\n",
      "Epoch [5155/10000] Avg train loss: 0.033679\n",
      "Epoch [5156/10000] Avg train loss: 0.033672\n",
      "Epoch [5157/10000] Avg train loss: 0.033666\n",
      "Epoch [5158/10000] Avg train loss: 0.033659\n",
      "Epoch [5159/10000] Avg train loss: 0.033653\n",
      "Epoch [5160/10000] Avg train loss: 0.033646\n",
      "Epoch [5161/10000] Avg train loss: 0.033639\n",
      "Epoch [5162/10000] Avg train loss: 0.033633\n",
      "Epoch [5163/10000] Avg train loss: 0.033626\n",
      "Epoch [5164/10000] Avg train loss: 0.033620\n",
      "Epoch [5165/10000] Avg train loss: 0.033613\n",
      "Epoch [5166/10000] Avg train loss: 0.033607\n",
      "Epoch [5167/10000] Avg train loss: 0.033600\n",
      "Epoch [5168/10000] Avg train loss: 0.033594\n",
      "Epoch [5169/10000] Avg train loss: 0.033587\n",
      "Epoch [5170/10000] Avg train loss: 0.033581\n",
      "Epoch [5171/10000] Avg train loss: 0.033574\n",
      "Epoch [5172/10000] Avg train loss: 0.033568\n",
      "Epoch [5173/10000] Avg train loss: 0.033561\n",
      "Epoch [5174/10000] Avg train loss: 0.033555\n",
      "Epoch [5175/10000] Avg train loss: 0.033548\n",
      "Epoch [5176/10000] Avg train loss: 0.033542\n",
      "Epoch [5177/10000] Avg train loss: 0.033536\n",
      "Epoch [5178/10000] Avg train loss: 0.033529\n",
      "Epoch [5179/10000] Avg train loss: 0.033523\n",
      "Epoch [5180/10000] Avg train loss: 0.033516\n",
      "Epoch [5181/10000] Avg train loss: 0.033510\n",
      "Epoch [5182/10000] Avg train loss: 0.033503\n",
      "Epoch [5183/10000] Avg train loss: 0.033497\n",
      "Epoch [5184/10000] Avg train loss: 0.033490\n",
      "Epoch [5185/10000] Avg train loss: 0.033484\n",
      "Epoch [5186/10000] Avg train loss: 0.033477\n",
      "Epoch [5187/10000] Avg train loss: 0.033471\n",
      "Epoch [5188/10000] Avg train loss: 0.033464\n",
      "Epoch [5189/10000] Avg train loss: 0.033458\n",
      "Epoch [5190/10000] Avg train loss: 0.033452\n",
      "Epoch [5191/10000] Avg train loss: 0.033445\n",
      "Epoch [5192/10000] Avg train loss: 0.033439\n",
      "Epoch [5193/10000] Avg train loss: 0.033432\n",
      "Epoch [5194/10000] Avg train loss: 0.033426\n",
      "Epoch [5195/10000] Avg train loss: 0.033419\n",
      "Epoch [5196/10000] Avg train loss: 0.033413\n",
      "Epoch [5197/10000] Avg train loss: 0.033406\n",
      "Epoch [5198/10000] Avg train loss: 0.033400\n",
      "Epoch [5199/10000] Avg train loss: 0.033394\n",
      "Epoch [5200/10000] Avg train loss: 0.033387\n",
      "Epoch [5201/10000] Avg train loss: 0.033381\n",
      "Epoch [5202/10000] Avg train loss: 0.033374\n",
      "Epoch [5203/10000] Avg train loss: 0.033368\n",
      "Epoch [5204/10000] Avg train loss: 0.033362\n",
      "Epoch [5205/10000] Avg train loss: 0.033355\n",
      "Epoch [5206/10000] Avg train loss: 0.033349\n",
      "Epoch [5207/10000] Avg train loss: 0.033342\n",
      "Epoch [5208/10000] Avg train loss: 0.033336\n",
      "Epoch [5209/10000] Avg train loss: 0.033330\n",
      "Epoch [5210/10000] Avg train loss: 0.033323\n",
      "Epoch [5211/10000] Avg train loss: 0.033317\n",
      "Epoch [5212/10000] Avg train loss: 0.033310\n",
      "Epoch [5213/10000] Avg train loss: 0.033304\n",
      "Epoch [5214/10000] Avg train loss: 0.033298\n",
      "Epoch [5215/10000] Avg train loss: 0.033291\n",
      "Epoch [5216/10000] Avg train loss: 0.033285\n",
      "Epoch [5217/10000] Avg train loss: 0.033278\n",
      "Epoch [5218/10000] Avg train loss: 0.033272\n",
      "Epoch [5219/10000] Avg train loss: 0.033266\n",
      "Epoch [5220/10000] Avg train loss: 0.033259\n",
      "Epoch [5221/10000] Avg train loss: 0.033253\n",
      "Epoch [5222/10000] Avg train loss: 0.033247\n",
      "Epoch [5223/10000] Avg train loss: 0.033240\n",
      "Epoch [5224/10000] Avg train loss: 0.033234\n",
      "Epoch [5225/10000] Avg train loss: 0.033227\n",
      "Epoch [5226/10000] Avg train loss: 0.033221\n",
      "Epoch [5227/10000] Avg train loss: 0.033215\n",
      "Epoch [5228/10000] Avg train loss: 0.033208\n",
      "Epoch [5229/10000] Avg train loss: 0.033202\n",
      "Epoch [5230/10000] Avg train loss: 0.033196\n",
      "Epoch [5231/10000] Avg train loss: 0.033189\n",
      "Epoch [5232/10000] Avg train loss: 0.033183\n",
      "Epoch [5233/10000] Avg train loss: 0.033177\n",
      "Epoch [5234/10000] Avg train loss: 0.033170\n",
      "Epoch [5235/10000] Avg train loss: 0.033164\n",
      "Epoch [5236/10000] Avg train loss: 0.033158\n",
      "Epoch [5237/10000] Avg train loss: 0.033151\n",
      "Epoch [5238/10000] Avg train loss: 0.033145\n",
      "Epoch [5239/10000] Avg train loss: 0.033139\n",
      "Epoch [5240/10000] Avg train loss: 0.033132\n",
      "Epoch [5241/10000] Avg train loss: 0.033126\n",
      "Epoch [5242/10000] Avg train loss: 0.033120\n",
      "Epoch [5243/10000] Avg train loss: 0.033113\n",
      "Epoch [5244/10000] Avg train loss: 0.033107\n",
      "Epoch [5245/10000] Avg train loss: 0.033101\n",
      "Epoch [5246/10000] Avg train loss: 0.033094\n",
      "Epoch [5247/10000] Avg train loss: 0.033088\n",
      "Epoch [5248/10000] Avg train loss: 0.033082\n",
      "Epoch [5249/10000] Avg train loss: 0.033076\n",
      "Epoch [5250/10000] Avg train loss: 0.033069\n",
      "Epoch [5251/10000] Avg train loss: 0.033063\n",
      "Epoch [5252/10000] Avg train loss: 0.033057\n",
      "Epoch [5253/10000] Avg train loss: 0.033050\n",
      "Epoch [5254/10000] Avg train loss: 0.033044\n",
      "Epoch [5255/10000] Avg train loss: 0.033038\n",
      "Epoch [5256/10000] Avg train loss: 0.033031\n",
      "Epoch [5257/10000] Avg train loss: 0.033025\n",
      "Epoch [5258/10000] Avg train loss: 0.033019\n",
      "Epoch [5259/10000] Avg train loss: 0.033013\n",
      "Epoch [5260/10000] Avg train loss: 0.033006\n",
      "Epoch [5261/10000] Avg train loss: 0.033000\n",
      "Epoch [5262/10000] Avg train loss: 0.032994\n",
      "Epoch [5263/10000] Avg train loss: 0.032988\n",
      "Epoch [5264/10000] Avg train loss: 0.032981\n",
      "Epoch [5265/10000] Avg train loss: 0.032975\n",
      "Epoch [5266/10000] Avg train loss: 0.032969\n",
      "Epoch [5267/10000] Avg train loss: 0.032962\n",
      "Epoch [5268/10000] Avg train loss: 0.032956\n",
      "Epoch [5269/10000] Avg train loss: 0.032950\n",
      "Epoch [5270/10000] Avg train loss: 0.032944\n",
      "Epoch [5271/10000] Avg train loss: 0.032937\n",
      "Epoch [5272/10000] Avg train loss: 0.032931\n",
      "Epoch [5273/10000] Avg train loss: 0.032925\n",
      "Epoch [5274/10000] Avg train loss: 0.032919\n",
      "Epoch [5275/10000] Avg train loss: 0.032913\n",
      "Epoch [5276/10000] Avg train loss: 0.032906\n",
      "Epoch [5277/10000] Avg train loss: 0.032900\n",
      "Epoch [5278/10000] Avg train loss: 0.032894\n",
      "Epoch [5279/10000] Avg train loss: 0.032888\n",
      "Epoch [5280/10000] Avg train loss: 0.032881\n",
      "Epoch [5281/10000] Avg train loss: 0.032875\n",
      "Epoch [5282/10000] Avg train loss: 0.032869\n",
      "Epoch [5283/10000] Avg train loss: 0.032863\n",
      "Epoch [5284/10000] Avg train loss: 0.032856\n",
      "Epoch [5285/10000] Avg train loss: 0.032850\n",
      "Epoch [5286/10000] Avg train loss: 0.032844\n",
      "Epoch [5287/10000] Avg train loss: 0.032838\n",
      "Epoch [5288/10000] Avg train loss: 0.032832\n",
      "Epoch [5289/10000] Avg train loss: 0.032825\n",
      "Epoch [5290/10000] Avg train loss: 0.032819\n",
      "Epoch [5291/10000] Avg train loss: 0.032813\n",
      "Epoch [5292/10000] Avg train loss: 0.032807\n",
      "Epoch [5293/10000] Avg train loss: 0.032801\n",
      "Epoch [5294/10000] Avg train loss: 0.032794\n",
      "Epoch [5295/10000] Avg train loss: 0.032788\n",
      "Epoch [5296/10000] Avg train loss: 0.032782\n",
      "Epoch [5297/10000] Avg train loss: 0.032776\n",
      "Epoch [5298/10000] Avg train loss: 0.032770\n",
      "Epoch [5299/10000] Avg train loss: 0.032763\n",
      "Epoch [5300/10000] Avg train loss: 0.032757\n",
      "Epoch [5301/10000] Avg train loss: 0.032751\n",
      "Epoch [5302/10000] Avg train loss: 0.032745\n",
      "Epoch [5303/10000] Avg train loss: 0.032739\n",
      "Epoch [5304/10000] Avg train loss: 0.032733\n",
      "Epoch [5305/10000] Avg train loss: 0.032726\n",
      "Epoch [5306/10000] Avg train loss: 0.032720\n",
      "Epoch [5307/10000] Avg train loss: 0.032714\n",
      "Epoch [5308/10000] Avg train loss: 0.032708\n",
      "Epoch [5309/10000] Avg train loss: 0.032702\n",
      "Epoch [5310/10000] Avg train loss: 0.032696\n",
      "Epoch [5311/10000] Avg train loss: 0.032689\n",
      "Epoch [5312/10000] Avg train loss: 0.032683\n",
      "Epoch [5313/10000] Avg train loss: 0.032677\n",
      "Epoch [5314/10000] Avg train loss: 0.032671\n",
      "Epoch [5315/10000] Avg train loss: 0.032665\n",
      "Epoch [5316/10000] Avg train loss: 0.032659\n",
      "Epoch [5317/10000] Avg train loss: 0.032653\n",
      "Epoch [5318/10000] Avg train loss: 0.032646\n",
      "Epoch [5319/10000] Avg train loss: 0.032640\n",
      "Epoch [5320/10000] Avg train loss: 0.032634\n",
      "Epoch [5321/10000] Avg train loss: 0.032628\n",
      "Epoch [5322/10000] Avg train loss: 0.032622\n",
      "Epoch [5323/10000] Avg train loss: 0.032616\n",
      "Epoch [5324/10000] Avg train loss: 0.032610\n",
      "Epoch [5325/10000] Avg train loss: 0.032603\n",
      "Epoch [5326/10000] Avg train loss: 0.032597\n",
      "Epoch [5327/10000] Avg train loss: 0.032591\n",
      "Epoch [5328/10000] Avg train loss: 0.032585\n",
      "Epoch [5329/10000] Avg train loss: 0.032579\n",
      "Epoch [5330/10000] Avg train loss: 0.032573\n",
      "Epoch [5331/10000] Avg train loss: 0.032567\n",
      "Epoch [5332/10000] Avg train loss: 0.032561\n",
      "Epoch [5333/10000] Avg train loss: 0.032555\n",
      "Epoch [5334/10000] Avg train loss: 0.032548\n",
      "Epoch [5335/10000] Avg train loss: 0.032542\n",
      "Epoch [5336/10000] Avg train loss: 0.032536\n",
      "Epoch [5337/10000] Avg train loss: 0.032530\n",
      "Epoch [5338/10000] Avg train loss: 0.032524\n",
      "Epoch [5339/10000] Avg train loss: 0.032518\n",
      "Epoch [5340/10000] Avg train loss: 0.032512\n",
      "Epoch [5341/10000] Avg train loss: 0.032506\n",
      "Epoch [5342/10000] Avg train loss: 0.032500\n",
      "Epoch [5343/10000] Avg train loss: 0.032494\n",
      "Epoch [5344/10000] Avg train loss: 0.032488\n",
      "Epoch [5345/10000] Avg train loss: 0.032481\n",
      "Epoch [5346/10000] Avg train loss: 0.032475\n",
      "Epoch [5347/10000] Avg train loss: 0.032469\n",
      "Epoch [5348/10000] Avg train loss: 0.032463\n",
      "Epoch [5349/10000] Avg train loss: 0.032457\n",
      "Epoch [5350/10000] Avg train loss: 0.032451\n",
      "Epoch [5351/10000] Avg train loss: 0.032445\n",
      "Epoch [5352/10000] Avg train loss: 0.032439\n",
      "Epoch [5353/10000] Avg train loss: 0.032433\n",
      "Epoch [5354/10000] Avg train loss: 0.032427\n",
      "Epoch [5355/10000] Avg train loss: 0.032421\n",
      "Epoch [5356/10000] Avg train loss: 0.032415\n",
      "Epoch [5357/10000] Avg train loss: 0.032409\n",
      "Epoch [5358/10000] Avg train loss: 0.032403\n",
      "Epoch [5359/10000] Avg train loss: 0.032397\n",
      "Epoch [5360/10000] Avg train loss: 0.032391\n",
      "Epoch [5361/10000] Avg train loss: 0.032385\n",
      "Epoch [5362/10000] Avg train loss: 0.032378\n",
      "Epoch [5363/10000] Avg train loss: 0.032372\n",
      "Epoch [5364/10000] Avg train loss: 0.032366\n",
      "Epoch [5365/10000] Avg train loss: 0.032360\n",
      "Epoch [5366/10000] Avg train loss: 0.032354\n",
      "Epoch [5367/10000] Avg train loss: 0.032348\n",
      "Epoch [5368/10000] Avg train loss: 0.032342\n",
      "Epoch [5369/10000] Avg train loss: 0.032336\n",
      "Epoch [5370/10000] Avg train loss: 0.032330\n",
      "Epoch [5371/10000] Avg train loss: 0.032324\n",
      "Epoch [5372/10000] Avg train loss: 0.032318\n",
      "Epoch [5373/10000] Avg train loss: 0.032312\n",
      "Epoch [5374/10000] Avg train loss: 0.032306\n",
      "Epoch [5375/10000] Avg train loss: 0.032300\n",
      "Epoch [5376/10000] Avg train loss: 0.032294\n",
      "Epoch [5377/10000] Avg train loss: 0.032288\n",
      "Epoch [5378/10000] Avg train loss: 0.032282\n",
      "Epoch [5379/10000] Avg train loss: 0.032276\n",
      "Epoch [5380/10000] Avg train loss: 0.032270\n",
      "Epoch [5381/10000] Avg train loss: 0.032264\n",
      "Epoch [5382/10000] Avg train loss: 0.032258\n",
      "Epoch [5383/10000] Avg train loss: 0.032252\n",
      "Epoch [5384/10000] Avg train loss: 0.032246\n",
      "Epoch [5385/10000] Avg train loss: 0.032240\n",
      "Epoch [5386/10000] Avg train loss: 0.032234\n",
      "Epoch [5387/10000] Avg train loss: 0.032228\n",
      "Epoch [5388/10000] Avg train loss: 0.032222\n",
      "Epoch [5389/10000] Avg train loss: 0.032216\n",
      "Epoch [5390/10000] Avg train loss: 0.032210\n",
      "Epoch [5391/10000] Avg train loss: 0.032204\n",
      "Epoch [5392/10000] Avg train loss: 0.032198\n",
      "Epoch [5393/10000] Avg train loss: 0.032192\n",
      "Epoch [5394/10000] Avg train loss: 0.032186\n",
      "Epoch [5395/10000] Avg train loss: 0.032180\n",
      "Epoch [5396/10000] Avg train loss: 0.032174\n",
      "Epoch [5397/10000] Avg train loss: 0.032169\n",
      "Epoch [5398/10000] Avg train loss: 0.032163\n",
      "Epoch [5399/10000] Avg train loss: 0.032157\n",
      "Epoch [5400/10000] Avg train loss: 0.032151\n",
      "Epoch [5401/10000] Avg train loss: 0.032145\n",
      "Epoch [5402/10000] Avg train loss: 0.032139\n",
      "Epoch [5403/10000] Avg train loss: 0.032133\n",
      "Epoch [5404/10000] Avg train loss: 0.032127\n",
      "Epoch [5405/10000] Avg train loss: 0.032121\n",
      "Epoch [5406/10000] Avg train loss: 0.032115\n",
      "Epoch [5407/10000] Avg train loss: 0.032109\n",
      "Epoch [5408/10000] Avg train loss: 0.032103\n",
      "Epoch [5409/10000] Avg train loss: 0.032097\n",
      "Epoch [5410/10000] Avg train loss: 0.032091\n",
      "Epoch [5411/10000] Avg train loss: 0.032085\n",
      "Epoch [5412/10000] Avg train loss: 0.032079\n",
      "Epoch [5413/10000] Avg train loss: 0.032073\n",
      "Epoch [5414/10000] Avg train loss: 0.032068\n",
      "Epoch [5415/10000] Avg train loss: 0.032062\n",
      "Epoch [5416/10000] Avg train loss: 0.032056\n",
      "Epoch [5417/10000] Avg train loss: 0.032050\n",
      "Epoch [5418/10000] Avg train loss: 0.032044\n",
      "Epoch [5419/10000] Avg train loss: 0.032038\n",
      "Epoch [5420/10000] Avg train loss: 0.032032\n",
      "Epoch [5421/10000] Avg train loss: 0.032026\n",
      "Epoch [5422/10000] Avg train loss: 0.032020\n",
      "Epoch [5423/10000] Avg train loss: 0.032014\n",
      "Epoch [5424/10000] Avg train loss: 0.032008\n",
      "Epoch [5425/10000] Avg train loss: 0.032002\n",
      "Epoch [5426/10000] Avg train loss: 0.031997\n",
      "Epoch [5427/10000] Avg train loss: 0.031991\n",
      "Epoch [5428/10000] Avg train loss: 0.031985\n",
      "Epoch [5429/10000] Avg train loss: 0.031979\n",
      "Epoch [5430/10000] Avg train loss: 0.031973\n",
      "Epoch [5431/10000] Avg train loss: 0.031967\n",
      "Epoch [5432/10000] Avg train loss: 0.031961\n",
      "Epoch [5433/10000] Avg train loss: 0.031955\n",
      "Epoch [5434/10000] Avg train loss: 0.031949\n",
      "Epoch [5435/10000] Avg train loss: 0.031944\n",
      "Epoch [5436/10000] Avg train loss: 0.031938\n",
      "Epoch [5437/10000] Avg train loss: 0.031932\n",
      "Epoch [5438/10000] Avg train loss: 0.031926\n",
      "Epoch [5439/10000] Avg train loss: 0.031920\n",
      "Epoch [5440/10000] Avg train loss: 0.031914\n",
      "Epoch [5441/10000] Avg train loss: 0.031908\n",
      "Epoch [5442/10000] Avg train loss: 0.031903\n",
      "Epoch [5443/10000] Avg train loss: 0.031897\n",
      "Epoch [5444/10000] Avg train loss: 0.031891\n",
      "Epoch [5445/10000] Avg train loss: 0.031885\n",
      "Epoch [5446/10000] Avg train loss: 0.031879\n",
      "Epoch [5447/10000] Avg train loss: 0.031873\n",
      "Epoch [5448/10000] Avg train loss: 0.031867\n",
      "Epoch [5449/10000] Avg train loss: 0.031862\n",
      "Epoch [5450/10000] Avg train loss: 0.031856\n",
      "Epoch [5451/10000] Avg train loss: 0.031850\n",
      "Epoch [5452/10000] Avg train loss: 0.031844\n",
      "Epoch [5453/10000] Avg train loss: 0.031838\n",
      "Epoch [5454/10000] Avg train loss: 0.031832\n",
      "Epoch [5455/10000] Avg train loss: 0.031826\n",
      "Epoch [5456/10000] Avg train loss: 0.031821\n",
      "Epoch [5457/10000] Avg train loss: 0.031815\n",
      "Epoch [5458/10000] Avg train loss: 0.031809\n",
      "Epoch [5459/10000] Avg train loss: 0.031803\n",
      "Epoch [5460/10000] Avg train loss: 0.031797\n",
      "Epoch [5461/10000] Avg train loss: 0.031792\n",
      "Epoch [5462/10000] Avg train loss: 0.031786\n",
      "Epoch [5463/10000] Avg train loss: 0.031780\n",
      "Epoch [5464/10000] Avg train loss: 0.031774\n",
      "Epoch [5465/10000] Avg train loss: 0.031768\n",
      "Epoch [5466/10000] Avg train loss: 0.031762\n",
      "Epoch [5467/10000] Avg train loss: 0.031757\n",
      "Epoch [5468/10000] Avg train loss: 0.031751\n",
      "Epoch [5469/10000] Avg train loss: 0.031745\n",
      "Epoch [5470/10000] Avg train loss: 0.031739\n",
      "Epoch [5471/10000] Avg train loss: 0.031733\n",
      "Epoch [5472/10000] Avg train loss: 0.031728\n",
      "Epoch [5473/10000] Avg train loss: 0.031722\n",
      "Epoch [5474/10000] Avg train loss: 0.031716\n",
      "Epoch [5475/10000] Avg train loss: 0.031710\n",
      "Epoch [5476/10000] Avg train loss: 0.031704\n",
      "Epoch [5477/10000] Avg train loss: 0.031699\n",
      "Epoch [5478/10000] Avg train loss: 0.031693\n",
      "Epoch [5479/10000] Avg train loss: 0.031687\n",
      "Epoch [5480/10000] Avg train loss: 0.031681\n",
      "Epoch [5481/10000] Avg train loss: 0.031676\n",
      "Epoch [5482/10000] Avg train loss: 0.031670\n",
      "Epoch [5483/10000] Avg train loss: 0.031664\n",
      "Epoch [5484/10000] Avg train loss: 0.031658\n",
      "Epoch [5485/10000] Avg train loss: 0.031652\n",
      "Epoch [5486/10000] Avg train loss: 0.031647\n",
      "Epoch [5487/10000] Avg train loss: 0.031641\n",
      "Epoch [5488/10000] Avg train loss: 0.031635\n",
      "Epoch [5489/10000] Avg train loss: 0.031629\n",
      "Epoch [5490/10000] Avg train loss: 0.031624\n",
      "Epoch [5491/10000] Avg train loss: 0.031618\n",
      "Epoch [5492/10000] Avg train loss: 0.031612\n",
      "Epoch [5493/10000] Avg train loss: 0.031606\n",
      "Epoch [5494/10000] Avg train loss: 0.031601\n",
      "Epoch [5495/10000] Avg train loss: 0.031595\n",
      "Epoch [5496/10000] Avg train loss: 0.031589\n",
      "Epoch [5497/10000] Avg train loss: 0.031583\n",
      "Epoch [5498/10000] Avg train loss: 0.031578\n",
      "Epoch [5499/10000] Avg train loss: 0.031572\n",
      "Epoch [5500/10000] Avg train loss: 0.031566\n",
      "Epoch [5501/10000] Avg train loss: 0.031560\n",
      "Epoch [5502/10000] Avg train loss: 0.031555\n",
      "Epoch [5503/10000] Avg train loss: 0.031549\n",
      "Epoch [5504/10000] Avg train loss: 0.031543\n",
      "Epoch [5505/10000] Avg train loss: 0.031537\n",
      "Epoch [5506/10000] Avg train loss: 0.031532\n",
      "Epoch [5507/10000] Avg train loss: 0.031526\n",
      "Epoch [5508/10000] Avg train loss: 0.031520\n",
      "Epoch [5509/10000] Avg train loss: 0.031515\n",
      "Epoch [5510/10000] Avg train loss: 0.031509\n",
      "Epoch [5511/10000] Avg train loss: 0.031503\n",
      "Epoch [5512/10000] Avg train loss: 0.031497\n",
      "Epoch [5513/10000] Avg train loss: 0.031492\n",
      "Epoch [5514/10000] Avg train loss: 0.031486\n",
      "Epoch [5515/10000] Avg train loss: 0.031480\n",
      "Epoch [5516/10000] Avg train loss: 0.031475\n",
      "Epoch [5517/10000] Avg train loss: 0.031469\n",
      "Epoch [5518/10000] Avg train loss: 0.031463\n",
      "Epoch [5519/10000] Avg train loss: 0.031457\n",
      "Epoch [5520/10000] Avg train loss: 0.031452\n",
      "Epoch [5521/10000] Avg train loss: 0.031446\n",
      "Epoch [5522/10000] Avg train loss: 0.031440\n",
      "Epoch [5523/10000] Avg train loss: 0.031435\n",
      "Epoch [5524/10000] Avg train loss: 0.031429\n",
      "Epoch [5525/10000] Avg train loss: 0.031423\n",
      "Epoch [5526/10000] Avg train loss: 0.031418\n",
      "Epoch [5527/10000] Avg train loss: 0.031412\n",
      "Epoch [5528/10000] Avg train loss: 0.031406\n",
      "Epoch [5529/10000] Avg train loss: 0.031401\n",
      "Epoch [5530/10000] Avg train loss: 0.031395\n",
      "Epoch [5531/10000] Avg train loss: 0.031389\n",
      "Epoch [5532/10000] Avg train loss: 0.031383\n",
      "Epoch [5533/10000] Avg train loss: 0.031378\n",
      "Epoch [5534/10000] Avg train loss: 0.031372\n",
      "Epoch [5535/10000] Avg train loss: 0.031366\n",
      "Epoch [5536/10000] Avg train loss: 0.031361\n",
      "Epoch [5537/10000] Avg train loss: 0.031355\n",
      "Epoch [5538/10000] Avg train loss: 0.031349\n",
      "Epoch [5539/10000] Avg train loss: 0.031344\n",
      "Epoch [5540/10000] Avg train loss: 0.031338\n",
      "Epoch [5541/10000] Avg train loss: 0.031333\n",
      "Epoch [5542/10000] Avg train loss: 0.031327\n",
      "Epoch [5543/10000] Avg train loss: 0.031321\n",
      "Epoch [5544/10000] Avg train loss: 0.031316\n",
      "Epoch [5545/10000] Avg train loss: 0.031310\n",
      "Epoch [5546/10000] Avg train loss: 0.031304\n",
      "Epoch [5547/10000] Avg train loss: 0.031299\n",
      "Epoch [5548/10000] Avg train loss: 0.031293\n",
      "Epoch [5549/10000] Avg train loss: 0.031287\n",
      "Epoch [5550/10000] Avg train loss: 0.031282\n",
      "Epoch [5551/10000] Avg train loss: 0.031276\n",
      "Epoch [5552/10000] Avg train loss: 0.031270\n",
      "Epoch [5553/10000] Avg train loss: 0.031265\n",
      "Epoch [5554/10000] Avg train loss: 0.031259\n",
      "Epoch [5555/10000] Avg train loss: 0.031254\n",
      "Epoch [5556/10000] Avg train loss: 0.031248\n",
      "Epoch [5557/10000] Avg train loss: 0.031242\n",
      "Epoch [5558/10000] Avg train loss: 0.031237\n",
      "Epoch [5559/10000] Avg train loss: 0.031231\n",
      "Epoch [5560/10000] Avg train loss: 0.031225\n",
      "Epoch [5561/10000] Avg train loss: 0.031220\n",
      "Epoch [5562/10000] Avg train loss: 0.031214\n",
      "Epoch [5563/10000] Avg train loss: 0.031209\n",
      "Epoch [5564/10000] Avg train loss: 0.031203\n",
      "Epoch [5565/10000] Avg train loss: 0.031197\n",
      "Epoch [5566/10000] Avg train loss: 0.031192\n",
      "Epoch [5567/10000] Avg train loss: 0.031186\n",
      "Epoch [5568/10000] Avg train loss: 0.031181\n",
      "Epoch [5569/10000] Avg train loss: 0.031175\n",
      "Epoch [5570/10000] Avg train loss: 0.031169\n",
      "Epoch [5571/10000] Avg train loss: 0.031164\n",
      "Epoch [5572/10000] Avg train loss: 0.031158\n",
      "Epoch [5573/10000] Avg train loss: 0.031153\n",
      "Epoch [5574/10000] Avg train loss: 0.031147\n",
      "Epoch [5575/10000] Avg train loss: 0.031141\n",
      "Epoch [5576/10000] Avg train loss: 0.031136\n",
      "Epoch [5577/10000] Avg train loss: 0.031130\n",
      "Epoch [5578/10000] Avg train loss: 0.031125\n",
      "Epoch [5579/10000] Avg train loss: 0.031119\n",
      "Epoch [5580/10000] Avg train loss: 0.031114\n",
      "Epoch [5581/10000] Avg train loss: 0.031108\n",
      "Epoch [5582/10000] Avg train loss: 0.031102\n",
      "Epoch [5583/10000] Avg train loss: 0.031097\n",
      "Epoch [5584/10000] Avg train loss: 0.031091\n",
      "Epoch [5585/10000] Avg train loss: 0.031086\n",
      "Epoch [5586/10000] Avg train loss: 0.031080\n",
      "Epoch [5587/10000] Avg train loss: 0.031075\n",
      "Epoch [5588/10000] Avg train loss: 0.031069\n",
      "Epoch [5589/10000] Avg train loss: 0.031063\n",
      "Epoch [5590/10000] Avg train loss: 0.031058\n",
      "Epoch [5591/10000] Avg train loss: 0.031052\n",
      "Epoch [5592/10000] Avg train loss: 0.031047\n",
      "Epoch [5593/10000] Avg train loss: 0.031041\n",
      "Epoch [5594/10000] Avg train loss: 0.031036\n",
      "Epoch [5595/10000] Avg train loss: 0.031030\n",
      "Epoch [5596/10000] Avg train loss: 0.031025\n",
      "Epoch [5597/10000] Avg train loss: 0.031019\n",
      "Epoch [5598/10000] Avg train loss: 0.031013\n",
      "Epoch [5599/10000] Avg train loss: 0.031008\n",
      "Epoch [5600/10000] Avg train loss: 0.031002\n",
      "Epoch [5601/10000] Avg train loss: 0.030997\n",
      "Epoch [5602/10000] Avg train loss: 0.030991\n",
      "Epoch [5603/10000] Avg train loss: 0.030986\n",
      "Epoch [5604/10000] Avg train loss: 0.030980\n",
      "Epoch [5605/10000] Avg train loss: 0.030975\n",
      "Epoch [5606/10000] Avg train loss: 0.030969\n",
      "Epoch [5607/10000] Avg train loss: 0.030964\n",
      "Epoch [5608/10000] Avg train loss: 0.030958\n",
      "Epoch [5609/10000] Avg train loss: 0.030953\n",
      "Epoch [5610/10000] Avg train loss: 0.030947\n",
      "Epoch [5611/10000] Avg train loss: 0.030942\n",
      "Epoch [5612/10000] Avg train loss: 0.030936\n",
      "Epoch [5613/10000] Avg train loss: 0.030931\n",
      "Epoch [5614/10000] Avg train loss: 0.030925\n",
      "Epoch [5615/10000] Avg train loss: 0.030920\n",
      "Epoch [5616/10000] Avg train loss: 0.030914\n",
      "Epoch [5617/10000] Avg train loss: 0.030909\n",
      "Epoch [5618/10000] Avg train loss: 0.030903\n",
      "Epoch [5619/10000] Avg train loss: 0.030898\n",
      "Epoch [5620/10000] Avg train loss: 0.030892\n",
      "Epoch [5621/10000] Avg train loss: 0.030887\n",
      "Epoch [5622/10000] Avg train loss: 0.030881\n",
      "Epoch [5623/10000] Avg train loss: 0.030876\n",
      "Epoch [5624/10000] Avg train loss: 0.030870\n",
      "Epoch [5625/10000] Avg train loss: 0.030865\n",
      "Epoch [5626/10000] Avg train loss: 0.030859\n",
      "Epoch [5627/10000] Avg train loss: 0.030854\n",
      "Epoch [5628/10000] Avg train loss: 0.030848\n",
      "Epoch [5629/10000] Avg train loss: 0.030843\n",
      "Epoch [5630/10000] Avg train loss: 0.030837\n",
      "Epoch [5631/10000] Avg train loss: 0.030832\n",
      "Epoch [5632/10000] Avg train loss: 0.030826\n",
      "Epoch [5633/10000] Avg train loss: 0.030821\n",
      "Epoch [5634/10000] Avg train loss: 0.030815\n",
      "Epoch [5635/10000] Avg train loss: 0.030810\n",
      "Epoch [5636/10000] Avg train loss: 0.030804\n",
      "Epoch [5637/10000] Avg train loss: 0.030799\n",
      "Epoch [5638/10000] Avg train loss: 0.030793\n",
      "Epoch [5639/10000] Avg train loss: 0.030788\n",
      "Epoch [5640/10000] Avg train loss: 0.030783\n",
      "Epoch [5641/10000] Avg train loss: 0.030777\n",
      "Epoch [5642/10000] Avg train loss: 0.030772\n",
      "Epoch [5643/10000] Avg train loss: 0.030766\n",
      "Epoch [5644/10000] Avg train loss: 0.030761\n",
      "Epoch [5645/10000] Avg train loss: 0.030755\n",
      "Epoch [5646/10000] Avg train loss: 0.030750\n",
      "Epoch [5647/10000] Avg train loss: 0.030744\n",
      "Epoch [5648/10000] Avg train loss: 0.030739\n",
      "Epoch [5649/10000] Avg train loss: 0.030733\n",
      "Epoch [5650/10000] Avg train loss: 0.030728\n",
      "Epoch [5651/10000] Avg train loss: 0.030723\n",
      "Epoch [5652/10000] Avg train loss: 0.030717\n",
      "Epoch [5653/10000] Avg train loss: 0.030712\n",
      "Epoch [5654/10000] Avg train loss: 0.030706\n",
      "Epoch [5655/10000] Avg train loss: 0.030701\n",
      "Epoch [5656/10000] Avg train loss: 0.030695\n",
      "Epoch [5657/10000] Avg train loss: 0.030690\n",
      "Epoch [5658/10000] Avg train loss: 0.030685\n",
      "Epoch [5659/10000] Avg train loss: 0.030679\n",
      "Epoch [5660/10000] Avg train loss: 0.030674\n",
      "Epoch [5661/10000] Avg train loss: 0.030668\n",
      "Epoch [5662/10000] Avg train loss: 0.030663\n",
      "Epoch [5663/10000] Avg train loss: 0.030658\n",
      "Epoch [5664/10000] Avg train loss: 0.030652\n",
      "Epoch [5665/10000] Avg train loss: 0.030647\n",
      "Epoch [5666/10000] Avg train loss: 0.030641\n",
      "Epoch [5667/10000] Avg train loss: 0.030636\n",
      "Epoch [5668/10000] Avg train loss: 0.030630\n",
      "Epoch [5669/10000] Avg train loss: 0.030625\n",
      "Epoch [5670/10000] Avg train loss: 0.030620\n",
      "Epoch [5671/10000] Avg train loss: 0.030614\n",
      "Epoch [5672/10000] Avg train loss: 0.030609\n",
      "Epoch [5673/10000] Avg train loss: 0.030603\n",
      "Epoch [5674/10000] Avg train loss: 0.030598\n",
      "Epoch [5675/10000] Avg train loss: 0.030593\n",
      "Epoch [5676/10000] Avg train loss: 0.030587\n",
      "Epoch [5677/10000] Avg train loss: 0.030582\n",
      "Epoch [5678/10000] Avg train loss: 0.030577\n",
      "Epoch [5679/10000] Avg train loss: 0.030571\n",
      "Epoch [5680/10000] Avg train loss: 0.030566\n",
      "Epoch [5681/10000] Avg train loss: 0.030560\n",
      "Epoch [5682/10000] Avg train loss: 0.030555\n",
      "Epoch [5683/10000] Avg train loss: 0.030550\n",
      "Epoch [5684/10000] Avg train loss: 0.030544\n",
      "Epoch [5685/10000] Avg train loss: 0.030539\n",
      "Epoch [5686/10000] Avg train loss: 0.030534\n",
      "Epoch [5687/10000] Avg train loss: 0.030528\n",
      "Epoch [5688/10000] Avg train loss: 0.030523\n",
      "Epoch [5689/10000] Avg train loss: 0.030517\n",
      "Epoch [5690/10000] Avg train loss: 0.030512\n",
      "Epoch [5691/10000] Avg train loss: 0.030507\n",
      "Epoch [5692/10000] Avg train loss: 0.030501\n",
      "Epoch [5693/10000] Avg train loss: 0.030496\n",
      "Epoch [5694/10000] Avg train loss: 0.030491\n",
      "Epoch [5695/10000] Avg train loss: 0.030485\n",
      "Epoch [5696/10000] Avg train loss: 0.030480\n",
      "Epoch [5697/10000] Avg train loss: 0.030475\n",
      "Epoch [5698/10000] Avg train loss: 0.030469\n",
      "Epoch [5699/10000] Avg train loss: 0.030464\n",
      "Epoch [5700/10000] Avg train loss: 0.030459\n",
      "Epoch [5701/10000] Avg train loss: 0.030453\n",
      "Epoch [5702/10000] Avg train loss: 0.030448\n",
      "Epoch [5703/10000] Avg train loss: 0.030442\n",
      "Epoch [5704/10000] Avg train loss: 0.030437\n",
      "Epoch [5705/10000] Avg train loss: 0.030432\n",
      "Epoch [5706/10000] Avg train loss: 0.030426\n",
      "Epoch [5707/10000] Avg train loss: 0.030421\n",
      "Epoch [5708/10000] Avg train loss: 0.030416\n",
      "Epoch [5709/10000] Avg train loss: 0.030410\n",
      "Epoch [5710/10000] Avg train loss: 0.030405\n",
      "Epoch [5711/10000] Avg train loss: 0.030400\n",
      "Epoch [5712/10000] Avg train loss: 0.030395\n",
      "Epoch [5713/10000] Avg train loss: 0.030389\n",
      "Epoch [5714/10000] Avg train loss: 0.030384\n",
      "Epoch [5715/10000] Avg train loss: 0.030379\n",
      "Epoch [5716/10000] Avg train loss: 0.030373\n",
      "Epoch [5717/10000] Avg train loss: 0.030368\n",
      "Epoch [5718/10000] Avg train loss: 0.030363\n",
      "Epoch [5719/10000] Avg train loss: 0.030357\n",
      "Epoch [5720/10000] Avg train loss: 0.030352\n",
      "Epoch [5721/10000] Avg train loss: 0.030347\n",
      "Epoch [5722/10000] Avg train loss: 0.030341\n",
      "Epoch [5723/10000] Avg train loss: 0.030336\n",
      "Epoch [5724/10000] Avg train loss: 0.030331\n",
      "Epoch [5725/10000] Avg train loss: 0.030326\n",
      "Epoch [5726/10000] Avg train loss: 0.030320\n",
      "Epoch [5727/10000] Avg train loss: 0.030315\n",
      "Epoch [5728/10000] Avg train loss: 0.030310\n",
      "Epoch [5729/10000] Avg train loss: 0.030304\n",
      "Epoch [5730/10000] Avg train loss: 0.030299\n",
      "Epoch [5731/10000] Avg train loss: 0.030294\n",
      "Epoch [5732/10000] Avg train loss: 0.030288\n",
      "Epoch [5733/10000] Avg train loss: 0.030283\n",
      "Epoch [5734/10000] Avg train loss: 0.030278\n",
      "Epoch [5735/10000] Avg train loss: 0.030273\n",
      "Epoch [5736/10000] Avg train loss: 0.030267\n",
      "Epoch [5737/10000] Avg train loss: 0.030262\n",
      "Epoch [5738/10000] Avg train loss: 0.030257\n",
      "Epoch [5739/10000] Avg train loss: 0.030252\n",
      "Epoch [5740/10000] Avg train loss: 0.030246\n",
      "Epoch [5741/10000] Avg train loss: 0.030241\n",
      "Epoch [5742/10000] Avg train loss: 0.030236\n",
      "Epoch [5743/10000] Avg train loss: 0.030230\n",
      "Epoch [5744/10000] Avg train loss: 0.030225\n",
      "Epoch [5745/10000] Avg train loss: 0.030220\n",
      "Epoch [5746/10000] Avg train loss: 0.030215\n",
      "Epoch [5747/10000] Avg train loss: 0.030209\n",
      "Epoch [5748/10000] Avg train loss: 0.030204\n",
      "Epoch [5749/10000] Avg train loss: 0.030199\n",
      "Epoch [5750/10000] Avg train loss: 0.030194\n",
      "Epoch [5751/10000] Avg train loss: 0.030188\n",
      "Epoch [5752/10000] Avg train loss: 0.030183\n",
      "Epoch [5753/10000] Avg train loss: 0.030178\n",
      "Epoch [5754/10000] Avg train loss: 0.030173\n",
      "Epoch [5755/10000] Avg train loss: 0.030167\n",
      "Epoch [5756/10000] Avg train loss: 0.030162\n",
      "Epoch [5757/10000] Avg train loss: 0.030157\n",
      "Epoch [5758/10000] Avg train loss: 0.030152\n",
      "Epoch [5759/10000] Avg train loss: 0.030146\n",
      "Epoch [5760/10000] Avg train loss: 0.030141\n",
      "Epoch [5761/10000] Avg train loss: 0.030136\n",
      "Epoch [5762/10000] Avg train loss: 0.030131\n",
      "Epoch [5763/10000] Avg train loss: 0.030126\n",
      "Epoch [5764/10000] Avg train loss: 0.030120\n",
      "Epoch [5765/10000] Avg train loss: 0.030115\n",
      "Epoch [5766/10000] Avg train loss: 0.030110\n",
      "Epoch [5767/10000] Avg train loss: 0.030105\n",
      "Epoch [5768/10000] Avg train loss: 0.030099\n",
      "Epoch [5769/10000] Avg train loss: 0.030094\n",
      "Epoch [5770/10000] Avg train loss: 0.030089\n",
      "Epoch [5771/10000] Avg train loss: 0.030084\n",
      "Epoch [5772/10000] Avg train loss: 0.030079\n",
      "Epoch [5773/10000] Avg train loss: 0.030073\n",
      "Epoch [5774/10000] Avg train loss: 0.030068\n",
      "Epoch [5775/10000] Avg train loss: 0.030063\n",
      "Epoch [5776/10000] Avg train loss: 0.030058\n",
      "Epoch [5777/10000] Avg train loss: 0.030053\n",
      "Epoch [5778/10000] Avg train loss: 0.030047\n",
      "Epoch [5779/10000] Avg train loss: 0.030042\n",
      "Epoch [5780/10000] Avg train loss: 0.030037\n",
      "Epoch [5781/10000] Avg train loss: 0.030032\n",
      "Epoch [5782/10000] Avg train loss: 0.030027\n",
      "Epoch [5783/10000] Avg train loss: 0.030021\n",
      "Epoch [5784/10000] Avg train loss: 0.030016\n",
      "Epoch [5785/10000] Avg train loss: 0.030011\n",
      "Epoch [5786/10000] Avg train loss: 0.030006\n",
      "Epoch [5787/10000] Avg train loss: 0.030001\n",
      "Epoch [5788/10000] Avg train loss: 0.029995\n",
      "Epoch [5789/10000] Avg train loss: 0.029990\n",
      "Epoch [5790/10000] Avg train loss: 0.029985\n",
      "Epoch [5791/10000] Avg train loss: 0.029980\n",
      "Epoch [5792/10000] Avg train loss: 0.029975\n",
      "Epoch [5793/10000] Avg train loss: 0.029970\n",
      "Epoch [5794/10000] Avg train loss: 0.029964\n",
      "Epoch [5795/10000] Avg train loss: 0.029959\n",
      "Epoch [5796/10000] Avg train loss: 0.029954\n",
      "Epoch [5797/10000] Avg train loss: 0.029949\n",
      "Epoch [5798/10000] Avg train loss: 0.029944\n",
      "Epoch [5799/10000] Avg train loss: 0.029939\n",
      "Epoch [5800/10000] Avg train loss: 0.029933\n",
      "Epoch [5801/10000] Avg train loss: 0.029928\n",
      "Epoch [5802/10000] Avg train loss: 0.029923\n",
      "Epoch [5803/10000] Avg train loss: 0.029918\n",
      "Epoch [5804/10000] Avg train loss: 0.029913\n",
      "Epoch [5805/10000] Avg train loss: 0.029908\n",
      "Epoch [5806/10000] Avg train loss: 0.029902\n",
      "Epoch [5807/10000] Avg train loss: 0.029897\n",
      "Epoch [5808/10000] Avg train loss: 0.029892\n",
      "Epoch [5809/10000] Avg train loss: 0.029887\n",
      "Epoch [5810/10000] Avg train loss: 0.029882\n",
      "Epoch [5811/10000] Avg train loss: 0.029877\n",
      "Epoch [5812/10000] Avg train loss: 0.029872\n",
      "Epoch [5813/10000] Avg train loss: 0.029866\n",
      "Epoch [5814/10000] Avg train loss: 0.029861\n",
      "Epoch [5815/10000] Avg train loss: 0.029856\n",
      "Epoch [5816/10000] Avg train loss: 0.029851\n",
      "Epoch [5817/10000] Avg train loss: 0.029846\n",
      "Epoch [5818/10000] Avg train loss: 0.029841\n",
      "Epoch [5819/10000] Avg train loss: 0.029836\n",
      "Epoch [5820/10000] Avg train loss: 0.029831\n",
      "Epoch [5821/10000] Avg train loss: 0.029825\n",
      "Epoch [5822/10000] Avg train loss: 0.029820\n",
      "Epoch [5823/10000] Avg train loss: 0.029815\n",
      "Epoch [5824/10000] Avg train loss: 0.029810\n",
      "Epoch [5825/10000] Avg train loss: 0.029805\n",
      "Epoch [5826/10000] Avg train loss: 0.029800\n",
      "Epoch [5827/10000] Avg train loss: 0.029795\n",
      "Epoch [5828/10000] Avg train loss: 0.029790\n",
      "Epoch [5829/10000] Avg train loss: 0.029784\n",
      "Epoch [5830/10000] Avg train loss: 0.029779\n",
      "Epoch [5831/10000] Avg train loss: 0.029774\n",
      "Epoch [5832/10000] Avg train loss: 0.029769\n",
      "Epoch [5833/10000] Avg train loss: 0.029764\n",
      "Epoch [5834/10000] Avg train loss: 0.029759\n",
      "Epoch [5835/10000] Avg train loss: 0.029754\n",
      "Epoch [5836/10000] Avg train loss: 0.029749\n",
      "Epoch [5837/10000] Avg train loss: 0.029744\n",
      "Epoch [5838/10000] Avg train loss: 0.029739\n",
      "Epoch [5839/10000] Avg train loss: 0.029733\n",
      "Epoch [5840/10000] Avg train loss: 0.029728\n",
      "Epoch [5841/10000] Avg train loss: 0.029723\n",
      "Epoch [5842/10000] Avg train loss: 0.029718\n",
      "Epoch [5843/10000] Avg train loss: 0.029713\n",
      "Epoch [5844/10000] Avg train loss: 0.029708\n",
      "Epoch [5845/10000] Avg train loss: 0.029703\n",
      "Epoch [5846/10000] Avg train loss: 0.029698\n",
      "Epoch [5847/10000] Avg train loss: 0.029693\n",
      "Epoch [5848/10000] Avg train loss: 0.029688\n",
      "Epoch [5849/10000] Avg train loss: 0.029683\n",
      "Epoch [5850/10000] Avg train loss: 0.029678\n",
      "Epoch [5851/10000] Avg train loss: 0.029672\n",
      "Epoch [5852/10000] Avg train loss: 0.029667\n",
      "Epoch [5853/10000] Avg train loss: 0.029662\n",
      "Epoch [5854/10000] Avg train loss: 0.029657\n",
      "Epoch [5855/10000] Avg train loss: 0.029652\n",
      "Epoch [5856/10000] Avg train loss: 0.029647\n",
      "Epoch [5857/10000] Avg train loss: 0.029642\n",
      "Epoch [5858/10000] Avg train loss: 0.029637\n",
      "Epoch [5859/10000] Avg train loss: 0.029632\n",
      "Epoch [5860/10000] Avg train loss: 0.029627\n",
      "Epoch [5861/10000] Avg train loss: 0.029622\n",
      "Epoch [5862/10000] Avg train loss: 0.029617\n",
      "Epoch [5863/10000] Avg train loss: 0.029612\n",
      "Epoch [5864/10000] Avg train loss: 0.029607\n",
      "Epoch [5865/10000] Avg train loss: 0.029602\n",
      "Epoch [5866/10000] Avg train loss: 0.029597\n",
      "Epoch [5867/10000] Avg train loss: 0.029592\n",
      "Epoch [5868/10000] Avg train loss: 0.029586\n",
      "Epoch [5869/10000] Avg train loss: 0.029581\n",
      "Epoch [5870/10000] Avg train loss: 0.029576\n",
      "Epoch [5871/10000] Avg train loss: 0.029571\n",
      "Epoch [5872/10000] Avg train loss: 0.029566\n",
      "Epoch [5873/10000] Avg train loss: 0.029561\n",
      "Epoch [5874/10000] Avg train loss: 0.029556\n",
      "Epoch [5875/10000] Avg train loss: 0.029551\n",
      "Epoch [5876/10000] Avg train loss: 0.029546\n",
      "Epoch [5877/10000] Avg train loss: 0.029541\n",
      "Epoch [5878/10000] Avg train loss: 0.029536\n",
      "Epoch [5879/10000] Avg train loss: 0.029531\n",
      "Epoch [5880/10000] Avg train loss: 0.029526\n",
      "Epoch [5881/10000] Avg train loss: 0.029521\n",
      "Epoch [5882/10000] Avg train loss: 0.029516\n",
      "Epoch [5883/10000] Avg train loss: 0.029511\n",
      "Epoch [5884/10000] Avg train loss: 0.029506\n",
      "Epoch [5885/10000] Avg train loss: 0.029501\n",
      "Epoch [5886/10000] Avg train loss: 0.029496\n",
      "Epoch [5887/10000] Avg train loss: 0.029491\n",
      "Epoch [5888/10000] Avg train loss: 0.029486\n",
      "Epoch [5889/10000] Avg train loss: 0.029481\n",
      "Epoch [5890/10000] Avg train loss: 0.029476\n",
      "Epoch [5891/10000] Avg train loss: 0.029471\n",
      "Epoch [5892/10000] Avg train loss: 0.029466\n",
      "Epoch [5893/10000] Avg train loss: 0.029461\n",
      "Epoch [5894/10000] Avg train loss: 0.029456\n",
      "Epoch [5895/10000] Avg train loss: 0.029451\n",
      "Epoch [5896/10000] Avg train loss: 0.029446\n",
      "Epoch [5897/10000] Avg train loss: 0.029441\n",
      "Epoch [5898/10000] Avg train loss: 0.029436\n",
      "Epoch [5899/10000] Avg train loss: 0.029431\n",
      "Epoch [5900/10000] Avg train loss: 0.029426\n",
      "Epoch [5901/10000] Avg train loss: 0.029421\n",
      "Epoch [5902/10000] Avg train loss: 0.029416\n",
      "Epoch [5903/10000] Avg train loss: 0.029411\n",
      "Epoch [5904/10000] Avg train loss: 0.029406\n",
      "Epoch [5905/10000] Avg train loss: 0.029401\n",
      "Epoch [5906/10000] Avg train loss: 0.029396\n",
      "Epoch [5907/10000] Avg train loss: 0.029391\n",
      "Epoch [5908/10000] Avg train loss: 0.029386\n",
      "Epoch [5909/10000] Avg train loss: 0.029381\n",
      "Epoch [5910/10000] Avg train loss: 0.029376\n",
      "Epoch [5911/10000] Avg train loss: 0.029371\n",
      "Epoch [5912/10000] Avg train loss: 0.029366\n",
      "Epoch [5913/10000] Avg train loss: 0.029361\n",
      "Epoch [5914/10000] Avg train loss: 0.029356\n",
      "Epoch [5915/10000] Avg train loss: 0.029351\n",
      "Epoch [5916/10000] Avg train loss: 0.029346\n",
      "Epoch [5917/10000] Avg train loss: 0.029341\n",
      "Epoch [5918/10000] Avg train loss: 0.029337\n",
      "Epoch [5919/10000] Avg train loss: 0.029332\n",
      "Epoch [5920/10000] Avg train loss: 0.029327\n",
      "Epoch [5921/10000] Avg train loss: 0.029322\n",
      "Epoch [5922/10000] Avg train loss: 0.029317\n",
      "Epoch [5923/10000] Avg train loss: 0.029312\n",
      "Epoch [5924/10000] Avg train loss: 0.029307\n",
      "Epoch [5925/10000] Avg train loss: 0.029302\n",
      "Epoch [5926/10000] Avg train loss: 0.029297\n",
      "Epoch [5927/10000] Avg train loss: 0.029292\n",
      "Epoch [5928/10000] Avg train loss: 0.029287\n",
      "Epoch [5929/10000] Avg train loss: 0.029282\n",
      "Epoch [5930/10000] Avg train loss: 0.029277\n",
      "Epoch [5931/10000] Avg train loss: 0.029272\n",
      "Epoch [5932/10000] Avg train loss: 0.029267\n",
      "Epoch [5933/10000] Avg train loss: 0.029262\n",
      "Epoch [5934/10000] Avg train loss: 0.029257\n",
      "Epoch [5935/10000] Avg train loss: 0.029252\n",
      "Epoch [5936/10000] Avg train loss: 0.029248\n",
      "Epoch [5937/10000] Avg train loss: 0.029243\n",
      "Epoch [5938/10000] Avg train loss: 0.029238\n",
      "Epoch [5939/10000] Avg train loss: 0.029233\n",
      "Epoch [5940/10000] Avg train loss: 0.029228\n",
      "Epoch [5941/10000] Avg train loss: 0.029223\n",
      "Epoch [5942/10000] Avg train loss: 0.029218\n",
      "Epoch [5943/10000] Avg train loss: 0.029213\n",
      "Epoch [5944/10000] Avg train loss: 0.029208\n",
      "Epoch [5945/10000] Avg train loss: 0.029203\n",
      "Epoch [5946/10000] Avg train loss: 0.029198\n",
      "Epoch [5947/10000] Avg train loss: 0.029193\n",
      "Epoch [5948/10000] Avg train loss: 0.029189\n",
      "Epoch [5949/10000] Avg train loss: 0.029184\n",
      "Epoch [5950/10000] Avg train loss: 0.029179\n",
      "Epoch [5951/10000] Avg train loss: 0.029174\n",
      "Epoch [5952/10000] Avg train loss: 0.029169\n",
      "Epoch [5953/10000] Avg train loss: 0.029164\n",
      "Epoch [5954/10000] Avg train loss: 0.029159\n",
      "Epoch [5955/10000] Avg train loss: 0.029154\n",
      "Epoch [5956/10000] Avg train loss: 0.029149\n",
      "Epoch [5957/10000] Avg train loss: 0.029144\n",
      "Epoch [5958/10000] Avg train loss: 0.029140\n",
      "Epoch [5959/10000] Avg train loss: 0.029135\n",
      "Epoch [5960/10000] Avg train loss: 0.029130\n",
      "Epoch [5961/10000] Avg train loss: 0.029125\n",
      "Epoch [5962/10000] Avg train loss: 0.029120\n",
      "Epoch [5963/10000] Avg train loss: 0.029115\n",
      "Epoch [5964/10000] Avg train loss: 0.029110\n",
      "Epoch [5965/10000] Avg train loss: 0.029105\n",
      "Epoch [5966/10000] Avg train loss: 0.029100\n",
      "Epoch [5967/10000] Avg train loss: 0.029096\n",
      "Epoch [5968/10000] Avg train loss: 0.029091\n",
      "Epoch [5969/10000] Avg train loss: 0.029086\n",
      "Epoch [5970/10000] Avg train loss: 0.029081\n",
      "Epoch [5971/10000] Avg train loss: 0.029076\n",
      "Epoch [5972/10000] Avg train loss: 0.029071\n",
      "Epoch [5973/10000] Avg train loss: 0.029066\n",
      "Epoch [5974/10000] Avg train loss: 0.029062\n",
      "Epoch [5975/10000] Avg train loss: 0.029057\n",
      "Epoch [5976/10000] Avg train loss: 0.029052\n",
      "Epoch [5977/10000] Avg train loss: 0.029047\n",
      "Epoch [5978/10000] Avg train loss: 0.029042\n",
      "Epoch [5979/10000] Avg train loss: 0.029037\n",
      "Epoch [5980/10000] Avg train loss: 0.029032\n",
      "Epoch [5981/10000] Avg train loss: 0.029028\n",
      "Epoch [5982/10000] Avg train loss: 0.029023\n",
      "Epoch [5983/10000] Avg train loss: 0.029018\n",
      "Epoch [5984/10000] Avg train loss: 0.029013\n",
      "Epoch [5985/10000] Avg train loss: 0.029008\n",
      "Epoch [5986/10000] Avg train loss: 0.029003\n",
      "Epoch [5987/10000] Avg train loss: 0.028998\n",
      "Epoch [5988/10000] Avg train loss: 0.028994\n",
      "Epoch [5989/10000] Avg train loss: 0.028989\n",
      "Epoch [5990/10000] Avg train loss: 0.028984\n",
      "Epoch [5991/10000] Avg train loss: 0.028979\n",
      "Epoch [5992/10000] Avg train loss: 0.028974\n",
      "Epoch [5993/10000] Avg train loss: 0.028969\n",
      "Epoch [5994/10000] Avg train loss: 0.028965\n",
      "Epoch [5995/10000] Avg train loss: 0.028960\n",
      "Epoch [5996/10000] Avg train loss: 0.028955\n",
      "Epoch [5997/10000] Avg train loss: 0.028950\n",
      "Epoch [5998/10000] Avg train loss: 0.028945\n",
      "Epoch [5999/10000] Avg train loss: 0.028940\n",
      "Epoch [6000/10000] Avg train loss: 0.028936\n",
      "Epoch [6001/10000] Avg train loss: 0.028931\n",
      "Epoch [6002/10000] Avg train loss: 0.028926\n",
      "Epoch [6003/10000] Avg train loss: 0.028921\n",
      "Epoch [6004/10000] Avg train loss: 0.028916\n",
      "Epoch [6005/10000] Avg train loss: 0.028911\n",
      "Epoch [6006/10000] Avg train loss: 0.028907\n",
      "Epoch [6007/10000] Avg train loss: 0.028902\n",
      "Epoch [6008/10000] Avg train loss: 0.028897\n",
      "Epoch [6009/10000] Avg train loss: 0.028892\n",
      "Epoch [6010/10000] Avg train loss: 0.028887\n",
      "Epoch [6011/10000] Avg train loss: 0.028883\n",
      "Epoch [6012/10000] Avg train loss: 0.028878\n",
      "Epoch [6013/10000] Avg train loss: 0.028873\n",
      "Epoch [6014/10000] Avg train loss: 0.028868\n",
      "Epoch [6015/10000] Avg train loss: 0.028863\n",
      "Epoch [6016/10000] Avg train loss: 0.028859\n",
      "Epoch [6017/10000] Avg train loss: 0.028854\n",
      "Epoch [6018/10000] Avg train loss: 0.028849\n",
      "Epoch [6019/10000] Avg train loss: 0.028844\n",
      "Epoch [6020/10000] Avg train loss: 0.028839\n",
      "Epoch [6021/10000] Avg train loss: 0.028835\n",
      "Epoch [6022/10000] Avg train loss: 0.028830\n",
      "Epoch [6023/10000] Avg train loss: 0.028825\n",
      "Epoch [6024/10000] Avg train loss: 0.028820\n",
      "Epoch [6025/10000] Avg train loss: 0.028816\n",
      "Epoch [6026/10000] Avg train loss: 0.028811\n",
      "Epoch [6027/10000] Avg train loss: 0.028806\n",
      "Epoch [6028/10000] Avg train loss: 0.028801\n",
      "Epoch [6029/10000] Avg train loss: 0.028796\n",
      "Epoch [6030/10000] Avg train loss: 0.028792\n",
      "Epoch [6031/10000] Avg train loss: 0.028787\n",
      "Epoch [6032/10000] Avg train loss: 0.028782\n",
      "Epoch [6033/10000] Avg train loss: 0.028777\n",
      "Epoch [6034/10000] Avg train loss: 0.028773\n",
      "Epoch [6035/10000] Avg train loss: 0.028768\n",
      "Epoch [6036/10000] Avg train loss: 0.028763\n",
      "Epoch [6037/10000] Avg train loss: 0.028758\n",
      "Epoch [6038/10000] Avg train loss: 0.028753\n",
      "Epoch [6039/10000] Avg train loss: 0.028749\n",
      "Epoch [6040/10000] Avg train loss: 0.028744\n",
      "Epoch [6041/10000] Avg train loss: 0.028739\n",
      "Epoch [6042/10000] Avg train loss: 0.028734\n",
      "Epoch [6043/10000] Avg train loss: 0.028730\n",
      "Epoch [6044/10000] Avg train loss: 0.028725\n",
      "Epoch [6045/10000] Avg train loss: 0.028720\n",
      "Epoch [6046/10000] Avg train loss: 0.028715\n",
      "Epoch [6047/10000] Avg train loss: 0.028711\n",
      "Epoch [6048/10000] Avg train loss: 0.028706\n",
      "Epoch [6049/10000] Avg train loss: 0.028701\n",
      "Epoch [6050/10000] Avg train loss: 0.028696\n",
      "Epoch [6051/10000] Avg train loss: 0.028692\n",
      "Epoch [6052/10000] Avg train loss: 0.028687\n",
      "Epoch [6053/10000] Avg train loss: 0.028682\n",
      "Epoch [6054/10000] Avg train loss: 0.028677\n",
      "Epoch [6055/10000] Avg train loss: 0.028673\n",
      "Epoch [6056/10000] Avg train loss: 0.028668\n",
      "Epoch [6057/10000] Avg train loss: 0.028663\n",
      "Epoch [6058/10000] Avg train loss: 0.028659\n",
      "Epoch [6059/10000] Avg train loss: 0.028654\n",
      "Epoch [6060/10000] Avg train loss: 0.028649\n",
      "Epoch [6061/10000] Avg train loss: 0.028644\n",
      "Epoch [6062/10000] Avg train loss: 0.028640\n",
      "Epoch [6063/10000] Avg train loss: 0.028635\n",
      "Epoch [6064/10000] Avg train loss: 0.028630\n",
      "Epoch [6065/10000] Avg train loss: 0.028625\n",
      "Epoch [6066/10000] Avg train loss: 0.028621\n",
      "Epoch [6067/10000] Avg train loss: 0.028616\n",
      "Epoch [6068/10000] Avg train loss: 0.028611\n",
      "Epoch [6069/10000] Avg train loss: 0.028607\n",
      "Epoch [6070/10000] Avg train loss: 0.028602\n",
      "Epoch [6071/10000] Avg train loss: 0.028597\n",
      "Epoch [6072/10000] Avg train loss: 0.028592\n",
      "Epoch [6073/10000] Avg train loss: 0.028588\n",
      "Epoch [6074/10000] Avg train loss: 0.028583\n",
      "Epoch [6075/10000] Avg train loss: 0.028578\n",
      "Epoch [6076/10000] Avg train loss: 0.028574\n",
      "Epoch [6077/10000] Avg train loss: 0.028569\n",
      "Epoch [6078/10000] Avg train loss: 0.028564\n",
      "Epoch [6079/10000] Avg train loss: 0.028560\n",
      "Epoch [6080/10000] Avg train loss: 0.028555\n",
      "Epoch [6081/10000] Avg train loss: 0.028550\n",
      "Epoch [6082/10000] Avg train loss: 0.028545\n",
      "Epoch [6083/10000] Avg train loss: 0.028541\n",
      "Epoch [6084/10000] Avg train loss: 0.028536\n",
      "Epoch [6085/10000] Avg train loss: 0.028531\n",
      "Epoch [6086/10000] Avg train loss: 0.028527\n",
      "Epoch [6087/10000] Avg train loss: 0.028522\n",
      "Epoch [6088/10000] Avg train loss: 0.028517\n",
      "Epoch [6089/10000] Avg train loss: 0.028513\n",
      "Epoch [6090/10000] Avg train loss: 0.028508\n",
      "Epoch [6091/10000] Avg train loss: 0.028503\n",
      "Epoch [6092/10000] Avg train loss: 0.028499\n",
      "Epoch [6093/10000] Avg train loss: 0.028494\n",
      "Epoch [6094/10000] Avg train loss: 0.028489\n",
      "Epoch [6095/10000] Avg train loss: 0.028485\n",
      "Epoch [6096/10000] Avg train loss: 0.028480\n",
      "Epoch [6097/10000] Avg train loss: 0.028475\n",
      "Epoch [6098/10000] Avg train loss: 0.028471\n",
      "Epoch [6099/10000] Avg train loss: 0.028466\n",
      "Epoch [6100/10000] Avg train loss: 0.028461\n",
      "Epoch [6101/10000] Avg train loss: 0.028457\n",
      "Epoch [6102/10000] Avg train loss: 0.028452\n",
      "Epoch [6103/10000] Avg train loss: 0.028447\n",
      "Epoch [6104/10000] Avg train loss: 0.028443\n",
      "Epoch [6105/10000] Avg train loss: 0.028438\n",
      "Epoch [6106/10000] Avg train loss: 0.028433\n",
      "Epoch [6107/10000] Avg train loss: 0.028429\n",
      "Epoch [6108/10000] Avg train loss: 0.028424\n",
      "Epoch [6109/10000] Avg train loss: 0.028419\n",
      "Epoch [6110/10000] Avg train loss: 0.028415\n",
      "Epoch [6111/10000] Avg train loss: 0.028410\n",
      "Epoch [6112/10000] Avg train loss: 0.028405\n",
      "Epoch [6113/10000] Avg train loss: 0.028401\n",
      "Epoch [6114/10000] Avg train loss: 0.028396\n",
      "Epoch [6115/10000] Avg train loss: 0.028391\n",
      "Epoch [6116/10000] Avg train loss: 0.028387\n",
      "Epoch [6117/10000] Avg train loss: 0.028382\n",
      "Epoch [6118/10000] Avg train loss: 0.028377\n",
      "Epoch [6119/10000] Avg train loss: 0.028373\n",
      "Epoch [6120/10000] Avg train loss: 0.028368\n",
      "Epoch [6121/10000] Avg train loss: 0.028364\n",
      "Epoch [6122/10000] Avg train loss: 0.028359\n",
      "Epoch [6123/10000] Avg train loss: 0.028354\n",
      "Epoch [6124/10000] Avg train loss: 0.028350\n",
      "Epoch [6125/10000] Avg train loss: 0.028345\n",
      "Epoch [6126/10000] Avg train loss: 0.028340\n",
      "Epoch [6127/10000] Avg train loss: 0.028336\n",
      "Epoch [6128/10000] Avg train loss: 0.028331\n",
      "Epoch [6129/10000] Avg train loss: 0.028327\n",
      "Epoch [6130/10000] Avg train loss: 0.028322\n",
      "Epoch [6131/10000] Avg train loss: 0.028317\n",
      "Epoch [6132/10000] Avg train loss: 0.028313\n",
      "Epoch [6133/10000] Avg train loss: 0.028308\n",
      "Epoch [6134/10000] Avg train loss: 0.028303\n",
      "Epoch [6135/10000] Avg train loss: 0.028299\n",
      "Epoch [6136/10000] Avg train loss: 0.028294\n",
      "Epoch [6137/10000] Avg train loss: 0.028290\n",
      "Epoch [6138/10000] Avg train loss: 0.028285\n",
      "Epoch [6139/10000] Avg train loss: 0.028280\n",
      "Epoch [6140/10000] Avg train loss: 0.028276\n",
      "Epoch [6141/10000] Avg train loss: 0.028271\n",
      "Epoch [6142/10000] Avg train loss: 0.028267\n",
      "Epoch [6143/10000] Avg train loss: 0.028262\n",
      "Epoch [6144/10000] Avg train loss: 0.028257\n",
      "Epoch [6145/10000] Avg train loss: 0.028253\n",
      "Epoch [6146/10000] Avg train loss: 0.028248\n",
      "Epoch [6147/10000] Avg train loss: 0.028244\n",
      "Epoch [6148/10000] Avg train loss: 0.028239\n",
      "Epoch [6149/10000] Avg train loss: 0.028234\n",
      "Epoch [6150/10000] Avg train loss: 0.028230\n",
      "Epoch [6151/10000] Avg train loss: 0.028225\n",
      "Epoch [6152/10000] Avg train loss: 0.028221\n",
      "Epoch [6153/10000] Avg train loss: 0.028216\n",
      "Epoch [6154/10000] Avg train loss: 0.028211\n",
      "Epoch [6155/10000] Avg train loss: 0.028207\n",
      "Epoch [6156/10000] Avg train loss: 0.028202\n",
      "Epoch [6157/10000] Avg train loss: 0.028198\n",
      "Epoch [6158/10000] Avg train loss: 0.028193\n",
      "Epoch [6159/10000] Avg train loss: 0.028189\n",
      "Epoch [6160/10000] Avg train loss: 0.028184\n",
      "Epoch [6161/10000] Avg train loss: 0.028179\n",
      "Epoch [6162/10000] Avg train loss: 0.028175\n",
      "Epoch [6163/10000] Avg train loss: 0.028170\n",
      "Epoch [6164/10000] Avg train loss: 0.028166\n",
      "Epoch [6165/10000] Avg train loss: 0.028161\n",
      "Epoch [6166/10000] Avg train loss: 0.028157\n",
      "Epoch [6167/10000] Avg train loss: 0.028152\n",
      "Epoch [6168/10000] Avg train loss: 0.028147\n",
      "Epoch [6169/10000] Avg train loss: 0.028143\n",
      "Epoch [6170/10000] Avg train loss: 0.028138\n",
      "Epoch [6171/10000] Avg train loss: 0.028134\n",
      "Epoch [6172/10000] Avg train loss: 0.028129\n",
      "Epoch [6173/10000] Avg train loss: 0.028125\n",
      "Epoch [6174/10000] Avg train loss: 0.028120\n",
      "Epoch [6175/10000] Avg train loss: 0.028116\n",
      "Epoch [6176/10000] Avg train loss: 0.028111\n",
      "Epoch [6177/10000] Avg train loss: 0.028106\n",
      "Epoch [6178/10000] Avg train loss: 0.028102\n",
      "Epoch [6179/10000] Avg train loss: 0.028097\n",
      "Epoch [6180/10000] Avg train loss: 0.028093\n",
      "Epoch [6181/10000] Avg train loss: 0.028088\n",
      "Epoch [6182/10000] Avg train loss: 0.028084\n",
      "Epoch [6183/10000] Avg train loss: 0.028079\n",
      "Epoch [6184/10000] Avg train loss: 0.028075\n",
      "Epoch [6185/10000] Avg train loss: 0.028070\n",
      "Epoch [6186/10000] Avg train loss: 0.028066\n",
      "Epoch [6187/10000] Avg train loss: 0.028061\n",
      "Epoch [6188/10000] Avg train loss: 0.028056\n",
      "Epoch [6189/10000] Avg train loss: 0.028052\n",
      "Epoch [6190/10000] Avg train loss: 0.028047\n",
      "Epoch [6191/10000] Avg train loss: 0.028043\n",
      "Epoch [6192/10000] Avg train loss: 0.028038\n",
      "Epoch [6193/10000] Avg train loss: 0.028034\n",
      "Epoch [6194/10000] Avg train loss: 0.028029\n",
      "Epoch [6195/10000] Avg train loss: 0.028025\n",
      "Epoch [6196/10000] Avg train loss: 0.028020\n",
      "Epoch [6197/10000] Avg train loss: 0.028016\n",
      "Epoch [6198/10000] Avg train loss: 0.028011\n",
      "Epoch [6199/10000] Avg train loss: 0.028007\n",
      "Epoch [6200/10000] Avg train loss: 0.028002\n",
      "Epoch [6201/10000] Avg train loss: 0.027998\n",
      "Epoch [6202/10000] Avg train loss: 0.027993\n",
      "Epoch [6203/10000] Avg train loss: 0.027989\n",
      "Epoch [6204/10000] Avg train loss: 0.027984\n",
      "Epoch [6205/10000] Avg train loss: 0.027980\n",
      "Epoch [6206/10000] Avg train loss: 0.027975\n",
      "Epoch [6207/10000] Avg train loss: 0.027971\n",
      "Epoch [6208/10000] Avg train loss: 0.027966\n",
      "Epoch [6209/10000] Avg train loss: 0.027962\n",
      "Epoch [6210/10000] Avg train loss: 0.027957\n",
      "Epoch [6211/10000] Avg train loss: 0.027953\n",
      "Epoch [6212/10000] Avg train loss: 0.027948\n",
      "Epoch [6213/10000] Avg train loss: 0.027944\n",
      "Epoch [6214/10000] Avg train loss: 0.027939\n",
      "Epoch [6215/10000] Avg train loss: 0.027935\n",
      "Epoch [6216/10000] Avg train loss: 0.027930\n",
      "Epoch [6217/10000] Avg train loss: 0.027926\n",
      "Epoch [6218/10000] Avg train loss: 0.027921\n",
      "Epoch [6219/10000] Avg train loss: 0.027917\n",
      "Epoch [6220/10000] Avg train loss: 0.027912\n",
      "Epoch [6221/10000] Avg train loss: 0.027908\n",
      "Epoch [6222/10000] Avg train loss: 0.027903\n",
      "Epoch [6223/10000] Avg train loss: 0.027899\n",
      "Epoch [6224/10000] Avg train loss: 0.027894\n",
      "Epoch [6225/10000] Avg train loss: 0.027890\n",
      "Epoch [6226/10000] Avg train loss: 0.027885\n",
      "Epoch [6227/10000] Avg train loss: 0.027881\n",
      "Epoch [6228/10000] Avg train loss: 0.027876\n",
      "Epoch [6229/10000] Avg train loss: 0.027872\n",
      "Epoch [6230/10000] Avg train loss: 0.027867\n",
      "Epoch [6231/10000] Avg train loss: 0.027863\n",
      "Epoch [6232/10000] Avg train loss: 0.027858\n",
      "Epoch [6233/10000] Avg train loss: 0.027854\n",
      "Epoch [6234/10000] Avg train loss: 0.027849\n",
      "Epoch [6235/10000] Avg train loss: 0.027845\n",
      "Epoch [6236/10000] Avg train loss: 0.027841\n",
      "Epoch [6237/10000] Avg train loss: 0.027836\n",
      "Epoch [6238/10000] Avg train loss: 0.027832\n",
      "Epoch [6239/10000] Avg train loss: 0.027827\n",
      "Epoch [6240/10000] Avg train loss: 0.027823\n",
      "Epoch [6241/10000] Avg train loss: 0.027818\n",
      "Epoch [6242/10000] Avg train loss: 0.027814\n",
      "Epoch [6243/10000] Avg train loss: 0.027809\n",
      "Epoch [6244/10000] Avg train loss: 0.027805\n",
      "Epoch [6245/10000] Avg train loss: 0.027800\n",
      "Epoch [6246/10000] Avg train loss: 0.027796\n",
      "Epoch [6247/10000] Avg train loss: 0.027792\n",
      "Epoch [6248/10000] Avg train loss: 0.027787\n",
      "Epoch [6249/10000] Avg train loss: 0.027783\n",
      "Epoch [6250/10000] Avg train loss: 0.027778\n",
      "Epoch [6251/10000] Avg train loss: 0.027774\n",
      "Epoch [6252/10000] Avg train loss: 0.027769\n",
      "Epoch [6253/10000] Avg train loss: 0.027765\n",
      "Epoch [6254/10000] Avg train loss: 0.027760\n",
      "Epoch [6255/10000] Avg train loss: 0.027756\n",
      "Epoch [6256/10000] Avg train loss: 0.027752\n",
      "Epoch [6257/10000] Avg train loss: 0.027747\n",
      "Epoch [6258/10000] Avg train loss: 0.027743\n",
      "Epoch [6259/10000] Avg train loss: 0.027738\n",
      "Epoch [6260/10000] Avg train loss: 0.027734\n",
      "Epoch [6261/10000] Avg train loss: 0.027729\n",
      "Epoch [6262/10000] Avg train loss: 0.027725\n",
      "Epoch [6263/10000] Avg train loss: 0.027721\n",
      "Epoch [6264/10000] Avg train loss: 0.027716\n",
      "Epoch [6265/10000] Avg train loss: 0.027712\n",
      "Epoch [6266/10000] Avg train loss: 0.027707\n",
      "Epoch [6267/10000] Avg train loss: 0.027703\n",
      "Epoch [6268/10000] Avg train loss: 0.027698\n",
      "Epoch [6269/10000] Avg train loss: 0.027694\n",
      "Epoch [6270/10000] Avg train loss: 0.027690\n",
      "Epoch [6271/10000] Avg train loss: 0.027685\n",
      "Epoch [6272/10000] Avg train loss: 0.027681\n",
      "Epoch [6273/10000] Avg train loss: 0.027676\n",
      "Epoch [6274/10000] Avg train loss: 0.027672\n",
      "Epoch [6275/10000] Avg train loss: 0.027667\n",
      "Epoch [6276/10000] Avg train loss: 0.027663\n",
      "Epoch [6277/10000] Avg train loss: 0.027659\n",
      "Epoch [6278/10000] Avg train loss: 0.027654\n",
      "Epoch [6279/10000] Avg train loss: 0.027650\n",
      "Epoch [6280/10000] Avg train loss: 0.027645\n",
      "Epoch [6281/10000] Avg train loss: 0.027641\n",
      "Epoch [6282/10000] Avg train loss: 0.027637\n",
      "Epoch [6283/10000] Avg train loss: 0.027632\n",
      "Epoch [6284/10000] Avg train loss: 0.027628\n",
      "Epoch [6285/10000] Avg train loss: 0.027623\n",
      "Epoch [6286/10000] Avg train loss: 0.027619\n",
      "Epoch [6287/10000] Avg train loss: 0.027615\n",
      "Epoch [6288/10000] Avg train loss: 0.027610\n",
      "Epoch [6289/10000] Avg train loss: 0.027606\n",
      "Epoch [6290/10000] Avg train loss: 0.027602\n",
      "Epoch [6291/10000] Avg train loss: 0.027597\n",
      "Epoch [6292/10000] Avg train loss: 0.027593\n",
      "Epoch [6293/10000] Avg train loss: 0.027588\n",
      "Epoch [6294/10000] Avg train loss: 0.027584\n",
      "Epoch [6295/10000] Avg train loss: 0.027580\n",
      "Epoch [6296/10000] Avg train loss: 0.027575\n",
      "Epoch [6297/10000] Avg train loss: 0.027571\n",
      "Epoch [6298/10000] Avg train loss: 0.027566\n",
      "Epoch [6299/10000] Avg train loss: 0.027562\n",
      "Epoch [6300/10000] Avg train loss: 0.027558\n",
      "Epoch [6301/10000] Avg train loss: 0.027553\n",
      "Epoch [6302/10000] Avg train loss: 0.027549\n",
      "Epoch [6303/10000] Avg train loss: 0.027545\n",
      "Epoch [6304/10000] Avg train loss: 0.027540\n",
      "Epoch [6305/10000] Avg train loss: 0.027536\n",
      "Epoch [6306/10000] Avg train loss: 0.027531\n",
      "Epoch [6307/10000] Avg train loss: 0.027527\n",
      "Epoch [6308/10000] Avg train loss: 0.027523\n",
      "Epoch [6309/10000] Avg train loss: 0.027518\n",
      "Epoch [6310/10000] Avg train loss: 0.027514\n",
      "Epoch [6311/10000] Avg train loss: 0.027510\n",
      "Epoch [6312/10000] Avg train loss: 0.027505\n",
      "Epoch [6313/10000] Avg train loss: 0.027501\n",
      "Epoch [6314/10000] Avg train loss: 0.027497\n",
      "Epoch [6315/10000] Avg train loss: 0.027492\n",
      "Epoch [6316/10000] Avg train loss: 0.027488\n",
      "Epoch [6317/10000] Avg train loss: 0.027484\n",
      "Epoch [6318/10000] Avg train loss: 0.027479\n",
      "Epoch [6319/10000] Avg train loss: 0.027475\n",
      "Epoch [6320/10000] Avg train loss: 0.027470\n",
      "Epoch [6321/10000] Avg train loss: 0.027466\n",
      "Epoch [6322/10000] Avg train loss: 0.027462\n",
      "Epoch [6323/10000] Avg train loss: 0.027457\n",
      "Epoch [6324/10000] Avg train loss: 0.027453\n",
      "Epoch [6325/10000] Avg train loss: 0.027449\n",
      "Epoch [6326/10000] Avg train loss: 0.027444\n",
      "Epoch [6327/10000] Avg train loss: 0.027440\n",
      "Epoch [6328/10000] Avg train loss: 0.027436\n",
      "Epoch [6329/10000] Avg train loss: 0.027431\n",
      "Epoch [6330/10000] Avg train loss: 0.027427\n",
      "Epoch [6331/10000] Avg train loss: 0.027423\n",
      "Epoch [6332/10000] Avg train loss: 0.027418\n",
      "Epoch [6333/10000] Avg train loss: 0.027414\n",
      "Epoch [6334/10000] Avg train loss: 0.027410\n",
      "Epoch [6335/10000] Avg train loss: 0.027405\n",
      "Epoch [6336/10000] Avg train loss: 0.027401\n",
      "Epoch [6337/10000] Avg train loss: 0.027397\n",
      "Epoch [6338/10000] Avg train loss: 0.027392\n",
      "Epoch [6339/10000] Avg train loss: 0.027388\n",
      "Epoch [6340/10000] Avg train loss: 0.027384\n",
      "Epoch [6341/10000] Avg train loss: 0.027380\n",
      "Epoch [6342/10000] Avg train loss: 0.027375\n",
      "Epoch [6343/10000] Avg train loss: 0.027371\n",
      "Epoch [6344/10000] Avg train loss: 0.027367\n",
      "Epoch [6345/10000] Avg train loss: 0.027362\n",
      "Epoch [6346/10000] Avg train loss: 0.027358\n",
      "Epoch [6347/10000] Avg train loss: 0.027354\n",
      "Epoch [6348/10000] Avg train loss: 0.027349\n",
      "Epoch [6349/10000] Avg train loss: 0.027345\n",
      "Epoch [6350/10000] Avg train loss: 0.027341\n",
      "Epoch [6351/10000] Avg train loss: 0.027336\n",
      "Epoch [6352/10000] Avg train loss: 0.027332\n",
      "Epoch [6353/10000] Avg train loss: 0.027328\n",
      "Epoch [6354/10000] Avg train loss: 0.027324\n",
      "Epoch [6355/10000] Avg train loss: 0.027319\n",
      "Epoch [6356/10000] Avg train loss: 0.027315\n",
      "Epoch [6357/10000] Avg train loss: 0.027311\n",
      "Epoch [6358/10000] Avg train loss: 0.027306\n",
      "Epoch [6359/10000] Avg train loss: 0.027302\n",
      "Epoch [6360/10000] Avg train loss: 0.027298\n",
      "Epoch [6361/10000] Avg train loss: 0.027293\n",
      "Epoch [6362/10000] Avg train loss: 0.027289\n",
      "Epoch [6363/10000] Avg train loss: 0.027285\n",
      "Epoch [6364/10000] Avg train loss: 0.027281\n",
      "Epoch [6365/10000] Avg train loss: 0.027276\n",
      "Epoch [6366/10000] Avg train loss: 0.027272\n",
      "Epoch [6367/10000] Avg train loss: 0.027268\n",
      "Epoch [6368/10000] Avg train loss: 0.027263\n",
      "Epoch [6369/10000] Avg train loss: 0.027259\n",
      "Epoch [6370/10000] Avg train loss: 0.027255\n",
      "Epoch [6371/10000] Avg train loss: 0.027251\n",
      "Epoch [6372/10000] Avg train loss: 0.027246\n",
      "Epoch [6373/10000] Avg train loss: 0.027242\n",
      "Epoch [6374/10000] Avg train loss: 0.027238\n",
      "Epoch [6375/10000] Avg train loss: 0.027233\n",
      "Epoch [6376/10000] Avg train loss: 0.027229\n",
      "Epoch [6377/10000] Avg train loss: 0.027225\n",
      "Epoch [6378/10000] Avg train loss: 0.027221\n",
      "Epoch [6379/10000] Avg train loss: 0.027216\n",
      "Epoch [6380/10000] Avg train loss: 0.027212\n",
      "Epoch [6381/10000] Avg train loss: 0.027208\n",
      "Epoch [6382/10000] Avg train loss: 0.027204\n",
      "Epoch [6383/10000] Avg train loss: 0.027199\n",
      "Epoch [6384/10000] Avg train loss: 0.027195\n",
      "Epoch [6385/10000] Avg train loss: 0.027191\n",
      "Epoch [6386/10000] Avg train loss: 0.027187\n",
      "Epoch [6387/10000] Avg train loss: 0.027182\n",
      "Epoch [6388/10000] Avg train loss: 0.027178\n",
      "Epoch [6389/10000] Avg train loss: 0.027174\n",
      "Epoch [6390/10000] Avg train loss: 0.027170\n",
      "Epoch [6391/10000] Avg train loss: 0.027165\n",
      "Epoch [6392/10000] Avg train loss: 0.027161\n",
      "Epoch [6393/10000] Avg train loss: 0.027157\n",
      "Epoch [6394/10000] Avg train loss: 0.027153\n",
      "Epoch [6395/10000] Avg train loss: 0.027148\n",
      "Epoch [6396/10000] Avg train loss: 0.027144\n",
      "Epoch [6397/10000] Avg train loss: 0.027140\n",
      "Epoch [6398/10000] Avg train loss: 0.027136\n",
      "Epoch [6399/10000] Avg train loss: 0.027131\n",
      "Epoch [6400/10000] Avg train loss: 0.027127\n",
      "Epoch [6401/10000] Avg train loss: 0.027123\n",
      "Epoch [6402/10000] Avg train loss: 0.027119\n",
      "Epoch [6403/10000] Avg train loss: 0.027114\n",
      "Epoch [6404/10000] Avg train loss: 0.027110\n",
      "Epoch [6405/10000] Avg train loss: 0.027106\n",
      "Epoch [6406/10000] Avg train loss: 0.027102\n",
      "Epoch [6407/10000] Avg train loss: 0.027097\n",
      "Epoch [6408/10000] Avg train loss: 0.027093\n",
      "Epoch [6409/10000] Avg train loss: 0.027089\n",
      "Epoch [6410/10000] Avg train loss: 0.027085\n",
      "Epoch [6411/10000] Avg train loss: 0.027081\n",
      "Epoch [6412/10000] Avg train loss: 0.027076\n",
      "Epoch [6413/10000] Avg train loss: 0.027072\n",
      "Epoch [6414/10000] Avg train loss: 0.027068\n",
      "Epoch [6415/10000] Avg train loss: 0.027064\n",
      "Epoch [6416/10000] Avg train loss: 0.027059\n",
      "Epoch [6417/10000] Avg train loss: 0.027055\n",
      "Epoch [6418/10000] Avg train loss: 0.027051\n",
      "Epoch [6419/10000] Avg train loss: 0.027047\n",
      "Epoch [6420/10000] Avg train loss: 0.027043\n",
      "Epoch [6421/10000] Avg train loss: 0.027038\n",
      "Epoch [6422/10000] Avg train loss: 0.027034\n",
      "Epoch [6423/10000] Avg train loss: 0.027030\n",
      "Epoch [6424/10000] Avg train loss: 0.027026\n",
      "Epoch [6425/10000] Avg train loss: 0.027022\n",
      "Epoch [6426/10000] Avg train loss: 0.027017\n",
      "Epoch [6427/10000] Avg train loss: 0.027013\n",
      "Epoch [6428/10000] Avg train loss: 0.027009\n",
      "Epoch [6429/10000] Avg train loss: 0.027005\n",
      "Epoch [6430/10000] Avg train loss: 0.027001\n",
      "Epoch [6431/10000] Avg train loss: 0.026996\n",
      "Epoch [6432/10000] Avg train loss: 0.026992\n",
      "Epoch [6433/10000] Avg train loss: 0.026988\n",
      "Epoch [6434/10000] Avg train loss: 0.026984\n",
      "Epoch [6435/10000] Avg train loss: 0.026980\n",
      "Epoch [6436/10000] Avg train loss: 0.026975\n",
      "Epoch [6437/10000] Avg train loss: 0.026971\n",
      "Epoch [6438/10000] Avg train loss: 0.026967\n",
      "Epoch [6439/10000] Avg train loss: 0.026963\n",
      "Epoch [6440/10000] Avg train loss: 0.026959\n",
      "Epoch [6441/10000] Avg train loss: 0.026954\n",
      "Epoch [6442/10000] Avg train loss: 0.026950\n",
      "Epoch [6443/10000] Avg train loss: 0.026946\n",
      "Epoch [6444/10000] Avg train loss: 0.026942\n",
      "Epoch [6445/10000] Avg train loss: 0.026938\n",
      "Epoch [6446/10000] Avg train loss: 0.026934\n",
      "Epoch [6447/10000] Avg train loss: 0.026929\n",
      "Epoch [6448/10000] Avg train loss: 0.026925\n",
      "Epoch [6449/10000] Avg train loss: 0.026921\n",
      "Epoch [6450/10000] Avg train loss: 0.026917\n",
      "Epoch [6451/10000] Avg train loss: 0.026913\n",
      "Epoch [6452/10000] Avg train loss: 0.026908\n",
      "Epoch [6453/10000] Avg train loss: 0.026904\n",
      "Epoch [6454/10000] Avg train loss: 0.026900\n",
      "Epoch [6455/10000] Avg train loss: 0.026896\n",
      "Epoch [6456/10000] Avg train loss: 0.026892\n",
      "Epoch [6457/10000] Avg train loss: 0.026888\n",
      "Epoch [6458/10000] Avg train loss: 0.026883\n",
      "Epoch [6459/10000] Avg train loss: 0.026879\n",
      "Epoch [6460/10000] Avg train loss: 0.026875\n",
      "Epoch [6461/10000] Avg train loss: 0.026871\n",
      "Epoch [6462/10000] Avg train loss: 0.026867\n",
      "Epoch [6463/10000] Avg train loss: 0.026863\n",
      "Epoch [6464/10000] Avg train loss: 0.026859\n",
      "Epoch [6465/10000] Avg train loss: 0.026854\n",
      "Epoch [6466/10000] Avg train loss: 0.026850\n",
      "Epoch [6467/10000] Avg train loss: 0.026846\n",
      "Epoch [6468/10000] Avg train loss: 0.026842\n",
      "Epoch [6469/10000] Avg train loss: 0.026838\n",
      "Epoch [6470/10000] Avg train loss: 0.026834\n",
      "Epoch [6471/10000] Avg train loss: 0.026829\n",
      "Epoch [6472/10000] Avg train loss: 0.026825\n",
      "Epoch [6473/10000] Avg train loss: 0.026821\n",
      "Epoch [6474/10000] Avg train loss: 0.026817\n",
      "Epoch [6475/10000] Avg train loss: 0.026813\n",
      "Epoch [6476/10000] Avg train loss: 0.026809\n",
      "Epoch [6477/10000] Avg train loss: 0.026805\n",
      "Epoch [6478/10000] Avg train loss: 0.026800\n",
      "Epoch [6479/10000] Avg train loss: 0.026796\n",
      "Epoch [6480/10000] Avg train loss: 0.026792\n",
      "Epoch [6481/10000] Avg train loss: 0.026788\n",
      "Epoch [6482/10000] Avg train loss: 0.026784\n",
      "Epoch [6483/10000] Avg train loss: 0.026780\n",
      "Epoch [6484/10000] Avg train loss: 0.026776\n",
      "Epoch [6485/10000] Avg train loss: 0.026772\n",
      "Epoch [6486/10000] Avg train loss: 0.026767\n",
      "Epoch [6487/10000] Avg train loss: 0.026763\n",
      "Epoch [6488/10000] Avg train loss: 0.026759\n",
      "Epoch [6489/10000] Avg train loss: 0.026755\n",
      "Epoch [6490/10000] Avg train loss: 0.026751\n",
      "Epoch [6491/10000] Avg train loss: 0.026747\n",
      "Epoch [6492/10000] Avg train loss: 0.026743\n",
      "Epoch [6493/10000] Avg train loss: 0.026739\n",
      "Epoch [6494/10000] Avg train loss: 0.026734\n",
      "Epoch [6495/10000] Avg train loss: 0.026730\n",
      "Epoch [6496/10000] Avg train loss: 0.026726\n",
      "Epoch [6497/10000] Avg train loss: 0.026722\n",
      "Epoch [6498/10000] Avg train loss: 0.026718\n",
      "Epoch [6499/10000] Avg train loss: 0.026714\n",
      "Epoch [6500/10000] Avg train loss: 0.026710\n",
      "Epoch [6501/10000] Avg train loss: 0.026706\n",
      "Epoch [6502/10000] Avg train loss: 0.026702\n",
      "Epoch [6503/10000] Avg train loss: 0.026697\n",
      "Epoch [6504/10000] Avg train loss: 0.026693\n",
      "Epoch [6505/10000] Avg train loss: 0.026689\n",
      "Epoch [6506/10000] Avg train loss: 0.026685\n",
      "Epoch [6507/10000] Avg train loss: 0.026681\n",
      "Epoch [6508/10000] Avg train loss: 0.026677\n",
      "Epoch [6509/10000] Avg train loss: 0.026673\n",
      "Epoch [6510/10000] Avg train loss: 0.026669\n",
      "Epoch [6511/10000] Avg train loss: 0.026665\n",
      "Epoch [6512/10000] Avg train loss: 0.026661\n",
      "Epoch [6513/10000] Avg train loss: 0.026656\n",
      "Epoch [6514/10000] Avg train loss: 0.026652\n",
      "Epoch [6515/10000] Avg train loss: 0.026648\n",
      "Epoch [6516/10000] Avg train loss: 0.026644\n",
      "Epoch [6517/10000] Avg train loss: 0.026640\n",
      "Epoch [6518/10000] Avg train loss: 0.026636\n",
      "Epoch [6519/10000] Avg train loss: 0.026632\n",
      "Epoch [6520/10000] Avg train loss: 0.026628\n",
      "Epoch [6521/10000] Avg train loss: 0.026624\n",
      "Epoch [6522/10000] Avg train loss: 0.026620\n",
      "Epoch [6523/10000] Avg train loss: 0.026616\n",
      "Epoch [6524/10000] Avg train loss: 0.026612\n",
      "Epoch [6525/10000] Avg train loss: 0.026607\n",
      "Epoch [6526/10000] Avg train loss: 0.026603\n",
      "Epoch [6527/10000] Avg train loss: 0.026599\n",
      "Epoch [6528/10000] Avg train loss: 0.026595\n",
      "Epoch [6529/10000] Avg train loss: 0.026591\n",
      "Epoch [6530/10000] Avg train loss: 0.026587\n",
      "Epoch [6531/10000] Avg train loss: 0.026583\n",
      "Epoch [6532/10000] Avg train loss: 0.026579\n",
      "Epoch [6533/10000] Avg train loss: 0.026575\n",
      "Epoch [6534/10000] Avg train loss: 0.026571\n",
      "Epoch [6535/10000] Avg train loss: 0.026567\n",
      "Epoch [6536/10000] Avg train loss: 0.026563\n",
      "Epoch [6537/10000] Avg train loss: 0.026559\n",
      "Epoch [6538/10000] Avg train loss: 0.026555\n",
      "Epoch [6539/10000] Avg train loss: 0.026550\n",
      "Epoch [6540/10000] Avg train loss: 0.026546\n",
      "Epoch [6541/10000] Avg train loss: 0.026542\n",
      "Epoch [6542/10000] Avg train loss: 0.026538\n",
      "Epoch [6543/10000] Avg train loss: 0.026534\n",
      "Epoch [6544/10000] Avg train loss: 0.026530\n",
      "Epoch [6545/10000] Avg train loss: 0.026526\n",
      "Epoch [6546/10000] Avg train loss: 0.026522\n",
      "Epoch [6547/10000] Avg train loss: 0.026518\n",
      "Epoch [6548/10000] Avg train loss: 0.026514\n",
      "Epoch [6549/10000] Avg train loss: 0.026510\n",
      "Epoch [6550/10000] Avg train loss: 0.026506\n",
      "Epoch [6551/10000] Avg train loss: 0.026502\n",
      "Epoch [6552/10000] Avg train loss: 0.026498\n",
      "Epoch [6553/10000] Avg train loss: 0.026494\n",
      "Epoch [6554/10000] Avg train loss: 0.026490\n",
      "Epoch [6555/10000] Avg train loss: 0.026486\n",
      "Epoch [6556/10000] Avg train loss: 0.026482\n",
      "Epoch [6557/10000] Avg train loss: 0.026478\n",
      "Epoch [6558/10000] Avg train loss: 0.026474\n",
      "Epoch [6559/10000] Avg train loss: 0.026470\n",
      "Epoch [6560/10000] Avg train loss: 0.026465\n",
      "Epoch [6561/10000] Avg train loss: 0.026461\n",
      "Epoch [6562/10000] Avg train loss: 0.026457\n",
      "Epoch [6563/10000] Avg train loss: 0.026453\n",
      "Epoch [6564/10000] Avg train loss: 0.026449\n",
      "Epoch [6565/10000] Avg train loss: 0.026445\n",
      "Epoch [6566/10000] Avg train loss: 0.026441\n",
      "Epoch [6567/10000] Avg train loss: 0.026437\n",
      "Epoch [6568/10000] Avg train loss: 0.026433\n",
      "Epoch [6569/10000] Avg train loss: 0.026429\n",
      "Epoch [6570/10000] Avg train loss: 0.026425\n",
      "Epoch [6571/10000] Avg train loss: 0.026421\n",
      "Epoch [6572/10000] Avg train loss: 0.026417\n",
      "Epoch [6573/10000] Avg train loss: 0.026413\n",
      "Epoch [6574/10000] Avg train loss: 0.026409\n",
      "Epoch [6575/10000] Avg train loss: 0.026405\n",
      "Epoch [6576/10000] Avg train loss: 0.026401\n",
      "Epoch [6577/10000] Avg train loss: 0.026397\n",
      "Epoch [6578/10000] Avg train loss: 0.026393\n",
      "Epoch [6579/10000] Avg train loss: 0.026389\n",
      "Epoch [6580/10000] Avg train loss: 0.026385\n",
      "Epoch [6581/10000] Avg train loss: 0.026381\n",
      "Epoch [6582/10000] Avg train loss: 0.026377\n",
      "Epoch [6583/10000] Avg train loss: 0.026373\n",
      "Epoch [6584/10000] Avg train loss: 0.026369\n",
      "Epoch [6585/10000] Avg train loss: 0.026365\n",
      "Epoch [6586/10000] Avg train loss: 0.026361\n",
      "Epoch [6587/10000] Avg train loss: 0.026357\n",
      "Epoch [6588/10000] Avg train loss: 0.026353\n",
      "Epoch [6589/10000] Avg train loss: 0.026349\n",
      "Epoch [6590/10000] Avg train loss: 0.026345\n",
      "Epoch [6591/10000] Avg train loss: 0.026341\n",
      "Epoch [6592/10000] Avg train loss: 0.026337\n",
      "Epoch [6593/10000] Avg train loss: 0.026333\n",
      "Epoch [6594/10000] Avg train loss: 0.026329\n",
      "Epoch [6595/10000] Avg train loss: 0.026325\n",
      "Epoch [6596/10000] Avg train loss: 0.026321\n",
      "Epoch [6597/10000] Avg train loss: 0.026317\n",
      "Epoch [6598/10000] Avg train loss: 0.026313\n",
      "Epoch [6599/10000] Avg train loss: 0.026309\n",
      "Epoch [6600/10000] Avg train loss: 0.026305\n",
      "Epoch [6601/10000] Avg train loss: 0.026301\n",
      "Epoch [6602/10000] Avg train loss: 0.026297\n",
      "Epoch [6603/10000] Avg train loss: 0.026293\n",
      "Epoch [6604/10000] Avg train loss: 0.026289\n",
      "Epoch [6605/10000] Avg train loss: 0.026285\n",
      "Epoch [6606/10000] Avg train loss: 0.026281\n",
      "Epoch [6607/10000] Avg train loss: 0.026277\n",
      "Epoch [6608/10000] Avg train loss: 0.026273\n",
      "Epoch [6609/10000] Avg train loss: 0.026269\n",
      "Epoch [6610/10000] Avg train loss: 0.026265\n",
      "Epoch [6611/10000] Avg train loss: 0.026261\n",
      "Epoch [6612/10000] Avg train loss: 0.026257\n",
      "Epoch [6613/10000] Avg train loss: 0.026253\n",
      "Epoch [6614/10000] Avg train loss: 0.026249\n",
      "Epoch [6615/10000] Avg train loss: 0.026245\n",
      "Epoch [6616/10000] Avg train loss: 0.026241\n",
      "Epoch [6617/10000] Avg train loss: 0.026237\n",
      "Epoch [6618/10000] Avg train loss: 0.026234\n",
      "Epoch [6619/10000] Avg train loss: 0.026230\n",
      "Epoch [6620/10000] Avg train loss: 0.026226\n",
      "Epoch [6621/10000] Avg train loss: 0.026222\n",
      "Epoch [6622/10000] Avg train loss: 0.026218\n",
      "Epoch [6623/10000] Avg train loss: 0.026214\n",
      "Epoch [6624/10000] Avg train loss: 0.026210\n",
      "Epoch [6625/10000] Avg train loss: 0.026206\n",
      "Epoch [6626/10000] Avg train loss: 0.026202\n",
      "Epoch [6627/10000] Avg train loss: 0.026198\n",
      "Epoch [6628/10000] Avg train loss: 0.026194\n",
      "Epoch [6629/10000] Avg train loss: 0.026190\n",
      "Epoch [6630/10000] Avg train loss: 0.026186\n",
      "Epoch [6631/10000] Avg train loss: 0.026182\n",
      "Epoch [6632/10000] Avg train loss: 0.026178\n",
      "Epoch [6633/10000] Avg train loss: 0.026174\n",
      "Epoch [6634/10000] Avg train loss: 0.026170\n",
      "Epoch [6635/10000] Avg train loss: 0.026166\n",
      "Epoch [6636/10000] Avg train loss: 0.026162\n",
      "Epoch [6637/10000] Avg train loss: 0.026158\n",
      "Epoch [6638/10000] Avg train loss: 0.026154\n",
      "Epoch [6639/10000] Avg train loss: 0.026151\n",
      "Epoch [6640/10000] Avg train loss: 0.026147\n",
      "Epoch [6641/10000] Avg train loss: 0.026143\n",
      "Epoch [6642/10000] Avg train loss: 0.026139\n",
      "Epoch [6643/10000] Avg train loss: 0.026135\n",
      "Epoch [6644/10000] Avg train loss: 0.026131\n",
      "Epoch [6645/10000] Avg train loss: 0.026127\n",
      "Epoch [6646/10000] Avg train loss: 0.026123\n",
      "Epoch [6647/10000] Avg train loss: 0.026119\n",
      "Epoch [6648/10000] Avg train loss: 0.026115\n",
      "Epoch [6649/10000] Avg train loss: 0.026111\n",
      "Epoch [6650/10000] Avg train loss: 0.026107\n",
      "Epoch [6651/10000] Avg train loss: 0.026103\n",
      "Epoch [6652/10000] Avg train loss: 0.026099\n",
      "Epoch [6653/10000] Avg train loss: 0.026096\n",
      "Epoch [6654/10000] Avg train loss: 0.026092\n",
      "Epoch [6655/10000] Avg train loss: 0.026088\n",
      "Epoch [6656/10000] Avg train loss: 0.026084\n",
      "Epoch [6657/10000] Avg train loss: 0.026080\n",
      "Epoch [6658/10000] Avg train loss: 0.026076\n",
      "Epoch [6659/10000] Avg train loss: 0.026072\n",
      "Epoch [6660/10000] Avg train loss: 0.026068\n",
      "Epoch [6661/10000] Avg train loss: 0.026064\n",
      "Epoch [6662/10000] Avg train loss: 0.026060\n",
      "Epoch [6663/10000] Avg train loss: 0.026056\n",
      "Epoch [6664/10000] Avg train loss: 0.026052\n",
      "Epoch [6665/10000] Avg train loss: 0.026049\n",
      "Epoch [6666/10000] Avg train loss: 0.026045\n",
      "Epoch [6667/10000] Avg train loss: 0.026041\n",
      "Epoch [6668/10000] Avg train loss: 0.026037\n",
      "Epoch [6669/10000] Avg train loss: 0.026033\n",
      "Epoch [6670/10000] Avg train loss: 0.026029\n",
      "Epoch [6671/10000] Avg train loss: 0.026025\n",
      "Epoch [6672/10000] Avg train loss: 0.026021\n",
      "Epoch [6673/10000] Avg train loss: 0.026017\n",
      "Epoch [6674/10000] Avg train loss: 0.026013\n",
      "Epoch [6675/10000] Avg train loss: 0.026010\n",
      "Epoch [6676/10000] Avg train loss: 0.026006\n",
      "Epoch [6677/10000] Avg train loss: 0.026002\n",
      "Epoch [6678/10000] Avg train loss: 0.025998\n",
      "Epoch [6679/10000] Avg train loss: 0.025994\n",
      "Epoch [6680/10000] Avg train loss: 0.025990\n",
      "Epoch [6681/10000] Avg train loss: 0.025986\n",
      "Epoch [6682/10000] Avg train loss: 0.025982\n",
      "Epoch [6683/10000] Avg train loss: 0.025978\n",
      "Epoch [6684/10000] Avg train loss: 0.025974\n",
      "Epoch [6685/10000] Avg train loss: 0.025971\n",
      "Epoch [6686/10000] Avg train loss: 0.025967\n",
      "Epoch [6687/10000] Avg train loss: 0.025963\n",
      "Epoch [6688/10000] Avg train loss: 0.025959\n",
      "Epoch [6689/10000] Avg train loss: 0.025955\n",
      "Epoch [6690/10000] Avg train loss: 0.025951\n",
      "Epoch [6691/10000] Avg train loss: 0.025947\n",
      "Epoch [6692/10000] Avg train loss: 0.025943\n",
      "Epoch [6693/10000] Avg train loss: 0.025940\n",
      "Epoch [6694/10000] Avg train loss: 0.025936\n",
      "Epoch [6695/10000] Avg train loss: 0.025932\n",
      "Epoch [6696/10000] Avg train loss: 0.025928\n",
      "Epoch [6697/10000] Avg train loss: 0.025924\n",
      "Epoch [6698/10000] Avg train loss: 0.025920\n",
      "Epoch [6699/10000] Avg train loss: 0.025916\n",
      "Epoch [6700/10000] Avg train loss: 0.025912\n",
      "Epoch [6701/10000] Avg train loss: 0.025909\n",
      "Epoch [6702/10000] Avg train loss: 0.025905\n",
      "Epoch [6703/10000] Avg train loss: 0.025901\n",
      "Epoch [6704/10000] Avg train loss: 0.025897\n",
      "Epoch [6705/10000] Avg train loss: 0.025893\n",
      "Epoch [6706/10000] Avg train loss: 0.025889\n",
      "Epoch [6707/10000] Avg train loss: 0.025885\n",
      "Epoch [6708/10000] Avg train loss: 0.025882\n",
      "Epoch [6709/10000] Avg train loss: 0.025878\n",
      "Epoch [6710/10000] Avg train loss: 0.025874\n",
      "Epoch [6711/10000] Avg train loss: 0.025870\n",
      "Epoch [6712/10000] Avg train loss: 0.025866\n",
      "Epoch [6713/10000] Avg train loss: 0.025862\n",
      "Epoch [6714/10000] Avg train loss: 0.025858\n",
      "Epoch [6715/10000] Avg train loss: 0.025855\n",
      "Epoch [6716/10000] Avg train loss: 0.025851\n",
      "Epoch [6717/10000] Avg train loss: 0.025847\n",
      "Epoch [6718/10000] Avg train loss: 0.025843\n",
      "Epoch [6719/10000] Avg train loss: 0.025839\n",
      "Epoch [6720/10000] Avg train loss: 0.025835\n",
      "Epoch [6721/10000] Avg train loss: 0.025832\n",
      "Epoch [6722/10000] Avg train loss: 0.025828\n",
      "Epoch [6723/10000] Avg train loss: 0.025824\n",
      "Epoch [6724/10000] Avg train loss: 0.025820\n",
      "Epoch [6725/10000] Avg train loss: 0.025816\n",
      "Epoch [6726/10000] Avg train loss: 0.025812\n",
      "Epoch [6727/10000] Avg train loss: 0.025808\n",
      "Epoch [6728/10000] Avg train loss: 0.025805\n",
      "Epoch [6729/10000] Avg train loss: 0.025801\n",
      "Epoch [6730/10000] Avg train loss: 0.025797\n",
      "Epoch [6731/10000] Avg train loss: 0.025793\n",
      "Epoch [6732/10000] Avg train loss: 0.025789\n",
      "Epoch [6733/10000] Avg train loss: 0.025785\n",
      "Epoch [6734/10000] Avg train loss: 0.025782\n",
      "Epoch [6735/10000] Avg train loss: 0.025778\n",
      "Epoch [6736/10000] Avg train loss: 0.025774\n",
      "Epoch [6737/10000] Avg train loss: 0.025770\n",
      "Epoch [6738/10000] Avg train loss: 0.025766\n",
      "Epoch [6739/10000] Avg train loss: 0.025763\n",
      "Epoch [6740/10000] Avg train loss: 0.025759\n",
      "Epoch [6741/10000] Avg train loss: 0.025755\n",
      "Epoch [6742/10000] Avg train loss: 0.025751\n",
      "Epoch [6743/10000] Avg train loss: 0.025747\n",
      "Epoch [6744/10000] Avg train loss: 0.025743\n",
      "Epoch [6745/10000] Avg train loss: 0.025740\n",
      "Epoch [6746/10000] Avg train loss: 0.025736\n",
      "Epoch [6747/10000] Avg train loss: 0.025732\n",
      "Epoch [6748/10000] Avg train loss: 0.025728\n",
      "Epoch [6749/10000] Avg train loss: 0.025724\n",
      "Epoch [6750/10000] Avg train loss: 0.025721\n",
      "Epoch [6751/10000] Avg train loss: 0.025717\n",
      "Epoch [6752/10000] Avg train loss: 0.025713\n",
      "Epoch [6753/10000] Avg train loss: 0.025709\n",
      "Epoch [6754/10000] Avg train loss: 0.025705\n",
      "Epoch [6755/10000] Avg train loss: 0.025701\n",
      "Epoch [6756/10000] Avg train loss: 0.025698\n",
      "Epoch [6757/10000] Avg train loss: 0.025694\n",
      "Epoch [6758/10000] Avg train loss: 0.025690\n",
      "Epoch [6759/10000] Avg train loss: 0.025686\n",
      "Epoch [6760/10000] Avg train loss: 0.025682\n",
      "Epoch [6761/10000] Avg train loss: 0.025679\n",
      "Epoch [6762/10000] Avg train loss: 0.025675\n",
      "Epoch [6763/10000] Avg train loss: 0.025671\n",
      "Epoch [6764/10000] Avg train loss: 0.025667\n",
      "Epoch [6765/10000] Avg train loss: 0.025663\n",
      "Epoch [6766/10000] Avg train loss: 0.025660\n",
      "Epoch [6767/10000] Avg train loss: 0.025656\n",
      "Epoch [6768/10000] Avg train loss: 0.025652\n",
      "Epoch [6769/10000] Avg train loss: 0.025648\n",
      "Epoch [6770/10000] Avg train loss: 0.025645\n",
      "Epoch [6771/10000] Avg train loss: 0.025641\n",
      "Epoch [6772/10000] Avg train loss: 0.025637\n",
      "Epoch [6773/10000] Avg train loss: 0.025633\n",
      "Epoch [6774/10000] Avg train loss: 0.025629\n",
      "Epoch [6775/10000] Avg train loss: 0.025626\n",
      "Epoch [6776/10000] Avg train loss: 0.025622\n",
      "Epoch [6777/10000] Avg train loss: 0.025618\n",
      "Epoch [6778/10000] Avg train loss: 0.025614\n",
      "Epoch [6779/10000] Avg train loss: 0.025610\n",
      "Epoch [6780/10000] Avg train loss: 0.025607\n",
      "Epoch [6781/10000] Avg train loss: 0.025603\n",
      "Epoch [6782/10000] Avg train loss: 0.025599\n",
      "Epoch [6783/10000] Avg train loss: 0.025595\n",
      "Epoch [6784/10000] Avg train loss: 0.025592\n",
      "Epoch [6785/10000] Avg train loss: 0.025588\n",
      "Epoch [6786/10000] Avg train loss: 0.025584\n",
      "Epoch [6787/10000] Avg train loss: 0.025580\n",
      "Epoch [6788/10000] Avg train loss: 0.025577\n",
      "Epoch [6789/10000] Avg train loss: 0.025573\n",
      "Epoch [6790/10000] Avg train loss: 0.025569\n",
      "Epoch [6791/10000] Avg train loss: 0.025565\n",
      "Epoch [6792/10000] Avg train loss: 0.025561\n",
      "Epoch [6793/10000] Avg train loss: 0.025558\n",
      "Epoch [6794/10000] Avg train loss: 0.025554\n",
      "Epoch [6795/10000] Avg train loss: 0.025550\n",
      "Epoch [6796/10000] Avg train loss: 0.025546\n",
      "Epoch [6797/10000] Avg train loss: 0.025543\n",
      "Epoch [6798/10000] Avg train loss: 0.025539\n",
      "Epoch [6799/10000] Avg train loss: 0.025535\n",
      "Epoch [6800/10000] Avg train loss: 0.025531\n",
      "Epoch [6801/10000] Avg train loss: 0.025528\n",
      "Epoch [6802/10000] Avg train loss: 0.025524\n",
      "Epoch [6803/10000] Avg train loss: 0.025520\n",
      "Epoch [6804/10000] Avg train loss: 0.025516\n",
      "Epoch [6805/10000] Avg train loss: 0.025513\n",
      "Epoch [6806/10000] Avg train loss: 0.025509\n",
      "Epoch [6807/10000] Avg train loss: 0.025505\n",
      "Epoch [6808/10000] Avg train loss: 0.025501\n",
      "Epoch [6809/10000] Avg train loss: 0.025498\n",
      "Epoch [6810/10000] Avg train loss: 0.025494\n",
      "Epoch [6811/10000] Avg train loss: 0.025490\n",
      "Epoch [6812/10000] Avg train loss: 0.025486\n",
      "Epoch [6813/10000] Avg train loss: 0.025483\n",
      "Epoch [6814/10000] Avg train loss: 0.025479\n",
      "Epoch [6815/10000] Avg train loss: 0.025475\n",
      "Epoch [6816/10000] Avg train loss: 0.025471\n",
      "Epoch [6817/10000] Avg train loss: 0.025468\n",
      "Epoch [6818/10000] Avg train loss: 0.025464\n",
      "Epoch [6819/10000] Avg train loss: 0.025460\n",
      "Epoch [6820/10000] Avg train loss: 0.025457\n",
      "Epoch [6821/10000] Avg train loss: 0.025453\n",
      "Epoch [6822/10000] Avg train loss: 0.025449\n",
      "Epoch [6823/10000] Avg train loss: 0.025445\n",
      "Epoch [6824/10000] Avg train loss: 0.025442\n",
      "Epoch [6825/10000] Avg train loss: 0.025438\n",
      "Epoch [6826/10000] Avg train loss: 0.025434\n",
      "Epoch [6827/10000] Avg train loss: 0.025430\n",
      "Epoch [6828/10000] Avg train loss: 0.025427\n",
      "Epoch [6829/10000] Avg train loss: 0.025423\n",
      "Epoch [6830/10000] Avg train loss: 0.025419\n",
      "Epoch [6831/10000] Avg train loss: 0.025416\n",
      "Epoch [6832/10000] Avg train loss: 0.025412\n",
      "Epoch [6833/10000] Avg train loss: 0.025408\n",
      "Epoch [6834/10000] Avg train loss: 0.025404\n",
      "Epoch [6835/10000] Avg train loss: 0.025401\n",
      "Epoch [6836/10000] Avg train loss: 0.025397\n",
      "Epoch [6837/10000] Avg train loss: 0.025393\n",
      "Epoch [6838/10000] Avg train loss: 0.025390\n",
      "Epoch [6839/10000] Avg train loss: 0.025386\n",
      "Epoch [6840/10000] Avg train loss: 0.025382\n",
      "Epoch [6841/10000] Avg train loss: 0.025378\n",
      "Epoch [6842/10000] Avg train loss: 0.025375\n",
      "Epoch [6843/10000] Avg train loss: 0.025371\n",
      "Epoch [6844/10000] Avg train loss: 0.025367\n",
      "Epoch [6845/10000] Avg train loss: 0.025364\n",
      "Epoch [6846/10000] Avg train loss: 0.025360\n",
      "Epoch [6847/10000] Avg train loss: 0.025356\n",
      "Epoch [6848/10000] Avg train loss: 0.025352\n",
      "Epoch [6849/10000] Avg train loss: 0.025349\n",
      "Epoch [6850/10000] Avg train loss: 0.025345\n",
      "Epoch [6851/10000] Avg train loss: 0.025341\n",
      "Epoch [6852/10000] Avg train loss: 0.025338\n",
      "Epoch [6853/10000] Avg train loss: 0.025334\n",
      "Epoch [6854/10000] Avg train loss: 0.025330\n",
      "Epoch [6855/10000] Avg train loss: 0.025327\n",
      "Epoch [6856/10000] Avg train loss: 0.025323\n",
      "Epoch [6857/10000] Avg train loss: 0.025319\n",
      "Epoch [6858/10000] Avg train loss: 0.025315\n",
      "Epoch [6859/10000] Avg train loss: 0.025312\n",
      "Epoch [6860/10000] Avg train loss: 0.025308\n",
      "Epoch [6861/10000] Avg train loss: 0.025304\n",
      "Epoch [6862/10000] Avg train loss: 0.025301\n",
      "Epoch [6863/10000] Avg train loss: 0.025297\n",
      "Epoch [6864/10000] Avg train loss: 0.025293\n",
      "Epoch [6865/10000] Avg train loss: 0.025290\n",
      "Epoch [6866/10000] Avg train loss: 0.025286\n",
      "Epoch [6867/10000] Avg train loss: 0.025282\n",
      "Epoch [6868/10000] Avg train loss: 0.025279\n",
      "Epoch [6869/10000] Avg train loss: 0.025275\n",
      "Epoch [6870/10000] Avg train loss: 0.025271\n",
      "Epoch [6871/10000] Avg train loss: 0.025268\n",
      "Epoch [6872/10000] Avg train loss: 0.025264\n",
      "Epoch [6873/10000] Avg train loss: 0.025260\n",
      "Epoch [6874/10000] Avg train loss: 0.025257\n",
      "Epoch [6875/10000] Avg train loss: 0.025253\n",
      "Epoch [6876/10000] Avg train loss: 0.025249\n",
      "Epoch [6877/10000] Avg train loss: 0.025246\n",
      "Epoch [6878/10000] Avg train loss: 0.025242\n",
      "Epoch [6879/10000] Avg train loss: 0.025238\n",
      "Epoch [6880/10000] Avg train loss: 0.025235\n",
      "Epoch [6881/10000] Avg train loss: 0.025231\n",
      "Epoch [6882/10000] Avg train loss: 0.025227\n",
      "Epoch [6883/10000] Avg train loss: 0.025224\n",
      "Epoch [6884/10000] Avg train loss: 0.025220\n",
      "Epoch [6885/10000] Avg train loss: 0.025216\n",
      "Epoch [6886/10000] Avg train loss: 0.025213\n",
      "Epoch [6887/10000] Avg train loss: 0.025209\n",
      "Epoch [6888/10000] Avg train loss: 0.025205\n",
      "Epoch [6889/10000] Avg train loss: 0.025202\n",
      "Epoch [6890/10000] Avg train loss: 0.025198\n",
      "Epoch [6891/10000] Avg train loss: 0.025194\n",
      "Epoch [6892/10000] Avg train loss: 0.025191\n",
      "Epoch [6893/10000] Avg train loss: 0.025187\n",
      "Epoch [6894/10000] Avg train loss: 0.025183\n",
      "Epoch [6895/10000] Avg train loss: 0.025180\n",
      "Epoch [6896/10000] Avg train loss: 0.025176\n",
      "Epoch [6897/10000] Avg train loss: 0.025172\n",
      "Epoch [6898/10000] Avg train loss: 0.025169\n",
      "Epoch [6899/10000] Avg train loss: 0.025165\n",
      "Epoch [6900/10000] Avg train loss: 0.025161\n",
      "Epoch [6901/10000] Avg train loss: 0.025158\n",
      "Epoch [6902/10000] Avg train loss: 0.025154\n",
      "Epoch [6903/10000] Avg train loss: 0.025150\n",
      "Epoch [6904/10000] Avg train loss: 0.025147\n",
      "Epoch [6905/10000] Avg train loss: 0.025143\n",
      "Epoch [6906/10000] Avg train loss: 0.025140\n",
      "Epoch [6907/10000] Avg train loss: 0.025136\n",
      "Epoch [6908/10000] Avg train loss: 0.025132\n",
      "Epoch [6909/10000] Avg train loss: 0.025129\n",
      "Epoch [6910/10000] Avg train loss: 0.025125\n",
      "Epoch [6911/10000] Avg train loss: 0.025121\n",
      "Epoch [6912/10000] Avg train loss: 0.025118\n",
      "Epoch [6913/10000] Avg train loss: 0.025114\n",
      "Epoch [6914/10000] Avg train loss: 0.025110\n",
      "Epoch [6915/10000] Avg train loss: 0.025107\n",
      "Epoch [6916/10000] Avg train loss: 0.025103\n",
      "Epoch [6917/10000] Avg train loss: 0.025100\n",
      "Epoch [6918/10000] Avg train loss: 0.025096\n",
      "Epoch [6919/10000] Avg train loss: 0.025092\n",
      "Epoch [6920/10000] Avg train loss: 0.025089\n",
      "Epoch [6921/10000] Avg train loss: 0.025085\n",
      "Epoch [6922/10000] Avg train loss: 0.025081\n",
      "Epoch [6923/10000] Avg train loss: 0.025078\n",
      "Epoch [6924/10000] Avg train loss: 0.025074\n",
      "Epoch [6925/10000] Avg train loss: 0.025071\n",
      "Epoch [6926/10000] Avg train loss: 0.025067\n",
      "Epoch [6927/10000] Avg train loss: 0.025063\n",
      "Epoch [6928/10000] Avg train loss: 0.025060\n",
      "Epoch [6929/10000] Avg train loss: 0.025056\n",
      "Epoch [6930/10000] Avg train loss: 0.025052\n",
      "Epoch [6931/10000] Avg train loss: 0.025049\n",
      "Epoch [6932/10000] Avg train loss: 0.025045\n",
      "Epoch [6933/10000] Avg train loss: 0.025042\n",
      "Epoch [6934/10000] Avg train loss: 0.025038\n",
      "Epoch [6935/10000] Avg train loss: 0.025034\n",
      "Epoch [6936/10000] Avg train loss: 0.025031\n",
      "Epoch [6937/10000] Avg train loss: 0.025027\n",
      "Epoch [6938/10000] Avg train loss: 0.025024\n",
      "Epoch [6939/10000] Avg train loss: 0.025020\n",
      "Epoch [6940/10000] Avg train loss: 0.025016\n",
      "Epoch [6941/10000] Avg train loss: 0.025013\n",
      "Epoch [6942/10000] Avg train loss: 0.025009\n",
      "Epoch [6943/10000] Avg train loss: 0.025006\n",
      "Epoch [6944/10000] Avg train loss: 0.025002\n",
      "Epoch [6945/10000] Avg train loss: 0.024998\n",
      "Epoch [6946/10000] Avg train loss: 0.024995\n",
      "Epoch [6947/10000] Avg train loss: 0.024991\n",
      "Epoch [6948/10000] Avg train loss: 0.024988\n",
      "Epoch [6949/10000] Avg train loss: 0.024984\n",
      "Epoch [6950/10000] Avg train loss: 0.024980\n",
      "Epoch [6951/10000] Avg train loss: 0.024977\n",
      "Epoch [6952/10000] Avg train loss: 0.024973\n",
      "Epoch [6953/10000] Avg train loss: 0.024970\n",
      "Epoch [6954/10000] Avg train loss: 0.024966\n",
      "Epoch [6955/10000] Avg train loss: 0.024962\n",
      "Epoch [6956/10000] Avg train loss: 0.024959\n",
      "Epoch [6957/10000] Avg train loss: 0.024955\n",
      "Epoch [6958/10000] Avg train loss: 0.024952\n",
      "Epoch [6959/10000] Avg train loss: 0.024948\n",
      "Epoch [6960/10000] Avg train loss: 0.024944\n",
      "Epoch [6961/10000] Avg train loss: 0.024941\n",
      "Epoch [6962/10000] Avg train loss: 0.024937\n",
      "Epoch [6963/10000] Avg train loss: 0.024934\n",
      "Epoch [6964/10000] Avg train loss: 0.024930\n",
      "Epoch [6965/10000] Avg train loss: 0.024927\n",
      "Epoch [6966/10000] Avg train loss: 0.024923\n",
      "Epoch [6967/10000] Avg train loss: 0.024919\n",
      "Epoch [6968/10000] Avg train loss: 0.024916\n",
      "Epoch [6969/10000] Avg train loss: 0.024912\n",
      "Epoch [6970/10000] Avg train loss: 0.024909\n",
      "Epoch [6971/10000] Avg train loss: 0.024905\n",
      "Epoch [6972/10000] Avg train loss: 0.024902\n",
      "Epoch [6973/10000] Avg train loss: 0.024898\n",
      "Epoch [6974/10000] Avg train loss: 0.024894\n",
      "Epoch [6975/10000] Avg train loss: 0.024891\n",
      "Epoch [6976/10000] Avg train loss: 0.024887\n",
      "Epoch [6977/10000] Avg train loss: 0.024884\n",
      "Epoch [6978/10000] Avg train loss: 0.024880\n",
      "Epoch [6979/10000] Avg train loss: 0.024877\n",
      "Epoch [6980/10000] Avg train loss: 0.024873\n",
      "Epoch [6981/10000] Avg train loss: 0.024869\n",
      "Epoch [6982/10000] Avg train loss: 0.024866\n",
      "Epoch [6983/10000] Avg train loss: 0.024862\n",
      "Epoch [6984/10000] Avg train loss: 0.024859\n",
      "Epoch [6985/10000] Avg train loss: 0.024855\n",
      "Epoch [6986/10000] Avg train loss: 0.024852\n",
      "Epoch [6987/10000] Avg train loss: 0.024848\n",
      "Epoch [6988/10000] Avg train loss: 0.024845\n",
      "Epoch [6989/10000] Avg train loss: 0.024841\n",
      "Epoch [6990/10000] Avg train loss: 0.024837\n",
      "Epoch [6991/10000] Avg train loss: 0.024834\n",
      "Epoch [6992/10000] Avg train loss: 0.024830\n",
      "Epoch [6993/10000] Avg train loss: 0.024827\n",
      "Epoch [6994/10000] Avg train loss: 0.024823\n",
      "Epoch [6995/10000] Avg train loss: 0.024820\n",
      "Epoch [6996/10000] Avg train loss: 0.024816\n",
      "Epoch [6997/10000] Avg train loss: 0.024813\n",
      "Epoch [6998/10000] Avg train loss: 0.024809\n",
      "Epoch [6999/10000] Avg train loss: 0.024805\n",
      "Epoch [7000/10000] Avg train loss: 0.024802\n",
      "Epoch [7001/10000] Avg train loss: 0.024798\n",
      "Epoch [7002/10000] Avg train loss: 0.024795\n",
      "Epoch [7003/10000] Avg train loss: 0.024791\n",
      "Epoch [7004/10000] Avg train loss: 0.024788\n",
      "Epoch [7005/10000] Avg train loss: 0.024784\n",
      "Epoch [7006/10000] Avg train loss: 0.024781\n",
      "Epoch [7007/10000] Avg train loss: 0.024777\n",
      "Epoch [7008/10000] Avg train loss: 0.024774\n",
      "Epoch [7009/10000] Avg train loss: 0.024770\n",
      "Epoch [7010/10000] Avg train loss: 0.024767\n",
      "Epoch [7011/10000] Avg train loss: 0.024763\n",
      "Epoch [7012/10000] Avg train loss: 0.024759\n",
      "Epoch [7013/10000] Avg train loss: 0.024756\n",
      "Epoch [7014/10000] Avg train loss: 0.024752\n",
      "Epoch [7015/10000] Avg train loss: 0.024749\n",
      "Epoch [7016/10000] Avg train loss: 0.024745\n",
      "Epoch [7017/10000] Avg train loss: 0.024742\n",
      "Epoch [7018/10000] Avg train loss: 0.024738\n",
      "Epoch [7019/10000] Avg train loss: 0.024735\n",
      "Epoch [7020/10000] Avg train loss: 0.024731\n",
      "Epoch [7021/10000] Avg train loss: 0.024728\n",
      "Epoch [7022/10000] Avg train loss: 0.024724\n",
      "Epoch [7023/10000] Avg train loss: 0.024721\n",
      "Epoch [7024/10000] Avg train loss: 0.024717\n",
      "Epoch [7025/10000] Avg train loss: 0.024714\n",
      "Epoch [7026/10000] Avg train loss: 0.024710\n",
      "Epoch [7027/10000] Avg train loss: 0.024707\n",
      "Epoch [7028/10000] Avg train loss: 0.024703\n",
      "Epoch [7029/10000] Avg train loss: 0.024700\n",
      "Epoch [7030/10000] Avg train loss: 0.024696\n",
      "Epoch [7031/10000] Avg train loss: 0.024693\n",
      "Epoch [7032/10000] Avg train loss: 0.024689\n",
      "Epoch [7033/10000] Avg train loss: 0.024686\n",
      "Epoch [7034/10000] Avg train loss: 0.024682\n",
      "Epoch [7035/10000] Avg train loss: 0.024679\n",
      "Epoch [7036/10000] Avg train loss: 0.024675\n",
      "Epoch [7037/10000] Avg train loss: 0.024672\n",
      "Epoch [7038/10000] Avg train loss: 0.024668\n",
      "Epoch [7039/10000] Avg train loss: 0.024665\n",
      "Epoch [7040/10000] Avg train loss: 0.024661\n",
      "Epoch [7041/10000] Avg train loss: 0.024658\n",
      "Epoch [7042/10000] Avg train loss: 0.024654\n",
      "Epoch [7043/10000] Avg train loss: 0.024651\n",
      "Epoch [7044/10000] Avg train loss: 0.024647\n",
      "Epoch [7045/10000] Avg train loss: 0.024644\n",
      "Epoch [7046/10000] Avg train loss: 0.024640\n",
      "Epoch [7047/10000] Avg train loss: 0.024637\n",
      "Epoch [7048/10000] Avg train loss: 0.024633\n",
      "Epoch [7049/10000] Avg train loss: 0.024630\n",
      "Epoch [7050/10000] Avg train loss: 0.024626\n",
      "Epoch [7051/10000] Avg train loss: 0.024623\n",
      "Epoch [7052/10000] Avg train loss: 0.024619\n",
      "Epoch [7053/10000] Avg train loss: 0.024616\n",
      "Epoch [7054/10000] Avg train loss: 0.024612\n",
      "Epoch [7055/10000] Avg train loss: 0.024609\n",
      "Epoch [7056/10000] Avg train loss: 0.024605\n",
      "Epoch [7057/10000] Avg train loss: 0.024602\n",
      "Epoch [7058/10000] Avg train loss: 0.024598\n",
      "Epoch [7059/10000] Avg train loss: 0.024595\n",
      "Epoch [7060/10000] Avg train loss: 0.024591\n",
      "Epoch [7061/10000] Avg train loss: 0.024588\n",
      "Epoch [7062/10000] Avg train loss: 0.024584\n",
      "Epoch [7063/10000] Avg train loss: 0.024581\n",
      "Epoch [7064/10000] Avg train loss: 0.024577\n",
      "Epoch [7065/10000] Avg train loss: 0.024574\n",
      "Epoch [7066/10000] Avg train loss: 0.024570\n",
      "Epoch [7067/10000] Avg train loss: 0.024567\n",
      "Epoch [7068/10000] Avg train loss: 0.024563\n",
      "Epoch [7069/10000] Avg train loss: 0.024560\n",
      "Epoch [7070/10000] Avg train loss: 0.024556\n",
      "Epoch [7071/10000] Avg train loss: 0.024553\n",
      "Epoch [7072/10000] Avg train loss: 0.024549\n",
      "Epoch [7073/10000] Avg train loss: 0.024546\n",
      "Epoch [7074/10000] Avg train loss: 0.024542\n",
      "Epoch [7075/10000] Avg train loss: 0.024539\n",
      "Epoch [7076/10000] Avg train loss: 0.024536\n",
      "Epoch [7077/10000] Avg train loss: 0.024532\n",
      "Epoch [7078/10000] Avg train loss: 0.024529\n",
      "Epoch [7079/10000] Avg train loss: 0.024525\n",
      "Epoch [7080/10000] Avg train loss: 0.024522\n",
      "Epoch [7081/10000] Avg train loss: 0.024518\n",
      "Epoch [7082/10000] Avg train loss: 0.024515\n",
      "Epoch [7083/10000] Avg train loss: 0.024511\n",
      "Epoch [7084/10000] Avg train loss: 0.024508\n",
      "Epoch [7085/10000] Avg train loss: 0.024504\n",
      "Epoch [7086/10000] Avg train loss: 0.024501\n",
      "Epoch [7087/10000] Avg train loss: 0.024497\n",
      "Epoch [7088/10000] Avg train loss: 0.024494\n",
      "Epoch [7089/10000] Avg train loss: 0.024491\n",
      "Epoch [7090/10000] Avg train loss: 0.024487\n",
      "Epoch [7091/10000] Avg train loss: 0.024484\n",
      "Epoch [7092/10000] Avg train loss: 0.024480\n",
      "Epoch [7093/10000] Avg train loss: 0.024477\n",
      "Epoch [7094/10000] Avg train loss: 0.024473\n",
      "Epoch [7095/10000] Avg train loss: 0.024470\n",
      "Epoch [7096/10000] Avg train loss: 0.024466\n",
      "Epoch [7097/10000] Avg train loss: 0.024463\n",
      "Epoch [7098/10000] Avg train loss: 0.024460\n",
      "Epoch [7099/10000] Avg train loss: 0.024456\n",
      "Epoch [7100/10000] Avg train loss: 0.024453\n",
      "Epoch [7101/10000] Avg train loss: 0.024449\n",
      "Epoch [7102/10000] Avg train loss: 0.024446\n",
      "Epoch [7103/10000] Avg train loss: 0.024442\n",
      "Epoch [7104/10000] Avg train loss: 0.024439\n",
      "Epoch [7105/10000] Avg train loss: 0.024435\n",
      "Epoch [7106/10000] Avg train loss: 0.024432\n",
      "Epoch [7107/10000] Avg train loss: 0.024429\n",
      "Epoch [7108/10000] Avg train loss: 0.024425\n",
      "Epoch [7109/10000] Avg train loss: 0.024422\n",
      "Epoch [7110/10000] Avg train loss: 0.024418\n",
      "Epoch [7111/10000] Avg train loss: 0.024415\n",
      "Epoch [7112/10000] Avg train loss: 0.024411\n",
      "Epoch [7113/10000] Avg train loss: 0.024408\n",
      "Epoch [7114/10000] Avg train loss: 0.024404\n",
      "Epoch [7115/10000] Avg train loss: 0.024401\n",
      "Epoch [7116/10000] Avg train loss: 0.024398\n",
      "Epoch [7117/10000] Avg train loss: 0.024394\n",
      "Epoch [7118/10000] Avg train loss: 0.024391\n",
      "Epoch [7119/10000] Avg train loss: 0.024387\n",
      "Epoch [7120/10000] Avg train loss: 0.024384\n",
      "Epoch [7121/10000] Avg train loss: 0.024380\n",
      "Epoch [7122/10000] Avg train loss: 0.024377\n",
      "Epoch [7123/10000] Avg train loss: 0.024374\n",
      "Epoch [7124/10000] Avg train loss: 0.024370\n",
      "Epoch [7125/10000] Avg train loss: 0.024367\n",
      "Epoch [7126/10000] Avg train loss: 0.024363\n",
      "Epoch [7127/10000] Avg train loss: 0.024360\n",
      "Epoch [7128/10000] Avg train loss: 0.024357\n",
      "Epoch [7129/10000] Avg train loss: 0.024353\n",
      "Epoch [7130/10000] Avg train loss: 0.024350\n",
      "Epoch [7131/10000] Avg train loss: 0.024346\n",
      "Epoch [7132/10000] Avg train loss: 0.024343\n",
      "Epoch [7133/10000] Avg train loss: 0.024339\n",
      "Epoch [7134/10000] Avg train loss: 0.024336\n",
      "Epoch [7135/10000] Avg train loss: 0.024333\n",
      "Epoch [7136/10000] Avg train loss: 0.024329\n",
      "Epoch [7137/10000] Avg train loss: 0.024326\n",
      "Epoch [7138/10000] Avg train loss: 0.024322\n",
      "Epoch [7139/10000] Avg train loss: 0.024319\n",
      "Epoch [7140/10000] Avg train loss: 0.024316\n",
      "Epoch [7141/10000] Avg train loss: 0.024312\n",
      "Epoch [7142/10000] Avg train loss: 0.024309\n",
      "Epoch [7143/10000] Avg train loss: 0.024305\n",
      "Epoch [7144/10000] Avg train loss: 0.024302\n",
      "Epoch [7145/10000] Avg train loss: 0.024299\n",
      "Epoch [7146/10000] Avg train loss: 0.024295\n",
      "Epoch [7147/10000] Avg train loss: 0.024292\n",
      "Epoch [7148/10000] Avg train loss: 0.024288\n",
      "Epoch [7149/10000] Avg train loss: 0.024285\n",
      "Epoch [7150/10000] Avg train loss: 0.024282\n",
      "Epoch [7151/10000] Avg train loss: 0.024278\n",
      "Epoch [7152/10000] Avg train loss: 0.024275\n",
      "Epoch [7153/10000] Avg train loss: 0.024271\n",
      "Epoch [7154/10000] Avg train loss: 0.024268\n",
      "Epoch [7155/10000] Avg train loss: 0.024265\n",
      "Epoch [7156/10000] Avg train loss: 0.024261\n",
      "Epoch [7157/10000] Avg train loss: 0.024258\n",
      "Epoch [7158/10000] Avg train loss: 0.024254\n",
      "Epoch [7159/10000] Avg train loss: 0.024251\n",
      "Epoch [7160/10000] Avg train loss: 0.024248\n",
      "Epoch [7161/10000] Avg train loss: 0.024244\n",
      "Epoch [7162/10000] Avg train loss: 0.024241\n",
      "Epoch [7163/10000] Avg train loss: 0.024238\n",
      "Epoch [7164/10000] Avg train loss: 0.024234\n",
      "Epoch [7165/10000] Avg train loss: 0.024231\n",
      "Epoch [7166/10000] Avg train loss: 0.024227\n",
      "Epoch [7167/10000] Avg train loss: 0.024224\n",
      "Epoch [7168/10000] Avg train loss: 0.024221\n",
      "Epoch [7169/10000] Avg train loss: 0.024217\n",
      "Epoch [7170/10000] Avg train loss: 0.024214\n",
      "Epoch [7171/10000] Avg train loss: 0.024211\n",
      "Epoch [7172/10000] Avg train loss: 0.024207\n",
      "Epoch [7173/10000] Avg train loss: 0.024204\n",
      "Epoch [7174/10000] Avg train loss: 0.024200\n",
      "Epoch [7175/10000] Avg train loss: 0.024197\n",
      "Epoch [7176/10000] Avg train loss: 0.024194\n",
      "Epoch [7177/10000] Avg train loss: 0.024190\n",
      "Epoch [7178/10000] Avg train loss: 0.024187\n",
      "Epoch [7179/10000] Avg train loss: 0.024184\n",
      "Epoch [7180/10000] Avg train loss: 0.024180\n",
      "Epoch [7181/10000] Avg train loss: 0.024177\n",
      "Epoch [7182/10000] Avg train loss: 0.024173\n",
      "Epoch [7183/10000] Avg train loss: 0.024170\n",
      "Epoch [7184/10000] Avg train loss: 0.024167\n",
      "Epoch [7185/10000] Avg train loss: 0.024163\n",
      "Epoch [7186/10000] Avg train loss: 0.024160\n",
      "Epoch [7187/10000] Avg train loss: 0.024157\n",
      "Epoch [7188/10000] Avg train loss: 0.024153\n",
      "Epoch [7189/10000] Avg train loss: 0.024150\n",
      "Epoch [7190/10000] Avg train loss: 0.024147\n",
      "Epoch [7191/10000] Avg train loss: 0.024143\n",
      "Epoch [7192/10000] Avg train loss: 0.024140\n",
      "Epoch [7193/10000] Avg train loss: 0.024136\n",
      "Epoch [7194/10000] Avg train loss: 0.024133\n",
      "Epoch [7195/10000] Avg train loss: 0.024130\n",
      "Epoch [7196/10000] Avg train loss: 0.024126\n",
      "Epoch [7197/10000] Avg train loss: 0.024123\n",
      "Epoch [7198/10000] Avg train loss: 0.024120\n",
      "Epoch [7199/10000] Avg train loss: 0.024116\n",
      "Epoch [7200/10000] Avg train loss: 0.024113\n",
      "Epoch [7201/10000] Avg train loss: 0.024110\n",
      "Epoch [7202/10000] Avg train loss: 0.024106\n",
      "Epoch [7203/10000] Avg train loss: 0.024103\n",
      "Epoch [7204/10000] Avg train loss: 0.024100\n",
      "Epoch [7205/10000] Avg train loss: 0.024096\n",
      "Epoch [7206/10000] Avg train loss: 0.024093\n",
      "Epoch [7207/10000] Avg train loss: 0.024090\n",
      "Epoch [7208/10000] Avg train loss: 0.024086\n",
      "Epoch [7209/10000] Avg train loss: 0.024083\n",
      "Epoch [7210/10000] Avg train loss: 0.024080\n",
      "Epoch [7211/10000] Avg train loss: 0.024076\n",
      "Epoch [7212/10000] Avg train loss: 0.024073\n",
      "Epoch [7213/10000] Avg train loss: 0.024070\n",
      "Epoch [7214/10000] Avg train loss: 0.024066\n",
      "Epoch [7215/10000] Avg train loss: 0.024063\n",
      "Epoch [7216/10000] Avg train loss: 0.024060\n",
      "Epoch [7217/10000] Avg train loss: 0.024056\n",
      "Epoch [7218/10000] Avg train loss: 0.024053\n",
      "Epoch [7219/10000] Avg train loss: 0.024050\n",
      "Epoch [7220/10000] Avg train loss: 0.024046\n",
      "Epoch [7221/10000] Avg train loss: 0.024043\n",
      "Epoch [7222/10000] Avg train loss: 0.024040\n",
      "Epoch [7223/10000] Avg train loss: 0.024036\n",
      "Epoch [7224/10000] Avg train loss: 0.024033\n",
      "Epoch [7225/10000] Avg train loss: 0.024030\n",
      "Epoch [7226/10000] Avg train loss: 0.024026\n",
      "Epoch [7227/10000] Avg train loss: 0.024023\n",
      "Epoch [7228/10000] Avg train loss: 0.024020\n",
      "Epoch [7229/10000] Avg train loss: 0.024016\n",
      "Epoch [7230/10000] Avg train loss: 0.024013\n",
      "Epoch [7231/10000] Avg train loss: 0.024010\n",
      "Epoch [7232/10000] Avg train loss: 0.024006\n",
      "Epoch [7233/10000] Avg train loss: 0.024003\n",
      "Epoch [7234/10000] Avg train loss: 0.024000\n",
      "Epoch [7235/10000] Avg train loss: 0.023996\n",
      "Epoch [7236/10000] Avg train loss: 0.023993\n",
      "Epoch [7237/10000] Avg train loss: 0.023990\n",
      "Epoch [7238/10000] Avg train loss: 0.023986\n",
      "Epoch [7239/10000] Avg train loss: 0.023983\n",
      "Epoch [7240/10000] Avg train loss: 0.023980\n",
      "Epoch [7241/10000] Avg train loss: 0.023976\n",
      "Epoch [7242/10000] Avg train loss: 0.023973\n",
      "Epoch [7243/10000] Avg train loss: 0.023970\n",
      "Epoch [7244/10000] Avg train loss: 0.023967\n",
      "Epoch [7245/10000] Avg train loss: 0.023963\n",
      "Epoch [7246/10000] Avg train loss: 0.023960\n",
      "Epoch [7247/10000] Avg train loss: 0.023957\n",
      "Epoch [7248/10000] Avg train loss: 0.023953\n",
      "Epoch [7249/10000] Avg train loss: 0.023950\n",
      "Epoch [7250/10000] Avg train loss: 0.023947\n",
      "Epoch [7251/10000] Avg train loss: 0.023943\n",
      "Epoch [7252/10000] Avg train loss: 0.023940\n",
      "Epoch [7253/10000] Avg train loss: 0.023937\n",
      "Epoch [7254/10000] Avg train loss: 0.023933\n",
      "Epoch [7255/10000] Avg train loss: 0.023930\n",
      "Epoch [7256/10000] Avg train loss: 0.023927\n",
      "Epoch [7257/10000] Avg train loss: 0.023924\n",
      "Epoch [7258/10000] Avg train loss: 0.023920\n",
      "Epoch [7259/10000] Avg train loss: 0.023917\n",
      "Epoch [7260/10000] Avg train loss: 0.023914\n",
      "Epoch [7261/10000] Avg train loss: 0.023910\n",
      "Epoch [7262/10000] Avg train loss: 0.023907\n",
      "Epoch [7263/10000] Avg train loss: 0.023904\n",
      "Epoch [7264/10000] Avg train loss: 0.023901\n",
      "Epoch [7265/10000] Avg train loss: 0.023897\n",
      "Epoch [7266/10000] Avg train loss: 0.023894\n",
      "Epoch [7267/10000] Avg train loss: 0.023891\n",
      "Epoch [7268/10000] Avg train loss: 0.023887\n",
      "Epoch [7269/10000] Avg train loss: 0.023884\n",
      "Epoch [7270/10000] Avg train loss: 0.023881\n",
      "Epoch [7271/10000] Avg train loss: 0.023878\n",
      "Epoch [7272/10000] Avg train loss: 0.023874\n",
      "Epoch [7273/10000] Avg train loss: 0.023871\n",
      "Epoch [7274/10000] Avg train loss: 0.023868\n",
      "Epoch [7275/10000] Avg train loss: 0.023864\n",
      "Epoch [7276/10000] Avg train loss: 0.023861\n",
      "Epoch [7277/10000] Avg train loss: 0.023858\n",
      "Epoch [7278/10000] Avg train loss: 0.023855\n",
      "Epoch [7279/10000] Avg train loss: 0.023851\n",
      "Epoch [7280/10000] Avg train loss: 0.023848\n",
      "Epoch [7281/10000] Avg train loss: 0.023845\n",
      "Epoch [7282/10000] Avg train loss: 0.023841\n",
      "Epoch [7283/10000] Avg train loss: 0.023838\n",
      "Epoch [7284/10000] Avg train loss: 0.023835\n",
      "Epoch [7285/10000] Avg train loss: 0.023832\n",
      "Epoch [7286/10000] Avg train loss: 0.023828\n",
      "Epoch [7287/10000] Avg train loss: 0.023825\n",
      "Epoch [7288/10000] Avg train loss: 0.023822\n",
      "Epoch [7289/10000] Avg train loss: 0.023819\n",
      "Epoch [7290/10000] Avg train loss: 0.023815\n",
      "Epoch [7291/10000] Avg train loss: 0.023812\n",
      "Epoch [7292/10000] Avg train loss: 0.023809\n",
      "Epoch [7293/10000] Avg train loss: 0.023806\n",
      "Epoch [7294/10000] Avg train loss: 0.023802\n",
      "Epoch [7295/10000] Avg train loss: 0.023799\n",
      "Epoch [7296/10000] Avg train loss: 0.023796\n",
      "Epoch [7297/10000] Avg train loss: 0.023792\n",
      "Epoch [7298/10000] Avg train loss: 0.023789\n",
      "Epoch [7299/10000] Avg train loss: 0.023786\n",
      "Epoch [7300/10000] Avg train loss: 0.023783\n",
      "Epoch [7301/10000] Avg train loss: 0.023779\n",
      "Epoch [7302/10000] Avg train loss: 0.023776\n",
      "Epoch [7303/10000] Avg train loss: 0.023773\n",
      "Epoch [7304/10000] Avg train loss: 0.023770\n",
      "Epoch [7305/10000] Avg train loss: 0.023766\n",
      "Epoch [7306/10000] Avg train loss: 0.023763\n",
      "Epoch [7307/10000] Avg train loss: 0.023760\n",
      "Epoch [7308/10000] Avg train loss: 0.023757\n",
      "Epoch [7309/10000] Avg train loss: 0.023753\n",
      "Epoch [7310/10000] Avg train loss: 0.023750\n",
      "Epoch [7311/10000] Avg train loss: 0.023747\n",
      "Epoch [7312/10000] Avg train loss: 0.023744\n",
      "Epoch [7313/10000] Avg train loss: 0.023740\n",
      "Epoch [7314/10000] Avg train loss: 0.023737\n",
      "Epoch [7315/10000] Avg train loss: 0.023734\n",
      "Epoch [7316/10000] Avg train loss: 0.023731\n",
      "Epoch [7317/10000] Avg train loss: 0.023727\n",
      "Epoch [7318/10000] Avg train loss: 0.023724\n",
      "Epoch [7319/10000] Avg train loss: 0.023721\n",
      "Epoch [7320/10000] Avg train loss: 0.023718\n",
      "Epoch [7321/10000] Avg train loss: 0.023714\n",
      "Epoch [7322/10000] Avg train loss: 0.023711\n",
      "Epoch [7323/10000] Avg train loss: 0.023708\n",
      "Epoch [7324/10000] Avg train loss: 0.023705\n",
      "Epoch [7325/10000] Avg train loss: 0.023702\n",
      "Epoch [7326/10000] Avg train loss: 0.023698\n",
      "Epoch [7327/10000] Avg train loss: 0.023695\n",
      "Epoch [7328/10000] Avg train loss: 0.023692\n",
      "Epoch [7329/10000] Avg train loss: 0.023689\n",
      "Epoch [7330/10000] Avg train loss: 0.023685\n",
      "Epoch [7331/10000] Avg train loss: 0.023682\n",
      "Epoch [7332/10000] Avg train loss: 0.023679\n",
      "Epoch [7333/10000] Avg train loss: 0.023676\n",
      "Epoch [7334/10000] Avg train loss: 0.023672\n",
      "Epoch [7335/10000] Avg train loss: 0.023669\n",
      "Epoch [7336/10000] Avg train loss: 0.023666\n",
      "Epoch [7337/10000] Avg train loss: 0.023663\n",
      "Epoch [7338/10000] Avg train loss: 0.023660\n",
      "Epoch [7339/10000] Avg train loss: 0.023656\n",
      "Epoch [7340/10000] Avg train loss: 0.023653\n",
      "Epoch [7341/10000] Avg train loss: 0.023650\n",
      "Epoch [7342/10000] Avg train loss: 0.023647\n",
      "Epoch [7343/10000] Avg train loss: 0.023643\n",
      "Epoch [7344/10000] Avg train loss: 0.023640\n",
      "Epoch [7345/10000] Avg train loss: 0.023637\n",
      "Epoch [7346/10000] Avg train loss: 0.023634\n",
      "Epoch [7347/10000] Avg train loss: 0.023631\n",
      "Epoch [7348/10000] Avg train loss: 0.023627\n",
      "Epoch [7349/10000] Avg train loss: 0.023624\n",
      "Epoch [7350/10000] Avg train loss: 0.023621\n",
      "Epoch [7351/10000] Avg train loss: 0.023618\n",
      "Epoch [7352/10000] Avg train loss: 0.023614\n",
      "Epoch [7353/10000] Avg train loss: 0.023611\n",
      "Epoch [7354/10000] Avg train loss: 0.023608\n",
      "Epoch [7355/10000] Avg train loss: 0.023605\n",
      "Epoch [7356/10000] Avg train loss: 0.023602\n",
      "Epoch [7357/10000] Avg train loss: 0.023598\n",
      "Epoch [7358/10000] Avg train loss: 0.023595\n",
      "Epoch [7359/10000] Avg train loss: 0.023592\n",
      "Epoch [7360/10000] Avg train loss: 0.023589\n",
      "Epoch [7361/10000] Avg train loss: 0.023586\n",
      "Epoch [7362/10000] Avg train loss: 0.023582\n",
      "Epoch [7363/10000] Avg train loss: 0.023579\n",
      "Epoch [7364/10000] Avg train loss: 0.023576\n",
      "Epoch [7365/10000] Avg train loss: 0.023573\n",
      "Epoch [7366/10000] Avg train loss: 0.023570\n",
      "Epoch [7367/10000] Avg train loss: 0.023566\n",
      "Epoch [7368/10000] Avg train loss: 0.023563\n",
      "Epoch [7369/10000] Avg train loss: 0.023560\n",
      "Epoch [7370/10000] Avg train loss: 0.023557\n",
      "Epoch [7371/10000] Avg train loss: 0.023554\n",
      "Epoch [7372/10000] Avg train loss: 0.023550\n",
      "Epoch [7373/10000] Avg train loss: 0.023547\n",
      "Epoch [7374/10000] Avg train loss: 0.023544\n",
      "Epoch [7375/10000] Avg train loss: 0.023541\n",
      "Epoch [7376/10000] Avg train loss: 0.023538\n",
      "Epoch [7377/10000] Avg train loss: 0.023534\n",
      "Epoch [7378/10000] Avg train loss: 0.023531\n",
      "Epoch [7379/10000] Avg train loss: 0.023528\n",
      "Epoch [7380/10000] Avg train loss: 0.023525\n",
      "Epoch [7381/10000] Avg train loss: 0.023522\n",
      "Epoch [7382/10000] Avg train loss: 0.023518\n",
      "Epoch [7383/10000] Avg train loss: 0.023515\n",
      "Epoch [7384/10000] Avg train loss: 0.023512\n",
      "Epoch [7385/10000] Avg train loss: 0.023509\n",
      "Epoch [7386/10000] Avg train loss: 0.023506\n",
      "Epoch [7387/10000] Avg train loss: 0.023503\n",
      "Epoch [7388/10000] Avg train loss: 0.023499\n",
      "Epoch [7389/10000] Avg train loss: 0.023496\n",
      "Epoch [7390/10000] Avg train loss: 0.023493\n",
      "Epoch [7391/10000] Avg train loss: 0.023490\n",
      "Epoch [7392/10000] Avg train loss: 0.023487\n",
      "Epoch [7393/10000] Avg train loss: 0.023484\n",
      "Epoch [7394/10000] Avg train loss: 0.023480\n",
      "Epoch [7395/10000] Avg train loss: 0.023477\n",
      "Epoch [7396/10000] Avg train loss: 0.023474\n",
      "Epoch [7397/10000] Avg train loss: 0.023471\n",
      "Epoch [7398/10000] Avg train loss: 0.023468\n",
      "Epoch [7399/10000] Avg train loss: 0.023464\n",
      "Epoch [7400/10000] Avg train loss: 0.023461\n",
      "Epoch [7401/10000] Avg train loss: 0.023458\n",
      "Epoch [7402/10000] Avg train loss: 0.023455\n",
      "Epoch [7403/10000] Avg train loss: 0.023452\n",
      "Epoch [7404/10000] Avg train loss: 0.023449\n",
      "Epoch [7405/10000] Avg train loss: 0.023445\n",
      "Epoch [7406/10000] Avg train loss: 0.023442\n",
      "Epoch [7407/10000] Avg train loss: 0.023439\n",
      "Epoch [7408/10000] Avg train loss: 0.023436\n",
      "Epoch [7409/10000] Avg train loss: 0.023433\n",
      "Epoch [7410/10000] Avg train loss: 0.023430\n",
      "Epoch [7411/10000] Avg train loss: 0.023426\n",
      "Epoch [7412/10000] Avg train loss: 0.023423\n",
      "Epoch [7413/10000] Avg train loss: 0.023420\n",
      "Epoch [7414/10000] Avg train loss: 0.023417\n",
      "Epoch [7415/10000] Avg train loss: 0.023414\n",
      "Epoch [7416/10000] Avg train loss: 0.023411\n",
      "Epoch [7417/10000] Avg train loss: 0.023408\n",
      "Epoch [7418/10000] Avg train loss: 0.023404\n",
      "Epoch [7419/10000] Avg train loss: 0.023401\n",
      "Epoch [7420/10000] Avg train loss: 0.023398\n",
      "Epoch [7421/10000] Avg train loss: 0.023395\n",
      "Epoch [7422/10000] Avg train loss: 0.023392\n",
      "Epoch [7423/10000] Avg train loss: 0.023389\n",
      "Epoch [7424/10000] Avg train loss: 0.023385\n",
      "Epoch [7425/10000] Avg train loss: 0.023382\n",
      "Epoch [7426/10000] Avg train loss: 0.023379\n",
      "Epoch [7427/10000] Avg train loss: 0.023376\n",
      "Epoch [7428/10000] Avg train loss: 0.023373\n",
      "Epoch [7429/10000] Avg train loss: 0.023370\n",
      "Epoch [7430/10000] Avg train loss: 0.023367\n",
      "Epoch [7431/10000] Avg train loss: 0.023363\n",
      "Epoch [7432/10000] Avg train loss: 0.023360\n",
      "Epoch [7433/10000] Avg train loss: 0.023357\n",
      "Epoch [7434/10000] Avg train loss: 0.023354\n",
      "Epoch [7435/10000] Avg train loss: 0.023351\n",
      "Epoch [7436/10000] Avg train loss: 0.023348\n",
      "Epoch [7437/10000] Avg train loss: 0.023345\n",
      "Epoch [7438/10000] Avg train loss: 0.023341\n",
      "Epoch [7439/10000] Avg train loss: 0.023338\n",
      "Epoch [7440/10000] Avg train loss: 0.023335\n",
      "Epoch [7441/10000] Avg train loss: 0.023332\n",
      "Epoch [7442/10000] Avg train loss: 0.023329\n",
      "Epoch [7443/10000] Avg train loss: 0.023326\n",
      "Epoch [7444/10000] Avg train loss: 0.023323\n",
      "Epoch [7445/10000] Avg train loss: 0.023319\n",
      "Epoch [7446/10000] Avg train loss: 0.023316\n",
      "Epoch [7447/10000] Avg train loss: 0.023313\n",
      "Epoch [7448/10000] Avg train loss: 0.023310\n",
      "Epoch [7449/10000] Avg train loss: 0.023307\n",
      "Epoch [7450/10000] Avg train loss: 0.023304\n",
      "Epoch [7451/10000] Avg train loss: 0.023301\n",
      "Epoch [7452/10000] Avg train loss: 0.023298\n",
      "Epoch [7453/10000] Avg train loss: 0.023294\n",
      "Epoch [7454/10000] Avg train loss: 0.023291\n",
      "Epoch [7455/10000] Avg train loss: 0.023288\n",
      "Epoch [7456/10000] Avg train loss: 0.023285\n",
      "Epoch [7457/10000] Avg train loss: 0.023282\n",
      "Epoch [7458/10000] Avg train loss: 0.023279\n",
      "Epoch [7459/10000] Avg train loss: 0.023276\n",
      "Epoch [7460/10000] Avg train loss: 0.023273\n",
      "Epoch [7461/10000] Avg train loss: 0.023269\n",
      "Epoch [7462/10000] Avg train loss: 0.023266\n",
      "Epoch [7463/10000] Avg train loss: 0.023263\n",
      "Epoch [7464/10000] Avg train loss: 0.023260\n",
      "Epoch [7465/10000] Avg train loss: 0.023257\n",
      "Epoch [7466/10000] Avg train loss: 0.023254\n",
      "Epoch [7467/10000] Avg train loss: 0.023251\n",
      "Epoch [7468/10000] Avg train loss: 0.023248\n",
      "Epoch [7469/10000] Avg train loss: 0.023245\n",
      "Epoch [7470/10000] Avg train loss: 0.023241\n",
      "Epoch [7471/10000] Avg train loss: 0.023238\n",
      "Epoch [7472/10000] Avg train loss: 0.023235\n",
      "Epoch [7473/10000] Avg train loss: 0.023232\n",
      "Epoch [7474/10000] Avg train loss: 0.023229\n",
      "Epoch [7475/10000] Avg train loss: 0.023226\n",
      "Epoch [7476/10000] Avg train loss: 0.023223\n",
      "Epoch [7477/10000] Avg train loss: 0.023220\n",
      "Epoch [7478/10000] Avg train loss: 0.023217\n",
      "Epoch [7479/10000] Avg train loss: 0.023213\n",
      "Epoch [7480/10000] Avg train loss: 0.023210\n",
      "Epoch [7481/10000] Avg train loss: 0.023207\n",
      "Epoch [7482/10000] Avg train loss: 0.023204\n",
      "Epoch [7483/10000] Avg train loss: 0.023201\n",
      "Epoch [7484/10000] Avg train loss: 0.023198\n",
      "Epoch [7485/10000] Avg train loss: 0.023195\n",
      "Epoch [7486/10000] Avg train loss: 0.023192\n",
      "Epoch [7487/10000] Avg train loss: 0.023189\n",
      "Epoch [7488/10000] Avg train loss: 0.023186\n",
      "Epoch [7489/10000] Avg train loss: 0.023182\n",
      "Epoch [7490/10000] Avg train loss: 0.023179\n",
      "Epoch [7491/10000] Avg train loss: 0.023176\n",
      "Epoch [7492/10000] Avg train loss: 0.023173\n",
      "Epoch [7493/10000] Avg train loss: 0.023170\n",
      "Epoch [7494/10000] Avg train loss: 0.023167\n",
      "Epoch [7495/10000] Avg train loss: 0.023164\n",
      "Epoch [7496/10000] Avg train loss: 0.023161\n",
      "Epoch [7497/10000] Avg train loss: 0.023158\n",
      "Epoch [7498/10000] Avg train loss: 0.023155\n",
      "Epoch [7499/10000] Avg train loss: 0.023152\n",
      "Epoch [7500/10000] Avg train loss: 0.023148\n",
      "Epoch [7501/10000] Avg train loss: 0.023145\n",
      "Epoch [7502/10000] Avg train loss: 0.023142\n",
      "Epoch [7503/10000] Avg train loss: 0.023139\n",
      "Epoch [7504/10000] Avg train loss: 0.023136\n",
      "Epoch [7505/10000] Avg train loss: 0.023133\n",
      "Epoch [7506/10000] Avg train loss: 0.023130\n",
      "Epoch [7507/10000] Avg train loss: 0.023127\n",
      "Epoch [7508/10000] Avg train loss: 0.023124\n",
      "Epoch [7509/10000] Avg train loss: 0.023121\n",
      "Epoch [7510/10000] Avg train loss: 0.023118\n",
      "Epoch [7511/10000] Avg train loss: 0.023115\n",
      "Epoch [7512/10000] Avg train loss: 0.023111\n",
      "Epoch [7513/10000] Avg train loss: 0.023108\n",
      "Epoch [7514/10000] Avg train loss: 0.023105\n",
      "Epoch [7515/10000] Avg train loss: 0.023102\n",
      "Epoch [7516/10000] Avg train loss: 0.023099\n",
      "Epoch [7517/10000] Avg train loss: 0.023096\n",
      "Epoch [7518/10000] Avg train loss: 0.023093\n",
      "Epoch [7519/10000] Avg train loss: 0.023090\n",
      "Epoch [7520/10000] Avg train loss: 0.023087\n",
      "Epoch [7521/10000] Avg train loss: 0.023084\n",
      "Epoch [7522/10000] Avg train loss: 0.023081\n",
      "Epoch [7523/10000] Avg train loss: 0.023078\n",
      "Epoch [7524/10000] Avg train loss: 0.023075\n",
      "Epoch [7525/10000] Avg train loss: 0.023072\n",
      "Epoch [7526/10000] Avg train loss: 0.023069\n",
      "Epoch [7527/10000] Avg train loss: 0.023065\n",
      "Epoch [7528/10000] Avg train loss: 0.023062\n",
      "Epoch [7529/10000] Avg train loss: 0.023059\n",
      "Epoch [7530/10000] Avg train loss: 0.023056\n",
      "Epoch [7531/10000] Avg train loss: 0.023053\n",
      "Epoch [7532/10000] Avg train loss: 0.023050\n",
      "Epoch [7533/10000] Avg train loss: 0.023047\n",
      "Epoch [7534/10000] Avg train loss: 0.023044\n",
      "Epoch [7535/10000] Avg train loss: 0.023041\n",
      "Epoch [7536/10000] Avg train loss: 0.023038\n",
      "Epoch [7537/10000] Avg train loss: 0.023035\n",
      "Epoch [7538/10000] Avg train loss: 0.023032\n",
      "Epoch [7539/10000] Avg train loss: 0.023029\n",
      "Epoch [7540/10000] Avg train loss: 0.023026\n",
      "Epoch [7541/10000] Avg train loss: 0.023023\n",
      "Epoch [7542/10000] Avg train loss: 0.023020\n",
      "Epoch [7543/10000] Avg train loss: 0.023017\n",
      "Epoch [7544/10000] Avg train loss: 0.023013\n",
      "Epoch [7545/10000] Avg train loss: 0.023010\n",
      "Epoch [7546/10000] Avg train loss: 0.023007\n",
      "Epoch [7547/10000] Avg train loss: 0.023004\n",
      "Epoch [7548/10000] Avg train loss: 0.023001\n",
      "Epoch [7549/10000] Avg train loss: 0.022998\n",
      "Epoch [7550/10000] Avg train loss: 0.022995\n",
      "Epoch [7551/10000] Avg train loss: 0.022992\n",
      "Epoch [7552/10000] Avg train loss: 0.022989\n",
      "Epoch [7553/10000] Avg train loss: 0.022986\n",
      "Epoch [7554/10000] Avg train loss: 0.022983\n",
      "Epoch [7555/10000] Avg train loss: 0.022980\n",
      "Epoch [7556/10000] Avg train loss: 0.022977\n",
      "Epoch [7557/10000] Avg train loss: 0.022974\n",
      "Epoch [7558/10000] Avg train loss: 0.022971\n",
      "Epoch [7559/10000] Avg train loss: 0.022968\n",
      "Epoch [7560/10000] Avg train loss: 0.022965\n",
      "Epoch [7561/10000] Avg train loss: 0.022962\n",
      "Epoch [7562/10000] Avg train loss: 0.022959\n",
      "Epoch [7563/10000] Avg train loss: 0.022956\n",
      "Epoch [7564/10000] Avg train loss: 0.022953\n",
      "Epoch [7565/10000] Avg train loss: 0.022950\n",
      "Epoch [7566/10000] Avg train loss: 0.022947\n",
      "Epoch [7567/10000] Avg train loss: 0.022944\n",
      "Epoch [7568/10000] Avg train loss: 0.022940\n",
      "Epoch [7569/10000] Avg train loss: 0.022937\n",
      "Epoch [7570/10000] Avg train loss: 0.022934\n",
      "Epoch [7571/10000] Avg train loss: 0.022931\n",
      "Epoch [7572/10000] Avg train loss: 0.022928\n",
      "Epoch [7573/10000] Avg train loss: 0.022925\n",
      "Epoch [7574/10000] Avg train loss: 0.022922\n",
      "Epoch [7575/10000] Avg train loss: 0.022919\n",
      "Epoch [7576/10000] Avg train loss: 0.022916\n",
      "Epoch [7577/10000] Avg train loss: 0.022913\n",
      "Epoch [7578/10000] Avg train loss: 0.022910\n",
      "Epoch [7579/10000] Avg train loss: 0.022907\n",
      "Epoch [7580/10000] Avg train loss: 0.022904\n",
      "Epoch [7581/10000] Avg train loss: 0.022901\n",
      "Epoch [7582/10000] Avg train loss: 0.022898\n",
      "Epoch [7583/10000] Avg train loss: 0.022895\n",
      "Epoch [7584/10000] Avg train loss: 0.022892\n",
      "Epoch [7585/10000] Avg train loss: 0.022889\n",
      "Epoch [7586/10000] Avg train loss: 0.022886\n",
      "Epoch [7587/10000] Avg train loss: 0.022883\n",
      "Epoch [7588/10000] Avg train loss: 0.022880\n",
      "Epoch [7589/10000] Avg train loss: 0.022877\n",
      "Epoch [7590/10000] Avg train loss: 0.022874\n",
      "Epoch [7591/10000] Avg train loss: 0.022871\n",
      "Epoch [7592/10000] Avg train loss: 0.022868\n",
      "Epoch [7593/10000] Avg train loss: 0.022865\n",
      "Epoch [7594/10000] Avg train loss: 0.022862\n",
      "Epoch [7595/10000] Avg train loss: 0.022859\n",
      "Epoch [7596/10000] Avg train loss: 0.022856\n",
      "Epoch [7597/10000] Avg train loss: 0.022853\n",
      "Epoch [7598/10000] Avg train loss: 0.022850\n",
      "Epoch [7599/10000] Avg train loss: 0.022847\n",
      "Epoch [7600/10000] Avg train loss: 0.022844\n",
      "Epoch [7601/10000] Avg train loss: 0.022841\n",
      "Epoch [7602/10000] Avg train loss: 0.022838\n",
      "Epoch [7603/10000] Avg train loss: 0.022835\n",
      "Epoch [7604/10000] Avg train loss: 0.022832\n",
      "Epoch [7605/10000] Avg train loss: 0.022829\n",
      "Epoch [7606/10000] Avg train loss: 0.022826\n",
      "Epoch [7607/10000] Avg train loss: 0.022823\n",
      "Epoch [7608/10000] Avg train loss: 0.022820\n",
      "Epoch [7609/10000] Avg train loss: 0.022817\n",
      "Epoch [7610/10000] Avg train loss: 0.022814\n",
      "Epoch [7611/10000] Avg train loss: 0.022811\n",
      "Epoch [7612/10000] Avg train loss: 0.022808\n",
      "Epoch [7613/10000] Avg train loss: 0.022805\n",
      "Epoch [7614/10000] Avg train loss: 0.022802\n",
      "Epoch [7615/10000] Avg train loss: 0.022799\n",
      "Epoch [7616/10000] Avg train loss: 0.022796\n",
      "Epoch [7617/10000] Avg train loss: 0.022793\n",
      "Epoch [7618/10000] Avg train loss: 0.022790\n",
      "Epoch [7619/10000] Avg train loss: 0.022787\n",
      "Epoch [7620/10000] Avg train loss: 0.022784\n",
      "Epoch [7621/10000] Avg train loss: 0.022781\n",
      "Epoch [7622/10000] Avg train loss: 0.022778\n",
      "Epoch [7623/10000] Avg train loss: 0.022775\n",
      "Epoch [7624/10000] Avg train loss: 0.022772\n",
      "Epoch [7625/10000] Avg train loss: 0.022769\n",
      "Epoch [7626/10000] Avg train loss: 0.022766\n",
      "Epoch [7627/10000] Avg train loss: 0.022763\n",
      "Epoch [7628/10000] Avg train loss: 0.022760\n",
      "Epoch [7629/10000] Avg train loss: 0.022757\n",
      "Epoch [7630/10000] Avg train loss: 0.022754\n",
      "Epoch [7631/10000] Avg train loss: 0.022751\n",
      "Epoch [7632/10000] Avg train loss: 0.022748\n",
      "Epoch [7633/10000] Avg train loss: 0.022745\n",
      "Epoch [7634/10000] Avg train loss: 0.022742\n",
      "Epoch [7635/10000] Avg train loss: 0.022739\n",
      "Epoch [7636/10000] Avg train loss: 0.022736\n",
      "Epoch [7637/10000] Avg train loss: 0.022733\n",
      "Epoch [7638/10000] Avg train loss: 0.022730\n",
      "Epoch [7639/10000] Avg train loss: 0.022727\n",
      "Epoch [7640/10000] Avg train loss: 0.022724\n",
      "Epoch [7641/10000] Avg train loss: 0.022721\n",
      "Epoch [7642/10000] Avg train loss: 0.022718\n",
      "Epoch [7643/10000] Avg train loss: 0.022715\n",
      "Epoch [7644/10000] Avg train loss: 0.022712\n",
      "Epoch [7645/10000] Avg train loss: 0.022709\n",
      "Epoch [7646/10000] Avg train loss: 0.022706\n",
      "Epoch [7647/10000] Avg train loss: 0.022703\n",
      "Epoch [7648/10000] Avg train loss: 0.022701\n",
      "Epoch [7649/10000] Avg train loss: 0.022698\n",
      "Epoch [7650/10000] Avg train loss: 0.022695\n",
      "Epoch [7651/10000] Avg train loss: 0.022692\n",
      "Epoch [7652/10000] Avg train loss: 0.022689\n",
      "Epoch [7653/10000] Avg train loss: 0.022686\n",
      "Epoch [7654/10000] Avg train loss: 0.022683\n",
      "Epoch [7655/10000] Avg train loss: 0.022680\n",
      "Epoch [7656/10000] Avg train loss: 0.022677\n",
      "Epoch [7657/10000] Avg train loss: 0.022674\n",
      "Epoch [7658/10000] Avg train loss: 0.022671\n",
      "Epoch [7659/10000] Avg train loss: 0.022668\n",
      "Epoch [7660/10000] Avg train loss: 0.022665\n",
      "Epoch [7661/10000] Avg train loss: 0.022662\n",
      "Epoch [7662/10000] Avg train loss: 0.022659\n",
      "Epoch [7663/10000] Avg train loss: 0.022656\n",
      "Epoch [7664/10000] Avg train loss: 0.022653\n",
      "Epoch [7665/10000] Avg train loss: 0.022650\n",
      "Epoch [7666/10000] Avg train loss: 0.022647\n",
      "Epoch [7667/10000] Avg train loss: 0.022644\n",
      "Epoch [7668/10000] Avg train loss: 0.022641\n",
      "Epoch [7669/10000] Avg train loss: 0.022638\n",
      "Epoch [7670/10000] Avg train loss: 0.022635\n",
      "Epoch [7671/10000] Avg train loss: 0.022632\n",
      "Epoch [7672/10000] Avg train loss: 0.022630\n",
      "Epoch [7673/10000] Avg train loss: 0.022627\n",
      "Epoch [7674/10000] Avg train loss: 0.022624\n",
      "Epoch [7675/10000] Avg train loss: 0.022621\n",
      "Epoch [7676/10000] Avg train loss: 0.022618\n",
      "Epoch [7677/10000] Avg train loss: 0.022615\n",
      "Epoch [7678/10000] Avg train loss: 0.022612\n",
      "Epoch [7679/10000] Avg train loss: 0.022609\n",
      "Epoch [7680/10000] Avg train loss: 0.022606\n",
      "Epoch [7681/10000] Avg train loss: 0.022603\n",
      "Epoch [7682/10000] Avg train loss: 0.022600\n",
      "Epoch [7683/10000] Avg train loss: 0.022597\n",
      "Epoch [7684/10000] Avg train loss: 0.022594\n",
      "Epoch [7685/10000] Avg train loss: 0.022591\n",
      "Epoch [7686/10000] Avg train loss: 0.022588\n",
      "Epoch [7687/10000] Avg train loss: 0.022585\n",
      "Epoch [7688/10000] Avg train loss: 0.022582\n",
      "Epoch [7689/10000] Avg train loss: 0.022579\n",
      "Epoch [7690/10000] Avg train loss: 0.022577\n",
      "Epoch [7691/10000] Avg train loss: 0.022574\n",
      "Epoch [7692/10000] Avg train loss: 0.022571\n",
      "Epoch [7693/10000] Avg train loss: 0.022568\n",
      "Epoch [7694/10000] Avg train loss: 0.022565\n",
      "Epoch [7695/10000] Avg train loss: 0.022562\n",
      "Epoch [7696/10000] Avg train loss: 0.022559\n",
      "Epoch [7697/10000] Avg train loss: 0.022556\n",
      "Epoch [7698/10000] Avg train loss: 0.022553\n",
      "Epoch [7699/10000] Avg train loss: 0.022550\n",
      "Epoch [7700/10000] Avg train loss: 0.022547\n",
      "Epoch [7701/10000] Avg train loss: 0.022544\n",
      "Epoch [7702/10000] Avg train loss: 0.022541\n",
      "Epoch [7703/10000] Avg train loss: 0.022538\n",
      "Epoch [7704/10000] Avg train loss: 0.022536\n",
      "Epoch [7705/10000] Avg train loss: 0.022533\n",
      "Epoch [7706/10000] Avg train loss: 0.022530\n",
      "Epoch [7707/10000] Avg train loss: 0.022527\n",
      "Epoch [7708/10000] Avg train loss: 0.022524\n",
      "Epoch [7709/10000] Avg train loss: 0.022521\n",
      "Epoch [7710/10000] Avg train loss: 0.022518\n",
      "Epoch [7711/10000] Avg train loss: 0.022515\n",
      "Epoch [7712/10000] Avg train loss: 0.022512\n",
      "Epoch [7713/10000] Avg train loss: 0.022509\n",
      "Epoch [7714/10000] Avg train loss: 0.022506\n",
      "Epoch [7715/10000] Avg train loss: 0.022503\n",
      "Epoch [7716/10000] Avg train loss: 0.022500\n",
      "Epoch [7717/10000] Avg train loss: 0.022498\n",
      "Epoch [7718/10000] Avg train loss: 0.022495\n",
      "Epoch [7719/10000] Avg train loss: 0.022492\n",
      "Epoch [7720/10000] Avg train loss: 0.022489\n",
      "Epoch [7721/10000] Avg train loss: 0.022486\n",
      "Epoch [7722/10000] Avg train loss: 0.022483\n",
      "Epoch [7723/10000] Avg train loss: 0.022480\n",
      "Epoch [7724/10000] Avg train loss: 0.022477\n",
      "Epoch [7725/10000] Avg train loss: 0.022474\n",
      "Epoch [7726/10000] Avg train loss: 0.022471\n",
      "Epoch [7727/10000] Avg train loss: 0.022468\n",
      "Epoch [7728/10000] Avg train loss: 0.022466\n",
      "Epoch [7729/10000] Avg train loss: 0.022463\n",
      "Epoch [7730/10000] Avg train loss: 0.022460\n",
      "Epoch [7731/10000] Avg train loss: 0.022457\n",
      "Epoch [7732/10000] Avg train loss: 0.022454\n",
      "Epoch [7733/10000] Avg train loss: 0.022451\n",
      "Epoch [7734/10000] Avg train loss: 0.022448\n",
      "Epoch [7735/10000] Avg train loss: 0.022445\n",
      "Epoch [7736/10000] Avg train loss: 0.022442\n",
      "Epoch [7737/10000] Avg train loss: 0.022439\n",
      "Epoch [7738/10000] Avg train loss: 0.022436\n",
      "Epoch [7739/10000] Avg train loss: 0.022434\n",
      "Epoch [7740/10000] Avg train loss: 0.022431\n",
      "Epoch [7741/10000] Avg train loss: 0.022428\n",
      "Epoch [7742/10000] Avg train loss: 0.022425\n",
      "Epoch [7743/10000] Avg train loss: 0.022422\n",
      "Epoch [7744/10000] Avg train loss: 0.022419\n",
      "Epoch [7745/10000] Avg train loss: 0.022416\n",
      "Epoch [7746/10000] Avg train loss: 0.022413\n",
      "Epoch [7747/10000] Avg train loss: 0.022410\n",
      "Epoch [7748/10000] Avg train loss: 0.022408\n",
      "Epoch [7749/10000] Avg train loss: 0.022405\n",
      "Epoch [7750/10000] Avg train loss: 0.022402\n",
      "Epoch [7751/10000] Avg train loss: 0.022399\n",
      "Epoch [7752/10000] Avg train loss: 0.022396\n",
      "Epoch [7753/10000] Avg train loss: 0.022393\n",
      "Epoch [7754/10000] Avg train loss: 0.022390\n",
      "Epoch [7755/10000] Avg train loss: 0.022387\n",
      "Epoch [7756/10000] Avg train loss: 0.022384\n",
      "Epoch [7757/10000] Avg train loss: 0.022382\n",
      "Epoch [7758/10000] Avg train loss: 0.022379\n",
      "Epoch [7759/10000] Avg train loss: 0.022376\n",
      "Epoch [7760/10000] Avg train loss: 0.022373\n",
      "Epoch [7761/10000] Avg train loss: 0.022370\n",
      "Epoch [7762/10000] Avg train loss: 0.022367\n",
      "Epoch [7763/10000] Avg train loss: 0.022364\n",
      "Epoch [7764/10000] Avg train loss: 0.022361\n",
      "Epoch [7765/10000] Avg train loss: 0.022358\n",
      "Epoch [7766/10000] Avg train loss: 0.022356\n",
      "Epoch [7767/10000] Avg train loss: 0.022353\n",
      "Epoch [7768/10000] Avg train loss: 0.022350\n",
      "Epoch [7769/10000] Avg train loss: 0.022347\n",
      "Epoch [7770/10000] Avg train loss: 0.022344\n",
      "Epoch [7771/10000] Avg train loss: 0.022341\n",
      "Epoch [7772/10000] Avg train loss: 0.022338\n",
      "Epoch [7773/10000] Avg train loss: 0.022335\n",
      "Epoch [7774/10000] Avg train loss: 0.022333\n",
      "Epoch [7775/10000] Avg train loss: 0.022330\n",
      "Epoch [7776/10000] Avg train loss: 0.022327\n",
      "Epoch [7777/10000] Avg train loss: 0.022324\n",
      "Epoch [7778/10000] Avg train loss: 0.022321\n",
      "Epoch [7779/10000] Avg train loss: 0.022318\n",
      "Epoch [7780/10000] Avg train loss: 0.022315\n",
      "Epoch [7781/10000] Avg train loss: 0.022312\n",
      "Epoch [7782/10000] Avg train loss: 0.022310\n",
      "Epoch [7783/10000] Avg train loss: 0.022307\n",
      "Epoch [7784/10000] Avg train loss: 0.022304\n",
      "Epoch [7785/10000] Avg train loss: 0.022301\n",
      "Epoch [7786/10000] Avg train loss: 0.022298\n",
      "Epoch [7787/10000] Avg train loss: 0.022295\n",
      "Epoch [7788/10000] Avg train loss: 0.022292\n",
      "Epoch [7789/10000] Avg train loss: 0.022290\n",
      "Epoch [7790/10000] Avg train loss: 0.022287\n",
      "Epoch [7791/10000] Avg train loss: 0.022284\n",
      "Epoch [7792/10000] Avg train loss: 0.022281\n",
      "Epoch [7793/10000] Avg train loss: 0.022278\n",
      "Epoch [7794/10000] Avg train loss: 0.022275\n",
      "Epoch [7795/10000] Avg train loss: 0.022272\n",
      "Epoch [7796/10000] Avg train loss: 0.022270\n",
      "Epoch [7797/10000] Avg train loss: 0.022267\n",
      "Epoch [7798/10000] Avg train loss: 0.022264\n",
      "Epoch [7799/10000] Avg train loss: 0.022261\n",
      "Epoch [7800/10000] Avg train loss: 0.022258\n",
      "Epoch [7801/10000] Avg train loss: 0.022255\n",
      "Epoch [7802/10000] Avg train loss: 0.022252\n",
      "Epoch [7803/10000] Avg train loss: 0.022250\n",
      "Epoch [7804/10000] Avg train loss: 0.022247\n",
      "Epoch [7805/10000] Avg train loss: 0.022244\n",
      "Epoch [7806/10000] Avg train loss: 0.022241\n",
      "Epoch [7807/10000] Avg train loss: 0.022238\n",
      "Epoch [7808/10000] Avg train loss: 0.022235\n",
      "Epoch [7809/10000] Avg train loss: 0.022232\n",
      "Epoch [7810/10000] Avg train loss: 0.022230\n",
      "Epoch [7811/10000] Avg train loss: 0.022227\n",
      "Epoch [7812/10000] Avg train loss: 0.022224\n",
      "Epoch [7813/10000] Avg train loss: 0.022221\n",
      "Epoch [7814/10000] Avg train loss: 0.022218\n",
      "Epoch [7815/10000] Avg train loss: 0.022215\n",
      "Epoch [7816/10000] Avg train loss: 0.022213\n",
      "Epoch [7817/10000] Avg train loss: 0.022210\n",
      "Epoch [7818/10000] Avg train loss: 0.022207\n",
      "Epoch [7819/10000] Avg train loss: 0.022204\n",
      "Epoch [7820/10000] Avg train loss: 0.022201\n",
      "Epoch [7821/10000] Avg train loss: 0.022198\n",
      "Epoch [7822/10000] Avg train loss: 0.022196\n",
      "Epoch [7823/10000] Avg train loss: 0.022193\n",
      "Epoch [7824/10000] Avg train loss: 0.022190\n",
      "Epoch [7825/10000] Avg train loss: 0.022187\n",
      "Epoch [7826/10000] Avg train loss: 0.022184\n",
      "Epoch [7827/10000] Avg train loss: 0.022181\n",
      "Epoch [7828/10000] Avg train loss: 0.022179\n",
      "Epoch [7829/10000] Avg train loss: 0.022176\n",
      "Epoch [7830/10000] Avg train loss: 0.022173\n",
      "Epoch [7831/10000] Avg train loss: 0.022170\n",
      "Epoch [7832/10000] Avg train loss: 0.022167\n",
      "Epoch [7833/10000] Avg train loss: 0.022164\n",
      "Epoch [7834/10000] Avg train loss: 0.022162\n",
      "Epoch [7835/10000] Avg train loss: 0.022159\n",
      "Epoch [7836/10000] Avg train loss: 0.022156\n",
      "Epoch [7837/10000] Avg train loss: 0.022153\n",
      "Epoch [7838/10000] Avg train loss: 0.022150\n",
      "Epoch [7839/10000] Avg train loss: 0.022147\n",
      "Epoch [7840/10000] Avg train loss: 0.022145\n",
      "Epoch [7841/10000] Avg train loss: 0.022142\n",
      "Epoch [7842/10000] Avg train loss: 0.022139\n",
      "Epoch [7843/10000] Avg train loss: 0.022136\n",
      "Epoch [7844/10000] Avg train loss: 0.022133\n",
      "Epoch [7845/10000] Avg train loss: 0.022130\n",
      "Epoch [7846/10000] Avg train loss: 0.022128\n",
      "Epoch [7847/10000] Avg train loss: 0.022125\n",
      "Epoch [7848/10000] Avg train loss: 0.022122\n",
      "Epoch [7849/10000] Avg train loss: 0.022119\n",
      "Epoch [7850/10000] Avg train loss: 0.022116\n",
      "Epoch [7851/10000] Avg train loss: 0.022114\n",
      "Epoch [7852/10000] Avg train loss: 0.022111\n",
      "Epoch [7853/10000] Avg train loss: 0.022108\n",
      "Epoch [7854/10000] Avg train loss: 0.022105\n",
      "Epoch [7855/10000] Avg train loss: 0.022102\n",
      "Epoch [7856/10000] Avg train loss: 0.022099\n",
      "Epoch [7857/10000] Avg train loss: 0.022097\n",
      "Epoch [7858/10000] Avg train loss: 0.022094\n",
      "Epoch [7859/10000] Avg train loss: 0.022091\n",
      "Epoch [7860/10000] Avg train loss: 0.022088\n",
      "Epoch [7861/10000] Avg train loss: 0.022085\n",
      "Epoch [7862/10000] Avg train loss: 0.022083\n",
      "Epoch [7863/10000] Avg train loss: 0.022080\n",
      "Epoch [7864/10000] Avg train loss: 0.022077\n",
      "Epoch [7865/10000] Avg train loss: 0.022074\n",
      "Epoch [7866/10000] Avg train loss: 0.022071\n",
      "Epoch [7867/10000] Avg train loss: 0.022069\n",
      "Epoch [7868/10000] Avg train loss: 0.022066\n",
      "Epoch [7869/10000] Avg train loss: 0.022063\n",
      "Epoch [7870/10000] Avg train loss: 0.022060\n",
      "Epoch [7871/10000] Avg train loss: 0.022057\n",
      "Epoch [7872/10000] Avg train loss: 0.022055\n",
      "Epoch [7873/10000] Avg train loss: 0.022052\n",
      "Epoch [7874/10000] Avg train loss: 0.022049\n",
      "Epoch [7875/10000] Avg train loss: 0.022046\n",
      "Epoch [7876/10000] Avg train loss: 0.022043\n",
      "Epoch [7877/10000] Avg train loss: 0.022041\n",
      "Epoch [7878/10000] Avg train loss: 0.022038\n",
      "Epoch [7879/10000] Avg train loss: 0.022035\n",
      "Epoch [7880/10000] Avg train loss: 0.022032\n",
      "Epoch [7881/10000] Avg train loss: 0.022029\n",
      "Epoch [7882/10000] Avg train loss: 0.022027\n",
      "Epoch [7883/10000] Avg train loss: 0.022024\n",
      "Epoch [7884/10000] Avg train loss: 0.022021\n",
      "Epoch [7885/10000] Avg train loss: 0.022018\n",
      "Epoch [7886/10000] Avg train loss: 0.022015\n",
      "Epoch [7887/10000] Avg train loss: 0.022013\n",
      "Epoch [7888/10000] Avg train loss: 0.022010\n",
      "Epoch [7889/10000] Avg train loss: 0.022007\n",
      "Epoch [7890/10000] Avg train loss: 0.022004\n",
      "Epoch [7891/10000] Avg train loss: 0.022001\n",
      "Epoch [7892/10000] Avg train loss: 0.021999\n",
      "Epoch [7893/10000] Avg train loss: 0.021996\n",
      "Epoch [7894/10000] Avg train loss: 0.021993\n",
      "Epoch [7895/10000] Avg train loss: 0.021990\n",
      "Epoch [7896/10000] Avg train loss: 0.021988\n",
      "Epoch [7897/10000] Avg train loss: 0.021985\n",
      "Epoch [7898/10000] Avg train loss: 0.021982\n",
      "Epoch [7899/10000] Avg train loss: 0.021979\n",
      "Epoch [7900/10000] Avg train loss: 0.021976\n",
      "Epoch [7901/10000] Avg train loss: 0.021974\n",
      "Epoch [7902/10000] Avg train loss: 0.021971\n",
      "Epoch [7903/10000] Avg train loss: 0.021968\n",
      "Epoch [7904/10000] Avg train loss: 0.021965\n",
      "Epoch [7905/10000] Avg train loss: 0.021962\n",
      "Epoch [7906/10000] Avg train loss: 0.021960\n",
      "Epoch [7907/10000] Avg train loss: 0.021957\n",
      "Epoch [7908/10000] Avg train loss: 0.021954\n",
      "Epoch [7909/10000] Avg train loss: 0.021951\n",
      "Epoch [7910/10000] Avg train loss: 0.021949\n",
      "Epoch [7911/10000] Avg train loss: 0.021946\n",
      "Epoch [7912/10000] Avg train loss: 0.021943\n",
      "Epoch [7913/10000] Avg train loss: 0.021940\n",
      "Epoch [7914/10000] Avg train loss: 0.021938\n",
      "Epoch [7915/10000] Avg train loss: 0.021935\n",
      "Epoch [7916/10000] Avg train loss: 0.021932\n",
      "Epoch [7917/10000] Avg train loss: 0.021929\n",
      "Epoch [7918/10000] Avg train loss: 0.021926\n",
      "Epoch [7919/10000] Avg train loss: 0.021924\n",
      "Epoch [7920/10000] Avg train loss: 0.021921\n",
      "Epoch [7921/10000] Avg train loss: 0.021918\n",
      "Epoch [7922/10000] Avg train loss: 0.021915\n",
      "Epoch [7923/10000] Avg train loss: 0.021913\n",
      "Epoch [7924/10000] Avg train loss: 0.021910\n",
      "Epoch [7925/10000] Avg train loss: 0.021907\n",
      "Epoch [7926/10000] Avg train loss: 0.021904\n",
      "Epoch [7927/10000] Avg train loss: 0.021902\n",
      "Epoch [7928/10000] Avg train loss: 0.021899\n",
      "Epoch [7929/10000] Avg train loss: 0.021896\n",
      "Epoch [7930/10000] Avg train loss: 0.021893\n",
      "Epoch [7931/10000] Avg train loss: 0.021890\n",
      "Epoch [7932/10000] Avg train loss: 0.021888\n",
      "Epoch [7933/10000] Avg train loss: 0.021885\n",
      "Epoch [7934/10000] Avg train loss: 0.021882\n",
      "Epoch [7935/10000] Avg train loss: 0.021879\n",
      "Epoch [7936/10000] Avg train loss: 0.021877\n",
      "Epoch [7937/10000] Avg train loss: 0.021874\n",
      "Epoch [7938/10000] Avg train loss: 0.021871\n",
      "Epoch [7939/10000] Avg train loss: 0.021868\n",
      "Epoch [7940/10000] Avg train loss: 0.021866\n",
      "Epoch [7941/10000] Avg train loss: 0.021863\n",
      "Epoch [7942/10000] Avg train loss: 0.021860\n",
      "Epoch [7943/10000] Avg train loss: 0.021857\n",
      "Epoch [7944/10000] Avg train loss: 0.021855\n",
      "Epoch [7945/10000] Avg train loss: 0.021852\n",
      "Epoch [7946/10000] Avg train loss: 0.021849\n",
      "Epoch [7947/10000] Avg train loss: 0.021846\n",
      "Epoch [7948/10000] Avg train loss: 0.021844\n",
      "Epoch [7949/10000] Avg train loss: 0.021841\n",
      "Epoch [7950/10000] Avg train loss: 0.021838\n",
      "Epoch [7951/10000] Avg train loss: 0.021835\n",
      "Epoch [7952/10000] Avg train loss: 0.021833\n",
      "Epoch [7953/10000] Avg train loss: 0.021830\n",
      "Epoch [7954/10000] Avg train loss: 0.021827\n",
      "Epoch [7955/10000] Avg train loss: 0.021824\n",
      "Epoch [7956/10000] Avg train loss: 0.021822\n",
      "Epoch [7957/10000] Avg train loss: 0.021819\n",
      "Epoch [7958/10000] Avg train loss: 0.021816\n",
      "Epoch [7959/10000] Avg train loss: 0.021813\n",
      "Epoch [7960/10000] Avg train loss: 0.021811\n",
      "Epoch [7961/10000] Avg train loss: 0.021808\n",
      "Epoch [7962/10000] Avg train loss: 0.021805\n",
      "Epoch [7963/10000] Avg train loss: 0.021803\n",
      "Epoch [7964/10000] Avg train loss: 0.021800\n",
      "Epoch [7965/10000] Avg train loss: 0.021797\n",
      "Epoch [7966/10000] Avg train loss: 0.021794\n",
      "Epoch [7967/10000] Avg train loss: 0.021792\n",
      "Epoch [7968/10000] Avg train loss: 0.021789\n",
      "Epoch [7969/10000] Avg train loss: 0.021786\n",
      "Epoch [7970/10000] Avg train loss: 0.021783\n",
      "Epoch [7971/10000] Avg train loss: 0.021781\n",
      "Epoch [7972/10000] Avg train loss: 0.021778\n",
      "Epoch [7973/10000] Avg train loss: 0.021775\n",
      "Epoch [7974/10000] Avg train loss: 0.021772\n",
      "Epoch [7975/10000] Avg train loss: 0.021770\n",
      "Epoch [7976/10000] Avg train loss: 0.021767\n",
      "Epoch [7977/10000] Avg train loss: 0.021764\n",
      "Epoch [7978/10000] Avg train loss: 0.021762\n",
      "Epoch [7979/10000] Avg train loss: 0.021759\n",
      "Epoch [7980/10000] Avg train loss: 0.021756\n",
      "Epoch [7981/10000] Avg train loss: 0.021753\n",
      "Epoch [7982/10000] Avg train loss: 0.021751\n",
      "Epoch [7983/10000] Avg train loss: 0.021748\n",
      "Epoch [7984/10000] Avg train loss: 0.021745\n",
      "Epoch [7985/10000] Avg train loss: 0.021742\n",
      "Epoch [7986/10000] Avg train loss: 0.021740\n",
      "Epoch [7987/10000] Avg train loss: 0.021737\n",
      "Epoch [7988/10000] Avg train loss: 0.021734\n",
      "Epoch [7989/10000] Avg train loss: 0.021732\n",
      "Epoch [7990/10000] Avg train loss: 0.021729\n",
      "Epoch [7991/10000] Avg train loss: 0.021726\n",
      "Epoch [7992/10000] Avg train loss: 0.021723\n",
      "Epoch [7993/10000] Avg train loss: 0.021721\n",
      "Epoch [7994/10000] Avg train loss: 0.021718\n",
      "Epoch [7995/10000] Avg train loss: 0.021715\n",
      "Epoch [7996/10000] Avg train loss: 0.021713\n",
      "Epoch [7997/10000] Avg train loss: 0.021710\n",
      "Epoch [7998/10000] Avg train loss: 0.021707\n",
      "Epoch [7999/10000] Avg train loss: 0.021704\n",
      "Epoch [8000/10000] Avg train loss: 0.021702\n",
      "Epoch [8001/10000] Avg train loss: 0.021699\n",
      "Epoch [8002/10000] Avg train loss: 0.021696\n",
      "Epoch [8003/10000] Avg train loss: 0.021694\n",
      "Epoch [8004/10000] Avg train loss: 0.021691\n",
      "Epoch [8005/10000] Avg train loss: 0.021688\n",
      "Epoch [8006/10000] Avg train loss: 0.021685\n",
      "Epoch [8007/10000] Avg train loss: 0.021683\n",
      "Epoch [8008/10000] Avg train loss: 0.021680\n",
      "Epoch [8009/10000] Avg train loss: 0.021677\n",
      "Epoch [8010/10000] Avg train loss: 0.021675\n",
      "Epoch [8011/10000] Avg train loss: 0.021672\n",
      "Epoch [8012/10000] Avg train loss: 0.021669\n",
      "Epoch [8013/10000] Avg train loss: 0.021666\n",
      "Epoch [8014/10000] Avg train loss: 0.021664\n",
      "Epoch [8015/10000] Avg train loss: 0.021661\n",
      "Epoch [8016/10000] Avg train loss: 0.021658\n",
      "Epoch [8017/10000] Avg train loss: 0.021656\n",
      "Epoch [8018/10000] Avg train loss: 0.021653\n",
      "Epoch [8019/10000] Avg train loss: 0.021650\n",
      "Epoch [8020/10000] Avg train loss: 0.021648\n",
      "Epoch [8021/10000] Avg train loss: 0.021645\n",
      "Epoch [8022/10000] Avg train loss: 0.021642\n",
      "Epoch [8023/10000] Avg train loss: 0.021639\n",
      "Epoch [8024/10000] Avg train loss: 0.021637\n",
      "Epoch [8025/10000] Avg train loss: 0.021634\n",
      "Epoch [8026/10000] Avg train loss: 0.021631\n",
      "Epoch [8027/10000] Avg train loss: 0.021629\n",
      "Epoch [8028/10000] Avg train loss: 0.021626\n",
      "Epoch [8029/10000] Avg train loss: 0.021623\n",
      "Epoch [8030/10000] Avg train loss: 0.021621\n",
      "Epoch [8031/10000] Avg train loss: 0.021618\n",
      "Epoch [8032/10000] Avg train loss: 0.021615\n",
      "Epoch [8033/10000] Avg train loss: 0.021613\n",
      "Epoch [8034/10000] Avg train loss: 0.021610\n",
      "Epoch [8035/10000] Avg train loss: 0.021607\n",
      "Epoch [8036/10000] Avg train loss: 0.021604\n",
      "Epoch [8037/10000] Avg train loss: 0.021602\n",
      "Epoch [8038/10000] Avg train loss: 0.021599\n",
      "Epoch [8039/10000] Avg train loss: 0.021596\n",
      "Epoch [8040/10000] Avg train loss: 0.021594\n",
      "Epoch [8041/10000] Avg train loss: 0.021591\n",
      "Epoch [8042/10000] Avg train loss: 0.021588\n",
      "Epoch [8043/10000] Avg train loss: 0.021586\n",
      "Epoch [8044/10000] Avg train loss: 0.021583\n",
      "Epoch [8045/10000] Avg train loss: 0.021580\n",
      "Epoch [8046/10000] Avg train loss: 0.021578\n",
      "Epoch [8047/10000] Avg train loss: 0.021575\n",
      "Epoch [8048/10000] Avg train loss: 0.021572\n",
      "Epoch [8049/10000] Avg train loss: 0.021570\n",
      "Epoch [8050/10000] Avg train loss: 0.021567\n",
      "Epoch [8051/10000] Avg train loss: 0.021564\n",
      "Epoch [8052/10000] Avg train loss: 0.021562\n",
      "Epoch [8053/10000] Avg train loss: 0.021559\n",
      "Epoch [8054/10000] Avg train loss: 0.021556\n",
      "Epoch [8055/10000] Avg train loss: 0.021554\n",
      "Epoch [8056/10000] Avg train loss: 0.021551\n",
      "Epoch [8057/10000] Avg train loss: 0.021548\n",
      "Epoch [8058/10000] Avg train loss: 0.021545\n",
      "Epoch [8059/10000] Avg train loss: 0.021543\n",
      "Epoch [8060/10000] Avg train loss: 0.021540\n",
      "Epoch [8061/10000] Avg train loss: 0.021537\n",
      "Epoch [8062/10000] Avg train loss: 0.021535\n",
      "Epoch [8063/10000] Avg train loss: 0.021532\n",
      "Epoch [8064/10000] Avg train loss: 0.021529\n",
      "Epoch [8065/10000] Avg train loss: 0.021527\n",
      "Epoch [8066/10000] Avg train loss: 0.021524\n",
      "Epoch [8067/10000] Avg train loss: 0.021521\n",
      "Epoch [8068/10000] Avg train loss: 0.021519\n",
      "Epoch [8069/10000] Avg train loss: 0.021516\n",
      "Epoch [8070/10000] Avg train loss: 0.021513\n",
      "Epoch [8071/10000] Avg train loss: 0.021511\n",
      "Epoch [8072/10000] Avg train loss: 0.021508\n",
      "Epoch [8073/10000] Avg train loss: 0.021505\n",
      "Epoch [8074/10000] Avg train loss: 0.021503\n",
      "Epoch [8075/10000] Avg train loss: 0.021500\n",
      "Epoch [8076/10000] Avg train loss: 0.021497\n",
      "Epoch [8077/10000] Avg train loss: 0.021495\n",
      "Epoch [8078/10000] Avg train loss: 0.021492\n",
      "Epoch [8079/10000] Avg train loss: 0.021489\n",
      "Epoch [8080/10000] Avg train loss: 0.021487\n",
      "Epoch [8081/10000] Avg train loss: 0.021484\n",
      "Epoch [8082/10000] Avg train loss: 0.021482\n",
      "Epoch [8083/10000] Avg train loss: 0.021479\n",
      "Epoch [8084/10000] Avg train loss: 0.021476\n",
      "Epoch [8085/10000] Avg train loss: 0.021474\n",
      "Epoch [8086/10000] Avg train loss: 0.021471\n",
      "Epoch [8087/10000] Avg train loss: 0.021468\n",
      "Epoch [8088/10000] Avg train loss: 0.021466\n",
      "Epoch [8089/10000] Avg train loss: 0.021463\n",
      "Epoch [8090/10000] Avg train loss: 0.021460\n",
      "Epoch [8091/10000] Avg train loss: 0.021458\n",
      "Epoch [8092/10000] Avg train loss: 0.021455\n",
      "Epoch [8093/10000] Avg train loss: 0.021452\n",
      "Epoch [8094/10000] Avg train loss: 0.021450\n",
      "Epoch [8095/10000] Avg train loss: 0.021447\n",
      "Epoch [8096/10000] Avg train loss: 0.021444\n",
      "Epoch [8097/10000] Avg train loss: 0.021442\n",
      "Epoch [8098/10000] Avg train loss: 0.021439\n",
      "Epoch [8099/10000] Avg train loss: 0.021436\n",
      "Epoch [8100/10000] Avg train loss: 0.021434\n",
      "Epoch [8101/10000] Avg train loss: 0.021431\n",
      "Epoch [8102/10000] Avg train loss: 0.021428\n",
      "Epoch [8103/10000] Avg train loss: 0.021426\n",
      "Epoch [8104/10000] Avg train loss: 0.021423\n",
      "Epoch [8105/10000] Avg train loss: 0.021421\n",
      "Epoch [8106/10000] Avg train loss: 0.021418\n",
      "Epoch [8107/10000] Avg train loss: 0.021415\n",
      "Epoch [8108/10000] Avg train loss: 0.021413\n",
      "Epoch [8109/10000] Avg train loss: 0.021410\n",
      "Epoch [8110/10000] Avg train loss: 0.021407\n",
      "Epoch [8111/10000] Avg train loss: 0.021405\n",
      "Epoch [8112/10000] Avg train loss: 0.021402\n",
      "Epoch [8113/10000] Avg train loss: 0.021399\n",
      "Epoch [8114/10000] Avg train loss: 0.021397\n",
      "Epoch [8115/10000] Avg train loss: 0.021394\n",
      "Epoch [8116/10000] Avg train loss: 0.021392\n",
      "Epoch [8117/10000] Avg train loss: 0.021389\n",
      "Epoch [8118/10000] Avg train loss: 0.021386\n",
      "Epoch [8119/10000] Avg train loss: 0.021384\n",
      "Epoch [8120/10000] Avg train loss: 0.021381\n",
      "Epoch [8121/10000] Avg train loss: 0.021378\n",
      "Epoch [8122/10000] Avg train loss: 0.021376\n",
      "Epoch [8123/10000] Avg train loss: 0.021373\n",
      "Epoch [8124/10000] Avg train loss: 0.021370\n",
      "Epoch [8125/10000] Avg train loss: 0.021368\n",
      "Epoch [8126/10000] Avg train loss: 0.021365\n",
      "Epoch [8127/10000] Avg train loss: 0.021363\n",
      "Epoch [8128/10000] Avg train loss: 0.021360\n",
      "Epoch [8129/10000] Avg train loss: 0.021357\n",
      "Epoch [8130/10000] Avg train loss: 0.021355\n",
      "Epoch [8131/10000] Avg train loss: 0.021352\n",
      "Epoch [8132/10000] Avg train loss: 0.021349\n",
      "Epoch [8133/10000] Avg train loss: 0.021347\n",
      "Epoch [8134/10000] Avg train loss: 0.021344\n",
      "Epoch [8135/10000] Avg train loss: 0.021342\n",
      "Epoch [8136/10000] Avg train loss: 0.021339\n",
      "Epoch [8137/10000] Avg train loss: 0.021336\n",
      "Epoch [8138/10000] Avg train loss: 0.021334\n",
      "Epoch [8139/10000] Avg train loss: 0.021331\n",
      "Epoch [8140/10000] Avg train loss: 0.021328\n",
      "Epoch [8141/10000] Avg train loss: 0.021326\n",
      "Epoch [8142/10000] Avg train loss: 0.021323\n",
      "Epoch [8143/10000] Avg train loss: 0.021321\n",
      "Epoch [8144/10000] Avg train loss: 0.021318\n",
      "Epoch [8145/10000] Avg train loss: 0.021315\n",
      "Epoch [8146/10000] Avg train loss: 0.021313\n",
      "Epoch [8147/10000] Avg train loss: 0.021310\n",
      "Epoch [8148/10000] Avg train loss: 0.021308\n",
      "Epoch [8149/10000] Avg train loss: 0.021305\n",
      "Epoch [8150/10000] Avg train loss: 0.021302\n",
      "Epoch [8151/10000] Avg train loss: 0.021300\n",
      "Epoch [8152/10000] Avg train loss: 0.021297\n",
      "Epoch [8153/10000] Avg train loss: 0.021294\n",
      "Epoch [8154/10000] Avg train loss: 0.021292\n",
      "Epoch [8155/10000] Avg train loss: 0.021289\n",
      "Epoch [8156/10000] Avg train loss: 0.021287\n",
      "Epoch [8157/10000] Avg train loss: 0.021284\n",
      "Epoch [8158/10000] Avg train loss: 0.021281\n",
      "Epoch [8159/10000] Avg train loss: 0.021279\n",
      "Epoch [8160/10000] Avg train loss: 0.021276\n",
      "Epoch [8161/10000] Avg train loss: 0.021274\n",
      "Epoch [8162/10000] Avg train loss: 0.021271\n",
      "Epoch [8163/10000] Avg train loss: 0.021268\n",
      "Epoch [8164/10000] Avg train loss: 0.021266\n",
      "Epoch [8165/10000] Avg train loss: 0.021263\n",
      "Epoch [8166/10000] Avg train loss: 0.021261\n",
      "Epoch [8167/10000] Avg train loss: 0.021258\n",
      "Epoch [8168/10000] Avg train loss: 0.021255\n",
      "Epoch [8169/10000] Avg train loss: 0.021253\n",
      "Epoch [8170/10000] Avg train loss: 0.021250\n",
      "Epoch [8171/10000] Avg train loss: 0.021248\n",
      "Epoch [8172/10000] Avg train loss: 0.021245\n",
      "Epoch [8173/10000] Avg train loss: 0.021242\n",
      "Epoch [8174/10000] Avg train loss: 0.021240\n",
      "Epoch [8175/10000] Avg train loss: 0.021237\n",
      "Epoch [8176/10000] Avg train loss: 0.021235\n",
      "Epoch [8177/10000] Avg train loss: 0.021232\n",
      "Epoch [8178/10000] Avg train loss: 0.021229\n",
      "Epoch [8179/10000] Avg train loss: 0.021227\n",
      "Epoch [8180/10000] Avg train loss: 0.021224\n",
      "Epoch [8181/10000] Avg train loss: 0.021222\n",
      "Epoch [8182/10000] Avg train loss: 0.021219\n",
      "Epoch [8183/10000] Avg train loss: 0.021216\n",
      "Epoch [8184/10000] Avg train loss: 0.021214\n",
      "Epoch [8185/10000] Avg train loss: 0.021211\n",
      "Epoch [8186/10000] Avg train loss: 0.021209\n",
      "Epoch [8187/10000] Avg train loss: 0.021206\n",
      "Epoch [8188/10000] Avg train loss: 0.021203\n",
      "Epoch [8189/10000] Avg train loss: 0.021201\n",
      "Epoch [8190/10000] Avg train loss: 0.021198\n",
      "Epoch [8191/10000] Avg train loss: 0.021196\n",
      "Epoch [8192/10000] Avg train loss: 0.021193\n",
      "Epoch [8193/10000] Avg train loss: 0.021190\n",
      "Epoch [8194/10000] Avg train loss: 0.021188\n",
      "Epoch [8195/10000] Avg train loss: 0.021185\n",
      "Epoch [8196/10000] Avg train loss: 0.021183\n",
      "Epoch [8197/10000] Avg train loss: 0.021180\n",
      "Epoch [8198/10000] Avg train loss: 0.021178\n",
      "Epoch [8199/10000] Avg train loss: 0.021175\n",
      "Epoch [8200/10000] Avg train loss: 0.021172\n",
      "Epoch [8201/10000] Avg train loss: 0.021170\n",
      "Epoch [8202/10000] Avg train loss: 0.021167\n",
      "Epoch [8203/10000] Avg train loss: 0.021165\n",
      "Epoch [8204/10000] Avg train loss: 0.021162\n",
      "Epoch [8205/10000] Avg train loss: 0.021159\n",
      "Epoch [8206/10000] Avg train loss: 0.021157\n",
      "Epoch [8207/10000] Avg train loss: 0.021154\n",
      "Epoch [8208/10000] Avg train loss: 0.021152\n",
      "Epoch [8209/10000] Avg train loss: 0.021149\n",
      "Epoch [8210/10000] Avg train loss: 0.021147\n",
      "Epoch [8211/10000] Avg train loss: 0.021144\n",
      "Epoch [8212/10000] Avg train loss: 0.021141\n",
      "Epoch [8213/10000] Avg train loss: 0.021139\n",
      "Epoch [8214/10000] Avg train loss: 0.021136\n",
      "Epoch [8215/10000] Avg train loss: 0.021134\n",
      "Epoch [8216/10000] Avg train loss: 0.021131\n",
      "Epoch [8217/10000] Avg train loss: 0.021129\n",
      "Epoch [8218/10000] Avg train loss: 0.021126\n",
      "Epoch [8219/10000] Avg train loss: 0.021123\n",
      "Epoch [8220/10000] Avg train loss: 0.021121\n",
      "Epoch [8221/10000] Avg train loss: 0.021118\n",
      "Epoch [8222/10000] Avg train loss: 0.021116\n",
      "Epoch [8223/10000] Avg train loss: 0.021113\n",
      "Epoch [8224/10000] Avg train loss: 0.021111\n",
      "Epoch [8225/10000] Avg train loss: 0.021108\n",
      "Epoch [8226/10000] Avg train loss: 0.021105\n",
      "Epoch [8227/10000] Avg train loss: 0.021103\n",
      "Epoch [8228/10000] Avg train loss: 0.021100\n",
      "Epoch [8229/10000] Avg train loss: 0.021098\n",
      "Epoch [8230/10000] Avg train loss: 0.021095\n",
      "Epoch [8231/10000] Avg train loss: 0.021093\n",
      "Epoch [8232/10000] Avg train loss: 0.021090\n",
      "Epoch [8233/10000] Avg train loss: 0.021088\n",
      "Epoch [8234/10000] Avg train loss: 0.021085\n",
      "Epoch [8235/10000] Avg train loss: 0.021082\n",
      "Epoch [8236/10000] Avg train loss: 0.021080\n",
      "Epoch [8237/10000] Avg train loss: 0.021077\n",
      "Epoch [8238/10000] Avg train loss: 0.021075\n",
      "Epoch [8239/10000] Avg train loss: 0.021072\n",
      "Epoch [8240/10000] Avg train loss: 0.021070\n",
      "Epoch [8241/10000] Avg train loss: 0.021067\n",
      "Epoch [8242/10000] Avg train loss: 0.021064\n",
      "Epoch [8243/10000] Avg train loss: 0.021062\n",
      "Epoch [8244/10000] Avg train loss: 0.021059\n",
      "Epoch [8245/10000] Avg train loss: 0.021057\n",
      "Epoch [8246/10000] Avg train loss: 0.021054\n",
      "Epoch [8247/10000] Avg train loss: 0.021052\n",
      "Epoch [8248/10000] Avg train loss: 0.021049\n",
      "Epoch [8249/10000] Avg train loss: 0.021047\n",
      "Epoch [8250/10000] Avg train loss: 0.021044\n",
      "Epoch [8251/10000] Avg train loss: 0.021042\n",
      "Epoch [8252/10000] Avg train loss: 0.021039\n",
      "Epoch [8253/10000] Avg train loss: 0.021036\n",
      "Epoch [8254/10000] Avg train loss: 0.021034\n",
      "Epoch [8255/10000] Avg train loss: 0.021031\n",
      "Epoch [8256/10000] Avg train loss: 0.021029\n",
      "Epoch [8257/10000] Avg train loss: 0.021026\n",
      "Epoch [8258/10000] Avg train loss: 0.021024\n",
      "Epoch [8259/10000] Avg train loss: 0.021021\n",
      "Epoch [8260/10000] Avg train loss: 0.021019\n",
      "Epoch [8261/10000] Avg train loss: 0.021016\n",
      "Epoch [8262/10000] Avg train loss: 0.021013\n",
      "Epoch [8263/10000] Avg train loss: 0.021011\n",
      "Epoch [8264/10000] Avg train loss: 0.021008\n",
      "Epoch [8265/10000] Avg train loss: 0.021006\n",
      "Epoch [8266/10000] Avg train loss: 0.021003\n",
      "Epoch [8267/10000] Avg train loss: 0.021001\n",
      "Epoch [8268/10000] Avg train loss: 0.020998\n",
      "Epoch [8269/10000] Avg train loss: 0.020996\n",
      "Epoch [8270/10000] Avg train loss: 0.020993\n",
      "Epoch [8271/10000] Avg train loss: 0.020991\n",
      "Epoch [8272/10000] Avg train loss: 0.020988\n",
      "Epoch [8273/10000] Avg train loss: 0.020986\n",
      "Epoch [8274/10000] Avg train loss: 0.020983\n",
      "Epoch [8275/10000] Avg train loss: 0.020980\n",
      "Epoch [8276/10000] Avg train loss: 0.020978\n",
      "Epoch [8277/10000] Avg train loss: 0.020975\n",
      "Epoch [8278/10000] Avg train loss: 0.020973\n",
      "Epoch [8279/10000] Avg train loss: 0.020970\n",
      "Epoch [8280/10000] Avg train loss: 0.020968\n",
      "Epoch [8281/10000] Avg train loss: 0.020965\n",
      "Epoch [8282/10000] Avg train loss: 0.020963\n",
      "Epoch [8283/10000] Avg train loss: 0.020960\n",
      "Epoch [8284/10000] Avg train loss: 0.020958\n",
      "Epoch [8285/10000] Avg train loss: 0.020955\n",
      "Epoch [8286/10000] Avg train loss: 0.020953\n",
      "Epoch [8287/10000] Avg train loss: 0.020950\n",
      "Epoch [8288/10000] Avg train loss: 0.020948\n",
      "Epoch [8289/10000] Avg train loss: 0.020945\n",
      "Epoch [8290/10000] Avg train loss: 0.020943\n",
      "Epoch [8291/10000] Avg train loss: 0.020940\n",
      "Epoch [8292/10000] Avg train loss: 0.020937\n",
      "Epoch [8293/10000] Avg train loss: 0.020935\n",
      "Epoch [8294/10000] Avg train loss: 0.020932\n",
      "Epoch [8295/10000] Avg train loss: 0.020930\n",
      "Epoch [8296/10000] Avg train loss: 0.020927\n",
      "Epoch [8297/10000] Avg train loss: 0.020925\n",
      "Epoch [8298/10000] Avg train loss: 0.020922\n",
      "Epoch [8299/10000] Avg train loss: 0.020920\n",
      "Epoch [8300/10000] Avg train loss: 0.020917\n",
      "Epoch [8301/10000] Avg train loss: 0.020915\n",
      "Epoch [8302/10000] Avg train loss: 0.020912\n",
      "Epoch [8303/10000] Avg train loss: 0.020910\n",
      "Epoch [8304/10000] Avg train loss: 0.020907\n",
      "Epoch [8305/10000] Avg train loss: 0.020905\n",
      "Epoch [8306/10000] Avg train loss: 0.020902\n",
      "Epoch [8307/10000] Avg train loss: 0.020900\n",
      "Epoch [8308/10000] Avg train loss: 0.020897\n",
      "Epoch [8309/10000] Avg train loss: 0.020895\n",
      "Epoch [8310/10000] Avg train loss: 0.020892\n",
      "Epoch [8311/10000] Avg train loss: 0.020890\n",
      "Epoch [8312/10000] Avg train loss: 0.020887\n",
      "Epoch [8313/10000] Avg train loss: 0.020885\n",
      "Epoch [8314/10000] Avg train loss: 0.020882\n",
      "Epoch [8315/10000] Avg train loss: 0.020880\n",
      "Epoch [8316/10000] Avg train loss: 0.020877\n",
      "Epoch [8317/10000] Avg train loss: 0.020875\n",
      "Epoch [8318/10000] Avg train loss: 0.020872\n",
      "Epoch [8319/10000] Avg train loss: 0.020870\n",
      "Epoch [8320/10000] Avg train loss: 0.020867\n",
      "Epoch [8321/10000] Avg train loss: 0.020865\n",
      "Epoch [8322/10000] Avg train loss: 0.020862\n",
      "Epoch [8323/10000] Avg train loss: 0.020859\n",
      "Epoch [8324/10000] Avg train loss: 0.020857\n",
      "Epoch [8325/10000] Avg train loss: 0.020854\n",
      "Epoch [8326/10000] Avg train loss: 0.020852\n",
      "Epoch [8327/10000] Avg train loss: 0.020849\n",
      "Epoch [8328/10000] Avg train loss: 0.020847\n",
      "Epoch [8329/10000] Avg train loss: 0.020844\n",
      "Epoch [8330/10000] Avg train loss: 0.020842\n",
      "Epoch [8331/10000] Avg train loss: 0.020839\n",
      "Epoch [8332/10000] Avg train loss: 0.020837\n",
      "Epoch [8333/10000] Avg train loss: 0.020834\n",
      "Epoch [8334/10000] Avg train loss: 0.020832\n",
      "Epoch [8335/10000] Avg train loss: 0.020829\n",
      "Epoch [8336/10000] Avg train loss: 0.020827\n",
      "Epoch [8337/10000] Avg train loss: 0.020824\n",
      "Epoch [8338/10000] Avg train loss: 0.020822\n",
      "Epoch [8339/10000] Avg train loss: 0.020819\n",
      "Epoch [8340/10000] Avg train loss: 0.020817\n",
      "Epoch [8341/10000] Avg train loss: 0.020814\n",
      "Epoch [8342/10000] Avg train loss: 0.020812\n",
      "Epoch [8343/10000] Avg train loss: 0.020809\n",
      "Epoch [8344/10000] Avg train loss: 0.020807\n",
      "Epoch [8345/10000] Avg train loss: 0.020804\n",
      "Epoch [8346/10000] Avg train loss: 0.020802\n",
      "Epoch [8347/10000] Avg train loss: 0.020800\n",
      "Epoch [8348/10000] Avg train loss: 0.020797\n",
      "Epoch [8349/10000] Avg train loss: 0.020795\n",
      "Epoch [8350/10000] Avg train loss: 0.020792\n",
      "Epoch [8351/10000] Avg train loss: 0.020790\n",
      "Epoch [8352/10000] Avg train loss: 0.020787\n",
      "Epoch [8353/10000] Avg train loss: 0.020785\n",
      "Epoch [8354/10000] Avg train loss: 0.020782\n",
      "Epoch [8355/10000] Avg train loss: 0.020780\n",
      "Epoch [8356/10000] Avg train loss: 0.020777\n",
      "Epoch [8357/10000] Avg train loss: 0.020775\n",
      "Epoch [8358/10000] Avg train loss: 0.020772\n",
      "Epoch [8359/10000] Avg train loss: 0.020770\n",
      "Epoch [8360/10000] Avg train loss: 0.020767\n",
      "Epoch [8361/10000] Avg train loss: 0.020765\n",
      "Epoch [8362/10000] Avg train loss: 0.020762\n",
      "Epoch [8363/10000] Avg train loss: 0.020760\n",
      "Epoch [8364/10000] Avg train loss: 0.020757\n",
      "Epoch [8365/10000] Avg train loss: 0.020755\n",
      "Epoch [8366/10000] Avg train loss: 0.020752\n",
      "Epoch [8367/10000] Avg train loss: 0.020750\n",
      "Epoch [8368/10000] Avg train loss: 0.020747\n",
      "Epoch [8369/10000] Avg train loss: 0.020745\n",
      "Epoch [8370/10000] Avg train loss: 0.020742\n",
      "Epoch [8371/10000] Avg train loss: 0.020740\n",
      "Epoch [8372/10000] Avg train loss: 0.020737\n",
      "Epoch [8373/10000] Avg train loss: 0.020735\n",
      "Epoch [8374/10000] Avg train loss: 0.020732\n",
      "Epoch [8375/10000] Avg train loss: 0.020730\n",
      "Epoch [8376/10000] Avg train loss: 0.020727\n",
      "Epoch [8377/10000] Avg train loss: 0.020725\n",
      "Epoch [8378/10000] Avg train loss: 0.020723\n",
      "Epoch [8379/10000] Avg train loss: 0.020720\n",
      "Epoch [8380/10000] Avg train loss: 0.020718\n",
      "Epoch [8381/10000] Avg train loss: 0.020715\n",
      "Epoch [8382/10000] Avg train loss: 0.020713\n",
      "Epoch [8383/10000] Avg train loss: 0.020710\n",
      "Epoch [8384/10000] Avg train loss: 0.020708\n",
      "Epoch [8385/10000] Avg train loss: 0.020705\n",
      "Epoch [8386/10000] Avg train loss: 0.020703\n",
      "Epoch [8387/10000] Avg train loss: 0.020700\n",
      "Epoch [8388/10000] Avg train loss: 0.020698\n",
      "Epoch [8389/10000] Avg train loss: 0.020695\n",
      "Epoch [8390/10000] Avg train loss: 0.020693\n",
      "Epoch [8391/10000] Avg train loss: 0.020690\n",
      "Epoch [8392/10000] Avg train loss: 0.020688\n",
      "Epoch [8393/10000] Avg train loss: 0.020686\n",
      "Epoch [8394/10000] Avg train loss: 0.020683\n",
      "Epoch [8395/10000] Avg train loss: 0.020681\n",
      "Epoch [8396/10000] Avg train loss: 0.020678\n",
      "Epoch [8397/10000] Avg train loss: 0.020676\n",
      "Epoch [8398/10000] Avg train loss: 0.020673\n",
      "Epoch [8399/10000] Avg train loss: 0.020671\n",
      "Epoch [8400/10000] Avg train loss: 0.020668\n",
      "Epoch [8401/10000] Avg train loss: 0.020666\n",
      "Epoch [8402/10000] Avg train loss: 0.020663\n",
      "Epoch [8403/10000] Avg train loss: 0.020661\n",
      "Epoch [8404/10000] Avg train loss: 0.020658\n",
      "Epoch [8405/10000] Avg train loss: 0.020656\n",
      "Epoch [8406/10000] Avg train loss: 0.020654\n",
      "Epoch [8407/10000] Avg train loss: 0.020651\n",
      "Epoch [8408/10000] Avg train loss: 0.020649\n",
      "Epoch [8409/10000] Avg train loss: 0.020646\n",
      "Epoch [8410/10000] Avg train loss: 0.020644\n",
      "Epoch [8411/10000] Avg train loss: 0.020641\n",
      "Epoch [8412/10000] Avg train loss: 0.020639\n",
      "Epoch [8413/10000] Avg train loss: 0.020636\n",
      "Epoch [8414/10000] Avg train loss: 0.020634\n",
      "Epoch [8415/10000] Avg train loss: 0.020631\n",
      "Epoch [8416/10000] Avg train loss: 0.020629\n",
      "Epoch [8417/10000] Avg train loss: 0.020627\n",
      "Epoch [8418/10000] Avg train loss: 0.020624\n",
      "Epoch [8419/10000] Avg train loss: 0.020622\n",
      "Epoch [8420/10000] Avg train loss: 0.020619\n",
      "Epoch [8421/10000] Avg train loss: 0.020617\n",
      "Epoch [8422/10000] Avg train loss: 0.020614\n",
      "Epoch [8423/10000] Avg train loss: 0.020612\n",
      "Epoch [8424/10000] Avg train loss: 0.020609\n",
      "Epoch [8425/10000] Avg train loss: 0.020607\n",
      "Epoch [8426/10000] Avg train loss: 0.020605\n",
      "Epoch [8427/10000] Avg train loss: 0.020602\n",
      "Epoch [8428/10000] Avg train loss: 0.020600\n",
      "Epoch [8429/10000] Avg train loss: 0.020597\n",
      "Epoch [8430/10000] Avg train loss: 0.020595\n",
      "Epoch [8431/10000] Avg train loss: 0.020592\n",
      "Epoch [8432/10000] Avg train loss: 0.020590\n",
      "Epoch [8433/10000] Avg train loss: 0.020587\n",
      "Epoch [8434/10000] Avg train loss: 0.020585\n",
      "Epoch [8435/10000] Avg train loss: 0.020583\n",
      "Epoch [8436/10000] Avg train loss: 0.020580\n",
      "Epoch [8437/10000] Avg train loss: 0.020578\n",
      "Epoch [8438/10000] Avg train loss: 0.020575\n",
      "Epoch [8439/10000] Avg train loss: 0.020573\n",
      "Epoch [8440/10000] Avg train loss: 0.020570\n",
      "Epoch [8441/10000] Avg train loss: 0.020568\n",
      "Epoch [8442/10000] Avg train loss: 0.020565\n",
      "Epoch [8443/10000] Avg train loss: 0.020563\n",
      "Epoch [8444/10000] Avg train loss: 0.020561\n",
      "Epoch [8445/10000] Avg train loss: 0.020558\n",
      "Epoch [8446/10000] Avg train loss: 0.020556\n",
      "Epoch [8447/10000] Avg train loss: 0.020553\n",
      "Epoch [8448/10000] Avg train loss: 0.020551\n",
      "Epoch [8449/10000] Avg train loss: 0.020548\n",
      "Epoch [8450/10000] Avg train loss: 0.020546\n",
      "Epoch [8451/10000] Avg train loss: 0.020544\n",
      "Epoch [8452/10000] Avg train loss: 0.020541\n",
      "Epoch [8453/10000] Avg train loss: 0.020539\n",
      "Epoch [8454/10000] Avg train loss: 0.020536\n",
      "Epoch [8455/10000] Avg train loss: 0.020534\n",
      "Epoch [8456/10000] Avg train loss: 0.020531\n",
      "Epoch [8457/10000] Avg train loss: 0.020529\n",
      "Epoch [8458/10000] Avg train loss: 0.020527\n",
      "Epoch [8459/10000] Avg train loss: 0.020524\n",
      "Epoch [8460/10000] Avg train loss: 0.020522\n",
      "Epoch [8461/10000] Avg train loss: 0.020519\n",
      "Epoch [8462/10000] Avg train loss: 0.020517\n",
      "Epoch [8463/10000] Avg train loss: 0.020514\n",
      "Epoch [8464/10000] Avg train loss: 0.020512\n",
      "Epoch [8465/10000] Avg train loss: 0.020510\n",
      "Epoch [8466/10000] Avg train loss: 0.020507\n",
      "Epoch [8467/10000] Avg train loss: 0.020505\n",
      "Epoch [8468/10000] Avg train loss: 0.020502\n",
      "Epoch [8469/10000] Avg train loss: 0.020500\n",
      "Epoch [8470/10000] Avg train loss: 0.020497\n",
      "Epoch [8471/10000] Avg train loss: 0.020495\n",
      "Epoch [8472/10000] Avg train loss: 0.020493\n",
      "Epoch [8473/10000] Avg train loss: 0.020490\n",
      "Epoch [8474/10000] Avg train loss: 0.020488\n",
      "Epoch [8475/10000] Avg train loss: 0.020485\n",
      "Epoch [8476/10000] Avg train loss: 0.020483\n",
      "Epoch [8477/10000] Avg train loss: 0.020481\n",
      "Epoch [8478/10000] Avg train loss: 0.020478\n",
      "Epoch [8479/10000] Avg train loss: 0.020476\n",
      "Epoch [8480/10000] Avg train loss: 0.020473\n",
      "Epoch [8481/10000] Avg train loss: 0.020471\n",
      "Epoch [8482/10000] Avg train loss: 0.020468\n",
      "Epoch [8483/10000] Avg train loss: 0.020466\n",
      "Epoch [8484/10000] Avg train loss: 0.020464\n",
      "Epoch [8485/10000] Avg train loss: 0.020461\n",
      "Epoch [8486/10000] Avg train loss: 0.020459\n",
      "Epoch [8487/10000] Avg train loss: 0.020456\n",
      "Epoch [8488/10000] Avg train loss: 0.020454\n",
      "Epoch [8489/10000] Avg train loss: 0.020452\n",
      "Epoch [8490/10000] Avg train loss: 0.020449\n",
      "Epoch [8491/10000] Avg train loss: 0.020447\n",
      "Epoch [8492/10000] Avg train loss: 0.020444\n",
      "Epoch [8493/10000] Avg train loss: 0.020442\n",
      "Epoch [8494/10000] Avg train loss: 0.020440\n",
      "Epoch [8495/10000] Avg train loss: 0.020437\n",
      "Epoch [8496/10000] Avg train loss: 0.020435\n",
      "Epoch [8497/10000] Avg train loss: 0.020432\n",
      "Epoch [8498/10000] Avg train loss: 0.020430\n",
      "Epoch [8499/10000] Avg train loss: 0.020428\n",
      "Epoch [8500/10000] Avg train loss: 0.020425\n",
      "Epoch [8501/10000] Avg train loss: 0.020423\n",
      "Epoch [8502/10000] Avg train loss: 0.020420\n",
      "Epoch [8503/10000] Avg train loss: 0.020418\n",
      "Epoch [8504/10000] Avg train loss: 0.020416\n",
      "Epoch [8505/10000] Avg train loss: 0.020413\n",
      "Epoch [8506/10000] Avg train loss: 0.020411\n",
      "Epoch [8507/10000] Avg train loss: 0.020408\n",
      "Epoch [8508/10000] Avg train loss: 0.020406\n",
      "Epoch [8509/10000] Avg train loss: 0.020404\n",
      "Epoch [8510/10000] Avg train loss: 0.020401\n",
      "Epoch [8511/10000] Avg train loss: 0.020399\n",
      "Epoch [8512/10000] Avg train loss: 0.020396\n",
      "Epoch [8513/10000] Avg train loss: 0.020394\n",
      "Epoch [8514/10000] Avg train loss: 0.020392\n",
      "Epoch [8515/10000] Avg train loss: 0.020389\n",
      "Epoch [8516/10000] Avg train loss: 0.020387\n",
      "Epoch [8517/10000] Avg train loss: 0.020384\n",
      "Epoch [8518/10000] Avg train loss: 0.020382\n",
      "Epoch [8519/10000] Avg train loss: 0.020380\n",
      "Epoch [8520/10000] Avg train loss: 0.020377\n",
      "Epoch [8521/10000] Avg train loss: 0.020375\n",
      "Epoch [8522/10000] Avg train loss: 0.020372\n",
      "Epoch [8523/10000] Avg train loss: 0.020370\n",
      "Epoch [8524/10000] Avg train loss: 0.020368\n",
      "Epoch [8525/10000] Avg train loss: 0.020365\n",
      "Epoch [8526/10000] Avg train loss: 0.020363\n",
      "Epoch [8527/10000] Avg train loss: 0.020360\n",
      "Epoch [8528/10000] Avg train loss: 0.020358\n",
      "Epoch [8529/10000] Avg train loss: 0.020356\n",
      "Epoch [8530/10000] Avg train loss: 0.020353\n",
      "Epoch [8531/10000] Avg train loss: 0.020351\n",
      "Epoch [8532/10000] Avg train loss: 0.020349\n",
      "Epoch [8533/10000] Avg train loss: 0.020346\n",
      "Epoch [8534/10000] Avg train loss: 0.020344\n",
      "Epoch [8535/10000] Avg train loss: 0.020341\n",
      "Epoch [8536/10000] Avg train loss: 0.020339\n",
      "Epoch [8537/10000] Avg train loss: 0.020337\n",
      "Epoch [8538/10000] Avg train loss: 0.020334\n",
      "Epoch [8539/10000] Avg train loss: 0.020332\n",
      "Epoch [8540/10000] Avg train loss: 0.020329\n",
      "Epoch [8541/10000] Avg train loss: 0.020327\n",
      "Epoch [8542/10000] Avg train loss: 0.020325\n",
      "Epoch [8543/10000] Avg train loss: 0.020322\n",
      "Epoch [8544/10000] Avg train loss: 0.020320\n",
      "Epoch [8545/10000] Avg train loss: 0.020318\n",
      "Epoch [8546/10000] Avg train loss: 0.020315\n",
      "Epoch [8547/10000] Avg train loss: 0.020313\n",
      "Epoch [8548/10000] Avg train loss: 0.020310\n",
      "Epoch [8549/10000] Avg train loss: 0.020308\n",
      "Epoch [8550/10000] Avg train loss: 0.020306\n",
      "Epoch [8551/10000] Avg train loss: 0.020303\n",
      "Epoch [8552/10000] Avg train loss: 0.020301\n",
      "Epoch [8553/10000] Avg train loss: 0.020299\n",
      "Epoch [8554/10000] Avg train loss: 0.020296\n",
      "Epoch [8555/10000] Avg train loss: 0.020294\n",
      "Epoch [8556/10000] Avg train loss: 0.020291\n",
      "Epoch [8557/10000] Avg train loss: 0.020289\n",
      "Epoch [8558/10000] Avg train loss: 0.020287\n",
      "Epoch [8559/10000] Avg train loss: 0.020284\n",
      "Epoch [8560/10000] Avg train loss: 0.020282\n",
      "Epoch [8561/10000] Avg train loss: 0.020280\n",
      "Epoch [8562/10000] Avg train loss: 0.020277\n",
      "Epoch [8563/10000] Avg train loss: 0.020275\n",
      "Epoch [8564/10000] Avg train loss: 0.020272\n",
      "Epoch [8565/10000] Avg train loss: 0.020270\n",
      "Epoch [8566/10000] Avg train loss: 0.020268\n",
      "Epoch [8567/10000] Avg train loss: 0.020265\n",
      "Epoch [8568/10000] Avg train loss: 0.020263\n",
      "Epoch [8569/10000] Avg train loss: 0.020261\n",
      "Epoch [8570/10000] Avg train loss: 0.020258\n",
      "Epoch [8571/10000] Avg train loss: 0.020256\n",
      "Epoch [8572/10000] Avg train loss: 0.020254\n",
      "Epoch [8573/10000] Avg train loss: 0.020251\n",
      "Epoch [8574/10000] Avg train loss: 0.020249\n",
      "Epoch [8575/10000] Avg train loss: 0.020246\n",
      "Epoch [8576/10000] Avg train loss: 0.020244\n",
      "Epoch [8577/10000] Avg train loss: 0.020242\n",
      "Epoch [8578/10000] Avg train loss: 0.020239\n",
      "Epoch [8579/10000] Avg train loss: 0.020237\n",
      "Epoch [8580/10000] Avg train loss: 0.020235\n",
      "Epoch [8581/10000] Avg train loss: 0.020232\n",
      "Epoch [8582/10000] Avg train loss: 0.020230\n",
      "Epoch [8583/10000] Avg train loss: 0.020228\n",
      "Epoch [8584/10000] Avg train loss: 0.020225\n",
      "Epoch [8585/10000] Avg train loss: 0.020223\n",
      "Epoch [8586/10000] Avg train loss: 0.020221\n",
      "Epoch [8587/10000] Avg train loss: 0.020218\n",
      "Epoch [8588/10000] Avg train loss: 0.020216\n",
      "Epoch [8589/10000] Avg train loss: 0.020213\n",
      "Epoch [8590/10000] Avg train loss: 0.020211\n",
      "Epoch [8591/10000] Avg train loss: 0.020209\n",
      "Epoch [8592/10000] Avg train loss: 0.020206\n",
      "Epoch [8593/10000] Avg train loss: 0.020204\n",
      "Epoch [8594/10000] Avg train loss: 0.020202\n",
      "Epoch [8595/10000] Avg train loss: 0.020199\n",
      "Epoch [8596/10000] Avg train loss: 0.020197\n",
      "Epoch [8597/10000] Avg train loss: 0.020195\n",
      "Epoch [8598/10000] Avg train loss: 0.020192\n",
      "Epoch [8599/10000] Avg train loss: 0.020190\n",
      "Epoch [8600/10000] Avg train loss: 0.020188\n",
      "Epoch [8601/10000] Avg train loss: 0.020185\n",
      "Epoch [8602/10000] Avg train loss: 0.020183\n",
      "Epoch [8603/10000] Avg train loss: 0.020181\n",
      "Epoch [8604/10000] Avg train loss: 0.020178\n",
      "Epoch [8605/10000] Avg train loss: 0.020176\n",
      "Epoch [8606/10000] Avg train loss: 0.020174\n",
      "Epoch [8607/10000] Avg train loss: 0.020171\n",
      "Epoch [8608/10000] Avg train loss: 0.020169\n",
      "Epoch [8609/10000] Avg train loss: 0.020167\n",
      "Epoch [8610/10000] Avg train loss: 0.020164\n",
      "Epoch [8611/10000] Avg train loss: 0.020162\n",
      "Epoch [8612/10000] Avg train loss: 0.020159\n",
      "Epoch [8613/10000] Avg train loss: 0.020157\n",
      "Epoch [8614/10000] Avg train loss: 0.020155\n",
      "Epoch [8615/10000] Avg train loss: 0.020152\n",
      "Epoch [8616/10000] Avg train loss: 0.020150\n",
      "Epoch [8617/10000] Avg train loss: 0.020148\n",
      "Epoch [8618/10000] Avg train loss: 0.020145\n",
      "Epoch [8619/10000] Avg train loss: 0.020143\n",
      "Epoch [8620/10000] Avg train loss: 0.020141\n",
      "Epoch [8621/10000] Avg train loss: 0.020138\n",
      "Epoch [8622/10000] Avg train loss: 0.020136\n",
      "Epoch [8623/10000] Avg train loss: 0.020134\n",
      "Epoch [8624/10000] Avg train loss: 0.020131\n",
      "Epoch [8625/10000] Avg train loss: 0.020129\n",
      "Epoch [8626/10000] Avg train loss: 0.020127\n",
      "Epoch [8627/10000] Avg train loss: 0.020124\n",
      "Epoch [8628/10000] Avg train loss: 0.020122\n",
      "Epoch [8629/10000] Avg train loss: 0.020120\n",
      "Epoch [8630/10000] Avg train loss: 0.020117\n",
      "Epoch [8631/10000] Avg train loss: 0.020115\n",
      "Epoch [8632/10000] Avg train loss: 0.020113\n",
      "Epoch [8633/10000] Avg train loss: 0.020110\n",
      "Epoch [8634/10000] Avg train loss: 0.020108\n",
      "Epoch [8635/10000] Avg train loss: 0.020106\n",
      "Epoch [8636/10000] Avg train loss: 0.020103\n",
      "Epoch [8637/10000] Avg train loss: 0.020101\n",
      "Epoch [8638/10000] Avg train loss: 0.020099\n",
      "Epoch [8639/10000] Avg train loss: 0.020096\n",
      "Epoch [8640/10000] Avg train loss: 0.020094\n",
      "Epoch [8641/10000] Avg train loss: 0.020092\n",
      "Epoch [8642/10000] Avg train loss: 0.020090\n",
      "Epoch [8643/10000] Avg train loss: 0.020087\n",
      "Epoch [8644/10000] Avg train loss: 0.020085\n",
      "Epoch [8645/10000] Avg train loss: 0.020083\n",
      "Epoch [8646/10000] Avg train loss: 0.020080\n",
      "Epoch [8647/10000] Avg train loss: 0.020078\n",
      "Epoch [8648/10000] Avg train loss: 0.020076\n",
      "Epoch [8649/10000] Avg train loss: 0.020073\n",
      "Epoch [8650/10000] Avg train loss: 0.020071\n",
      "Epoch [8651/10000] Avg train loss: 0.020069\n",
      "Epoch [8652/10000] Avg train loss: 0.020066\n",
      "Epoch [8653/10000] Avg train loss: 0.020064\n",
      "Epoch [8654/10000] Avg train loss: 0.020062\n",
      "Epoch [8655/10000] Avg train loss: 0.020059\n",
      "Epoch [8656/10000] Avg train loss: 0.020057\n",
      "Epoch [8657/10000] Avg train loss: 0.020055\n",
      "Epoch [8658/10000] Avg train loss: 0.020052\n",
      "Epoch [8659/10000] Avg train loss: 0.020050\n",
      "Epoch [8660/10000] Avg train loss: 0.020048\n",
      "Epoch [8661/10000] Avg train loss: 0.020045\n",
      "Epoch [8662/10000] Avg train loss: 0.020043\n",
      "Epoch [8663/10000] Avg train loss: 0.020041\n",
      "Epoch [8664/10000] Avg train loss: 0.020038\n",
      "Epoch [8665/10000] Avg train loss: 0.020036\n",
      "Epoch [8666/10000] Avg train loss: 0.020034\n",
      "Epoch [8667/10000] Avg train loss: 0.020032\n",
      "Epoch [8668/10000] Avg train loss: 0.020029\n",
      "Epoch [8669/10000] Avg train loss: 0.020027\n",
      "Epoch [8670/10000] Avg train loss: 0.020025\n",
      "Epoch [8671/10000] Avg train loss: 0.020022\n",
      "Epoch [8672/10000] Avg train loss: 0.020020\n",
      "Epoch [8673/10000] Avg train loss: 0.020018\n",
      "Epoch [8674/10000] Avg train loss: 0.020015\n",
      "Epoch [8675/10000] Avg train loss: 0.020013\n",
      "Epoch [8676/10000] Avg train loss: 0.020011\n",
      "Epoch [8677/10000] Avg train loss: 0.020008\n",
      "Epoch [8678/10000] Avg train loss: 0.020006\n",
      "Epoch [8679/10000] Avg train loss: 0.020004\n",
      "Epoch [8680/10000] Avg train loss: 0.020002\n",
      "Epoch [8681/10000] Avg train loss: 0.019999\n",
      "Epoch [8682/10000] Avg train loss: 0.019997\n",
      "Epoch [8683/10000] Avg train loss: 0.019995\n",
      "Epoch [8684/10000] Avg train loss: 0.019992\n",
      "Epoch [8685/10000] Avg train loss: 0.019990\n",
      "Epoch [8686/10000] Avg train loss: 0.019988\n",
      "Epoch [8687/10000] Avg train loss: 0.019985\n",
      "Epoch [8688/10000] Avg train loss: 0.019983\n",
      "Epoch [8689/10000] Avg train loss: 0.019981\n",
      "Epoch [8690/10000] Avg train loss: 0.019979\n",
      "Epoch [8691/10000] Avg train loss: 0.019976\n",
      "Epoch [8692/10000] Avg train loss: 0.019974\n",
      "Epoch [8693/10000] Avg train loss: 0.019972\n",
      "Epoch [8694/10000] Avg train loss: 0.019969\n",
      "Epoch [8695/10000] Avg train loss: 0.019967\n",
      "Epoch [8696/10000] Avg train loss: 0.019965\n",
      "Epoch [8697/10000] Avg train loss: 0.019962\n",
      "Epoch [8698/10000] Avg train loss: 0.019960\n",
      "Epoch [8699/10000] Avg train loss: 0.019958\n",
      "Epoch [8700/10000] Avg train loss: 0.019956\n",
      "Epoch [8701/10000] Avg train loss: 0.019953\n",
      "Epoch [8702/10000] Avg train loss: 0.019951\n",
      "Epoch [8703/10000] Avg train loss: 0.019949\n",
      "Epoch [8704/10000] Avg train loss: 0.019946\n",
      "Epoch [8705/10000] Avg train loss: 0.019944\n",
      "Epoch [8706/10000] Avg train loss: 0.019942\n",
      "Epoch [8707/10000] Avg train loss: 0.019940\n",
      "Epoch [8708/10000] Avg train loss: 0.019937\n",
      "Epoch [8709/10000] Avg train loss: 0.019935\n",
      "Epoch [8710/10000] Avg train loss: 0.019933\n",
      "Epoch [8711/10000] Avg train loss: 0.019930\n",
      "Epoch [8712/10000] Avg train loss: 0.019928\n",
      "Epoch [8713/10000] Avg train loss: 0.019926\n",
      "Epoch [8714/10000] Avg train loss: 0.019924\n",
      "Epoch [8715/10000] Avg train loss: 0.019921\n",
      "Epoch [8716/10000] Avg train loss: 0.019919\n",
      "Epoch [8717/10000] Avg train loss: 0.019917\n",
      "Epoch [8718/10000] Avg train loss: 0.019914\n",
      "Epoch [8719/10000] Avg train loss: 0.019912\n",
      "Epoch [8720/10000] Avg train loss: 0.019910\n",
      "Epoch [8721/10000] Avg train loss: 0.019908\n",
      "Epoch [8722/10000] Avg train loss: 0.019905\n",
      "Epoch [8723/10000] Avg train loss: 0.019903\n",
      "Epoch [8724/10000] Avg train loss: 0.019901\n",
      "Epoch [8725/10000] Avg train loss: 0.019898\n",
      "Epoch [8726/10000] Avg train loss: 0.019896\n",
      "Epoch [8727/10000] Avg train loss: 0.019894\n",
      "Epoch [8728/10000] Avg train loss: 0.019892\n",
      "Epoch [8729/10000] Avg train loss: 0.019889\n",
      "Epoch [8730/10000] Avg train loss: 0.019887\n",
      "Epoch [8731/10000] Avg train loss: 0.019885\n",
      "Epoch [8732/10000] Avg train loss: 0.019882\n",
      "Epoch [8733/10000] Avg train loss: 0.019880\n",
      "Epoch [8734/10000] Avg train loss: 0.019878\n",
      "Epoch [8735/10000] Avg train loss: 0.019876\n",
      "Epoch [8736/10000] Avg train loss: 0.019873\n",
      "Epoch [8737/10000] Avg train loss: 0.019871\n",
      "Epoch [8738/10000] Avg train loss: 0.019869\n",
      "Epoch [8739/10000] Avg train loss: 0.019867\n",
      "Epoch [8740/10000] Avg train loss: 0.019864\n",
      "Epoch [8741/10000] Avg train loss: 0.019862\n",
      "Epoch [8742/10000] Avg train loss: 0.019860\n",
      "Epoch [8743/10000] Avg train loss: 0.019857\n",
      "Epoch [8744/10000] Avg train loss: 0.019855\n",
      "Epoch [8745/10000] Avg train loss: 0.019853\n",
      "Epoch [8746/10000] Avg train loss: 0.019851\n",
      "Epoch [8747/10000] Avg train loss: 0.019848\n",
      "Epoch [8748/10000] Avg train loss: 0.019846\n",
      "Epoch [8749/10000] Avg train loss: 0.019844\n",
      "Epoch [8750/10000] Avg train loss: 0.019842\n",
      "Epoch [8751/10000] Avg train loss: 0.019839\n",
      "Epoch [8752/10000] Avg train loss: 0.019837\n",
      "Epoch [8753/10000] Avg train loss: 0.019835\n",
      "Epoch [8754/10000] Avg train loss: 0.019832\n",
      "Epoch [8755/10000] Avg train loss: 0.019830\n",
      "Epoch [8756/10000] Avg train loss: 0.019828\n",
      "Epoch [8757/10000] Avg train loss: 0.019826\n",
      "Epoch [8758/10000] Avg train loss: 0.019823\n",
      "Epoch [8759/10000] Avg train loss: 0.019821\n",
      "Epoch [8760/10000] Avg train loss: 0.019819\n",
      "Epoch [8761/10000] Avg train loss: 0.019817\n",
      "Epoch [8762/10000] Avg train loss: 0.019814\n",
      "Epoch [8763/10000] Avg train loss: 0.019812\n",
      "Epoch [8764/10000] Avg train loss: 0.019810\n",
      "Epoch [8765/10000] Avg train loss: 0.019808\n",
      "Epoch [8766/10000] Avg train loss: 0.019805\n",
      "Epoch [8767/10000] Avg train loss: 0.019803\n",
      "Epoch [8768/10000] Avg train loss: 0.019801\n",
      "Epoch [8769/10000] Avg train loss: 0.019799\n",
      "Epoch [8770/10000] Avg train loss: 0.019796\n",
      "Epoch [8771/10000] Avg train loss: 0.019794\n",
      "Epoch [8772/10000] Avg train loss: 0.019792\n",
      "Epoch [8773/10000] Avg train loss: 0.019790\n",
      "Epoch [8774/10000] Avg train loss: 0.019787\n",
      "Epoch [8775/10000] Avg train loss: 0.019785\n",
      "Epoch [8776/10000] Avg train loss: 0.019783\n",
      "Epoch [8777/10000] Avg train loss: 0.019781\n",
      "Epoch [8778/10000] Avg train loss: 0.019778\n",
      "Epoch [8779/10000] Avg train loss: 0.019776\n",
      "Epoch [8780/10000] Avg train loss: 0.019774\n",
      "Epoch [8781/10000] Avg train loss: 0.019771\n",
      "Epoch [8782/10000] Avg train loss: 0.019769\n",
      "Epoch [8783/10000] Avg train loss: 0.019767\n",
      "Epoch [8784/10000] Avg train loss: 0.019765\n",
      "Epoch [8785/10000] Avg train loss: 0.019762\n",
      "Epoch [8786/10000] Avg train loss: 0.019760\n",
      "Epoch [8787/10000] Avg train loss: 0.019758\n",
      "Epoch [8788/10000] Avg train loss: 0.019756\n",
      "Epoch [8789/10000] Avg train loss: 0.019754\n",
      "Epoch [8790/10000] Avg train loss: 0.019751\n",
      "Epoch [8791/10000] Avg train loss: 0.019749\n",
      "Epoch [8792/10000] Avg train loss: 0.019747\n",
      "Epoch [8793/10000] Avg train loss: 0.019745\n",
      "Epoch [8794/10000] Avg train loss: 0.019742\n",
      "Epoch [8795/10000] Avg train loss: 0.019740\n",
      "Epoch [8796/10000] Avg train loss: 0.019738\n",
      "Epoch [8797/10000] Avg train loss: 0.019736\n",
      "Epoch [8798/10000] Avg train loss: 0.019733\n",
      "Epoch [8799/10000] Avg train loss: 0.019731\n",
      "Epoch [8800/10000] Avg train loss: 0.019729\n",
      "Epoch [8801/10000] Avg train loss: 0.019727\n",
      "Epoch [8802/10000] Avg train loss: 0.019724\n",
      "Epoch [8803/10000] Avg train loss: 0.019722\n",
      "Epoch [8804/10000] Avg train loss: 0.019720\n",
      "Epoch [8805/10000] Avg train loss: 0.019718\n",
      "Epoch [8806/10000] Avg train loss: 0.019715\n",
      "Epoch [8807/10000] Avg train loss: 0.019713\n",
      "Epoch [8808/10000] Avg train loss: 0.019711\n",
      "Epoch [8809/10000] Avg train loss: 0.019709\n",
      "Epoch [8810/10000] Avg train loss: 0.019706\n",
      "Epoch [8811/10000] Avg train loss: 0.019704\n",
      "Epoch [8812/10000] Avg train loss: 0.019702\n",
      "Epoch [8813/10000] Avg train loss: 0.019700\n",
      "Epoch [8814/10000] Avg train loss: 0.019697\n",
      "Epoch [8815/10000] Avg train loss: 0.019695\n",
      "Epoch [8816/10000] Avg train loss: 0.019693\n",
      "Epoch [8817/10000] Avg train loss: 0.019691\n",
      "Epoch [8818/10000] Avg train loss: 0.019689\n",
      "Epoch [8819/10000] Avg train loss: 0.019686\n",
      "Epoch [8820/10000] Avg train loss: 0.019684\n",
      "Epoch [8821/10000] Avg train loss: 0.019682\n",
      "Epoch [8822/10000] Avg train loss: 0.019680\n",
      "Epoch [8823/10000] Avg train loss: 0.019677\n",
      "Epoch [8824/10000] Avg train loss: 0.019675\n",
      "Epoch [8825/10000] Avg train loss: 0.019673\n",
      "Epoch [8826/10000] Avg train loss: 0.019671\n",
      "Epoch [8827/10000] Avg train loss: 0.019668\n",
      "Epoch [8828/10000] Avg train loss: 0.019666\n",
      "Epoch [8829/10000] Avg train loss: 0.019664\n",
      "Epoch [8830/10000] Avg train loss: 0.019662\n",
      "Epoch [8831/10000] Avg train loss: 0.019660\n",
      "Epoch [8832/10000] Avg train loss: 0.019657\n",
      "Epoch [8833/10000] Avg train loss: 0.019655\n",
      "Epoch [8834/10000] Avg train loss: 0.019653\n",
      "Epoch [8835/10000] Avg train loss: 0.019651\n",
      "Epoch [8836/10000] Avg train loss: 0.019648\n",
      "Epoch [8837/10000] Avg train loss: 0.019646\n",
      "Epoch [8838/10000] Avg train loss: 0.019644\n",
      "Epoch [8839/10000] Avg train loss: 0.019642\n",
      "Epoch [8840/10000] Avg train loss: 0.019640\n",
      "Epoch [8841/10000] Avg train loss: 0.019637\n",
      "Epoch [8842/10000] Avg train loss: 0.019635\n",
      "Epoch [8843/10000] Avg train loss: 0.019633\n",
      "Epoch [8844/10000] Avg train loss: 0.019631\n",
      "Epoch [8845/10000] Avg train loss: 0.019628\n",
      "Epoch [8846/10000] Avg train loss: 0.019626\n",
      "Epoch [8847/10000] Avg train loss: 0.019624\n",
      "Epoch [8848/10000] Avg train loss: 0.019622\n",
      "Epoch [8849/10000] Avg train loss: 0.019620\n",
      "Epoch [8850/10000] Avg train loss: 0.019617\n",
      "Epoch [8851/10000] Avg train loss: 0.019615\n",
      "Epoch [8852/10000] Avg train loss: 0.019613\n",
      "Epoch [8853/10000] Avg train loss: 0.019611\n",
      "Epoch [8854/10000] Avg train loss: 0.019608\n",
      "Epoch [8855/10000] Avg train loss: 0.019606\n",
      "Epoch [8856/10000] Avg train loss: 0.019604\n",
      "Epoch [8857/10000] Avg train loss: 0.019602\n",
      "Epoch [8858/10000] Avg train loss: 0.019600\n",
      "Epoch [8859/10000] Avg train loss: 0.019597\n",
      "Epoch [8860/10000] Avg train loss: 0.019595\n",
      "Epoch [8861/10000] Avg train loss: 0.019593\n",
      "Epoch [8862/10000] Avg train loss: 0.019591\n",
      "Epoch [8863/10000] Avg train loss: 0.019589\n",
      "Epoch [8864/10000] Avg train loss: 0.019586\n",
      "Epoch [8865/10000] Avg train loss: 0.019584\n",
      "Epoch [8866/10000] Avg train loss: 0.019582\n",
      "Epoch [8867/10000] Avg train loss: 0.019580\n",
      "Epoch [8868/10000] Avg train loss: 0.019578\n",
      "Epoch [8869/10000] Avg train loss: 0.019575\n",
      "Epoch [8870/10000] Avg train loss: 0.019573\n",
      "Epoch [8871/10000] Avg train loss: 0.019571\n",
      "Epoch [8872/10000] Avg train loss: 0.019569\n",
      "Epoch [8873/10000] Avg train loss: 0.019566\n",
      "Epoch [8874/10000] Avg train loss: 0.019564\n",
      "Epoch [8875/10000] Avg train loss: 0.019562\n",
      "Epoch [8876/10000] Avg train loss: 0.019560\n",
      "Epoch [8877/10000] Avg train loss: 0.019558\n",
      "Epoch [8878/10000] Avg train loss: 0.019555\n",
      "Epoch [8879/10000] Avg train loss: 0.019553\n",
      "Epoch [8880/10000] Avg train loss: 0.019551\n",
      "Epoch [8881/10000] Avg train loss: 0.019549\n",
      "Epoch [8882/10000] Avg train loss: 0.019547\n",
      "Epoch [8883/10000] Avg train loss: 0.019544\n",
      "Epoch [8884/10000] Avg train loss: 0.019542\n",
      "Epoch [8885/10000] Avg train loss: 0.019540\n",
      "Epoch [8886/10000] Avg train loss: 0.019538\n",
      "Epoch [8887/10000] Avg train loss: 0.019536\n",
      "Epoch [8888/10000] Avg train loss: 0.019533\n",
      "Epoch [8889/10000] Avg train loss: 0.019531\n",
      "Epoch [8890/10000] Avg train loss: 0.019529\n",
      "Epoch [8891/10000] Avg train loss: 0.019527\n",
      "Epoch [8892/10000] Avg train loss: 0.019525\n",
      "Epoch [8893/10000] Avg train loss: 0.019522\n",
      "Epoch [8894/10000] Avg train loss: 0.019520\n",
      "Epoch [8895/10000] Avg train loss: 0.019518\n",
      "Epoch [8896/10000] Avg train loss: 0.019516\n",
      "Epoch [8897/10000] Avg train loss: 0.019514\n",
      "Epoch [8898/10000] Avg train loss: 0.019512\n",
      "Epoch [8899/10000] Avg train loss: 0.019509\n",
      "Epoch [8900/10000] Avg train loss: 0.019507\n",
      "Epoch [8901/10000] Avg train loss: 0.019505\n",
      "Epoch [8902/10000] Avg train loss: 0.019503\n",
      "Epoch [8903/10000] Avg train loss: 0.019501\n",
      "Epoch [8904/10000] Avg train loss: 0.019498\n",
      "Epoch [8905/10000] Avg train loss: 0.019496\n",
      "Epoch [8906/10000] Avg train loss: 0.019494\n",
      "Epoch [8907/10000] Avg train loss: 0.019492\n",
      "Epoch [8908/10000] Avg train loss: 0.019490\n",
      "Epoch [8909/10000] Avg train loss: 0.019487\n",
      "Epoch [8910/10000] Avg train loss: 0.019485\n",
      "Epoch [8911/10000] Avg train loss: 0.019483\n",
      "Epoch [8912/10000] Avg train loss: 0.019481\n",
      "Epoch [8913/10000] Avg train loss: 0.019479\n",
      "Epoch [8914/10000] Avg train loss: 0.019477\n",
      "Epoch [8915/10000] Avg train loss: 0.019474\n",
      "Epoch [8916/10000] Avg train loss: 0.019472\n",
      "Epoch [8917/10000] Avg train loss: 0.019470\n",
      "Epoch [8918/10000] Avg train loss: 0.019468\n",
      "Epoch [8919/10000] Avg train loss: 0.019466\n",
      "Epoch [8920/10000] Avg train loss: 0.019463\n",
      "Epoch [8921/10000] Avg train loss: 0.019461\n",
      "Epoch [8922/10000] Avg train loss: 0.019459\n",
      "Epoch [8923/10000] Avg train loss: 0.019457\n",
      "Epoch [8924/10000] Avg train loss: 0.019455\n",
      "Epoch [8925/10000] Avg train loss: 0.019452\n",
      "Epoch [8926/10000] Avg train loss: 0.019450\n",
      "Epoch [8927/10000] Avg train loss: 0.019448\n",
      "Epoch [8928/10000] Avg train loss: 0.019446\n",
      "Epoch [8929/10000] Avg train loss: 0.019444\n",
      "Epoch [8930/10000] Avg train loss: 0.019442\n",
      "Epoch [8931/10000] Avg train loss: 0.019439\n",
      "Epoch [8932/10000] Avg train loss: 0.019437\n",
      "Epoch [8933/10000] Avg train loss: 0.019435\n",
      "Epoch [8934/10000] Avg train loss: 0.019433\n",
      "Epoch [8935/10000] Avg train loss: 0.019431\n",
      "Epoch [8936/10000] Avg train loss: 0.019429\n",
      "Epoch [8937/10000] Avg train loss: 0.019426\n",
      "Epoch [8938/10000] Avg train loss: 0.019424\n",
      "Epoch [8939/10000] Avg train loss: 0.019422\n",
      "Epoch [8940/10000] Avg train loss: 0.019420\n",
      "Epoch [8941/10000] Avg train loss: 0.019418\n",
      "Epoch [8942/10000] Avg train loss: 0.019416\n",
      "Epoch [8943/10000] Avg train loss: 0.019413\n",
      "Epoch [8944/10000] Avg train loss: 0.019411\n",
      "Epoch [8945/10000] Avg train loss: 0.019409\n",
      "Epoch [8946/10000] Avg train loss: 0.019407\n",
      "Epoch [8947/10000] Avg train loss: 0.019405\n",
      "Epoch [8948/10000] Avg train loss: 0.019402\n",
      "Epoch [8949/10000] Avg train loss: 0.019400\n",
      "Epoch [8950/10000] Avg train loss: 0.019398\n",
      "Epoch [8951/10000] Avg train loss: 0.019396\n",
      "Epoch [8952/10000] Avg train loss: 0.019394\n",
      "Epoch [8953/10000] Avg train loss: 0.019392\n",
      "Epoch [8954/10000] Avg train loss: 0.019389\n",
      "Epoch [8955/10000] Avg train loss: 0.019387\n",
      "Epoch [8956/10000] Avg train loss: 0.019385\n",
      "Epoch [8957/10000] Avg train loss: 0.019383\n",
      "Epoch [8958/10000] Avg train loss: 0.019381\n",
      "Epoch [8959/10000] Avg train loss: 0.019379\n",
      "Epoch [8960/10000] Avg train loss: 0.019377\n",
      "Epoch [8961/10000] Avg train loss: 0.019374\n",
      "Epoch [8962/10000] Avg train loss: 0.019372\n",
      "Epoch [8963/10000] Avg train loss: 0.019370\n",
      "Epoch [8964/10000] Avg train loss: 0.019368\n",
      "Epoch [8965/10000] Avg train loss: 0.019366\n",
      "Epoch [8966/10000] Avg train loss: 0.019364\n",
      "Epoch [8967/10000] Avg train loss: 0.019361\n",
      "Epoch [8968/10000] Avg train loss: 0.019359\n",
      "Epoch [8969/10000] Avg train loss: 0.019357\n",
      "Epoch [8970/10000] Avg train loss: 0.019355\n",
      "Epoch [8971/10000] Avg train loss: 0.019353\n",
      "Epoch [8972/10000] Avg train loss: 0.019351\n",
      "Epoch [8973/10000] Avg train loss: 0.019348\n",
      "Epoch [8974/10000] Avg train loss: 0.019346\n",
      "Epoch [8975/10000] Avg train loss: 0.019344\n",
      "Epoch [8976/10000] Avg train loss: 0.019342\n",
      "Epoch [8977/10000] Avg train loss: 0.019340\n",
      "Epoch [8978/10000] Avg train loss: 0.019338\n",
      "Epoch [8979/10000] Avg train loss: 0.019336\n",
      "Epoch [8980/10000] Avg train loss: 0.019333\n",
      "Epoch [8981/10000] Avg train loss: 0.019331\n",
      "Epoch [8982/10000] Avg train loss: 0.019329\n",
      "Epoch [8983/10000] Avg train loss: 0.019327\n",
      "Epoch [8984/10000] Avg train loss: 0.019325\n",
      "Epoch [8985/10000] Avg train loss: 0.019323\n",
      "Epoch [8986/10000] Avg train loss: 0.019320\n",
      "Epoch [8987/10000] Avg train loss: 0.019318\n",
      "Epoch [8988/10000] Avg train loss: 0.019316\n",
      "Epoch [8989/10000] Avg train loss: 0.019314\n",
      "Epoch [8990/10000] Avg train loss: 0.019312\n",
      "Epoch [8991/10000] Avg train loss: 0.019310\n",
      "Epoch [8992/10000] Avg train loss: 0.019308\n",
      "Epoch [8993/10000] Avg train loss: 0.019305\n",
      "Epoch [8994/10000] Avg train loss: 0.019303\n",
      "Epoch [8995/10000] Avg train loss: 0.019301\n",
      "Epoch [8996/10000] Avg train loss: 0.019299\n",
      "Epoch [8997/10000] Avg train loss: 0.019297\n",
      "Epoch [8998/10000] Avg train loss: 0.019295\n",
      "Epoch [8999/10000] Avg train loss: 0.019293\n",
      "Epoch [9000/10000] Avg train loss: 0.019290\n",
      "Epoch [9001/10000] Avg train loss: 0.019288\n",
      "Epoch [9002/10000] Avg train loss: 0.019286\n",
      "Epoch [9003/10000] Avg train loss: 0.019284\n",
      "Epoch [9004/10000] Avg train loss: 0.019282\n",
      "Epoch [9005/10000] Avg train loss: 0.019280\n",
      "Epoch [9006/10000] Avg train loss: 0.019278\n",
      "Epoch [9007/10000] Avg train loss: 0.019275\n",
      "Epoch [9008/10000] Avg train loss: 0.019273\n",
      "Epoch [9009/10000] Avg train loss: 0.019271\n",
      "Epoch [9010/10000] Avg train loss: 0.019269\n",
      "Epoch [9011/10000] Avg train loss: 0.019267\n",
      "Epoch [9012/10000] Avg train loss: 0.019265\n",
      "Epoch [9013/10000] Avg train loss: 0.019263\n",
      "Epoch [9014/10000] Avg train loss: 0.019260\n",
      "Epoch [9015/10000] Avg train loss: 0.019258\n",
      "Epoch [9016/10000] Avg train loss: 0.019256\n",
      "Epoch [9017/10000] Avg train loss: 0.019254\n",
      "Epoch [9018/10000] Avg train loss: 0.019252\n",
      "Epoch [9019/10000] Avg train loss: 0.019250\n",
      "Epoch [9020/10000] Avg train loss: 0.019248\n",
      "Epoch [9021/10000] Avg train loss: 0.019245\n",
      "Epoch [9022/10000] Avg train loss: 0.019243\n",
      "Epoch [9023/10000] Avg train loss: 0.019241\n",
      "Epoch [9024/10000] Avg train loss: 0.019239\n",
      "Epoch [9025/10000] Avg train loss: 0.019237\n",
      "Epoch [9026/10000] Avg train loss: 0.019235\n",
      "Epoch [9027/10000] Avg train loss: 0.019233\n",
      "Epoch [9028/10000] Avg train loss: 0.019231\n",
      "Epoch [9029/10000] Avg train loss: 0.019228\n",
      "Epoch [9030/10000] Avg train loss: 0.019226\n",
      "Epoch [9031/10000] Avg train loss: 0.019224\n",
      "Epoch [9032/10000] Avg train loss: 0.019222\n",
      "Epoch [9033/10000] Avg train loss: 0.019220\n",
      "Epoch [9034/10000] Avg train loss: 0.019218\n",
      "Epoch [9035/10000] Avg train loss: 0.019216\n",
      "Epoch [9036/10000] Avg train loss: 0.019214\n",
      "Epoch [9037/10000] Avg train loss: 0.019211\n",
      "Epoch [9038/10000] Avg train loss: 0.019209\n",
      "Epoch [9039/10000] Avg train loss: 0.019207\n",
      "Epoch [9040/10000] Avg train loss: 0.019205\n",
      "Epoch [9041/10000] Avg train loss: 0.019203\n",
      "Epoch [9042/10000] Avg train loss: 0.019201\n",
      "Epoch [9043/10000] Avg train loss: 0.019199\n",
      "Epoch [9044/10000] Avg train loss: 0.019197\n",
      "Epoch [9045/10000] Avg train loss: 0.019194\n",
      "Epoch [9046/10000] Avg train loss: 0.019192\n",
      "Epoch [9047/10000] Avg train loss: 0.019190\n",
      "Epoch [9048/10000] Avg train loss: 0.019188\n",
      "Epoch [9049/10000] Avg train loss: 0.019186\n",
      "Epoch [9050/10000] Avg train loss: 0.019184\n",
      "Epoch [9051/10000] Avg train loss: 0.019182\n",
      "Epoch [9052/10000] Avg train loss: 0.019180\n",
      "Epoch [9053/10000] Avg train loss: 0.019177\n",
      "Epoch [9054/10000] Avg train loss: 0.019175\n",
      "Epoch [9055/10000] Avg train loss: 0.019173\n",
      "Epoch [9056/10000] Avg train loss: 0.019171\n",
      "Epoch [9057/10000] Avg train loss: 0.019169\n",
      "Epoch [9058/10000] Avg train loss: 0.019167\n",
      "Epoch [9059/10000] Avg train loss: 0.019165\n",
      "Epoch [9060/10000] Avg train loss: 0.019163\n",
      "Epoch [9061/10000] Avg train loss: 0.019161\n",
      "Epoch [9062/10000] Avg train loss: 0.019158\n",
      "Epoch [9063/10000] Avg train loss: 0.019156\n",
      "Epoch [9064/10000] Avg train loss: 0.019154\n",
      "Epoch [9065/10000] Avg train loss: 0.019152\n",
      "Epoch [9066/10000] Avg train loss: 0.019150\n",
      "Epoch [9067/10000] Avg train loss: 0.019148\n",
      "Epoch [9068/10000] Avg train loss: 0.019146\n",
      "Epoch [9069/10000] Avg train loss: 0.019144\n",
      "Epoch [9070/10000] Avg train loss: 0.019142\n",
      "Epoch [9071/10000] Avg train loss: 0.019139\n",
      "Epoch [9072/10000] Avg train loss: 0.019137\n",
      "Epoch [9073/10000] Avg train loss: 0.019135\n",
      "Epoch [9074/10000] Avg train loss: 0.019133\n",
      "Epoch [9075/10000] Avg train loss: 0.019131\n",
      "Epoch [9076/10000] Avg train loss: 0.019129\n",
      "Epoch [9077/10000] Avg train loss: 0.019127\n",
      "Epoch [9078/10000] Avg train loss: 0.019125\n",
      "Epoch [9079/10000] Avg train loss: 0.019123\n",
      "Epoch [9080/10000] Avg train loss: 0.019120\n",
      "Epoch [9081/10000] Avg train loss: 0.019118\n",
      "Epoch [9082/10000] Avg train loss: 0.019116\n",
      "Epoch [9083/10000] Avg train loss: 0.019114\n",
      "Epoch [9084/10000] Avg train loss: 0.019112\n",
      "Epoch [9085/10000] Avg train loss: 0.019110\n",
      "Epoch [9086/10000] Avg train loss: 0.019108\n",
      "Epoch [9087/10000] Avg train loss: 0.019106\n",
      "Epoch [9088/10000] Avg train loss: 0.019104\n",
      "Epoch [9089/10000] Avg train loss: 0.019102\n",
      "Epoch [9090/10000] Avg train loss: 0.019099\n",
      "Epoch [9091/10000] Avg train loss: 0.019097\n",
      "Epoch [9092/10000] Avg train loss: 0.019095\n",
      "Epoch [9093/10000] Avg train loss: 0.019093\n",
      "Epoch [9094/10000] Avg train loss: 0.019091\n",
      "Epoch [9095/10000] Avg train loss: 0.019089\n",
      "Epoch [9096/10000] Avg train loss: 0.019087\n",
      "Epoch [9097/10000] Avg train loss: 0.019085\n",
      "Epoch [9098/10000] Avg train loss: 0.019083\n",
      "Epoch [9099/10000] Avg train loss: 0.019081\n",
      "Epoch [9100/10000] Avg train loss: 0.019078\n",
      "Epoch [9101/10000] Avg train loss: 0.019076\n",
      "Epoch [9102/10000] Avg train loss: 0.019074\n",
      "Epoch [9103/10000] Avg train loss: 0.019072\n",
      "Epoch [9104/10000] Avg train loss: 0.019070\n",
      "Epoch [9105/10000] Avg train loss: 0.019068\n",
      "Epoch [9106/10000] Avg train loss: 0.019066\n",
      "Epoch [9107/10000] Avg train loss: 0.019064\n",
      "Epoch [9108/10000] Avg train loss: 0.019062\n",
      "Epoch [9109/10000] Avg train loss: 0.019060\n",
      "Epoch [9110/10000] Avg train loss: 0.019057\n",
      "Epoch [9111/10000] Avg train loss: 0.019055\n",
      "Epoch [9112/10000] Avg train loss: 0.019053\n",
      "Epoch [9113/10000] Avg train loss: 0.019051\n",
      "Epoch [9114/10000] Avg train loss: 0.019049\n",
      "Epoch [9115/10000] Avg train loss: 0.019047\n",
      "Epoch [9116/10000] Avg train loss: 0.019045\n",
      "Epoch [9117/10000] Avg train loss: 0.019043\n",
      "Epoch [9118/10000] Avg train loss: 0.019041\n",
      "Epoch [9119/10000] Avg train loss: 0.019039\n",
      "Epoch [9120/10000] Avg train loss: 0.019037\n",
      "Epoch [9121/10000] Avg train loss: 0.019034\n",
      "Epoch [9122/10000] Avg train loss: 0.019032\n",
      "Epoch [9123/10000] Avg train loss: 0.019030\n",
      "Epoch [9124/10000] Avg train loss: 0.019028\n",
      "Epoch [9125/10000] Avg train loss: 0.019026\n",
      "Epoch [9126/10000] Avg train loss: 0.019024\n",
      "Epoch [9127/10000] Avg train loss: 0.019022\n",
      "Epoch [9128/10000] Avg train loss: 0.019020\n",
      "Epoch [9129/10000] Avg train loss: 0.019018\n",
      "Epoch [9130/10000] Avg train loss: 0.019016\n",
      "Epoch [9131/10000] Avg train loss: 0.019014\n",
      "Epoch [9132/10000] Avg train loss: 0.019012\n",
      "Epoch [9133/10000] Avg train loss: 0.019009\n",
      "Epoch [9134/10000] Avg train loss: 0.019007\n",
      "Epoch [9135/10000] Avg train loss: 0.019005\n",
      "Epoch [9136/10000] Avg train loss: 0.019003\n",
      "Epoch [9137/10000] Avg train loss: 0.019001\n",
      "Epoch [9138/10000] Avg train loss: 0.018999\n",
      "Epoch [9139/10000] Avg train loss: 0.018997\n",
      "Epoch [9140/10000] Avg train loss: 0.018995\n",
      "Epoch [9141/10000] Avg train loss: 0.018993\n",
      "Epoch [9142/10000] Avg train loss: 0.018991\n",
      "Epoch [9143/10000] Avg train loss: 0.018989\n",
      "Epoch [9144/10000] Avg train loss: 0.018987\n",
      "Epoch [9145/10000] Avg train loss: 0.018985\n",
      "Epoch [9146/10000] Avg train loss: 0.018982\n",
      "Epoch [9147/10000] Avg train loss: 0.018980\n",
      "Epoch [9148/10000] Avg train loss: 0.018978\n",
      "Epoch [9149/10000] Avg train loss: 0.018976\n",
      "Epoch [9150/10000] Avg train loss: 0.018974\n",
      "Epoch [9151/10000] Avg train loss: 0.018972\n",
      "Epoch [9152/10000] Avg train loss: 0.018970\n",
      "Epoch [9153/10000] Avg train loss: 0.018968\n",
      "Epoch [9154/10000] Avg train loss: 0.018966\n",
      "Epoch [9155/10000] Avg train loss: 0.018964\n",
      "Epoch [9156/10000] Avg train loss: 0.018962\n",
      "Epoch [9157/10000] Avg train loss: 0.018960\n",
      "Epoch [9158/10000] Avg train loss: 0.018958\n",
      "Epoch [9159/10000] Avg train loss: 0.018956\n",
      "Epoch [9160/10000] Avg train loss: 0.018953\n",
      "Epoch [9161/10000] Avg train loss: 0.018951\n",
      "Epoch [9162/10000] Avg train loss: 0.018949\n",
      "Epoch [9163/10000] Avg train loss: 0.018947\n",
      "Epoch [9164/10000] Avg train loss: 0.018945\n",
      "Epoch [9165/10000] Avg train loss: 0.018943\n",
      "Epoch [9166/10000] Avg train loss: 0.018941\n",
      "Epoch [9167/10000] Avg train loss: 0.018939\n",
      "Epoch [9168/10000] Avg train loss: 0.018937\n",
      "Epoch [9169/10000] Avg train loss: 0.018935\n",
      "Epoch [9170/10000] Avg train loss: 0.018933\n",
      "Epoch [9171/10000] Avg train loss: 0.018931\n",
      "Epoch [9172/10000] Avg train loss: 0.018929\n",
      "Epoch [9173/10000] Avg train loss: 0.018927\n",
      "Epoch [9174/10000] Avg train loss: 0.018925\n",
      "Epoch [9175/10000] Avg train loss: 0.018922\n",
      "Epoch [9176/10000] Avg train loss: 0.018920\n",
      "Epoch [9177/10000] Avg train loss: 0.018918\n",
      "Epoch [9178/10000] Avg train loss: 0.018916\n",
      "Epoch [9179/10000] Avg train loss: 0.018914\n",
      "Epoch [9180/10000] Avg train loss: 0.018912\n",
      "Epoch [9181/10000] Avg train loss: 0.018910\n",
      "Epoch [9182/10000] Avg train loss: 0.018908\n",
      "Epoch [9183/10000] Avg train loss: 0.018906\n",
      "Epoch [9184/10000] Avg train loss: 0.018904\n",
      "Epoch [9185/10000] Avg train loss: 0.018902\n",
      "Epoch [9186/10000] Avg train loss: 0.018900\n",
      "Epoch [9187/10000] Avg train loss: 0.018898\n",
      "Epoch [9188/10000] Avg train loss: 0.018896\n",
      "Epoch [9189/10000] Avg train loss: 0.018894\n",
      "Epoch [9190/10000] Avg train loss: 0.018892\n",
      "Epoch [9191/10000] Avg train loss: 0.018890\n",
      "Epoch [9192/10000] Avg train loss: 0.018887\n",
      "Epoch [9193/10000] Avg train loss: 0.018885\n",
      "Epoch [9194/10000] Avg train loss: 0.018883\n",
      "Epoch [9195/10000] Avg train loss: 0.018881\n",
      "Epoch [9196/10000] Avg train loss: 0.018879\n",
      "Epoch [9197/10000] Avg train loss: 0.018877\n",
      "Epoch [9198/10000] Avg train loss: 0.018875\n",
      "Epoch [9199/10000] Avg train loss: 0.018873\n",
      "Epoch [9200/10000] Avg train loss: 0.018871\n",
      "Epoch [9201/10000] Avg train loss: 0.018869\n",
      "Epoch [9202/10000] Avg train loss: 0.018867\n",
      "Epoch [9203/10000] Avg train loss: 0.018865\n",
      "Epoch [9204/10000] Avg train loss: 0.018863\n",
      "Epoch [9205/10000] Avg train loss: 0.018861\n",
      "Epoch [9206/10000] Avg train loss: 0.018859\n",
      "Epoch [9207/10000] Avg train loss: 0.018857\n",
      "Epoch [9208/10000] Avg train loss: 0.018855\n",
      "Epoch [9209/10000] Avg train loss: 0.018853\n",
      "Epoch [9210/10000] Avg train loss: 0.018851\n",
      "Epoch [9211/10000] Avg train loss: 0.018849\n",
      "Epoch [9212/10000] Avg train loss: 0.018846\n",
      "Epoch [9213/10000] Avg train loss: 0.018844\n",
      "Epoch [9214/10000] Avg train loss: 0.018842\n",
      "Epoch [9215/10000] Avg train loss: 0.018840\n",
      "Epoch [9216/10000] Avg train loss: 0.018838\n",
      "Epoch [9217/10000] Avg train loss: 0.018836\n",
      "Epoch [9218/10000] Avg train loss: 0.018834\n",
      "Epoch [9219/10000] Avg train loss: 0.018832\n",
      "Epoch [9220/10000] Avg train loss: 0.018830\n",
      "Epoch [9221/10000] Avg train loss: 0.018828\n",
      "Epoch [9222/10000] Avg train loss: 0.018826\n",
      "Epoch [9223/10000] Avg train loss: 0.018824\n",
      "Epoch [9224/10000] Avg train loss: 0.018822\n",
      "Epoch [9225/10000] Avg train loss: 0.018820\n",
      "Epoch [9226/10000] Avg train loss: 0.018818\n",
      "Epoch [9227/10000] Avg train loss: 0.018816\n",
      "Epoch [9228/10000] Avg train loss: 0.018814\n",
      "Epoch [9229/10000] Avg train loss: 0.018812\n",
      "Epoch [9230/10000] Avg train loss: 0.018810\n",
      "Epoch [9231/10000] Avg train loss: 0.018808\n",
      "Epoch [9232/10000] Avg train loss: 0.018806\n",
      "Epoch [9233/10000] Avg train loss: 0.018804\n",
      "Epoch [9234/10000] Avg train loss: 0.018802\n",
      "Epoch [9235/10000] Avg train loss: 0.018800\n",
      "Epoch [9236/10000] Avg train loss: 0.018797\n",
      "Epoch [9237/10000] Avg train loss: 0.018795\n",
      "Epoch [9238/10000] Avg train loss: 0.018793\n",
      "Epoch [9239/10000] Avg train loss: 0.018791\n",
      "Epoch [9240/10000] Avg train loss: 0.018789\n",
      "Epoch [9241/10000] Avg train loss: 0.018787\n",
      "Epoch [9242/10000] Avg train loss: 0.018785\n",
      "Epoch [9243/10000] Avg train loss: 0.018783\n",
      "Epoch [9244/10000] Avg train loss: 0.018781\n",
      "Epoch [9245/10000] Avg train loss: 0.018779\n",
      "Epoch [9246/10000] Avg train loss: 0.018777\n",
      "Epoch [9247/10000] Avg train loss: 0.018775\n",
      "Epoch [9248/10000] Avg train loss: 0.018773\n",
      "Epoch [9249/10000] Avg train loss: 0.018771\n",
      "Epoch [9250/10000] Avg train loss: 0.018769\n",
      "Epoch [9251/10000] Avg train loss: 0.018767\n",
      "Epoch [9252/10000] Avg train loss: 0.018765\n",
      "Epoch [9253/10000] Avg train loss: 0.018763\n",
      "Epoch [9254/10000] Avg train loss: 0.018761\n",
      "Epoch [9255/10000] Avg train loss: 0.018759\n",
      "Epoch [9256/10000] Avg train loss: 0.018757\n",
      "Epoch [9257/10000] Avg train loss: 0.018755\n",
      "Epoch [9258/10000] Avg train loss: 0.018753\n",
      "Epoch [9259/10000] Avg train loss: 0.018751\n",
      "Epoch [9260/10000] Avg train loss: 0.018749\n",
      "Epoch [9261/10000] Avg train loss: 0.018747\n",
      "Epoch [9262/10000] Avg train loss: 0.018745\n",
      "Epoch [9263/10000] Avg train loss: 0.018743\n",
      "Epoch [9264/10000] Avg train loss: 0.018741\n",
      "Epoch [9265/10000] Avg train loss: 0.018739\n",
      "Epoch [9266/10000] Avg train loss: 0.018737\n",
      "Epoch [9267/10000] Avg train loss: 0.018735\n",
      "Epoch [9268/10000] Avg train loss: 0.018733\n",
      "Epoch [9269/10000] Avg train loss: 0.018731\n",
      "Epoch [9270/10000] Avg train loss: 0.018729\n",
      "Epoch [9271/10000] Avg train loss: 0.018727\n",
      "Epoch [9272/10000] Avg train loss: 0.018724\n",
      "Epoch [9273/10000] Avg train loss: 0.018722\n",
      "Epoch [9274/10000] Avg train loss: 0.018720\n",
      "Epoch [9275/10000] Avg train loss: 0.018718\n",
      "Epoch [9276/10000] Avg train loss: 0.018716\n",
      "Epoch [9277/10000] Avg train loss: 0.018714\n",
      "Epoch [9278/10000] Avg train loss: 0.018712\n",
      "Epoch [9279/10000] Avg train loss: 0.018710\n",
      "Epoch [9280/10000] Avg train loss: 0.018708\n",
      "Epoch [9281/10000] Avg train loss: 0.018706\n",
      "Epoch [9282/10000] Avg train loss: 0.018704\n",
      "Epoch [9283/10000] Avg train loss: 0.018702\n",
      "Epoch [9284/10000] Avg train loss: 0.018700\n",
      "Epoch [9285/10000] Avg train loss: 0.018698\n",
      "Epoch [9286/10000] Avg train loss: 0.018696\n",
      "Epoch [9287/10000] Avg train loss: 0.018694\n",
      "Epoch [9288/10000] Avg train loss: 0.018692\n",
      "Epoch [9289/10000] Avg train loss: 0.018690\n",
      "Epoch [9290/10000] Avg train loss: 0.018688\n",
      "Epoch [9291/10000] Avg train loss: 0.018686\n",
      "Epoch [9292/10000] Avg train loss: 0.018684\n",
      "Epoch [9293/10000] Avg train loss: 0.018682\n",
      "Epoch [9294/10000] Avg train loss: 0.018680\n",
      "Epoch [9295/10000] Avg train loss: 0.018678\n",
      "Epoch [9296/10000] Avg train loss: 0.018676\n",
      "Epoch [9297/10000] Avg train loss: 0.018674\n",
      "Epoch [9298/10000] Avg train loss: 0.018672\n",
      "Epoch [9299/10000] Avg train loss: 0.018670\n",
      "Epoch [9300/10000] Avg train loss: 0.018668\n",
      "Epoch [9301/10000] Avg train loss: 0.018666\n",
      "Epoch [9302/10000] Avg train loss: 0.018664\n",
      "Epoch [9303/10000] Avg train loss: 0.018662\n",
      "Epoch [9304/10000] Avg train loss: 0.018660\n",
      "Epoch [9305/10000] Avg train loss: 0.018658\n",
      "Epoch [9306/10000] Avg train loss: 0.018656\n",
      "Epoch [9307/10000] Avg train loss: 0.018654\n",
      "Epoch [9308/10000] Avg train loss: 0.018652\n",
      "Epoch [9309/10000] Avg train loss: 0.018650\n",
      "Epoch [9310/10000] Avg train loss: 0.018648\n",
      "Epoch [9311/10000] Avg train loss: 0.018646\n",
      "Epoch [9312/10000] Avg train loss: 0.018644\n",
      "Epoch [9313/10000] Avg train loss: 0.018642\n",
      "Epoch [9314/10000] Avg train loss: 0.018640\n",
      "Epoch [9315/10000] Avg train loss: 0.018638\n",
      "Epoch [9316/10000] Avg train loss: 0.018636\n",
      "Epoch [9317/10000] Avg train loss: 0.018634\n",
      "Epoch [9318/10000] Avg train loss: 0.018632\n",
      "Epoch [9319/10000] Avg train loss: 0.018630\n",
      "Epoch [9320/10000] Avg train loss: 0.018628\n",
      "Epoch [9321/10000] Avg train loss: 0.018626\n",
      "Epoch [9322/10000] Avg train loss: 0.018624\n",
      "Epoch [9323/10000] Avg train loss: 0.018622\n",
      "Epoch [9324/10000] Avg train loss: 0.018620\n",
      "Epoch [9325/10000] Avg train loss: 0.018618\n",
      "Epoch [9326/10000] Avg train loss: 0.018616\n",
      "Epoch [9327/10000] Avg train loss: 0.018614\n",
      "Epoch [9328/10000] Avg train loss: 0.018612\n",
      "Epoch [9329/10000] Avg train loss: 0.018610\n",
      "Epoch [9330/10000] Avg train loss: 0.018608\n",
      "Epoch [9331/10000] Avg train loss: 0.018606\n",
      "Epoch [9332/10000] Avg train loss: 0.018604\n",
      "Epoch [9333/10000] Avg train loss: 0.018602\n",
      "Epoch [9334/10000] Avg train loss: 0.018600\n",
      "Epoch [9335/10000] Avg train loss: 0.018598\n",
      "Epoch [9336/10000] Avg train loss: 0.018596\n",
      "Epoch [9337/10000] Avg train loss: 0.018594\n",
      "Epoch [9338/10000] Avg train loss: 0.018592\n",
      "Epoch [9339/10000] Avg train loss: 0.018590\n",
      "Epoch [9340/10000] Avg train loss: 0.018588\n",
      "Epoch [9341/10000] Avg train loss: 0.018586\n",
      "Epoch [9342/10000] Avg train loss: 0.018584\n",
      "Epoch [9343/10000] Avg train loss: 0.018582\n",
      "Epoch [9344/10000] Avg train loss: 0.018580\n",
      "Epoch [9345/10000] Avg train loss: 0.018578\n",
      "Epoch [9346/10000] Avg train loss: 0.018576\n",
      "Epoch [9347/10000] Avg train loss: 0.018574\n",
      "Epoch [9348/10000] Avg train loss: 0.018572\n",
      "Epoch [9349/10000] Avg train loss: 0.018570\n",
      "Epoch [9350/10000] Avg train loss: 0.018568\n",
      "Epoch [9351/10000] Avg train loss: 0.018566\n",
      "Epoch [9352/10000] Avg train loss: 0.018564\n",
      "Epoch [9353/10000] Avg train loss: 0.018562\n",
      "Epoch [9354/10000] Avg train loss: 0.018560\n",
      "Epoch [9355/10000] Avg train loss: 0.018558\n",
      "Epoch [9356/10000] Avg train loss: 0.018556\n",
      "Epoch [9357/10000] Avg train loss: 0.018554\n",
      "Epoch [9358/10000] Avg train loss: 0.018552\n",
      "Epoch [9359/10000] Avg train loss: 0.018550\n",
      "Epoch [9360/10000] Avg train loss: 0.018548\n",
      "Epoch [9361/10000] Avg train loss: 0.018546\n",
      "Epoch [9362/10000] Avg train loss: 0.018544\n",
      "Epoch [9363/10000] Avg train loss: 0.018543\n",
      "Epoch [9364/10000] Avg train loss: 0.018541\n",
      "Epoch [9365/10000] Avg train loss: 0.018539\n",
      "Epoch [9366/10000] Avg train loss: 0.018537\n",
      "Epoch [9367/10000] Avg train loss: 0.018535\n",
      "Epoch [9368/10000] Avg train loss: 0.018533\n",
      "Epoch [9369/10000] Avg train loss: 0.018531\n",
      "Epoch [9370/10000] Avg train loss: 0.018529\n",
      "Epoch [9371/10000] Avg train loss: 0.018527\n",
      "Epoch [9372/10000] Avg train loss: 0.018525\n",
      "Epoch [9373/10000] Avg train loss: 0.018523\n",
      "Epoch [9374/10000] Avg train loss: 0.018521\n",
      "Epoch [9375/10000] Avg train loss: 0.018519\n",
      "Epoch [9376/10000] Avg train loss: 0.018517\n",
      "Epoch [9377/10000] Avg train loss: 0.018515\n",
      "Epoch [9378/10000] Avg train loss: 0.018513\n",
      "Epoch [9379/10000] Avg train loss: 0.018511\n",
      "Epoch [9380/10000] Avg train loss: 0.018509\n",
      "Epoch [9381/10000] Avg train loss: 0.018507\n",
      "Epoch [9382/10000] Avg train loss: 0.018505\n",
      "Epoch [9383/10000] Avg train loss: 0.018503\n",
      "Epoch [9384/10000] Avg train loss: 0.018501\n",
      "Epoch [9385/10000] Avg train loss: 0.018499\n",
      "Epoch [9386/10000] Avg train loss: 0.018497\n",
      "Epoch [9387/10000] Avg train loss: 0.018495\n",
      "Epoch [9388/10000] Avg train loss: 0.018493\n",
      "Epoch [9389/10000] Avg train loss: 0.018491\n",
      "Epoch [9390/10000] Avg train loss: 0.018489\n",
      "Epoch [9391/10000] Avg train loss: 0.018487\n",
      "Epoch [9392/10000] Avg train loss: 0.018485\n",
      "Epoch [9393/10000] Avg train loss: 0.018483\n",
      "Epoch [9394/10000] Avg train loss: 0.018481\n",
      "Epoch [9395/10000] Avg train loss: 0.018479\n",
      "Epoch [9396/10000] Avg train loss: 0.018477\n",
      "Epoch [9397/10000] Avg train loss: 0.018475\n",
      "Epoch [9398/10000] Avg train loss: 0.018473\n",
      "Epoch [9399/10000] Avg train loss: 0.018471\n",
      "Epoch [9400/10000] Avg train loss: 0.018470\n",
      "Epoch [9401/10000] Avg train loss: 0.018468\n",
      "Epoch [9402/10000] Avg train loss: 0.018466\n",
      "Epoch [9403/10000] Avg train loss: 0.018464\n",
      "Epoch [9404/10000] Avg train loss: 0.018462\n",
      "Epoch [9405/10000] Avg train loss: 0.018460\n",
      "Epoch [9406/10000] Avg train loss: 0.018458\n",
      "Epoch [9407/10000] Avg train loss: 0.018456\n",
      "Epoch [9408/10000] Avg train loss: 0.018454\n",
      "Epoch [9409/10000] Avg train loss: 0.018452\n",
      "Epoch [9410/10000] Avg train loss: 0.018450\n",
      "Epoch [9411/10000] Avg train loss: 0.018448\n",
      "Epoch [9412/10000] Avg train loss: 0.018446\n",
      "Epoch [9413/10000] Avg train loss: 0.018444\n",
      "Epoch [9414/10000] Avg train loss: 0.018442\n",
      "Epoch [9415/10000] Avg train loss: 0.018440\n",
      "Epoch [9416/10000] Avg train loss: 0.018438\n",
      "Epoch [9417/10000] Avg train loss: 0.018436\n",
      "Epoch [9418/10000] Avg train loss: 0.018434\n",
      "Epoch [9419/10000] Avg train loss: 0.018432\n",
      "Epoch [9420/10000] Avg train loss: 0.018430\n",
      "Epoch [9421/10000] Avg train loss: 0.018428\n",
      "Epoch [9422/10000] Avg train loss: 0.018426\n",
      "Epoch [9423/10000] Avg train loss: 0.018424\n",
      "Epoch [9424/10000] Avg train loss: 0.018422\n",
      "Epoch [9425/10000] Avg train loss: 0.018421\n",
      "Epoch [9426/10000] Avg train loss: 0.018419\n",
      "Epoch [9427/10000] Avg train loss: 0.018417\n",
      "Epoch [9428/10000] Avg train loss: 0.018415\n",
      "Epoch [9429/10000] Avg train loss: 0.018413\n",
      "Epoch [9430/10000] Avg train loss: 0.018411\n",
      "Epoch [9431/10000] Avg train loss: 0.018409\n",
      "Epoch [9432/10000] Avg train loss: 0.018407\n",
      "Epoch [9433/10000] Avg train loss: 0.018405\n",
      "Epoch [9434/10000] Avg train loss: 0.018403\n",
      "Epoch [9435/10000] Avg train loss: 0.018401\n",
      "Epoch [9436/10000] Avg train loss: 0.018399\n",
      "Epoch [9437/10000] Avg train loss: 0.018397\n",
      "Epoch [9438/10000] Avg train loss: 0.018395\n",
      "Epoch [9439/10000] Avg train loss: 0.018393\n",
      "Epoch [9440/10000] Avg train loss: 0.018391\n",
      "Epoch [9441/10000] Avg train loss: 0.018389\n",
      "Epoch [9442/10000] Avg train loss: 0.018387\n",
      "Epoch [9443/10000] Avg train loss: 0.018385\n",
      "Epoch [9444/10000] Avg train loss: 0.018383\n",
      "Epoch [9445/10000] Avg train loss: 0.018382\n",
      "Epoch [9446/10000] Avg train loss: 0.018380\n",
      "Epoch [9447/10000] Avg train loss: 0.018378\n",
      "Epoch [9448/10000] Avg train loss: 0.018376\n",
      "Epoch [9449/10000] Avg train loss: 0.018374\n",
      "Epoch [9450/10000] Avg train loss: 0.018372\n",
      "Epoch [9451/10000] Avg train loss: 0.018370\n",
      "Epoch [9452/10000] Avg train loss: 0.018368\n",
      "Epoch [9453/10000] Avg train loss: 0.018366\n",
      "Epoch [9454/10000] Avg train loss: 0.018364\n",
      "Epoch [9455/10000] Avg train loss: 0.018362\n",
      "Epoch [9456/10000] Avg train loss: 0.018360\n",
      "Epoch [9457/10000] Avg train loss: 0.018358\n",
      "Epoch [9458/10000] Avg train loss: 0.018356\n",
      "Epoch [9459/10000] Avg train loss: 0.018354\n",
      "Epoch [9460/10000] Avg train loss: 0.018352\n",
      "Epoch [9461/10000] Avg train loss: 0.018350\n",
      "Epoch [9462/10000] Avg train loss: 0.018349\n",
      "Epoch [9463/10000] Avg train loss: 0.018347\n",
      "Epoch [9464/10000] Avg train loss: 0.018345\n",
      "Epoch [9465/10000] Avg train loss: 0.018343\n",
      "Epoch [9466/10000] Avg train loss: 0.018341\n",
      "Epoch [9467/10000] Avg train loss: 0.018339\n",
      "Epoch [9468/10000] Avg train loss: 0.018337\n",
      "Epoch [9469/10000] Avg train loss: 0.018335\n",
      "Epoch [9470/10000] Avg train loss: 0.018333\n",
      "Epoch [9471/10000] Avg train loss: 0.018331\n",
      "Epoch [9472/10000] Avg train loss: 0.018329\n",
      "Epoch [9473/10000] Avg train loss: 0.018327\n",
      "Epoch [9474/10000] Avg train loss: 0.018325\n",
      "Epoch [9475/10000] Avg train loss: 0.018323\n",
      "Epoch [9476/10000] Avg train loss: 0.018321\n",
      "Epoch [9477/10000] Avg train loss: 0.018319\n",
      "Epoch [9478/10000] Avg train loss: 0.018318\n",
      "Epoch [9479/10000] Avg train loss: 0.018316\n",
      "Epoch [9480/10000] Avg train loss: 0.018314\n",
      "Epoch [9481/10000] Avg train loss: 0.018312\n",
      "Epoch [9482/10000] Avg train loss: 0.018310\n",
      "Epoch [9483/10000] Avg train loss: 0.018308\n",
      "Epoch [9484/10000] Avg train loss: 0.018306\n",
      "Epoch [9485/10000] Avg train loss: 0.018304\n",
      "Epoch [9486/10000] Avg train loss: 0.018302\n",
      "Epoch [9487/10000] Avg train loss: 0.018300\n",
      "Epoch [9488/10000] Avg train loss: 0.018298\n",
      "Epoch [9489/10000] Avg train loss: 0.018296\n",
      "Epoch [9490/10000] Avg train loss: 0.018294\n",
      "Epoch [9491/10000] Avg train loss: 0.018292\n",
      "Epoch [9492/10000] Avg train loss: 0.018291\n",
      "Epoch [9493/10000] Avg train loss: 0.018289\n",
      "Epoch [9494/10000] Avg train loss: 0.018287\n",
      "Epoch [9495/10000] Avg train loss: 0.018285\n",
      "Epoch [9496/10000] Avg train loss: 0.018283\n",
      "Epoch [9497/10000] Avg train loss: 0.018281\n",
      "Epoch [9498/10000] Avg train loss: 0.018279\n",
      "Epoch [9499/10000] Avg train loss: 0.018277\n",
      "Epoch [9500/10000] Avg train loss: 0.018275\n",
      "Epoch [9501/10000] Avg train loss: 0.018273\n",
      "Epoch [9502/10000] Avg train loss: 0.018271\n",
      "Epoch [9503/10000] Avg train loss: 0.018269\n",
      "Epoch [9504/10000] Avg train loss: 0.018267\n",
      "Epoch [9505/10000] Avg train loss: 0.018265\n",
      "Epoch [9506/10000] Avg train loss: 0.018264\n",
      "Epoch [9507/10000] Avg train loss: 0.018262\n",
      "Epoch [9508/10000] Avg train loss: 0.018260\n",
      "Epoch [9509/10000] Avg train loss: 0.018258\n",
      "Epoch [9510/10000] Avg train loss: 0.018256\n",
      "Epoch [9511/10000] Avg train loss: 0.018254\n",
      "Epoch [9512/10000] Avg train loss: 0.018252\n",
      "Epoch [9513/10000] Avg train loss: 0.018250\n",
      "Epoch [9514/10000] Avg train loss: 0.018248\n",
      "Epoch [9515/10000] Avg train loss: 0.018246\n",
      "Epoch [9516/10000] Avg train loss: 0.018244\n",
      "Epoch [9517/10000] Avg train loss: 0.018242\n",
      "Epoch [9518/10000] Avg train loss: 0.018241\n",
      "Epoch [9519/10000] Avg train loss: 0.018239\n",
      "Epoch [9520/10000] Avg train loss: 0.018237\n",
      "Epoch [9521/10000] Avg train loss: 0.018235\n",
      "Epoch [9522/10000] Avg train loss: 0.018233\n",
      "Epoch [9523/10000] Avg train loss: 0.018231\n",
      "Epoch [9524/10000] Avg train loss: 0.018229\n",
      "Epoch [9525/10000] Avg train loss: 0.018227\n",
      "Epoch [9526/10000] Avg train loss: 0.018225\n",
      "Epoch [9527/10000] Avg train loss: 0.018223\n",
      "Epoch [9528/10000] Avg train loss: 0.018221\n",
      "Epoch [9529/10000] Avg train loss: 0.018219\n",
      "Epoch [9530/10000] Avg train loss: 0.018218\n",
      "Epoch [9531/10000] Avg train loss: 0.018216\n",
      "Epoch [9532/10000] Avg train loss: 0.018214\n",
      "Epoch [9533/10000] Avg train loss: 0.018212\n",
      "Epoch [9534/10000] Avg train loss: 0.018210\n",
      "Epoch [9535/10000] Avg train loss: 0.018208\n",
      "Epoch [9536/10000] Avg train loss: 0.018206\n",
      "Epoch [9537/10000] Avg train loss: 0.018204\n",
      "Epoch [9538/10000] Avg train loss: 0.018202\n",
      "Epoch [9539/10000] Avg train loss: 0.018200\n",
      "Epoch [9540/10000] Avg train loss: 0.018198\n",
      "Epoch [9541/10000] Avg train loss: 0.018197\n",
      "Epoch [9542/10000] Avg train loss: 0.018195\n",
      "Epoch [9543/10000] Avg train loss: 0.018193\n",
      "Epoch [9544/10000] Avg train loss: 0.018191\n",
      "Epoch [9545/10000] Avg train loss: 0.018189\n",
      "Epoch [9546/10000] Avg train loss: 0.018187\n",
      "Epoch [9547/10000] Avg train loss: 0.018185\n",
      "Epoch [9548/10000] Avg train loss: 0.018183\n",
      "Epoch [9549/10000] Avg train loss: 0.018181\n",
      "Epoch [9550/10000] Avg train loss: 0.018179\n",
      "Epoch [9551/10000] Avg train loss: 0.018178\n",
      "Epoch [9552/10000] Avg train loss: 0.018176\n",
      "Epoch [9553/10000] Avg train loss: 0.018174\n",
      "Epoch [9554/10000] Avg train loss: 0.018172\n",
      "Epoch [9555/10000] Avg train loss: 0.018170\n",
      "Epoch [9556/10000] Avg train loss: 0.018168\n",
      "Epoch [9557/10000] Avg train loss: 0.018166\n",
      "Epoch [9558/10000] Avg train loss: 0.018164\n",
      "Epoch [9559/10000] Avg train loss: 0.018162\n",
      "Epoch [9560/10000] Avg train loss: 0.018160\n",
      "Epoch [9561/10000] Avg train loss: 0.018159\n",
      "Epoch [9562/10000] Avg train loss: 0.018157\n",
      "Epoch [9563/10000] Avg train loss: 0.018155\n",
      "Epoch [9564/10000] Avg train loss: 0.018153\n",
      "Epoch [9565/10000] Avg train loss: 0.018151\n",
      "Epoch [9566/10000] Avg train loss: 0.018149\n",
      "Epoch [9567/10000] Avg train loss: 0.018147\n",
      "Epoch [9568/10000] Avg train loss: 0.018145\n",
      "Epoch [9569/10000] Avg train loss: 0.018143\n",
      "Epoch [9570/10000] Avg train loss: 0.018141\n",
      "Epoch [9571/10000] Avg train loss: 0.018140\n",
      "Epoch [9572/10000] Avg train loss: 0.018138\n",
      "Epoch [9573/10000] Avg train loss: 0.018136\n",
      "Epoch [9574/10000] Avg train loss: 0.018134\n",
      "Epoch [9575/10000] Avg train loss: 0.018132\n",
      "Epoch [9576/10000] Avg train loss: 0.018130\n",
      "Epoch [9577/10000] Avg train loss: 0.018128\n",
      "Epoch [9578/10000] Avg train loss: 0.018126\n",
      "Epoch [9579/10000] Avg train loss: 0.018124\n",
      "Epoch [9580/10000] Avg train loss: 0.018122\n",
      "Epoch [9581/10000] Avg train loss: 0.018121\n",
      "Epoch [9582/10000] Avg train loss: 0.018119\n",
      "Epoch [9583/10000] Avg train loss: 0.018117\n",
      "Epoch [9584/10000] Avg train loss: 0.018115\n",
      "Epoch [9585/10000] Avg train loss: 0.018113\n",
      "Epoch [9586/10000] Avg train loss: 0.018111\n",
      "Epoch [9587/10000] Avg train loss: 0.018109\n",
      "Epoch [9588/10000] Avg train loss: 0.018107\n",
      "Epoch [9589/10000] Avg train loss: 0.018105\n",
      "Epoch [9590/10000] Avg train loss: 0.018104\n",
      "Epoch [9591/10000] Avg train loss: 0.018102\n",
      "Epoch [9592/10000] Avg train loss: 0.018100\n",
      "Epoch [9593/10000] Avg train loss: 0.018098\n",
      "Epoch [9594/10000] Avg train loss: 0.018096\n",
      "Epoch [9595/10000] Avg train loss: 0.018094\n",
      "Epoch [9596/10000] Avg train loss: 0.018092\n",
      "Epoch [9597/10000] Avg train loss: 0.018090\n",
      "Epoch [9598/10000] Avg train loss: 0.018089\n",
      "Epoch [9599/10000] Avg train loss: 0.018087\n",
      "Epoch [9600/10000] Avg train loss: 0.018085\n",
      "Epoch [9601/10000] Avg train loss: 0.018083\n",
      "Epoch [9602/10000] Avg train loss: 0.018081\n",
      "Epoch [9603/10000] Avg train loss: 0.018079\n",
      "Epoch [9604/10000] Avg train loss: 0.018077\n",
      "Epoch [9605/10000] Avg train loss: 0.018075\n",
      "Epoch [9606/10000] Avg train loss: 0.018073\n",
      "Epoch [9607/10000] Avg train loss: 0.018072\n",
      "Epoch [9608/10000] Avg train loss: 0.018070\n",
      "Epoch [9609/10000] Avg train loss: 0.018068\n",
      "Epoch [9610/10000] Avg train loss: 0.018066\n",
      "Epoch [9611/10000] Avg train loss: 0.018064\n",
      "Epoch [9612/10000] Avg train loss: 0.018062\n",
      "Epoch [9613/10000] Avg train loss: 0.018060\n",
      "Epoch [9614/10000] Avg train loss: 0.018058\n",
      "Epoch [9615/10000] Avg train loss: 0.018057\n",
      "Epoch [9616/10000] Avg train loss: 0.018055\n",
      "Epoch [9617/10000] Avg train loss: 0.018053\n",
      "Epoch [9618/10000] Avg train loss: 0.018051\n",
      "Epoch [9619/10000] Avg train loss: 0.018049\n",
      "Epoch [9620/10000] Avg train loss: 0.018047\n",
      "Epoch [9621/10000] Avg train loss: 0.018045\n",
      "Epoch [9622/10000] Avg train loss: 0.018043\n",
      "Epoch [9623/10000] Avg train loss: 0.018042\n",
      "Epoch [9624/10000] Avg train loss: 0.018040\n",
      "Epoch [9625/10000] Avg train loss: 0.018038\n",
      "Epoch [9626/10000] Avg train loss: 0.018036\n",
      "Epoch [9627/10000] Avg train loss: 0.018034\n",
      "Epoch [9628/10000] Avg train loss: 0.018032\n",
      "Epoch [9629/10000] Avg train loss: 0.018030\n",
      "Epoch [9630/10000] Avg train loss: 0.018028\n",
      "Epoch [9631/10000] Avg train loss: 0.018027\n",
      "Epoch [9632/10000] Avg train loss: 0.018025\n",
      "Epoch [9633/10000] Avg train loss: 0.018023\n",
      "Epoch [9634/10000] Avg train loss: 0.018021\n",
      "Epoch [9635/10000] Avg train loss: 0.018019\n",
      "Epoch [9636/10000] Avg train loss: 0.018017\n",
      "Epoch [9637/10000] Avg train loss: 0.018015\n",
      "Epoch [9638/10000] Avg train loss: 0.018013\n",
      "Epoch [9639/10000] Avg train loss: 0.018012\n",
      "Epoch [9640/10000] Avg train loss: 0.018010\n",
      "Epoch [9641/10000] Avg train loss: 0.018008\n",
      "Epoch [9642/10000] Avg train loss: 0.018006\n",
      "Epoch [9643/10000] Avg train loss: 0.018004\n",
      "Epoch [9644/10000] Avg train loss: 0.018002\n",
      "Epoch [9645/10000] Avg train loss: 0.018000\n",
      "Epoch [9646/10000] Avg train loss: 0.017999\n",
      "Epoch [9647/10000] Avg train loss: 0.017997\n",
      "Epoch [9648/10000] Avg train loss: 0.017995\n",
      "Epoch [9649/10000] Avg train loss: 0.017993\n",
      "Epoch [9650/10000] Avg train loss: 0.017991\n",
      "Epoch [9651/10000] Avg train loss: 0.017989\n",
      "Epoch [9652/10000] Avg train loss: 0.017987\n",
      "Epoch [9653/10000] Avg train loss: 0.017985\n",
      "Epoch [9654/10000] Avg train loss: 0.017984\n",
      "Epoch [9655/10000] Avg train loss: 0.017982\n",
      "Epoch [9656/10000] Avg train loss: 0.017980\n",
      "Epoch [9657/10000] Avg train loss: 0.017978\n",
      "Epoch [9658/10000] Avg train loss: 0.017976\n",
      "Epoch [9659/10000] Avg train loss: 0.017974\n",
      "Epoch [9660/10000] Avg train loss: 0.017972\n",
      "Epoch [9661/10000] Avg train loss: 0.017971\n",
      "Epoch [9662/10000] Avg train loss: 0.017969\n",
      "Epoch [9663/10000] Avg train loss: 0.017967\n",
      "Epoch [9664/10000] Avg train loss: 0.017965\n",
      "Epoch [9665/10000] Avg train loss: 0.017963\n",
      "Epoch [9666/10000] Avg train loss: 0.017961\n",
      "Epoch [9667/10000] Avg train loss: 0.017959\n",
      "Epoch [9668/10000] Avg train loss: 0.017958\n",
      "Epoch [9669/10000] Avg train loss: 0.017956\n",
      "Epoch [9670/10000] Avg train loss: 0.017954\n",
      "Epoch [9671/10000] Avg train loss: 0.017952\n",
      "Epoch [9672/10000] Avg train loss: 0.017950\n",
      "Epoch [9673/10000] Avg train loss: 0.017948\n",
      "Epoch [9674/10000] Avg train loss: 0.017946\n",
      "Epoch [9675/10000] Avg train loss: 0.017945\n",
      "Epoch [9676/10000] Avg train loss: 0.017943\n",
      "Epoch [9677/10000] Avg train loss: 0.017941\n",
      "Epoch [9678/10000] Avg train loss: 0.017939\n",
      "Epoch [9679/10000] Avg train loss: 0.017937\n",
      "Epoch [9680/10000] Avg train loss: 0.017935\n",
      "Epoch [9681/10000] Avg train loss: 0.017933\n",
      "Epoch [9682/10000] Avg train loss: 0.017932\n",
      "Epoch [9683/10000] Avg train loss: 0.017930\n",
      "Epoch [9684/10000] Avg train loss: 0.017928\n",
      "Epoch [9685/10000] Avg train loss: 0.017926\n",
      "Epoch [9686/10000] Avg train loss: 0.017924\n",
      "Epoch [9687/10000] Avg train loss: 0.017922\n",
      "Epoch [9688/10000] Avg train loss: 0.017920\n",
      "Epoch [9689/10000] Avg train loss: 0.017919\n",
      "Epoch [9690/10000] Avg train loss: 0.017917\n",
      "Epoch [9691/10000] Avg train loss: 0.017915\n",
      "Epoch [9692/10000] Avg train loss: 0.017913\n",
      "Epoch [9693/10000] Avg train loss: 0.017911\n",
      "Epoch [9694/10000] Avg train loss: 0.017909\n",
      "Epoch [9695/10000] Avg train loss: 0.017908\n",
      "Epoch [9696/10000] Avg train loss: 0.017906\n",
      "Epoch [9697/10000] Avg train loss: 0.017904\n",
      "Epoch [9698/10000] Avg train loss: 0.017902\n",
      "Epoch [9699/10000] Avg train loss: 0.017900\n",
      "Epoch [9700/10000] Avg train loss: 0.017898\n",
      "Epoch [9701/10000] Avg train loss: 0.017896\n",
      "Epoch [9702/10000] Avg train loss: 0.017895\n",
      "Epoch [9703/10000] Avg train loss: 0.017893\n",
      "Epoch [9704/10000] Avg train loss: 0.017891\n",
      "Epoch [9705/10000] Avg train loss: 0.017889\n",
      "Epoch [9706/10000] Avg train loss: 0.017887\n",
      "Epoch [9707/10000] Avg train loss: 0.017885\n",
      "Epoch [9708/10000] Avg train loss: 0.017884\n",
      "Epoch [9709/10000] Avg train loss: 0.017882\n",
      "Epoch [9710/10000] Avg train loss: 0.017880\n",
      "Epoch [9711/10000] Avg train loss: 0.017878\n",
      "Epoch [9712/10000] Avg train loss: 0.017876\n",
      "Epoch [9713/10000] Avg train loss: 0.017874\n",
      "Epoch [9714/10000] Avg train loss: 0.017873\n",
      "Epoch [9715/10000] Avg train loss: 0.017871\n",
      "Epoch [9716/10000] Avg train loss: 0.017869\n",
      "Epoch [9717/10000] Avg train loss: 0.017867\n",
      "Epoch [9718/10000] Avg train loss: 0.017865\n",
      "Epoch [9719/10000] Avg train loss: 0.017863\n",
      "Epoch [9720/10000] Avg train loss: 0.017861\n",
      "Epoch [9721/10000] Avg train loss: 0.017860\n",
      "Epoch [9722/10000] Avg train loss: 0.017858\n",
      "Epoch [9723/10000] Avg train loss: 0.017856\n",
      "Epoch [9724/10000] Avg train loss: 0.017854\n",
      "Epoch [9725/10000] Avg train loss: 0.017852\n",
      "Epoch [9726/10000] Avg train loss: 0.017850\n",
      "Epoch [9727/10000] Avg train loss: 0.017849\n",
      "Epoch [9728/10000] Avg train loss: 0.017847\n",
      "Epoch [9729/10000] Avg train loss: 0.017845\n",
      "Epoch [9730/10000] Avg train loss: 0.017843\n",
      "Epoch [9731/10000] Avg train loss: 0.017841\n",
      "Epoch [9732/10000] Avg train loss: 0.017839\n",
      "Epoch [9733/10000] Avg train loss: 0.017838\n",
      "Epoch [9734/10000] Avg train loss: 0.017836\n",
      "Epoch [9735/10000] Avg train loss: 0.017834\n",
      "Epoch [9736/10000] Avg train loss: 0.017832\n",
      "Epoch [9737/10000] Avg train loss: 0.017830\n",
      "Epoch [9738/10000] Avg train loss: 0.017828\n",
      "Epoch [9739/10000] Avg train loss: 0.017827\n",
      "Epoch [9740/10000] Avg train loss: 0.017825\n",
      "Epoch [9741/10000] Avg train loss: 0.017823\n",
      "Epoch [9742/10000] Avg train loss: 0.017821\n",
      "Epoch [9743/10000] Avg train loss: 0.017819\n",
      "Epoch [9744/10000] Avg train loss: 0.017817\n",
      "Epoch [9745/10000] Avg train loss: 0.017816\n",
      "Epoch [9746/10000] Avg train loss: 0.017814\n",
      "Epoch [9747/10000] Avg train loss: 0.017812\n",
      "Epoch [9748/10000] Avg train loss: 0.017810\n",
      "Epoch [9749/10000] Avg train loss: 0.017808\n",
      "Epoch [9750/10000] Avg train loss: 0.017807\n",
      "Epoch [9751/10000] Avg train loss: 0.017805\n",
      "Epoch [9752/10000] Avg train loss: 0.017803\n",
      "Epoch [9753/10000] Avg train loss: 0.017801\n",
      "Epoch [9754/10000] Avg train loss: 0.017799\n",
      "Epoch [9755/10000] Avg train loss: 0.017797\n",
      "Epoch [9756/10000] Avg train loss: 0.017796\n",
      "Epoch [9757/10000] Avg train loss: 0.017794\n",
      "Epoch [9758/10000] Avg train loss: 0.017792\n",
      "Epoch [9759/10000] Avg train loss: 0.017790\n",
      "Epoch [9760/10000] Avg train loss: 0.017788\n",
      "Epoch [9761/10000] Avg train loss: 0.017786\n",
      "Epoch [9762/10000] Avg train loss: 0.017785\n",
      "Epoch [9763/10000] Avg train loss: 0.017783\n",
      "Epoch [9764/10000] Avg train loss: 0.017781\n",
      "Epoch [9765/10000] Avg train loss: 0.017779\n",
      "Epoch [9766/10000] Avg train loss: 0.017777\n",
      "Epoch [9767/10000] Avg train loss: 0.017776\n",
      "Epoch [9768/10000] Avg train loss: 0.017774\n",
      "Epoch [9769/10000] Avg train loss: 0.017772\n",
      "Epoch [9770/10000] Avg train loss: 0.017770\n",
      "Epoch [9771/10000] Avg train loss: 0.017768\n",
      "Epoch [9772/10000] Avg train loss: 0.017766\n",
      "Epoch [9773/10000] Avg train loss: 0.017765\n",
      "Epoch [9774/10000] Avg train loss: 0.017763\n",
      "Epoch [9775/10000] Avg train loss: 0.017761\n",
      "Epoch [9776/10000] Avg train loss: 0.017759\n",
      "Epoch [9777/10000] Avg train loss: 0.017757\n",
      "Epoch [9778/10000] Avg train loss: 0.017756\n",
      "Epoch [9779/10000] Avg train loss: 0.017754\n",
      "Epoch [9780/10000] Avg train loss: 0.017752\n",
      "Epoch [9781/10000] Avg train loss: 0.017750\n",
      "Epoch [9782/10000] Avg train loss: 0.017748\n",
      "Epoch [9783/10000] Avg train loss: 0.017746\n",
      "Epoch [9784/10000] Avg train loss: 0.017745\n",
      "Epoch [9785/10000] Avg train loss: 0.017743\n",
      "Epoch [9786/10000] Avg train loss: 0.017741\n",
      "Epoch [9787/10000] Avg train loss: 0.017739\n",
      "Epoch [9788/10000] Avg train loss: 0.017737\n",
      "Epoch [9789/10000] Avg train loss: 0.017736\n",
      "Epoch [9790/10000] Avg train loss: 0.017734\n",
      "Epoch [9791/10000] Avg train loss: 0.017732\n",
      "Epoch [9792/10000] Avg train loss: 0.017730\n",
      "Epoch [9793/10000] Avg train loss: 0.017728\n",
      "Epoch [9794/10000] Avg train loss: 0.017727\n",
      "Epoch [9795/10000] Avg train loss: 0.017725\n",
      "Epoch [9796/10000] Avg train loss: 0.017723\n",
      "Epoch [9797/10000] Avg train loss: 0.017721\n",
      "Epoch [9798/10000] Avg train loss: 0.017719\n",
      "Epoch [9799/10000] Avg train loss: 0.017717\n",
      "Epoch [9800/10000] Avg train loss: 0.017716\n",
      "Epoch [9801/10000] Avg train loss: 0.017714\n",
      "Epoch [9802/10000] Avg train loss: 0.017712\n",
      "Epoch [9803/10000] Avg train loss: 0.017710\n",
      "Epoch [9804/10000] Avg train loss: 0.017708\n",
      "Epoch [9805/10000] Avg train loss: 0.017707\n",
      "Epoch [9806/10000] Avg train loss: 0.017705\n",
      "Epoch [9807/10000] Avg train loss: 0.017703\n",
      "Epoch [9808/10000] Avg train loss: 0.017701\n",
      "Epoch [9809/10000] Avg train loss: 0.017699\n",
      "Epoch [9810/10000] Avg train loss: 0.017698\n",
      "Epoch [9811/10000] Avg train loss: 0.017696\n",
      "Epoch [9812/10000] Avg train loss: 0.017694\n",
      "Epoch [9813/10000] Avg train loss: 0.017692\n",
      "Epoch [9814/10000] Avg train loss: 0.017690\n",
      "Epoch [9815/10000] Avg train loss: 0.017689\n",
      "Epoch [9816/10000] Avg train loss: 0.017687\n",
      "Epoch [9817/10000] Avg train loss: 0.017685\n",
      "Epoch [9818/10000] Avg train loss: 0.017683\n",
      "Epoch [9819/10000] Avg train loss: 0.017681\n",
      "Epoch [9820/10000] Avg train loss: 0.017680\n",
      "Epoch [9821/10000] Avg train loss: 0.017678\n",
      "Epoch [9822/10000] Avg train loss: 0.017676\n",
      "Epoch [9823/10000] Avg train loss: 0.017674\n",
      "Epoch [9824/10000] Avg train loss: 0.017672\n",
      "Epoch [9825/10000] Avg train loss: 0.017671\n",
      "Epoch [9826/10000] Avg train loss: 0.017669\n",
      "Epoch [9827/10000] Avg train loss: 0.017667\n",
      "Epoch [9828/10000] Avg train loss: 0.017665\n",
      "Epoch [9829/10000] Avg train loss: 0.017663\n",
      "Epoch [9830/10000] Avg train loss: 0.017662\n",
      "Epoch [9831/10000] Avg train loss: 0.017660\n",
      "Epoch [9832/10000] Avg train loss: 0.017658\n",
      "Epoch [9833/10000] Avg train loss: 0.017656\n",
      "Epoch [9834/10000] Avg train loss: 0.017654\n",
      "Epoch [9835/10000] Avg train loss: 0.017653\n",
      "Epoch [9836/10000] Avg train loss: 0.017651\n",
      "Epoch [9837/10000] Avg train loss: 0.017649\n",
      "Epoch [9838/10000] Avg train loss: 0.017647\n",
      "Epoch [9839/10000] Avg train loss: 0.017645\n",
      "Epoch [9840/10000] Avg train loss: 0.017644\n",
      "Epoch [9841/10000] Avg train loss: 0.017642\n",
      "Epoch [9842/10000] Avg train loss: 0.017640\n",
      "Epoch [9843/10000] Avg train loss: 0.017638\n",
      "Epoch [9844/10000] Avg train loss: 0.017636\n",
      "Epoch [9845/10000] Avg train loss: 0.017635\n",
      "Epoch [9846/10000] Avg train loss: 0.017633\n",
      "Epoch [9847/10000] Avg train loss: 0.017631\n",
      "Epoch [9848/10000] Avg train loss: 0.017629\n",
      "Epoch [9849/10000] Avg train loss: 0.017628\n",
      "Epoch [9850/10000] Avg train loss: 0.017626\n",
      "Epoch [9851/10000] Avg train loss: 0.017624\n",
      "Epoch [9852/10000] Avg train loss: 0.017622\n",
      "Epoch [9853/10000] Avg train loss: 0.017620\n",
      "Epoch [9854/10000] Avg train loss: 0.017619\n",
      "Epoch [9855/10000] Avg train loss: 0.017617\n",
      "Epoch [9856/10000] Avg train loss: 0.017615\n",
      "Epoch [9857/10000] Avg train loss: 0.017613\n",
      "Epoch [9858/10000] Avg train loss: 0.017611\n",
      "Epoch [9859/10000] Avg train loss: 0.017610\n",
      "Epoch [9860/10000] Avg train loss: 0.017608\n",
      "Epoch [9861/10000] Avg train loss: 0.017606\n",
      "Epoch [9862/10000] Avg train loss: 0.017604\n",
      "Epoch [9863/10000] Avg train loss: 0.017603\n",
      "Epoch [9864/10000] Avg train loss: 0.017601\n",
      "Epoch [9865/10000] Avg train loss: 0.017599\n",
      "Epoch [9866/10000] Avg train loss: 0.017597\n",
      "Epoch [9867/10000] Avg train loss: 0.017595\n",
      "Epoch [9868/10000] Avg train loss: 0.017594\n",
      "Epoch [9869/10000] Avg train loss: 0.017592\n",
      "Epoch [9870/10000] Avg train loss: 0.017590\n",
      "Epoch [9871/10000] Avg train loss: 0.017588\n",
      "Epoch [9872/10000] Avg train loss: 0.017586\n",
      "Epoch [9873/10000] Avg train loss: 0.017585\n",
      "Epoch [9874/10000] Avg train loss: 0.017583\n",
      "Epoch [9875/10000] Avg train loss: 0.017581\n",
      "Epoch [9876/10000] Avg train loss: 0.017579\n",
      "Epoch [9877/10000] Avg train loss: 0.017578\n",
      "Epoch [9878/10000] Avg train loss: 0.017576\n",
      "Epoch [9879/10000] Avg train loss: 0.017574\n",
      "Epoch [9880/10000] Avg train loss: 0.017572\n",
      "Epoch [9881/10000] Avg train loss: 0.017570\n",
      "Epoch [9882/10000] Avg train loss: 0.017569\n",
      "Epoch [9883/10000] Avg train loss: 0.017567\n",
      "Epoch [9884/10000] Avg train loss: 0.017565\n",
      "Epoch [9885/10000] Avg train loss: 0.017563\n",
      "Epoch [9886/10000] Avg train loss: 0.017562\n",
      "Epoch [9887/10000] Avg train loss: 0.017560\n",
      "Epoch [9888/10000] Avg train loss: 0.017558\n",
      "Epoch [9889/10000] Avg train loss: 0.017556\n",
      "Epoch [9890/10000] Avg train loss: 0.017554\n",
      "Epoch [9891/10000] Avg train loss: 0.017553\n",
      "Epoch [9892/10000] Avg train loss: 0.017551\n",
      "Epoch [9893/10000] Avg train loss: 0.017549\n",
      "Epoch [9894/10000] Avg train loss: 0.017547\n",
      "Epoch [9895/10000] Avg train loss: 0.017546\n",
      "Epoch [9896/10000] Avg train loss: 0.017544\n",
      "Epoch [9897/10000] Avg train loss: 0.017542\n",
      "Epoch [9898/10000] Avg train loss: 0.017540\n",
      "Epoch [9899/10000] Avg train loss: 0.017538\n",
      "Epoch [9900/10000] Avg train loss: 0.017537\n",
      "Epoch [9901/10000] Avg train loss: 0.017535\n",
      "Epoch [9902/10000] Avg train loss: 0.017533\n",
      "Epoch [9903/10000] Avg train loss: 0.017531\n",
      "Epoch [9904/10000] Avg train loss: 0.017530\n",
      "Epoch [9905/10000] Avg train loss: 0.017528\n",
      "Epoch [9906/10000] Avg train loss: 0.017526\n",
      "Epoch [9907/10000] Avg train loss: 0.017524\n",
      "Epoch [9908/10000] Avg train loss: 0.017523\n",
      "Epoch [9909/10000] Avg train loss: 0.017521\n",
      "Epoch [9910/10000] Avg train loss: 0.017519\n",
      "Epoch [9911/10000] Avg train loss: 0.017517\n",
      "Epoch [9912/10000] Avg train loss: 0.017515\n",
      "Epoch [9913/10000] Avg train loss: 0.017514\n",
      "Epoch [9914/10000] Avg train loss: 0.017512\n",
      "Epoch [9915/10000] Avg train loss: 0.017510\n",
      "Epoch [9916/10000] Avg train loss: 0.017508\n",
      "Epoch [9917/10000] Avg train loss: 0.017507\n",
      "Epoch [9918/10000] Avg train loss: 0.017505\n",
      "Epoch [9919/10000] Avg train loss: 0.017503\n",
      "Epoch [9920/10000] Avg train loss: 0.017501\n",
      "Epoch [9921/10000] Avg train loss: 0.017500\n",
      "Epoch [9922/10000] Avg train loss: 0.017498\n",
      "Epoch [9923/10000] Avg train loss: 0.017496\n",
      "Epoch [9924/10000] Avg train loss: 0.017494\n",
      "Epoch [9925/10000] Avg train loss: 0.017493\n",
      "Epoch [9926/10000] Avg train loss: 0.017491\n",
      "Epoch [9927/10000] Avg train loss: 0.017489\n",
      "Epoch [9928/10000] Avg train loss: 0.017487\n",
      "Epoch [9929/10000] Avg train loss: 0.017486\n",
      "Epoch [9930/10000] Avg train loss: 0.017484\n",
      "Epoch [9931/10000] Avg train loss: 0.017482\n",
      "Epoch [9932/10000] Avg train loss: 0.017480\n",
      "Epoch [9933/10000] Avg train loss: 0.017478\n",
      "Epoch [9934/10000] Avg train loss: 0.017477\n",
      "Epoch [9935/10000] Avg train loss: 0.017475\n",
      "Epoch [9936/10000] Avg train loss: 0.017473\n",
      "Epoch [9937/10000] Avg train loss: 0.017471\n",
      "Epoch [9938/10000] Avg train loss: 0.017470\n",
      "Epoch [9939/10000] Avg train loss: 0.017468\n",
      "Epoch [9940/10000] Avg train loss: 0.017466\n",
      "Epoch [9941/10000] Avg train loss: 0.017464\n",
      "Epoch [9942/10000] Avg train loss: 0.017463\n",
      "Epoch [9943/10000] Avg train loss: 0.017461\n",
      "Epoch [9944/10000] Avg train loss: 0.017459\n",
      "Epoch [9945/10000] Avg train loss: 0.017457\n",
      "Epoch [9946/10000] Avg train loss: 0.017456\n",
      "Epoch [9947/10000] Avg train loss: 0.017454\n",
      "Epoch [9948/10000] Avg train loss: 0.017452\n",
      "Epoch [9949/10000] Avg train loss: 0.017450\n",
      "Epoch [9950/10000] Avg train loss: 0.017449\n",
      "Epoch [9951/10000] Avg train loss: 0.017447\n",
      "Epoch [9952/10000] Avg train loss: 0.017445\n",
      "Epoch [9953/10000] Avg train loss: 0.017443\n",
      "Epoch [9954/10000] Avg train loss: 0.017442\n",
      "Epoch [9955/10000] Avg train loss: 0.017440\n",
      "Epoch [9956/10000] Avg train loss: 0.017438\n",
      "Epoch [9957/10000] Avg train loss: 0.017436\n",
      "Epoch [9958/10000] Avg train loss: 0.017435\n",
      "Epoch [9959/10000] Avg train loss: 0.017433\n",
      "Epoch [9960/10000] Avg train loss: 0.017431\n",
      "Epoch [9961/10000] Avg train loss: 0.017429\n",
      "Epoch [9962/10000] Avg train loss: 0.017428\n",
      "Epoch [9963/10000] Avg train loss: 0.017426\n",
      "Epoch [9964/10000] Avg train loss: 0.017424\n",
      "Epoch [9965/10000] Avg train loss: 0.017422\n",
      "Epoch [9966/10000] Avg train loss: 0.017421\n",
      "Epoch [9967/10000] Avg train loss: 0.017419\n",
      "Epoch [9968/10000] Avg train loss: 0.017417\n",
      "Epoch [9969/10000] Avg train loss: 0.017415\n",
      "Epoch [9970/10000] Avg train loss: 0.017414\n",
      "Epoch [9971/10000] Avg train loss: 0.017412\n",
      "Epoch [9972/10000] Avg train loss: 0.017410\n",
      "Epoch [9973/10000] Avg train loss: 0.017408\n",
      "Epoch [9974/10000] Avg train loss: 0.017407\n",
      "Epoch [9975/10000] Avg train loss: 0.017405\n",
      "Epoch [9976/10000] Avg train loss: 0.017403\n",
      "Epoch [9977/10000] Avg train loss: 0.017401\n",
      "Epoch [9978/10000] Avg train loss: 0.017400\n",
      "Epoch [9979/10000] Avg train loss: 0.017398\n",
      "Epoch [9980/10000] Avg train loss: 0.017396\n",
      "Epoch [9981/10000] Avg train loss: 0.017394\n",
      "Epoch [9982/10000] Avg train loss: 0.017393\n",
      "Epoch [9983/10000] Avg train loss: 0.017391\n",
      "Epoch [9984/10000] Avg train loss: 0.017389\n",
      "Epoch [9985/10000] Avg train loss: 0.017387\n",
      "Epoch [9986/10000] Avg train loss: 0.017386\n",
      "Epoch [9987/10000] Avg train loss: 0.017384\n",
      "Epoch [9988/10000] Avg train loss: 0.017382\n",
      "Epoch [9989/10000] Avg train loss: 0.017380\n",
      "Epoch [9990/10000] Avg train loss: 0.017379\n",
      "Epoch [9991/10000] Avg train loss: 0.017377\n",
      "Epoch [9992/10000] Avg train loss: 0.017375\n",
      "Epoch [9993/10000] Avg train loss: 0.017374\n",
      "Epoch [9994/10000] Avg train loss: 0.017372\n",
      "Epoch [9995/10000] Avg train loss: 0.017370\n",
      "Epoch [9996/10000] Avg train loss: 0.017368\n",
      "Epoch [9997/10000] Avg train loss: 0.017367\n",
      "Epoch [9998/10000] Avg train loss: 0.017365\n",
      "Epoch [9999/10000] Avg train loss: 0.017363\n",
      "Epoch [10000/10000] Avg train loss: 0.017361\n"
     ]
    }
   ],
   "source": [
    "set_seed(42)  # Fixing the seed\n",
    "import torch.optim as optim\n",
    "# Proceed with the rest of the setup (loss, optimizer) and training loop as before\n",
    "# Training loop\n",
    "\n",
    "# careful, for minibatch 32, 400 is enough!!!\n",
    "num_epochs = 10000  # You might need more epochs for a deep network\n",
    "losses = []\n",
    "cool_loss = []\n",
    "for epoch in range(num_epochs):\n",
    "    # now we loop through the mini-batches\n",
    "    for inputs, targets in train_loader:\n",
    "        # Forward pass\n",
    "        outputs = model(inputs) # this is f(x;\\theta)\n",
    "        loss = criterion(outputs, targets) # this is \\sum_{t\\in mini batch} \\ell(y_t, f(x_t;\\theta))\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad() # forget the gradients from old steps of Gradient Descent (GD)\n",
    "        loss.backward() # compute the new gradient, \\nabla_\\theta L(\\theta)\n",
    "        optimizer.step() # \\theta_{k+1}=\\theta_k - \\eta \\nabla_\\theta L(\\theta_k).\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    cool_loss.append(np.mean(losses))\n",
    "    losses = []\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] Avg train loss: {np.mean(cool_loss):.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "6c30d56e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "14040164",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x304034a30>]"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plot_e = np.linspace(1,  10000, num= 10000)\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.plot(plot_e, np.array(cool_loss))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "e6e5b96f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure the model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Function to get predictions\n",
    "def get_predictions(loader):\n",
    "    targets = []\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            outputs = model(inputs)\n",
    "            targets.extend(labels.numpy())\n",
    "            predictions.extend(outputs.numpy())\n",
    "\n",
    "    return np.array(targets).flatten(), np.array(predictions).flatten()\n",
    "\n",
    "# Get predictions for both training and test sets\n",
    "train_targets, train_predictions = get_predictions(train_loader)\n",
    "test_targets, test_predictions = get_predictions(test_loader)\n",
    "#test_predictions=0.5*test_predictions\n",
    "\n",
    "# Calculate MSE for both training and test sets\n",
    "train_mse = np.mean((train_targets - train_predictions) ** 2) / np.mean((train_targets) ** 2)\n",
    "test_mse = np.mean((test_targets - test_predictions) ** 2) / np.mean((test_targets) ** 2)\n",
    "\n",
    "# Plotting\n",
    "fig, axs = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Training set subplot\n",
    "axs[0].scatter(train_targets, train_predictions)\n",
    "axs[0].plot([train_targets.min(), train_targets.max()], [train_targets.min(), train_targets.max()], 'k--', lw=2)  # Diagonal line\n",
    "axs[0].set_xlabel('Actual values')\n",
    "axs[0].set_ylabel('Predicted values')\n",
    "axs[0].set_title(f'Training Set Predicted vs Actual\\nMSE: {train_mse:.4f}')\n",
    "\n",
    "# Test set subplot\n",
    "axs[1].scatter(test_targets, test_predictions)\n",
    "axs[1].plot([test_targets.min(), test_targets.max()], [test_targets.min(), test_targets.max()], 'k--', lw=2)  # Diagonal line\n",
    "axs[1].set_xlabel('Actual values')\n",
    "axs[1].set_ylabel('Predicted values')\n",
    "axs[1].set_title(f'Test Set Predicted vs Actual\\nMSE: {test_mse:.4f}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "d25cfb0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test sign prediction accuracy: 49.77%\n",
      "99% CI under null: [44.76%, 54.78%]  Significant? NO\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "# === 1. SIGN PREDICTION ACCURACY (TEST ONLY) ===\n",
    "def sign_accuracy(y_true, y_pred):\n",
    "    return np.mean(np.sign(y_true) == np.sign(y_pred))\n",
    "\n",
    "test_sign_acc = sign_accuracy(test_targets, test_predictions)\n",
    "\n",
    "# === 1.1. Confidence interval around 0.5 (coin toss null hypothesis) ===\n",
    "n_test = len(test_targets)\n",
    "p_hat = test_sign_acc\n",
    "p_0 = 0.5\n",
    "z = norm.ppf(1 - 0.01 / 2)  # two-tailed 99%  z  2.576\n",
    "se = np.sqrt(p_0 * (1 - p_0) / n_test)\n",
    "ci_low = p_hat - z * se\n",
    "ci_high = p_hat + z * se\n",
    "significant = \"YES\" if ci_low > 0.5 or ci_high < 0.5 else \"NO\"\n",
    "\n",
    "print(f\"Test sign prediction accuracy: {test_sign_acc:.2%}\")\n",
    "print(f\"99% CI under null: [{ci_low:.2%}, {ci_high:.2%}]  Significant? {significant}\")\n",
    "\n",
    "# === 2. STRATEGY RETURNS (TEST ONLY) ===\n",
    "test_strategy_returns = np.sign(test_predictions) * test_targets\n",
    "\n",
    "# === 3. PLOT CUMULATIVE RETURNS (TEST ONLY) ===\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "\n",
    "ax.plot(np.cumsum(test_strategy_returns), label='Strategy (Test)', linestyle='--')\n",
    "ax.plot(np.cumsum(test_targets), label='Asset (Test)', alpha=0.7)\n",
    "\n",
    "ax.set_title('Cumulative Returns on Test Set')\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_ylabel('Cumulative Return')\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLfinanceHW2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
