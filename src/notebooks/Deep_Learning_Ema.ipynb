{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75ce5ebb",
   "metadata": {},
   "source": [
    "Here we will follow professor's colab in order to create a MLP. Here is the link to the colab:https://colab.research.google.com/drive/1gmm92ZzBF_pB9QcHA91dhRte2saC7pf1. No further explanation is needed. Attention is all you need.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "9f1e6af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch                     # for all things PyTorch\n",
    "import pandas as pd\n",
    "import torch.nn as nn            # for torch.nn.Module, the parent object for PyTorch models\n",
    "import torch.nn.functional as F  # for the activation function\n",
    "import numpy as np\n",
    "import random\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "75eba4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlexibleMLP(nn.Module):\n",
    "    def __init__(self, layers: list, scale: float = 1.0, bias_scale: float = 0.0, activation=nn.GELU()):\n",
    "        \"\"\"\n",
    "        Initialize a customizable Multi-Layer Perceptron (MLP) with flexible architecture and initialization.\n",
    "\n",
    "        Args:\n",
    "            layers (list of int): A list of integers where each value defines the size of each layer.\n",
    "                                  For example, layers=[10, 64, 32, 1] defines a network with input dimension 10,\n",
    "                                  two hidden layers with sizes 64 and 32, and output dimension 1.\n",
    "            scale (float): Scaling factor for weight initialization. Controls the standard deviation of\n",
    "                           the normal distribution used in initializing weights. Recommended to be 1.0 for LeCun initialization.\n",
    "            bias_scale (float): Scaling factor for bias initialization. Often set to 0.0 to start with no initial bias.\n",
    "            activation (nn.Module): Activation function applied after each linear transformation except the last layer.\n",
    "                                    Defaults to nn.GELU(), but can be any activation like nn.ReLU(), nn.Tanh(), etc.\n",
    "        \"\"\"\n",
    "        # Call the constructor of the parent class (nn.Module) to initialize all internal PyTorch machinery.\n",
    "        # This is crucial because nn.Module handles a lot of behind-the-scenes logic like:\n",
    "        # - registering parameters (weights and biases) for automatic optimization\n",
    "        # - setting up .to(device), .eval(), .train(), etc.\n",
    "        # - tracking submodules (layers, activations, etc.)\n",
    "        #\n",
    "        # If you omit this line, the module will NOT work correctly in PyTorch:\n",
    "        # things like model.cuda(), model.parameters(), model.state_dict(), etc. will all break.\n",
    "        #\n",
    "        # The super() call here:\n",
    "        # - FlexibleMLP is our class\n",
    "        # - nn.Module is the parent class\n",
    "        # - self.__init__() is the method we want to call from the parent\n",
    "        super(FlexibleMLP, self).__init__()\n",
    "\n",
    "        # Save arguments as attributes for reuse in reset_parameters\n",
    "        self.layer_sizes = layers\n",
    "        self.scale = scale\n",
    "        self.bias_scale = bias_scale\n",
    "        self.activation_fn = activation\n",
    "\n",
    "        # Create containers to hold layers and activations\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.activations = nn.ModuleList()\n",
    "\n",
    "        # Build network structure (but not weights yet)\n",
    "        self._build_layers()\n",
    "\n",
    "        # Initialize weights and biases\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def _build_layers(self):\n",
    "        \"\"\"\n",
    "        Build the linear layers and corresponding activations (except for initialization).\n",
    "        \"\"\"\n",
    "        for i in range(len(self.layer_sizes) - 1):\n",
    "            # Create a linear layer from layer i to layer i+1\n",
    "            layer = nn.Linear(self.layer_sizes[i], self.layer_sizes[i + 1])\n",
    "            self.layers.append(layer)\n",
    "\n",
    "            # Add an activation function unless it's the final layer\n",
    "            if i < len(self.layer_sizes) - 2:\n",
    "                self.activations.append(self.activation_fn)\n",
    "            else:\n",
    "                # Final layer doesn't use activation (use Identity to keep list structure consistent)\n",
    "                self.activations.append(nn.Identity())\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"\n",
    "        Apply custom initialization to all layers using the given scale and bias_scale.\n",
    "        \"\"\"\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            # Apply LeCun-style initialization for better gradient behavior\n",
    "            nn.init.normal_(layer.weight, mean=0.0, std=self.scale * np.sqrt(1 / self.layer_sizes[i]))\n",
    "            nn.init.normal_(layer.bias, mean=0.0, std=self.bias_scale * np.sqrt(1 / self.layer_sizes[i]))\n",
    "\n",
    "    def forward(self, x, return_last_hidden=False):\n",
    "        \"\"\"\n",
    "        Perform a forward pass through the network.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, input_dim).\n",
    "            return_last_hidden (bool): If True, returns both the final output and the last hidden layer's output.\n",
    "                                       Useful for feature extraction, analysis, or interpretability.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output of the final layer.\n",
    "            torch.Tensor (optional): Output of the last hidden layer (before final linear layer),\n",
    "                                     if return_last_hidden is set to True.\n",
    "        \"\"\"\n",
    "        last_hidden = None  # Will store the output of the last hidden layer\n",
    "\n",
    "        # Apply all but the last layer with activation\n",
    "        for layer, activation in zip(self.layers[:-1], self.activations[:-1]):\n",
    "            x = activation(layer(x))  # Apply linear transformation and activation\n",
    "            last_hidden = x  # Save the last hidden output\n",
    "\n",
    "        # Final layer (linear transformation only, no activation)\n",
    "        x = self.layers[-1](x)\n",
    "\n",
    "        if return_last_hidden:\n",
    "            return x, last_hidden\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "ecfe088c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Module.parameters at 0x159451cf0>\n",
      "Parameter containing:\n",
      "tensor([[ 6.3686e-01,  8.4078e-01, -1.4175e-02, -2.6850e-02,  5.4663e-01,\n",
      "         -1.9757e-01,  1.3496e-01,  6.5039e-01,  2.8905e-01, -1.3444e-01],\n",
      "        [-7.6251e-01,  7.6877e-01, -3.4985e-01,  9.5796e-01, -1.4475e-01,\n",
      "         -1.6878e-01, -1.0276e+00,  6.1902e-01, -1.3406e-01, -2.8436e-01],\n",
      "        [-9.3413e-01, -2.6702e-01,  6.8995e-01,  7.3949e-01,  3.3985e-01,\n",
      "          4.8398e-02,  2.0938e-01,  7.1564e-01, -2.2972e-01, -2.8483e-01],\n",
      "        [ 1.3444e-01,  1.0243e-01, -8.6104e-01, -1.0965e-01,  6.0615e-01,\n",
      "          1.7848e-01, -4.2529e-01, -4.2075e-01,  6.3001e-01, -5.2980e-01],\n",
      "        [-1.3480e-01, -1.4241e+00,  1.6077e-01, -3.2276e-02, -7.9266e-01,\n",
      "          7.3656e-01, -2.9789e-01,  1.2361e-02,  1.2839e+00, -1.0527e-01],\n",
      "        [-2.8496e-01, -1.1604e-01, -7.1703e-01, -7.9147e-01, -2.7793e-02,\n",
      "         -2.9027e-01, -1.1640e-01, -6.3503e-01, -5.9159e-01, -2.2021e-01],\n",
      "        [-2.5962e-01, -1.3775e-01, -1.7103e-02, -8.8796e-01,  7.6333e-02,\n",
      "         -1.1103e-01, -6.0235e-01, -1.2911e+00,  3.1086e-01, -7.9140e-01],\n",
      "        [-1.8516e-01, -3.4991e-01, -1.1843e-01, -2.1773e-01, -2.5309e-01,\n",
      "          5.4401e-01, -3.0858e-01,  3.1552e-01, -1.5325e-01, -1.4606e-01],\n",
      "        [-1.6877e+00,  1.3002e-01, -7.6148e-01, -2.3468e-01,  1.1332e+00,\n",
      "         -6.3397e-01, -8.7273e-01,  3.1259e-01, -6.3524e-01,  9.2978e-01],\n",
      "        [ 6.4514e-01, -1.2640e+00, -1.1334e+00, -6.3981e-01,  1.5615e-01,\n",
      "          3.9151e-01, -4.0670e-01,  4.9122e-02,  5.8816e-03, -1.0953e+00],\n",
      "        [ 1.0070e+00, -1.9266e-01, -5.9808e-01,  2.3961e-01, -4.8247e-01,\n",
      "         -6.1941e-01,  6.2223e-01, -5.0212e-01, -1.0822e+00, -3.1569e-01],\n",
      "        [ 1.9673e-01, -3.7861e-01, -4.7112e-01,  8.0216e-01, -4.4381e-02,\n",
      "          5.8876e-01,  3.3553e-01, -1.0079e-01,  6.7205e-01, -1.6693e-01],\n",
      "        [ 5.8848e-01,  2.6715e-01,  1.3825e-01,  1.9756e-02,  2.8196e-01,\n",
      "          5.8642e-01,  4.0670e-01, -1.3214e+00, -3.7245e-01,  7.2416e-01],\n",
      "        [-7.8366e-01, -4.5275e-01, -7.2834e-01, -3.9217e-01,  3.9981e-01,\n",
      "         -9.4724e-02,  2.7542e-01, -1.2746e-01,  1.6378e-01, -8.9057e-01],\n",
      "        [-3.1134e-01,  4.6752e-01,  1.0680e+00, -1.2899e+00, -4.3959e-01,\n",
      "         -7.2164e-02,  9.9418e-01,  4.6446e-01, -7.4932e-01, -4.1479e-01],\n",
      "        [ 3.1272e-01, -7.5111e-01, -2.8572e-01, -1.1504e-01,  8.7576e-01,\n",
      "          1.0623e+00,  1.1251e+00,  1.1905e-01,  1.3630e+00, -8.3658e-02],\n",
      "        [-4.0225e-01,  1.1729e+00,  3.5400e-01, -5.5417e-02,  5.9983e-01,\n",
      "          1.5972e+00,  7.9618e-01, -4.0100e-01, -2.7082e-01,  4.6096e-02],\n",
      "        [-7.3099e-01, -9.5610e-01, -7.5730e-01, -3.7570e-01,  3.4794e-01,\n",
      "         -5.7851e-01,  2.7238e-01,  1.8362e-01, -1.0494e+00,  3.2244e-01],\n",
      "        [-9.3412e-01, -8.9062e-01, -5.3629e-01,  8.9643e-01, -4.6741e-01,\n",
      "          1.6904e-02, -5.9957e-01,  4.5472e-01, -1.6530e-01,  6.3637e-02],\n",
      "        [ 4.3516e-02,  1.0564e-01, -5.6220e-01,  6.8282e-03, -8.3510e-01,\n",
      "         -2.2918e-01, -2.6001e-02, -1.5663e-01,  3.1732e-01, -7.1626e-01],\n",
      "        [-8.7137e-01, -7.9139e-01,  9.6220e-01,  7.3326e-01, -2.1328e-01,\n",
      "         -4.0147e-01,  2.1557e-01,  1.9857e-01,  7.4419e-01,  1.0962e+00],\n",
      "        [ 4.4814e-02,  1.6529e+00, -5.6662e-01,  3.5594e-01,  2.3535e-01,\n",
      "          6.8635e-02, -1.2676e+00,  5.9586e-01, -7.5106e-01,  1.4693e+00],\n",
      "        [-8.6382e-01, -7.1692e-01,  5.3928e-01,  1.4570e+00, -8.7254e-01,\n",
      "         -7.3755e-02,  1.2788e+00,  7.3837e-02, -1.1397e+00,  4.5333e-01],\n",
      "        [ 7.8548e-02,  5.1273e-01, -1.1309e+00,  9.8513e-01, -2.8108e-01,\n",
      "         -7.1567e-01,  1.0970e+00, -3.9339e-02,  4.7211e-01, -9.3819e-01],\n",
      "        [ 3.5892e-01, -1.2014e-01,  2.8776e-01,  4.2688e-01,  7.5762e-02,\n",
      "          4.5681e-01,  9.3950e-01, -1.4421e-01,  7.6050e-01,  2.2996e-01],\n",
      "        [ 7.0500e-01, -4.3400e-03, -3.2297e-01, -3.0672e-01,  1.4752e-01,\n",
      "          1.3390e+00, -6.9582e-01,  1.2994e+00,  4.1444e-01, -1.2404e-01],\n",
      "        [-5.6820e-01, -1.6334e-01, -3.4506e-01, -8.4108e-01,  2.0370e-02,\n",
      "         -4.6865e-01, -4.7538e-01, -3.0734e-01,  5.2286e-01,  3.5363e-01],\n",
      "        [-1.0662e+00,  7.9720e-01,  2.4121e-01, -3.0594e-01,  5.1985e-01,\n",
      "          1.0297e+00, -6.8403e-01, -1.8192e-01,  3.2059e-01,  1.3824e+00],\n",
      "        [-1.4722e+00, -3.1916e-01,  6.4412e-01, -6.4329e-01, -7.0223e-01,\n",
      "         -7.3666e-01, -6.2497e-02, -1.3228e+00, -5.0376e-01, -6.9964e-01],\n",
      "        [ 5.5366e-01,  3.7579e-01, -2.1571e-02, -1.7635e-01, -6.6505e-01,\n",
      "         -6.6190e-01,  7.3684e-01,  4.8718e-02, -5.2218e-01, -5.1281e-01],\n",
      "        [ 4.1717e-01, -2.4695e-01,  7.3814e-01,  4.6114e-01,  1.5621e-01,\n",
      "         -6.9821e-01, -8.4577e-01,  2.2931e-01,  7.6340e-01, -6.2801e-01],\n",
      "        [-1.1614e+00,  7.6866e-01, -5.4792e-01,  2.1557e-01,  1.3965e-01,\n",
      "          9.1951e-03, -1.1509e+00, -4.3018e-01,  5.0402e-01, -4.3589e-01],\n",
      "        [ 3.0837e-01, -1.2245e+00,  6.4235e-01,  8.6031e-01, -2.3757e-01,\n",
      "         -3.4460e-01,  8.7905e-01,  7.0584e-02,  4.5661e-01,  6.5857e-01],\n",
      "        [-7.9911e-02,  4.5961e-01,  1.1741e-01, -1.2172e+00,  1.0677e-01,\n",
      "          2.1656e-01,  4.1753e-01,  2.0129e-01, -2.0276e-02, -3.7870e-01],\n",
      "        [ 6.6593e-01, -2.4543e-01, -4.6781e-01,  1.2916e-01,  5.5652e-01,\n",
      "          1.7448e-01,  5.2464e-01,  1.5489e-02, -2.1172e-01,  1.5072e-01],\n",
      "        [-9.8711e-02,  9.5400e-03,  7.6911e-01,  6.5071e-01,  1.9717e-01,\n",
      "          7.9824e-01,  9.9014e-01, -2.1583e-02,  5.5488e-01,  3.6726e-02],\n",
      "        [ 5.2984e-01, -1.4537e-01,  6.3111e-01, -1.4882e+00, -2.7394e-01,\n",
      "         -7.2088e-01, -1.5422e-03,  7.0834e-01,  1.3587e-01,  5.3735e-01],\n",
      "        [ 3.2990e-01,  1.9489e-01,  7.4980e-01, -9.8562e-02, -1.1113e+00,\n",
      "         -2.9940e-01, -8.2025e-01, -8.0780e-01,  1.0297e-01,  3.0459e-02],\n",
      "        [-3.3994e-01,  1.4100e+00, -2.8338e-01,  9.5463e-01,  8.3702e-01,\n",
      "          6.3099e-01, -2.7873e-01,  1.9925e-01,  8.6771e-01,  8.1434e-03],\n",
      "        [-2.3938e-01, -3.5149e-01, -2.9402e-01,  5.0278e-01,  1.4438e+00,\n",
      "         -6.3537e-01,  2.8791e-01,  5.8898e-01, -1.1666e+00,  7.0461e-01],\n",
      "        [ 3.4994e-01,  6.5188e-01, -1.2948e+00,  6.1877e-01, -6.1839e-01,\n",
      "          9.3529e-02, -9.9558e-02,  8.2966e-01,  2.7973e-01, -2.7990e-01],\n",
      "        [-2.2345e-01, -4.8536e-01, -1.4071e-01,  1.7597e-02, -5.7828e-01,\n",
      "          7.1828e-01,  7.6849e-01,  1.7453e-01,  2.0611e-01,  9.3416e-01],\n",
      "        [ 1.1883e-01,  7.0712e-01, -4.4755e-01,  1.1755e-01,  3.3536e-01,\n",
      "          6.7805e-01, -7.8873e-02, -3.1074e-02, -4.3448e-01,  8.2712e-01],\n",
      "        [-3.6383e-01,  3.8489e-01, -1.5271e-01,  1.4983e+00, -1.0252e+00,\n",
      "          8.6720e-01, -4.1039e-01, -2.5613e-01,  2.7049e-01,  3.3743e-01],\n",
      "        [-1.0357e+00, -7.8209e-01,  1.0924e-01, -1.0552e-01, -5.3799e-01,\n",
      "          4.3921e-01, -3.7794e-01,  2.8961e-01, -5.5267e-03, -2.8466e-01],\n",
      "        [-4.9585e-01,  6.4436e-01, -7.5372e-01, -4.7821e-02, -7.1031e-02,\n",
      "         -6.2888e-02, -8.1437e-01,  3.4669e-01,  9.0169e-01, -4.2911e-01],\n",
      "        [ 7.4851e-02, -6.4241e-01, -3.2087e-01,  6.4430e-01, -3.5728e-01,\n",
      "         -1.8530e-01, -4.3225e-01, -1.1269e+00,  7.1263e-02,  1.1474e+00],\n",
      "        [ 4.2054e-01, -4.0922e-01, -5.1897e-01, -1.9898e-01, -9.7600e-01,\n",
      "         -7.4190e-01,  3.8752e-01,  7.2410e-01, -3.9792e-01, -9.1802e-01],\n",
      "        [ 6.3791e-01, -2.5877e-01, -5.2981e-01, -8.6449e-02,  9.1356e-01,\n",
      "          3.7639e-01, -1.8685e-01, -4.2278e-01, -3.4500e-01,  8.3246e-01],\n",
      "        [-3.0895e-01, -2.2394e-02, -7.0711e-01, -7.5790e-03, -5.3796e-01,\n",
      "          3.0599e-01,  4.7132e-01,  8.2803e-01, -2.1662e-01,  4.3903e-01],\n",
      "        [ 9.9819e-01,  8.7563e-02, -1.8779e-01,  1.0042e-01, -9.5633e-01,\n",
      "          4.3814e-01, -4.5401e-02,  5.5945e-01, -7.7313e-01,  2.8312e-01],\n",
      "        [-4.2054e-01,  4.7373e-01, -2.2541e-01,  3.3563e-01, -8.1061e-02,\n",
      "          2.9139e-01,  2.1351e-01,  3.5954e-01, -4.3126e-01,  9.8702e-01],\n",
      "        [-4.8727e-01,  6.8784e-01,  7.1089e-01, -2.7722e-01,  7.2975e-02,\n",
      "         -1.5194e-01,  6.7657e-02,  5.0155e-01,  4.6922e-01, -1.1019e+00],\n",
      "        [-7.1405e-01,  3.4042e-01,  5.3507e-01,  3.6981e-01,  5.9501e-02,\n",
      "         -3.1711e-01,  9.2621e-01, -4.8357e-01,  7.3346e-01, -4.9304e-01],\n",
      "        [ 4.3681e-02,  7.0178e-01, -3.7233e-01,  7.7456e-01, -9.9795e-01,\n",
      "          1.1154e-01,  6.2470e-01,  8.2424e-01, -2.2651e+00,  1.1800e+00],\n",
      "        [-1.0095e+00, -5.6923e-01, -8.2195e-01, -2.9217e-01,  6.7381e-01,\n",
      "          1.2824e-01,  7.9093e-02,  1.8972e-02, -1.8431e-01, -3.7194e-01],\n",
      "        [-9.2168e-01, -1.1265e+00, -1.3370e-01, -1.5550e+00, -1.4177e-01,\n",
      "         -6.0017e-02, -1.4388e-01, -6.0294e-01,  8.5115e-01,  5.4835e-01],\n",
      "        [ 2.8913e-02,  1.2781e+00,  2.6779e-01, -2.0157e-01, -5.7228e-01,\n",
      "          5.9954e-01,  1.0043e+00, -5.3547e-01,  5.4710e-01, -5.7789e-01],\n",
      "        [ 1.0295e+00, -1.3019e-01, -8.5549e-01, -9.7308e-01,  8.2318e-02,\n",
      "          9.9055e-01,  3.9396e-01, -3.0843e-01, -8.0015e-01, -7.1233e-01],\n",
      "        [ 1.0124e+00, -1.4768e+00, -1.1208e+00, -3.8972e-01,  8.2873e-01,\n",
      "          6.7547e-01, -4.6367e-01,  1.5770e-01, -1.0914e+00,  9.2830e-01],\n",
      "        [ 5.1512e-01, -1.3865e-02,  4.3135e-01,  8.6338e-02,  6.7199e-01,\n",
      "          5.2855e-01,  2.0813e-01, -4.2224e-01, -4.7361e-03, -4.6322e-01],\n",
      "        [-6.2986e-01,  3.4069e-01, -3.1465e-01,  2.1166e-01,  7.5290e-01,\n",
      "         -5.8630e-02,  8.2919e-01,  1.1105e-01, -2.5901e-01, -3.5786e-01],\n",
      "        [-1.9795e-01, -2.6169e-01,  9.1553e-01, -4.9387e-01,  1.4419e-01,\n",
      "          4.2293e-01, -2.4744e-01, -4.8010e-01, -4.8134e-01, -8.8153e-02],\n",
      "        [ 3.2840e-01, -1.9793e-01,  6.5442e-02,  4.7084e-01, -2.4400e-02,\n",
      "         -5.6256e-01, -1.4060e+00, -1.6387e-01,  3.5366e-01,  1.2556e+00]],\n",
      "       requires_grad=True)\n",
      "torch.Size([64, 10])\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "torch.Size([64])\n",
      "Parameter containing:\n",
      "tensor([[ 0.0237, -0.3428, -0.0050,  ...,  0.1682, -0.1085, -0.0116],\n",
      "        [-0.0813,  0.0453, -0.0099,  ..., -0.0930,  0.0608,  0.2083],\n",
      "        [ 0.1677,  0.2698,  0.2983,  ..., -0.1060,  0.2791, -0.2404],\n",
      "        ...,\n",
      "        [-0.1578, -0.3215, -0.3060,  ...,  0.1402,  0.0472, -0.2587],\n",
      "        [-0.0191,  0.3132,  0.0880,  ..., -0.1786, -0.1143,  0.0669],\n",
      "        [ 0.4330,  0.0881, -0.0992,  ...,  0.4499,  0.0514,  0.2521]],\n",
      "       requires_grad=True)\n",
      "torch.Size([32, 64])\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n",
      "torch.Size([32])\n",
      "Parameter containing:\n",
      "tensor([[ 0.1872,  0.1436,  0.2645,  0.3792,  0.1531,  0.6222,  0.1671,  0.2361,\n",
      "          0.0168,  0.1522, -0.2377, -0.2250, -0.1353, -0.1958, -0.1014, -0.2756,\n",
      "         -0.1450, -0.1964,  0.3030, -0.0168,  0.0673,  0.0850,  0.1246,  0.1874,\n",
      "         -0.0384, -0.1304, -0.0063, -0.1312,  0.3120, -0.0013,  0.3302,  0.2211]],\n",
      "       requires_grad=True)\n",
      "torch.Size([1, 32])\n",
      "Parameter containing:\n",
      "tensor([0.], requires_grad=True)\n",
      "torch.Size([1])\n",
      "Total number of parameters: 2817\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "# Create an MLP with input size of 10, two hidden layers with 64 and 32 neurons,\n",
    "# and an output size of 5.\n",
    "layer_sizes = [10, 64, 32, 1] # use this to see parameters: [2, 3, 2, 1]#\n",
    "model = FlexibleMLP(layer_sizes, scale=2., bias_scale=0.)\n",
    "print(model.parameters())\n",
    "\n",
    "num_param = 0\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "  print(param)\n",
    "  print(param.shape)\n",
    "  num_param += param.numel()\n",
    "\n",
    "print(f'Total number of parameters: {num_param}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "00532aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed_value=42):\n",
    "    \"\"\"\n",
    "    Set random seed across NumPy, Python, and PyTorch to ensure reproducibility.\n",
    "\n",
    "    Args:\n",
    "        seed_value (int): The seed value to use. Default is 42, a commonly used arbitrary number.\n",
    "\n",
    "    This function ensures that experiments produce the same results across different runs,\n",
    "    which is critical for debugging, comparing models, and scientific reproducibility.\n",
    "\n",
    "    It sets the seed for:\n",
    "        - NumPy (used for numerical ops like matrix generation)\n",
    "        - Python's built-in random module (used in random sampling, shuffling, etc.)\n",
    "        - PyTorch (both CPU and GPU)\n",
    "\n",
    "    For GPU reproducibility:\n",
    "        - It manually sets the CUDA seeds (for single and multi-GPU setups)\n",
    "        - It disables the CUDA backend benchmarking feature to ensure deterministic behavior\n",
    "          (at the potential cost of performance).\n",
    "    \"\"\"\n",
    "\n",
    "    # Set seed for NumPy (used in data shuffling, batch generation, etc.)\n",
    "    np.random.seed(seed_value)\n",
    "\n",
    "    # Set seed for PyTorch operations on CPU\n",
    "    torch.manual_seed(seed_value)\n",
    "\n",
    "    # Set seed for Python's built-in random module (e.g., random.shuffle, random.randint)\n",
    "    random.seed(seed_value)\n",
    "\n",
    "    # Set seeds for PyTorch operations on GPU\n",
    "    if torch.cuda.is_available():\n",
    "        # Set seed for single-GPU\n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "\n",
    "        # Set seed for all available GPUs (multi-GPU training)\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "        # Ensures that CUDA uses deterministic algorithms\n",
    "        # This disables non-deterministic optimizations and ensures reproducible behavior\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "        # Disables cuDNN auto-tuner which selects the best algorithm for each configuration\n",
    "        # When disabled, it uses deterministic algorithms, but this might make training slower\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Set the seed globally so every run starts from the same state\n",
    "set_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "60e922ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_lagged_mlp_data(df: pd.DataFrame, symbol: str, max_lag: int = 10, test_size: float = 0.8, seed: int = 42):\n",
    "    \"\"\"\n",
    "    Prepare lagged feature matrix and target vector from stock returns,\n",
    "    and split into PyTorch-ready training and test sets.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): must contain ['SYMBOL', 'DATE', 'TIME', 'RETURN']\n",
    "        symbol (str): asset to process\n",
    "        max_lag (int): number of lagged returns as features\n",
    "        test_size (float): fraction of data to allocate to test set\n",
    "        seed (int): random seed for reproducibility\n",
    "\n",
    "    Returns:\n",
    "        train_loader, test_loader: PyTorch DataLoader objects\n",
    "        input_dim: int, number of input features (should be max_lag)\n",
    "    \"\"\"\n",
    "    group = df[df['SYMBOL'] == symbol].sort_values(by=['DATE', 'TIME']).reset_index(drop=True)\n",
    "    returns = group['RETURN'].values.astype(np.float32)\n",
    "\n",
    "    if len(returns) < max_lag + 1:\n",
    "        raise ValueError(\"Not enough data to build lagged features.\")\n",
    "\n",
    "    # Build lagged features (X) and corresponding target (y)\n",
    "    X = np.column_stack([returns[i:len(returns)-max_lag+i] for i in range(max_lag)])\n",
    "    y = returns[max_lag:]\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=seed, shuffle=False\n",
    "    )\n",
    "    X_scaler = StandardScaler()\n",
    "    X_train_scaled = X_scaler.fit_transform(X_train)\n",
    "    X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "    y_scaler = StandardScaler()\n",
    "    y_train_scaled = y_scaler.fit_transform(y_train.reshape(-1, 1))\n",
    "    y_test_scaled = y_scaler.transform(y_test.reshape(-1, 1))\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "    X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train_scaled, dtype=torch.float32)\n",
    "    y_test_tensor = torch.tensor(y_test_scaled, dtype=torch.float32)\n",
    "\n",
    "    # Wrap in datasets\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "    # DataLoaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader, X.shape[1],y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "71322071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected symbol: SBSW\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "df = pd.read_parquet('/Users/emanueledurante/Desktop/LGMB/lausanne/epfl/MLfinance/High-Frequency-Trading-with-Deep-Learning/data/high_10m.parquet')\n",
    "\n",
    "# Pick a random symbol\n",
    "available_symbols = df['SYMBOL'].unique()\n",
    "symbol = np.random.choice(available_symbols)\n",
    "print(f\"Selected symbol: {symbol}\")\n",
    "\n",
    "# Prepare data\n",
    "input_dim = 10\n",
    "train_loader, test_loader, input_dim, y_test = prepare_lagged_mlp_data(df, symbol, max_lag=input_dim)\n",
    "\n",
    "# Initialize model\n",
    "model = FlexibleMLP([input_dim, 64, 32, 64, 32, 64,32, 1], scale=1.0, bias_scale=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "7ff5cafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)  # Fixing the seed\n",
    "criterion = nn.MSELoss()  # For prediction, MSE is the standard objective; but other, custom objective might be better;\n",
    "# choose loss appropriate for your task\n",
    "# experiment with learning rates, lr = 0.02, 0.01, 0.001\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=0.001) # this is one of the most popular gradient descent algorithms\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1) # experiment with 0.1, 0.2, 0.5. 0.5 is super interesting, achives well OOS!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "2cee9e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10000] Avg train loss: 0.908620\n",
      "Epoch [2/10000] Avg train loss: 0.906962\n",
      "Epoch [3/10000] Avg train loss: 0.910506\n",
      "Epoch [4/10000] Avg train loss: 0.912874\n",
      "Epoch [5/10000] Avg train loss: 0.939536\n",
      "Epoch [6/10000] Avg train loss: 0.929902\n",
      "Epoch [7/10000] Avg train loss: 0.924841\n",
      "Epoch [8/10000] Avg train loss: 0.924289\n",
      "Epoch [9/10000] Avg train loss: 0.934963\n",
      "Epoch [10/10000] Avg train loss: 0.927248\n",
      "Epoch [11/10000] Avg train loss: 0.937773\n",
      "Epoch [12/10000] Avg train loss: 0.929730\n",
      "Epoch [13/10000] Avg train loss: 0.923829\n",
      "Epoch [14/10000] Avg train loss: 0.933103\n",
      "Epoch [15/10000] Avg train loss: 0.921161\n",
      "Epoch [16/10000] Avg train loss: 0.914359\n",
      "Epoch [17/10000] Avg train loss: 0.920925\n",
      "Epoch [18/10000] Avg train loss: 0.920369\n",
      "Epoch [19/10000] Avg train loss: 0.917383\n",
      "Epoch [20/10000] Avg train loss: 0.909801\n",
      "Epoch [21/10000] Avg train loss: 0.899535\n",
      "Epoch [22/10000] Avg train loss: 0.886074\n",
      "Epoch [23/10000] Avg train loss: 0.887189\n",
      "Epoch [24/10000] Avg train loss: 0.878237\n",
      "Epoch [25/10000] Avg train loss: 0.866405\n",
      "Epoch [26/10000] Avg train loss: 0.859377\n",
      "Epoch [27/10000] Avg train loss: 0.859132\n",
      "Epoch [28/10000] Avg train loss: 0.860519\n",
      "Epoch [29/10000] Avg train loss: 0.851378\n",
      "Epoch [30/10000] Avg train loss: 0.841373\n",
      "Epoch [31/10000] Avg train loss: 0.829783\n",
      "Epoch [32/10000] Avg train loss: 0.817624\n",
      "Epoch [33/10000] Avg train loss: 0.806660\n",
      "Epoch [34/10000] Avg train loss: 0.795568\n",
      "Epoch [35/10000] Avg train loss: 0.785279\n",
      "Epoch [36/10000] Avg train loss: 0.775700\n",
      "Epoch [37/10000] Avg train loss: 0.773154\n",
      "Epoch [38/10000] Avg train loss: 0.764538\n",
      "Epoch [39/10000] Avg train loss: 0.755857\n",
      "Epoch [40/10000] Avg train loss: 0.748252\n",
      "Epoch [41/10000] Avg train loss: 0.743464\n",
      "Epoch [42/10000] Avg train loss: 0.734538\n",
      "Epoch [43/10000] Avg train loss: 0.725962\n",
      "Epoch [44/10000] Avg train loss: 0.717568\n",
      "Epoch [45/10000] Avg train loss: 0.707924\n",
      "Epoch [46/10000] Avg train loss: 0.699932\n",
      "Epoch [47/10000] Avg train loss: 0.698352\n",
      "Epoch [48/10000] Avg train loss: 0.694027\n",
      "Epoch [49/10000] Avg train loss: 0.685963\n",
      "Epoch [50/10000] Avg train loss: 0.678084\n",
      "Epoch [51/10000] Avg train loss: 0.669613\n",
      "Epoch [52/10000] Avg train loss: 0.667659\n",
      "Epoch [53/10000] Avg train loss: 0.663995\n",
      "Epoch [54/10000] Avg train loss: 0.656554\n",
      "Epoch [55/10000] Avg train loss: 0.648436\n",
      "Epoch [56/10000] Avg train loss: 0.640279\n",
      "Epoch [57/10000] Avg train loss: 0.632579\n",
      "Epoch [58/10000] Avg train loss: 0.624937\n",
      "Epoch [59/10000] Avg train loss: 0.618105\n",
      "Epoch [60/10000] Avg train loss: 0.611850\n",
      "Epoch [61/10000] Avg train loss: 0.605021\n",
      "Epoch [62/10000] Avg train loss: 0.598281\n",
      "Epoch [63/10000] Avg train loss: 0.595063\n",
      "Epoch [64/10000] Avg train loss: 0.589270\n",
      "Epoch [65/10000] Avg train loss: 0.582415\n",
      "Epoch [66/10000] Avg train loss: 0.576059\n",
      "Epoch [67/10000] Avg train loss: 0.569950\n",
      "Epoch [68/10000] Avg train loss: 0.563428\n",
      "Epoch [69/10000] Avg train loss: 0.557922\n",
      "Epoch [70/10000] Avg train loss: 0.552087\n",
      "Epoch [71/10000] Avg train loss: 0.546286\n",
      "Epoch [72/10000] Avg train loss: 0.540838\n",
      "Epoch [73/10000] Avg train loss: 0.535325\n",
      "Epoch [74/10000] Avg train loss: 0.530675\n",
      "Epoch [75/10000] Avg train loss: 0.525515\n",
      "Epoch [76/10000] Avg train loss: 0.520561\n",
      "Epoch [77/10000] Avg train loss: 0.515188\n",
      "Epoch [78/10000] Avg train loss: 0.510097\n",
      "Epoch [79/10000] Avg train loss: 0.505049\n",
      "Epoch [80/10000] Avg train loss: 0.500179\n",
      "Epoch [81/10000] Avg train loss: 0.495069\n",
      "Epoch [82/10000] Avg train loss: 0.490169\n",
      "Epoch [83/10000] Avg train loss: 0.485323\n",
      "Epoch [84/10000] Avg train loss: 0.481158\n",
      "Epoch [85/10000] Avg train loss: 0.476621\n",
      "Epoch [86/10000] Avg train loss: 0.472174\n",
      "Epoch [87/10000] Avg train loss: 0.467727\n",
      "Epoch [88/10000] Avg train loss: 0.463273\n",
      "Epoch [89/10000] Avg train loss: 0.459447\n",
      "Epoch [90/10000] Avg train loss: 0.455886\n",
      "Epoch [91/10000] Avg train loss: 0.451796\n",
      "Epoch [92/10000] Avg train loss: 0.447720\n",
      "Epoch [93/10000] Avg train loss: 0.443802\n",
      "Epoch [94/10000] Avg train loss: 0.439879\n",
      "Epoch [95/10000] Avg train loss: 0.436186\n",
      "Epoch [96/10000] Avg train loss: 0.432545\n",
      "Epoch [97/10000] Avg train loss: 0.430064\n",
      "Epoch [98/10000] Avg train loss: 0.426769\n",
      "Epoch [99/10000] Avg train loss: 0.423478\n",
      "Epoch [100/10000] Avg train loss: 0.419928\n",
      "Epoch [101/10000] Avg train loss: 0.416476\n",
      "Epoch [102/10000] Avg train loss: 0.414006\n",
      "Epoch [103/10000] Avg train loss: 0.411753\n",
      "Epoch [104/10000] Avg train loss: 0.409001\n",
      "Epoch [105/10000] Avg train loss: 0.405977\n",
      "Epoch [106/10000] Avg train loss: 0.402764\n",
      "Epoch [107/10000] Avg train loss: 0.399606\n",
      "Epoch [108/10000] Avg train loss: 0.396533\n",
      "Epoch [109/10000] Avg train loss: 0.393525\n",
      "Epoch [110/10000] Avg train loss: 0.390696\n",
      "Epoch [111/10000] Avg train loss: 0.387748\n",
      "Epoch [112/10000] Avg train loss: 0.384773\n",
      "Epoch [113/10000] Avg train loss: 0.381846\n",
      "Epoch [114/10000] Avg train loss: 0.379122\n",
      "Epoch [115/10000] Avg train loss: 0.376389\n",
      "Epoch [116/10000] Avg train loss: 0.373517\n",
      "Epoch [117/10000] Avg train loss: 0.370741\n",
      "Epoch [118/10000] Avg train loss: 0.368332\n",
      "Epoch [119/10000] Avg train loss: 0.365809\n",
      "Epoch [120/10000] Avg train loss: 0.363491\n",
      "Epoch [121/10000] Avg train loss: 0.361005\n",
      "Epoch [122/10000] Avg train loss: 0.358499\n",
      "Epoch [123/10000] Avg train loss: 0.356067\n",
      "Epoch [124/10000] Avg train loss: 0.353594\n",
      "Epoch [125/10000] Avg train loss: 0.351132\n",
      "Epoch [126/10000] Avg train loss: 0.348823\n",
      "Epoch [127/10000] Avg train loss: 0.346607\n",
      "Epoch [128/10000] Avg train loss: 0.344383\n",
      "Epoch [129/10000] Avg train loss: 0.342134\n",
      "Epoch [130/10000] Avg train loss: 0.339839\n",
      "Epoch [131/10000] Avg train loss: 0.337535\n",
      "Epoch [132/10000] Avg train loss: 0.335253\n",
      "Epoch [133/10000] Avg train loss: 0.333024\n",
      "Epoch [134/10000] Avg train loss: 0.330775\n",
      "Epoch [135/10000] Avg train loss: 0.328542\n",
      "Epoch [136/10000] Avg train loss: 0.326446\n",
      "Epoch [137/10000] Avg train loss: 0.324413\n",
      "Epoch [138/10000] Avg train loss: 0.322366\n",
      "Epoch [139/10000] Avg train loss: 0.320389\n",
      "Epoch [140/10000] Avg train loss: 0.318330\n",
      "Epoch [141/10000] Avg train loss: 0.316301\n",
      "Epoch [142/10000] Avg train loss: 0.314360\n",
      "Epoch [143/10000] Avg train loss: 0.312416\n",
      "Epoch [144/10000] Avg train loss: 0.310418\n",
      "Epoch [145/10000] Avg train loss: 0.308435\n",
      "Epoch [146/10000] Avg train loss: 0.306515\n",
      "Epoch [147/10000] Avg train loss: 0.304631\n",
      "Epoch [148/10000] Avg train loss: 0.302748\n",
      "Epoch [149/10000] Avg train loss: 0.300864\n",
      "Epoch [150/10000] Avg train loss: 0.298993\n",
      "Epoch [151/10000] Avg train loss: 0.297209\n",
      "Epoch [152/10000] Avg train loss: 0.295400\n",
      "Epoch [153/10000] Avg train loss: 0.293595\n",
      "Epoch [154/10000] Avg train loss: 0.291822\n",
      "Epoch [155/10000] Avg train loss: 0.290178\n",
      "Epoch [156/10000] Avg train loss: 0.288611\n",
      "Epoch [157/10000] Avg train loss: 0.287134\n",
      "Epoch [158/10000] Avg train loss: 0.285799\n",
      "Epoch [159/10000] Avg train loss: 0.284172\n",
      "Epoch [160/10000] Avg train loss: 0.282564\n",
      "Epoch [161/10000] Avg train loss: 0.281106\n",
      "Epoch [162/10000] Avg train loss: 0.279782\n",
      "Epoch [163/10000] Avg train loss: 0.278284\n",
      "Epoch [164/10000] Avg train loss: 0.276926\n",
      "Epoch [165/10000] Avg train loss: 0.275424\n",
      "Epoch [166/10000] Avg train loss: 0.273933\n",
      "Epoch [167/10000] Avg train loss: 0.272415\n",
      "Epoch [168/10000] Avg train loss: 0.270908\n",
      "Epoch [169/10000] Avg train loss: 0.269395\n",
      "Epoch [170/10000] Avg train loss: 0.268028\n",
      "Epoch [171/10000] Avg train loss: 0.266739\n",
      "Epoch [172/10000] Avg train loss: 0.265326\n",
      "Epoch [173/10000] Avg train loss: 0.263895\n",
      "Epoch [174/10000] Avg train loss: 0.262515\n",
      "Epoch [175/10000] Avg train loss: 0.261202\n",
      "Epoch [176/10000] Avg train loss: 0.259827\n",
      "Epoch [177/10000] Avg train loss: 0.258534\n",
      "Epoch [178/10000] Avg train loss: 0.257173\n",
      "Epoch [179/10000] Avg train loss: 0.255857\n",
      "Epoch [180/10000] Avg train loss: 0.254518\n",
      "Epoch [181/10000] Avg train loss: 0.253196\n",
      "Epoch [182/10000] Avg train loss: 0.251865\n",
      "Epoch [183/10000] Avg train loss: 0.250585\n",
      "Epoch [184/10000] Avg train loss: 0.249276\n",
      "Epoch [185/10000] Avg train loss: 0.247971\n",
      "Epoch [186/10000] Avg train loss: 0.246691\n",
      "Epoch [187/10000] Avg train loss: 0.245447\n",
      "Epoch [188/10000] Avg train loss: 0.244175\n",
      "Epoch [189/10000] Avg train loss: 0.242919\n",
      "Epoch [190/10000] Avg train loss: 0.241708\n",
      "Epoch [191/10000] Avg train loss: 0.240587\n",
      "Epoch [192/10000] Avg train loss: 0.239372\n",
      "Epoch [193/10000] Avg train loss: 0.238163\n",
      "Epoch [194/10000] Avg train loss: 0.236961\n",
      "Epoch [195/10000] Avg train loss: 0.235804\n",
      "Epoch [196/10000] Avg train loss: 0.235202\n",
      "Epoch [197/10000] Avg train loss: 0.234150\n",
      "Epoch [198/10000] Avg train loss: 0.233076\n",
      "Epoch [199/10000] Avg train loss: 0.231962\n",
      "Epoch [200/10000] Avg train loss: 0.230843\n",
      "Epoch [201/10000] Avg train loss: 0.229732\n",
      "Epoch [202/10000] Avg train loss: 0.228620\n",
      "Epoch [203/10000] Avg train loss: 0.227517\n",
      "Epoch [204/10000] Avg train loss: 0.226420\n",
      "Epoch [205/10000] Avg train loss: 0.225346\n",
      "Epoch [206/10000] Avg train loss: 0.224288\n",
      "Epoch [207/10000] Avg train loss: 0.223237\n",
      "Epoch [208/10000] Avg train loss: 0.222185\n",
      "Epoch [209/10000] Avg train loss: 0.221140\n",
      "Epoch [210/10000] Avg train loss: 0.220108\n",
      "Epoch [211/10000] Avg train loss: 0.219084\n",
      "Epoch [212/10000] Avg train loss: 0.218065\n",
      "Epoch [213/10000] Avg train loss: 0.217069\n",
      "Epoch [214/10000] Avg train loss: 0.216117\n",
      "Epoch [215/10000] Avg train loss: 0.215130\n",
      "Epoch [216/10000] Avg train loss: 0.214144\n",
      "Epoch [217/10000] Avg train loss: 0.213169\n",
      "Epoch [218/10000] Avg train loss: 0.212203\n",
      "Epoch [219/10000] Avg train loss: 0.211248\n",
      "Epoch [220/10000] Avg train loss: 0.210305\n",
      "Epoch [221/10000] Avg train loss: 0.209364\n",
      "Epoch [222/10000] Avg train loss: 0.208429\n",
      "Epoch [223/10000] Avg train loss: 0.207503\n",
      "Epoch [224/10000] Avg train loss: 0.206585\n",
      "Epoch [225/10000] Avg train loss: 0.205676\n",
      "Epoch [226/10000] Avg train loss: 0.204785\n",
      "Epoch [227/10000] Avg train loss: 0.203894\n",
      "Epoch [228/10000] Avg train loss: 0.203008\n",
      "Epoch [229/10000] Avg train loss: 0.202130\n",
      "Epoch [230/10000] Avg train loss: 0.201260\n",
      "Epoch [231/10000] Avg train loss: 0.200397\n",
      "Epoch [232/10000] Avg train loss: 0.199538\n",
      "Epoch [233/10000] Avg train loss: 0.198692\n",
      "Epoch [234/10000] Avg train loss: 0.197850\n",
      "Epoch [235/10000] Avg train loss: 0.197012\n",
      "Epoch [236/10000] Avg train loss: 0.196181\n",
      "Epoch [237/10000] Avg train loss: 0.195359\n",
      "Epoch [238/10000] Avg train loss: 0.194543\n",
      "Epoch [239/10000] Avg train loss: 0.193734\n",
      "Epoch [240/10000] Avg train loss: 0.192932\n",
      "Epoch [241/10000] Avg train loss: 0.192142\n",
      "Epoch [242/10000] Avg train loss: 0.191359\n",
      "Epoch [243/10000] Avg train loss: 0.190579\n",
      "Epoch [244/10000] Avg train loss: 0.189802\n",
      "Epoch [245/10000] Avg train loss: 0.189033\n",
      "Epoch [246/10000] Avg train loss: 0.188270\n",
      "Epoch [247/10000] Avg train loss: 0.187512\n",
      "Epoch [248/10000] Avg train loss: 0.186759\n",
      "Epoch [249/10000] Avg train loss: 0.186017\n",
      "Epoch [250/10000] Avg train loss: 0.185276\n",
      "Epoch [251/10000] Avg train loss: 0.184541\n",
      "Epoch [252/10000] Avg train loss: 0.183811\n",
      "Epoch [253/10000] Avg train loss: 0.183089\n",
      "Epoch [254/10000] Avg train loss: 0.182371\n",
      "Epoch [255/10000] Avg train loss: 0.181660\n",
      "Epoch [256/10000] Avg train loss: 0.180959\n",
      "Epoch [257/10000] Avg train loss: 0.180262\n",
      "Epoch [258/10000] Avg train loss: 0.179567\n",
      "Epoch [259/10000] Avg train loss: 0.178882\n",
      "Epoch [260/10000] Avg train loss: 0.178199\n",
      "Epoch [261/10000] Avg train loss: 0.177534\n",
      "Epoch [262/10000] Avg train loss: 0.176862\n",
      "Epoch [263/10000] Avg train loss: 0.176200\n",
      "Epoch [264/10000] Avg train loss: 0.175536\n",
      "Epoch [265/10000] Avg train loss: 0.174877\n",
      "Epoch [266/10000] Avg train loss: 0.174222\n",
      "Epoch [267/10000] Avg train loss: 0.173572\n",
      "Epoch [268/10000] Avg train loss: 0.172926\n",
      "Epoch [269/10000] Avg train loss: 0.172286\n",
      "Epoch [270/10000] Avg train loss: 0.171649\n",
      "Epoch [271/10000] Avg train loss: 0.171018\n",
      "Epoch [272/10000] Avg train loss: 0.170390\n",
      "Epoch [273/10000] Avg train loss: 0.169768\n",
      "Epoch [274/10000] Avg train loss: 0.169150\n",
      "Epoch [275/10000] Avg train loss: 0.168536\n",
      "Epoch [276/10000] Avg train loss: 0.167927\n",
      "Epoch [277/10000] Avg train loss: 0.167322\n",
      "Epoch [278/10000] Avg train loss: 0.166722\n",
      "Epoch [279/10000] Avg train loss: 0.166125\n",
      "Epoch [280/10000] Avg train loss: 0.165533\n",
      "Epoch [281/10000] Avg train loss: 0.164945\n",
      "Epoch [282/10000] Avg train loss: 0.164362\n",
      "Epoch [283/10000] Avg train loss: 0.163784\n",
      "Epoch [284/10000] Avg train loss: 0.163209\n",
      "Epoch [285/10000] Avg train loss: 0.162638\n",
      "Epoch [286/10000] Avg train loss: 0.162071\n",
      "Epoch [287/10000] Avg train loss: 0.161508\n",
      "Epoch [288/10000] Avg train loss: 0.160952\n",
      "Epoch [289/10000] Avg train loss: 0.160400\n",
      "Epoch [290/10000] Avg train loss: 0.159859\n",
      "Epoch [291/10000] Avg train loss: 0.159320\n",
      "Epoch [292/10000] Avg train loss: 0.158780\n",
      "Epoch [293/10000] Avg train loss: 0.158243\n",
      "Epoch [294/10000] Avg train loss: 0.157708\n",
      "Epoch [295/10000] Avg train loss: 0.157175\n",
      "Epoch [296/10000] Avg train loss: 0.156644\n",
      "Epoch [297/10000] Avg train loss: 0.156118\n",
      "Epoch [298/10000] Avg train loss: 0.155595\n",
      "Epoch [299/10000] Avg train loss: 0.155075\n",
      "Epoch [300/10000] Avg train loss: 0.154559\n",
      "Epoch [301/10000] Avg train loss: 0.154046\n",
      "Epoch [302/10000] Avg train loss: 0.153537\n",
      "Epoch [303/10000] Avg train loss: 0.153031\n",
      "Epoch [304/10000] Avg train loss: 0.152528\n",
      "Epoch [305/10000] Avg train loss: 0.152028\n",
      "Epoch [306/10000] Avg train loss: 0.151532\n",
      "Epoch [307/10000] Avg train loss: 0.151039\n",
      "Epoch [308/10000] Avg train loss: 0.150549\n",
      "Epoch [309/10000] Avg train loss: 0.150063\n",
      "Epoch [310/10000] Avg train loss: 0.149579\n",
      "Epoch [311/10000] Avg train loss: 0.149098\n",
      "Epoch [312/10000] Avg train loss: 0.148621\n",
      "Epoch [313/10000] Avg train loss: 0.148147\n",
      "Epoch [314/10000] Avg train loss: 0.147675\n",
      "Epoch [315/10000] Avg train loss: 0.147207\n",
      "Epoch [316/10000] Avg train loss: 0.146742\n",
      "Epoch [317/10000] Avg train loss: 0.146279\n",
      "Epoch [318/10000] Avg train loss: 0.145820\n",
      "Epoch [319/10000] Avg train loss: 0.145363\n",
      "Epoch [320/10000] Avg train loss: 0.144911\n",
      "Epoch [321/10000] Avg train loss: 0.144460\n",
      "Epoch [322/10000] Avg train loss: 0.144012\n",
      "Epoch [323/10000] Avg train loss: 0.143566\n",
      "Epoch [324/10000] Avg train loss: 0.143123\n",
      "Epoch [325/10000] Avg train loss: 0.142683\n",
      "Epoch [326/10000] Avg train loss: 0.142246\n",
      "Epoch [327/10000] Avg train loss: 0.141811\n",
      "Epoch [328/10000] Avg train loss: 0.141379\n",
      "Epoch [329/10000] Avg train loss: 0.140950\n",
      "Epoch [330/10000] Avg train loss: 0.140523\n",
      "Epoch [331/10000] Avg train loss: 0.140099\n",
      "Epoch [332/10000] Avg train loss: 0.139677\n",
      "Epoch [333/10000] Avg train loss: 0.139258\n",
      "Epoch [334/10000] Avg train loss: 0.138841\n",
      "Epoch [335/10000] Avg train loss: 0.138427\n",
      "Epoch [336/10000] Avg train loss: 0.138016\n",
      "Epoch [337/10000] Avg train loss: 0.137606\n",
      "Epoch [338/10000] Avg train loss: 0.137200\n",
      "Epoch [339/10000] Avg train loss: 0.136795\n",
      "Epoch [340/10000] Avg train loss: 0.136393\n",
      "Epoch [341/10000] Avg train loss: 0.135994\n",
      "Epoch [342/10000] Avg train loss: 0.135598\n",
      "Epoch [343/10000] Avg train loss: 0.135203\n",
      "Epoch [344/10000] Avg train loss: 0.134810\n",
      "Epoch [345/10000] Avg train loss: 0.134419\n",
      "Epoch [346/10000] Avg train loss: 0.134031\n",
      "Epoch [347/10000] Avg train loss: 0.133645\n",
      "Epoch [348/10000] Avg train loss: 0.133262\n",
      "Epoch [349/10000] Avg train loss: 0.132880\n",
      "Epoch [350/10000] Avg train loss: 0.132501\n",
      "Epoch [351/10000] Avg train loss: 0.132124\n",
      "Epoch [352/10000] Avg train loss: 0.131749\n",
      "Epoch [353/10000] Avg train loss: 0.131376\n",
      "Epoch [354/10000] Avg train loss: 0.131005\n",
      "Epoch [355/10000] Avg train loss: 0.130637\n",
      "Epoch [356/10000] Avg train loss: 0.130270\n",
      "Epoch [357/10000] Avg train loss: 0.129906\n",
      "Epoch [358/10000] Avg train loss: 0.129543\n",
      "Epoch [359/10000] Avg train loss: 0.129182\n",
      "Epoch [360/10000] Avg train loss: 0.128824\n",
      "Epoch [361/10000] Avg train loss: 0.128467\n",
      "Epoch [362/10000] Avg train loss: 0.128112\n",
      "Epoch [363/10000] Avg train loss: 0.127760\n",
      "Epoch [364/10000] Avg train loss: 0.127409\n",
      "Epoch [365/10000] Avg train loss: 0.127060\n",
      "Epoch [366/10000] Avg train loss: 0.126713\n",
      "Epoch [367/10000] Avg train loss: 0.126368\n",
      "Epoch [368/10000] Avg train loss: 0.126025\n",
      "Epoch [369/10000] Avg train loss: 0.125683\n",
      "Epoch [370/10000] Avg train loss: 0.125344\n",
      "Epoch [371/10000] Avg train loss: 0.125006\n",
      "Epoch [372/10000] Avg train loss: 0.124671\n",
      "Epoch [373/10000] Avg train loss: 0.124337\n",
      "Epoch [374/10000] Avg train loss: 0.124005\n",
      "Epoch [375/10000] Avg train loss: 0.123674\n",
      "Epoch [376/10000] Avg train loss: 0.123346\n",
      "Epoch [377/10000] Avg train loss: 0.123019\n",
      "Epoch [378/10000] Avg train loss: 0.122693\n",
      "Epoch [379/10000] Avg train loss: 0.122370\n",
      "Epoch [380/10000] Avg train loss: 0.122048\n",
      "Epoch [381/10000] Avg train loss: 0.121728\n",
      "Epoch [382/10000] Avg train loss: 0.121409\n",
      "Epoch [383/10000] Avg train loss: 0.121092\n",
      "Epoch [384/10000] Avg train loss: 0.120777\n",
      "Epoch [385/10000] Avg train loss: 0.120464\n",
      "Epoch [386/10000] Avg train loss: 0.120152\n",
      "Epoch [387/10000] Avg train loss: 0.119841\n",
      "Epoch [388/10000] Avg train loss: 0.119533\n",
      "Epoch [389/10000] Avg train loss: 0.119226\n",
      "Epoch [390/10000] Avg train loss: 0.118920\n",
      "Epoch [391/10000] Avg train loss: 0.118616\n",
      "Epoch [392/10000] Avg train loss: 0.118314\n",
      "Epoch [393/10000] Avg train loss: 0.118013\n",
      "Epoch [394/10000] Avg train loss: 0.117713\n",
      "Epoch [395/10000] Avg train loss: 0.117416\n",
      "Epoch [396/10000] Avg train loss: 0.117119\n",
      "Epoch [397/10000] Avg train loss: 0.116824\n",
      "Epoch [398/10000] Avg train loss: 0.116531\n",
      "Epoch [399/10000] Avg train loss: 0.116239\n",
      "Epoch [400/10000] Avg train loss: 0.115949\n",
      "Epoch [401/10000] Avg train loss: 0.115659\n",
      "Epoch [402/10000] Avg train loss: 0.115372\n",
      "Epoch [403/10000] Avg train loss: 0.115086\n",
      "Epoch [404/10000] Avg train loss: 0.114801\n",
      "Epoch [405/10000] Avg train loss: 0.114517\n",
      "Epoch [406/10000] Avg train loss: 0.114235\n",
      "Epoch [407/10000] Avg train loss: 0.113955\n",
      "Epoch [408/10000] Avg train loss: 0.113676\n",
      "Epoch [409/10000] Avg train loss: 0.113398\n",
      "Epoch [410/10000] Avg train loss: 0.113121\n",
      "Epoch [411/10000] Avg train loss: 0.112846\n",
      "Epoch [412/10000] Avg train loss: 0.112572\n",
      "Epoch [413/10000] Avg train loss: 0.112300\n",
      "Epoch [414/10000] Avg train loss: 0.112028\n",
      "Epoch [415/10000] Avg train loss: 0.111759\n",
      "Epoch [416/10000] Avg train loss: 0.111490\n",
      "Epoch [417/10000] Avg train loss: 0.111223\n",
      "Epoch [418/10000] Avg train loss: 0.110957\n",
      "Epoch [419/10000] Avg train loss: 0.110692\n",
      "Epoch [420/10000] Avg train loss: 0.110428\n",
      "Epoch [421/10000] Avg train loss: 0.110166\n",
      "Epoch [422/10000] Avg train loss: 0.109905\n",
      "Epoch [423/10000] Avg train loss: 0.109645\n",
      "Epoch [424/10000] Avg train loss: 0.109387\n",
      "Epoch [425/10000] Avg train loss: 0.109129\n",
      "Epoch [426/10000] Avg train loss: 0.108873\n",
      "Epoch [427/10000] Avg train loss: 0.108618\n",
      "Epoch [428/10000] Avg train loss: 0.108364\n",
      "Epoch [429/10000] Avg train loss: 0.108112\n",
      "Epoch [430/10000] Avg train loss: 0.107861\n",
      "Epoch [431/10000] Avg train loss: 0.107610\n",
      "Epoch [432/10000] Avg train loss: 0.107361\n",
      "Epoch [433/10000] Avg train loss: 0.107113\n",
      "Epoch [434/10000] Avg train loss: 0.106867\n",
      "Epoch [435/10000] Avg train loss: 0.106621\n",
      "Epoch [436/10000] Avg train loss: 0.106377\n",
      "Epoch [437/10000] Avg train loss: 0.106133\n",
      "Epoch [438/10000] Avg train loss: 0.105891\n",
      "Epoch [439/10000] Avg train loss: 0.105650\n",
      "Epoch [440/10000] Avg train loss: 0.105410\n",
      "Epoch [441/10000] Avg train loss: 0.105171\n",
      "Epoch [442/10000] Avg train loss: 0.104933\n",
      "Epoch [443/10000] Avg train loss: 0.104696\n",
      "Epoch [444/10000] Avg train loss: 0.104461\n",
      "Epoch [445/10000] Avg train loss: 0.104226\n",
      "Epoch [446/10000] Avg train loss: 0.103992\n",
      "Epoch [447/10000] Avg train loss: 0.103760\n",
      "Epoch [448/10000] Avg train loss: 0.103528\n",
      "Epoch [449/10000] Avg train loss: 0.103298\n",
      "Epoch [450/10000] Avg train loss: 0.103068\n",
      "Epoch [451/10000] Avg train loss: 0.102839\n",
      "Epoch [452/10000] Avg train loss: 0.102612\n",
      "Epoch [453/10000] Avg train loss: 0.102385\n",
      "Epoch [454/10000] Avg train loss: 0.102160\n",
      "Epoch [455/10000] Avg train loss: 0.101935\n",
      "Epoch [456/10000] Avg train loss: 0.101712\n",
      "Epoch [457/10000] Avg train loss: 0.101489\n",
      "Epoch [458/10000] Avg train loss: 0.101268\n",
      "Epoch [459/10000] Avg train loss: 0.101047\n",
      "Epoch [460/10000] Avg train loss: 0.100828\n",
      "Epoch [461/10000] Avg train loss: 0.100609\n",
      "Epoch [462/10000] Avg train loss: 0.100391\n",
      "Epoch [463/10000] Avg train loss: 0.100174\n",
      "Epoch [464/10000] Avg train loss: 0.099958\n",
      "Epoch [465/10000] Avg train loss: 0.099743\n",
      "Epoch [466/10000] Avg train loss: 0.099529\n",
      "Epoch [467/10000] Avg train loss: 0.099316\n",
      "Epoch [468/10000] Avg train loss: 0.099104\n",
      "Epoch [469/10000] Avg train loss: 0.098893\n",
      "Epoch [470/10000] Avg train loss: 0.098682\n",
      "Epoch [471/10000] Avg train loss: 0.098473\n",
      "Epoch [472/10000] Avg train loss: 0.098264\n",
      "Epoch [473/10000] Avg train loss: 0.098057\n",
      "Epoch [474/10000] Avg train loss: 0.097850\n",
      "Epoch [475/10000] Avg train loss: 0.097644\n",
      "Epoch [476/10000] Avg train loss: 0.097439\n",
      "Epoch [477/10000] Avg train loss: 0.097234\n",
      "Epoch [478/10000] Avg train loss: 0.097031\n",
      "Epoch [479/10000] Avg train loss: 0.096828\n",
      "Epoch [480/10000] Avg train loss: 0.096627\n",
      "Epoch [481/10000] Avg train loss: 0.096426\n",
      "Epoch [482/10000] Avg train loss: 0.096226\n",
      "Epoch [483/10000] Avg train loss: 0.096027\n",
      "Epoch [484/10000] Avg train loss: 0.095828\n",
      "Epoch [485/10000] Avg train loss: 0.095631\n",
      "Epoch [486/10000] Avg train loss: 0.095434\n",
      "Epoch [487/10000] Avg train loss: 0.095238\n",
      "Epoch [488/10000] Avg train loss: 0.095043\n",
      "Epoch [489/10000] Avg train loss: 0.094848\n",
      "Epoch [490/10000] Avg train loss: 0.094655\n",
      "Epoch [491/10000] Avg train loss: 0.094462\n",
      "Epoch [492/10000] Avg train loss: 0.094270\n",
      "Epoch [493/10000] Avg train loss: 0.094079\n",
      "Epoch [494/10000] Avg train loss: 0.093889\n",
      "Epoch [495/10000] Avg train loss: 0.093699\n",
      "Epoch [496/10000] Avg train loss: 0.093510\n",
      "Epoch [497/10000] Avg train loss: 0.093322\n",
      "Epoch [498/10000] Avg train loss: 0.093134\n",
      "Epoch [499/10000] Avg train loss: 0.092948\n",
      "Epoch [500/10000] Avg train loss: 0.092762\n",
      "Epoch [501/10000] Avg train loss: 0.092577\n",
      "Epoch [502/10000] Avg train loss: 0.092392\n",
      "Epoch [503/10000] Avg train loss: 0.092209\n",
      "Epoch [504/10000] Avg train loss: 0.092026\n",
      "Epoch [505/10000] Avg train loss: 0.091844\n",
      "Epoch [506/10000] Avg train loss: 0.091662\n",
      "Epoch [507/10000] Avg train loss: 0.091481\n",
      "Epoch [508/10000] Avg train loss: 0.091301\n",
      "Epoch [509/10000] Avg train loss: 0.091122\n",
      "Epoch [510/10000] Avg train loss: 0.090943\n",
      "Epoch [511/10000] Avg train loss: 0.090765\n",
      "Epoch [512/10000] Avg train loss: 0.090588\n",
      "Epoch [513/10000] Avg train loss: 0.090411\n",
      "Epoch [514/10000] Avg train loss: 0.090235\n",
      "Epoch [515/10000] Avg train loss: 0.090060\n",
      "Epoch [516/10000] Avg train loss: 0.089886\n",
      "Epoch [517/10000] Avg train loss: 0.089712\n",
      "Epoch [518/10000] Avg train loss: 0.089539\n",
      "Epoch [519/10000] Avg train loss: 0.089366\n",
      "Epoch [520/10000] Avg train loss: 0.089194\n",
      "Epoch [521/10000] Avg train loss: 0.089023\n",
      "Epoch [522/10000] Avg train loss: 0.088853\n",
      "Epoch [523/10000] Avg train loss: 0.088683\n",
      "Epoch [524/10000] Avg train loss: 0.088513\n",
      "Epoch [525/10000] Avg train loss: 0.088345\n",
      "Epoch [526/10000] Avg train loss: 0.088177\n",
      "Epoch [527/10000] Avg train loss: 0.088010\n",
      "Epoch [528/10000] Avg train loss: 0.087843\n",
      "Epoch [529/10000] Avg train loss: 0.087677\n",
      "Epoch [530/10000] Avg train loss: 0.087511\n",
      "Epoch [531/10000] Avg train loss: 0.087347\n",
      "Epoch [532/10000] Avg train loss: 0.087182\n",
      "Epoch [533/10000] Avg train loss: 0.087019\n",
      "Epoch [534/10000] Avg train loss: 0.086856\n",
      "Epoch [535/10000] Avg train loss: 0.086694\n",
      "Epoch [536/10000] Avg train loss: 0.086532\n",
      "Epoch [537/10000] Avg train loss: 0.086371\n",
      "Epoch [538/10000] Avg train loss: 0.086210\n",
      "Epoch [539/10000] Avg train loss: 0.086050\n",
      "Epoch [540/10000] Avg train loss: 0.085891\n",
      "Epoch [541/10000] Avg train loss: 0.085732\n",
      "Epoch [542/10000] Avg train loss: 0.085574\n",
      "Epoch [543/10000] Avg train loss: 0.085416\n",
      "Epoch [544/10000] Avg train loss: 0.085259\n",
      "Epoch [545/10000] Avg train loss: 0.085103\n",
      "Epoch [546/10000] Avg train loss: 0.084947\n",
      "Epoch [547/10000] Avg train loss: 0.084792\n",
      "Epoch [548/10000] Avg train loss: 0.084637\n",
      "Epoch [549/10000] Avg train loss: 0.084483\n",
      "Epoch [550/10000] Avg train loss: 0.084329\n",
      "Epoch [551/10000] Avg train loss: 0.084176\n",
      "Epoch [552/10000] Avg train loss: 0.084024\n",
      "Epoch [553/10000] Avg train loss: 0.083872\n",
      "Epoch [554/10000] Avg train loss: 0.083720\n",
      "Epoch [555/10000] Avg train loss: 0.083570\n",
      "Epoch [556/10000] Avg train loss: 0.083419\n",
      "Epoch [557/10000] Avg train loss: 0.083270\n",
      "Epoch [558/10000] Avg train loss: 0.083120\n",
      "Epoch [559/10000] Avg train loss: 0.082972\n",
      "Epoch [560/10000] Avg train loss: 0.082823\n",
      "Epoch [561/10000] Avg train loss: 0.082676\n",
      "Epoch [562/10000] Avg train loss: 0.082529\n",
      "Epoch [563/10000] Avg train loss: 0.082382\n",
      "Epoch [564/10000] Avg train loss: 0.082236\n",
      "Epoch [565/10000] Avg train loss: 0.082091\n",
      "Epoch [566/10000] Avg train loss: 0.081946\n",
      "Epoch [567/10000] Avg train loss: 0.081801\n",
      "Epoch [568/10000] Avg train loss: 0.081657\n",
      "Epoch [569/10000] Avg train loss: 0.081513\n",
      "Epoch [570/10000] Avg train loss: 0.081370\n",
      "Epoch [571/10000] Avg train loss: 0.081228\n",
      "Epoch [572/10000] Avg train loss: 0.081086\n",
      "Epoch [573/10000] Avg train loss: 0.080944\n",
      "Epoch [574/10000] Avg train loss: 0.080803\n",
      "Epoch [575/10000] Avg train loss: 0.080663\n",
      "Epoch [576/10000] Avg train loss: 0.080523\n",
      "Epoch [577/10000] Avg train loss: 0.080383\n",
      "Epoch [578/10000] Avg train loss: 0.080244\n",
      "Epoch [579/10000] Avg train loss: 0.080106\n",
      "Epoch [580/10000] Avg train loss: 0.079968\n",
      "Epoch [581/10000] Avg train loss: 0.079830\n",
      "Epoch [582/10000] Avg train loss: 0.079693\n",
      "Epoch [583/10000] Avg train loss: 0.079556\n",
      "Epoch [584/10000] Avg train loss: 0.079420\n",
      "Epoch [585/10000] Avg train loss: 0.079284\n",
      "Epoch [586/10000] Avg train loss: 0.079149\n",
      "Epoch [587/10000] Avg train loss: 0.079014\n",
      "Epoch [588/10000] Avg train loss: 0.078880\n",
      "Epoch [589/10000] Avg train loss: 0.078746\n",
      "Epoch [590/10000] Avg train loss: 0.078612\n",
      "Epoch [591/10000] Avg train loss: 0.078479\n",
      "Epoch [592/10000] Avg train loss: 0.078347\n",
      "Epoch [593/10000] Avg train loss: 0.078215\n",
      "Epoch [594/10000] Avg train loss: 0.078083\n",
      "Epoch [595/10000] Avg train loss: 0.077952\n",
      "Epoch [596/10000] Avg train loss: 0.077821\n",
      "Epoch [597/10000] Avg train loss: 0.077690\n",
      "Epoch [598/10000] Avg train loss: 0.077561\n",
      "Epoch [599/10000] Avg train loss: 0.077431\n",
      "Epoch [600/10000] Avg train loss: 0.077302\n",
      "Epoch [601/10000] Avg train loss: 0.077173\n",
      "Epoch [602/10000] Avg train loss: 0.077045\n",
      "Epoch [603/10000] Avg train loss: 0.076917\n",
      "Epoch [604/10000] Avg train loss: 0.076790\n",
      "Epoch [605/10000] Avg train loss: 0.076663\n",
      "Epoch [606/10000] Avg train loss: 0.076537\n",
      "Epoch [607/10000] Avg train loss: 0.076411\n",
      "Epoch [608/10000] Avg train loss: 0.076285\n",
      "Epoch [609/10000] Avg train loss: 0.076160\n",
      "Epoch [610/10000] Avg train loss: 0.076035\n",
      "Epoch [611/10000] Avg train loss: 0.075910\n",
      "Epoch [612/10000] Avg train loss: 0.075786\n",
      "Epoch [613/10000] Avg train loss: 0.075663\n",
      "Epoch [614/10000] Avg train loss: 0.075539\n",
      "Epoch [615/10000] Avg train loss: 0.075417\n",
      "Epoch [616/10000] Avg train loss: 0.075294\n",
      "Epoch [617/10000] Avg train loss: 0.075172\n",
      "Epoch [618/10000] Avg train loss: 0.075051\n",
      "Epoch [619/10000] Avg train loss: 0.074929\n",
      "Epoch [620/10000] Avg train loss: 0.074808\n",
      "Epoch [621/10000] Avg train loss: 0.074688\n",
      "Epoch [622/10000] Avg train loss: 0.074568\n",
      "Epoch [623/10000] Avg train loss: 0.074448\n",
      "Epoch [624/10000] Avg train loss: 0.074329\n",
      "Epoch [625/10000] Avg train loss: 0.074210\n",
      "Epoch [626/10000] Avg train loss: 0.074091\n",
      "Epoch [627/10000] Avg train loss: 0.073973\n",
      "Epoch [628/10000] Avg train loss: 0.073855\n",
      "Epoch [629/10000] Avg train loss: 0.073738\n",
      "Epoch [630/10000] Avg train loss: 0.073621\n",
      "Epoch [631/10000] Avg train loss: 0.073504\n",
      "Epoch [632/10000] Avg train loss: 0.073388\n",
      "Epoch [633/10000] Avg train loss: 0.073272\n",
      "Epoch [634/10000] Avg train loss: 0.073157\n",
      "Epoch [635/10000] Avg train loss: 0.073041\n",
      "Epoch [636/10000] Avg train loss: 0.072926\n",
      "Epoch [637/10000] Avg train loss: 0.072812\n",
      "Epoch [638/10000] Avg train loss: 0.072698\n",
      "Epoch [639/10000] Avg train loss: 0.072584\n",
      "Epoch [640/10000] Avg train loss: 0.072471\n",
      "Epoch [641/10000] Avg train loss: 0.072358\n",
      "Epoch [642/10000] Avg train loss: 0.072245\n",
      "Epoch [643/10000] Avg train loss: 0.072133\n",
      "Epoch [644/10000] Avg train loss: 0.072021\n",
      "Epoch [645/10000] Avg train loss: 0.071909\n",
      "Epoch [646/10000] Avg train loss: 0.071798\n",
      "Epoch [647/10000] Avg train loss: 0.071687\n",
      "Epoch [648/10000] Avg train loss: 0.071576\n",
      "Epoch [649/10000] Avg train loss: 0.071466\n",
      "Epoch [650/10000] Avg train loss: 0.071356\n",
      "Epoch [651/10000] Avg train loss: 0.071246\n",
      "Epoch [652/10000] Avg train loss: 0.071137\n",
      "Epoch [653/10000] Avg train loss: 0.071028\n",
      "Epoch [654/10000] Avg train loss: 0.070919\n",
      "Epoch [655/10000] Avg train loss: 0.070811\n",
      "Epoch [656/10000] Avg train loss: 0.070703\n",
      "Epoch [657/10000] Avg train loss: 0.070595\n",
      "Epoch [658/10000] Avg train loss: 0.070488\n",
      "Epoch [659/10000] Avg train loss: 0.070381\n",
      "Epoch [660/10000] Avg train loss: 0.070275\n",
      "Epoch [661/10000] Avg train loss: 0.070168\n",
      "Epoch [662/10000] Avg train loss: 0.070062\n",
      "Epoch [663/10000] Avg train loss: 0.069957\n",
      "Epoch [664/10000] Avg train loss: 0.069851\n",
      "Epoch [665/10000] Avg train loss: 0.069746\n",
      "Epoch [666/10000] Avg train loss: 0.069642\n",
      "Epoch [667/10000] Avg train loss: 0.069537\n",
      "Epoch [668/10000] Avg train loss: 0.069433\n",
      "Epoch [669/10000] Avg train loss: 0.069329\n",
      "Epoch [670/10000] Avg train loss: 0.069226\n",
      "Epoch [671/10000] Avg train loss: 0.069123\n",
      "Epoch [672/10000] Avg train loss: 0.069020\n",
      "Epoch [673/10000] Avg train loss: 0.068917\n",
      "Epoch [674/10000] Avg train loss: 0.068815\n",
      "Epoch [675/10000] Avg train loss: 0.068713\n",
      "Epoch [676/10000] Avg train loss: 0.068611\n",
      "Epoch [677/10000] Avg train loss: 0.068510\n",
      "Epoch [678/10000] Avg train loss: 0.068409\n",
      "Epoch [679/10000] Avg train loss: 0.068308\n",
      "Epoch [680/10000] Avg train loss: 0.068208\n",
      "Epoch [681/10000] Avg train loss: 0.068108\n",
      "Epoch [682/10000] Avg train loss: 0.068008\n",
      "Epoch [683/10000] Avg train loss: 0.067908\n",
      "Epoch [684/10000] Avg train loss: 0.067809\n",
      "Epoch [685/10000] Avg train loss: 0.067710\n",
      "Epoch [686/10000] Avg train loss: 0.067611\n",
      "Epoch [687/10000] Avg train loss: 0.067513\n",
      "Epoch [688/10000] Avg train loss: 0.067415\n",
      "Epoch [689/10000] Avg train loss: 0.067317\n",
      "Epoch [690/10000] Avg train loss: 0.067219\n",
      "Epoch [691/10000] Avg train loss: 0.067122\n",
      "Epoch [692/10000] Avg train loss: 0.067025\n",
      "Epoch [693/10000] Avg train loss: 0.066928\n",
      "Epoch [694/10000] Avg train loss: 0.066832\n",
      "Epoch [695/10000] Avg train loss: 0.066736\n",
      "Epoch [696/10000] Avg train loss: 0.066640\n",
      "Epoch [697/10000] Avg train loss: 0.066544\n",
      "Epoch [698/10000] Avg train loss: 0.066449\n",
      "Epoch [699/10000] Avg train loss: 0.066354\n",
      "Epoch [700/10000] Avg train loss: 0.066259\n",
      "Epoch [701/10000] Avg train loss: 0.066164\n",
      "Epoch [702/10000] Avg train loss: 0.066070\n",
      "Epoch [703/10000] Avg train loss: 0.065976\n",
      "Epoch [704/10000] Avg train loss: 0.065882\n",
      "Epoch [705/10000] Avg train loss: 0.065789\n",
      "Epoch [706/10000] Avg train loss: 0.065696\n",
      "Epoch [707/10000] Avg train loss: 0.065603\n",
      "Epoch [708/10000] Avg train loss: 0.065510\n",
      "Epoch [709/10000] Avg train loss: 0.065418\n",
      "Epoch [710/10000] Avg train loss: 0.065326\n",
      "Epoch [711/10000] Avg train loss: 0.065234\n",
      "Epoch [712/10000] Avg train loss: 0.065142\n",
      "Epoch [713/10000] Avg train loss: 0.065051\n",
      "Epoch [714/10000] Avg train loss: 0.064960\n",
      "Epoch [715/10000] Avg train loss: 0.064869\n",
      "Epoch [716/10000] Avg train loss: 0.064778\n",
      "Epoch [717/10000] Avg train loss: 0.064688\n",
      "Epoch [718/10000] Avg train loss: 0.064598\n",
      "Epoch [719/10000] Avg train loss: 0.064508\n",
      "Epoch [720/10000] Avg train loss: 0.064418\n",
      "Epoch [721/10000] Avg train loss: 0.064329\n",
      "Epoch [722/10000] Avg train loss: 0.064240\n",
      "Epoch [723/10000] Avg train loss: 0.064151\n",
      "Epoch [724/10000] Avg train loss: 0.064063\n",
      "Epoch [725/10000] Avg train loss: 0.063974\n",
      "Epoch [726/10000] Avg train loss: 0.063886\n",
      "Epoch [727/10000] Avg train loss: 0.063798\n",
      "Epoch [728/10000] Avg train loss: 0.063711\n",
      "Epoch [729/10000] Avg train loss: 0.063623\n",
      "Epoch [730/10000] Avg train loss: 0.063536\n",
      "Epoch [731/10000] Avg train loss: 0.063449\n",
      "Epoch [732/10000] Avg train loss: 0.063362\n",
      "Epoch [733/10000] Avg train loss: 0.063276\n",
      "Epoch [734/10000] Avg train loss: 0.063190\n",
      "Epoch [735/10000] Avg train loss: 0.063104\n",
      "Epoch [736/10000] Avg train loss: 0.063018\n",
      "Epoch [737/10000] Avg train loss: 0.062933\n",
      "Epoch [738/10000] Avg train loss: 0.062847\n",
      "Epoch [739/10000] Avg train loss: 0.062762\n",
      "Epoch [740/10000] Avg train loss: 0.062677\n",
      "Epoch [741/10000] Avg train loss: 0.062593\n",
      "Epoch [742/10000] Avg train loss: 0.062508\n",
      "Epoch [743/10000] Avg train loss: 0.062424\n",
      "Epoch [744/10000] Avg train loss: 0.062340\n",
      "Epoch [745/10000] Avg train loss: 0.062257\n",
      "Epoch [746/10000] Avg train loss: 0.062173\n",
      "Epoch [747/10000] Avg train loss: 0.062090\n",
      "Epoch [748/10000] Avg train loss: 0.062007\n",
      "Epoch [749/10000] Avg train loss: 0.061924\n",
      "Epoch [750/10000] Avg train loss: 0.061842\n",
      "Epoch [751/10000] Avg train loss: 0.061759\n",
      "Epoch [752/10000] Avg train loss: 0.061677\n",
      "Epoch [753/10000] Avg train loss: 0.061595\n",
      "Epoch [754/10000] Avg train loss: 0.061514\n",
      "Epoch [755/10000] Avg train loss: 0.061432\n",
      "Epoch [756/10000] Avg train loss: 0.061351\n",
      "Epoch [757/10000] Avg train loss: 0.061270\n",
      "Epoch [758/10000] Avg train loss: 0.061189\n",
      "Epoch [759/10000] Avg train loss: 0.061108\n",
      "Epoch [760/10000] Avg train loss: 0.061028\n",
      "Epoch [761/10000] Avg train loss: 0.060948\n",
      "Epoch [762/10000] Avg train loss: 0.060868\n",
      "Epoch [763/10000] Avg train loss: 0.060788\n",
      "Epoch [764/10000] Avg train loss: 0.060708\n",
      "Epoch [765/10000] Avg train loss: 0.060629\n",
      "Epoch [766/10000] Avg train loss: 0.060550\n",
      "Epoch [767/10000] Avg train loss: 0.060471\n",
      "Epoch [768/10000] Avg train loss: 0.060392\n",
      "Epoch [769/10000] Avg train loss: 0.060314\n",
      "Epoch [770/10000] Avg train loss: 0.060235\n",
      "Epoch [771/10000] Avg train loss: 0.060157\n",
      "Epoch [772/10000] Avg train loss: 0.060079\n",
      "Epoch [773/10000] Avg train loss: 0.060002\n",
      "Epoch [774/10000] Avg train loss: 0.059924\n",
      "Epoch [775/10000] Avg train loss: 0.059847\n",
      "Epoch [776/10000] Avg train loss: 0.059770\n",
      "Epoch [777/10000] Avg train loss: 0.059693\n",
      "Epoch [778/10000] Avg train loss: 0.059616\n",
      "Epoch [779/10000] Avg train loss: 0.059539\n",
      "Epoch [780/10000] Avg train loss: 0.059463\n",
      "Epoch [781/10000] Avg train loss: 0.059387\n",
      "Epoch [782/10000] Avg train loss: 0.059311\n",
      "Epoch [783/10000] Avg train loss: 0.059235\n",
      "Epoch [784/10000] Avg train loss: 0.059160\n",
      "Epoch [785/10000] Avg train loss: 0.059084\n",
      "Epoch [786/10000] Avg train loss: 0.059009\n",
      "Epoch [787/10000] Avg train loss: 0.058934\n",
      "Epoch [788/10000] Avg train loss: 0.058859\n",
      "Epoch [789/10000] Avg train loss: 0.058785\n",
      "Epoch [790/10000] Avg train loss: 0.058710\n",
      "Epoch [791/10000] Avg train loss: 0.058636\n",
      "Epoch [792/10000] Avg train loss: 0.058562\n",
      "Epoch [793/10000] Avg train loss: 0.058488\n",
      "Epoch [794/10000] Avg train loss: 0.058415\n",
      "Epoch [795/10000] Avg train loss: 0.058341\n",
      "Epoch [796/10000] Avg train loss: 0.058268\n",
      "Epoch [797/10000] Avg train loss: 0.058195\n",
      "Epoch [798/10000] Avg train loss: 0.058122\n",
      "Epoch [799/10000] Avg train loss: 0.058049\n",
      "Epoch [800/10000] Avg train loss: 0.057977\n",
      "Epoch [801/10000] Avg train loss: 0.057904\n",
      "Epoch [802/10000] Avg train loss: 0.057832\n",
      "Epoch [803/10000] Avg train loss: 0.057760\n",
      "Epoch [804/10000] Avg train loss: 0.057688\n",
      "Epoch [805/10000] Avg train loss: 0.057616\n",
      "Epoch [806/10000] Avg train loss: 0.057545\n",
      "Epoch [807/10000] Avg train loss: 0.057474\n",
      "Epoch [808/10000] Avg train loss: 0.057403\n",
      "Epoch [809/10000] Avg train loss: 0.057332\n",
      "Epoch [810/10000] Avg train loss: 0.057261\n",
      "Epoch [811/10000] Avg train loss: 0.057190\n",
      "Epoch [812/10000] Avg train loss: 0.057120\n",
      "Epoch [813/10000] Avg train loss: 0.057050\n",
      "Epoch [814/10000] Avg train loss: 0.056979\n",
      "Epoch [815/10000] Avg train loss: 0.056910\n",
      "Epoch [816/10000] Avg train loss: 0.056840\n",
      "Epoch [817/10000] Avg train loss: 0.056770\n",
      "Epoch [818/10000] Avg train loss: 0.056701\n",
      "Epoch [819/10000] Avg train loss: 0.056632\n",
      "Epoch [820/10000] Avg train loss: 0.056563\n",
      "Epoch [821/10000] Avg train loss: 0.056494\n",
      "Epoch [822/10000] Avg train loss: 0.056425\n",
      "Epoch [823/10000] Avg train loss: 0.056356\n",
      "Epoch [824/10000] Avg train loss: 0.056288\n",
      "Epoch [825/10000] Avg train loss: 0.056220\n",
      "Epoch [826/10000] Avg train loss: 0.056152\n",
      "Epoch [827/10000] Avg train loss: 0.056084\n",
      "Epoch [828/10000] Avg train loss: 0.056016\n",
      "Epoch [829/10000] Avg train loss: 0.055948\n",
      "Epoch [830/10000] Avg train loss: 0.055881\n",
      "Epoch [831/10000] Avg train loss: 0.055814\n",
      "Epoch [832/10000] Avg train loss: 0.055747\n",
      "Epoch [833/10000] Avg train loss: 0.055680\n",
      "Epoch [834/10000] Avg train loss: 0.055613\n",
      "Epoch [835/10000] Avg train loss: 0.055546\n",
      "Epoch [836/10000] Avg train loss: 0.055480\n",
      "Epoch [837/10000] Avg train loss: 0.055414\n",
      "Epoch [838/10000] Avg train loss: 0.055348\n",
      "Epoch [839/10000] Avg train loss: 0.055282\n",
      "Epoch [840/10000] Avg train loss: 0.055216\n",
      "Epoch [841/10000] Avg train loss: 0.055150\n",
      "Epoch [842/10000] Avg train loss: 0.055085\n",
      "Epoch [843/10000] Avg train loss: 0.055019\n",
      "Epoch [844/10000] Avg train loss: 0.054954\n",
      "Epoch [845/10000] Avg train loss: 0.054889\n",
      "Epoch [846/10000] Avg train loss: 0.054824\n",
      "Epoch [847/10000] Avg train loss: 0.054759\n",
      "Epoch [848/10000] Avg train loss: 0.054695\n",
      "Epoch [849/10000] Avg train loss: 0.054630\n",
      "Epoch [850/10000] Avg train loss: 0.054566\n",
      "Epoch [851/10000] Avg train loss: 0.054502\n",
      "Epoch [852/10000] Avg train loss: 0.054438\n",
      "Epoch [853/10000] Avg train loss: 0.054374\n",
      "Epoch [854/10000] Avg train loss: 0.054311\n",
      "Epoch [855/10000] Avg train loss: 0.054247\n",
      "Epoch [856/10000] Avg train loss: 0.054184\n",
      "Epoch [857/10000] Avg train loss: 0.054120\n",
      "Epoch [858/10000] Avg train loss: 0.054057\n",
      "Epoch [859/10000] Avg train loss: 0.053994\n",
      "Epoch [860/10000] Avg train loss: 0.053932\n",
      "Epoch [861/10000] Avg train loss: 0.053869\n",
      "Epoch [862/10000] Avg train loss: 0.053807\n",
      "Epoch [863/10000] Avg train loss: 0.053744\n",
      "Epoch [864/10000] Avg train loss: 0.053682\n",
      "Epoch [865/10000] Avg train loss: 0.053620\n",
      "Epoch [866/10000] Avg train loss: 0.053558\n",
      "Epoch [867/10000] Avg train loss: 0.053496\n",
      "Epoch [868/10000] Avg train loss: 0.053435\n",
      "Epoch [869/10000] Avg train loss: 0.053373\n",
      "Epoch [870/10000] Avg train loss: 0.053312\n",
      "Epoch [871/10000] Avg train loss: 0.053251\n",
      "Epoch [872/10000] Avg train loss: 0.053190\n",
      "Epoch [873/10000] Avg train loss: 0.053129\n",
      "Epoch [874/10000] Avg train loss: 0.053068\n",
      "Epoch [875/10000] Avg train loss: 0.053007\n",
      "Epoch [876/10000] Avg train loss: 0.052947\n",
      "Epoch [877/10000] Avg train loss: 0.052886\n",
      "Epoch [878/10000] Avg train loss: 0.052826\n",
      "Epoch [879/10000] Avg train loss: 0.052766\n",
      "Epoch [880/10000] Avg train loss: 0.052706\n",
      "Epoch [881/10000] Avg train loss: 0.052646\n",
      "Epoch [882/10000] Avg train loss: 0.052586\n",
      "Epoch [883/10000] Avg train loss: 0.052527\n",
      "Epoch [884/10000] Avg train loss: 0.052467\n",
      "Epoch [885/10000] Avg train loss: 0.052408\n",
      "Epoch [886/10000] Avg train loss: 0.052349\n",
      "Epoch [887/10000] Avg train loss: 0.052290\n",
      "Epoch [888/10000] Avg train loss: 0.052231\n",
      "Epoch [889/10000] Avg train loss: 0.052172\n",
      "Epoch [890/10000] Avg train loss: 0.052114\n",
      "Epoch [891/10000] Avg train loss: 0.052055\n",
      "Epoch [892/10000] Avg train loss: 0.051997\n",
      "Epoch [893/10000] Avg train loss: 0.051939\n",
      "Epoch [894/10000] Avg train loss: 0.051881\n",
      "Epoch [895/10000] Avg train loss: 0.051823\n",
      "Epoch [896/10000] Avg train loss: 0.051765\n",
      "Epoch [897/10000] Avg train loss: 0.051707\n",
      "Epoch [898/10000] Avg train loss: 0.051650\n",
      "Epoch [899/10000] Avg train loss: 0.051592\n",
      "Epoch [900/10000] Avg train loss: 0.051535\n",
      "Epoch [901/10000] Avg train loss: 0.051478\n",
      "Epoch [902/10000] Avg train loss: 0.051420\n",
      "Epoch [903/10000] Avg train loss: 0.051364\n",
      "Epoch [904/10000] Avg train loss: 0.051307\n",
      "Epoch [905/10000] Avg train loss: 0.051250\n",
      "Epoch [906/10000] Avg train loss: 0.051193\n",
      "Epoch [907/10000] Avg train loss: 0.051137\n",
      "Epoch [908/10000] Avg train loss: 0.051081\n",
      "Epoch [909/10000] Avg train loss: 0.051024\n",
      "Epoch [910/10000] Avg train loss: 0.050968\n",
      "Epoch [911/10000] Avg train loss: 0.050912\n",
      "Epoch [912/10000] Avg train loss: 0.050857\n",
      "Epoch [913/10000] Avg train loss: 0.050801\n",
      "Epoch [914/10000] Avg train loss: 0.050745\n",
      "Epoch [915/10000] Avg train loss: 0.050690\n",
      "Epoch [916/10000] Avg train loss: 0.050635\n",
      "Epoch [917/10000] Avg train loss: 0.050579\n",
      "Epoch [918/10000] Avg train loss: 0.050524\n",
      "Epoch [919/10000] Avg train loss: 0.050469\n",
      "Epoch [920/10000] Avg train loss: 0.050414\n",
      "Epoch [921/10000] Avg train loss: 0.050360\n",
      "Epoch [922/10000] Avg train loss: 0.050305\n",
      "Epoch [923/10000] Avg train loss: 0.050251\n",
      "Epoch [924/10000] Avg train loss: 0.050196\n",
      "Epoch [925/10000] Avg train loss: 0.050142\n",
      "Epoch [926/10000] Avg train loss: 0.050088\n",
      "Epoch [927/10000] Avg train loss: 0.050034\n",
      "Epoch [928/10000] Avg train loss: 0.049980\n",
      "Epoch [929/10000] Avg train loss: 0.049926\n",
      "Epoch [930/10000] Avg train loss: 0.049872\n",
      "Epoch [931/10000] Avg train loss: 0.049819\n",
      "Epoch [932/10000] Avg train loss: 0.049765\n",
      "Epoch [933/10000] Avg train loss: 0.049712\n",
      "Epoch [934/10000] Avg train loss: 0.049659\n",
      "Epoch [935/10000] Avg train loss: 0.049606\n",
      "Epoch [936/10000] Avg train loss: 0.049553\n",
      "Epoch [937/10000] Avg train loss: 0.049500\n",
      "Epoch [938/10000] Avg train loss: 0.049447\n",
      "Epoch [939/10000] Avg train loss: 0.049394\n",
      "Epoch [940/10000] Avg train loss: 0.049342\n",
      "Epoch [941/10000] Avg train loss: 0.049289\n",
      "Epoch [942/10000] Avg train loss: 0.049237\n",
      "Epoch [943/10000] Avg train loss: 0.049185\n",
      "Epoch [944/10000] Avg train loss: 0.049133\n",
      "Epoch [945/10000] Avg train loss: 0.049081\n",
      "Epoch [946/10000] Avg train loss: 0.049029\n",
      "Epoch [947/10000] Avg train loss: 0.048977\n",
      "Epoch [948/10000] Avg train loss: 0.048925\n",
      "Epoch [949/10000] Avg train loss: 0.048874\n",
      "Epoch [950/10000] Avg train loss: 0.048822\n",
      "Epoch [951/10000] Avg train loss: 0.048771\n",
      "Epoch [952/10000] Avg train loss: 0.048720\n",
      "Epoch [953/10000] Avg train loss: 0.048669\n",
      "Epoch [954/10000] Avg train loss: 0.048618\n",
      "Epoch [955/10000] Avg train loss: 0.048567\n",
      "Epoch [956/10000] Avg train loss: 0.048516\n",
      "Epoch [957/10000] Avg train loss: 0.048465\n",
      "Epoch [958/10000] Avg train loss: 0.048415\n",
      "Epoch [959/10000] Avg train loss: 0.048364\n",
      "Epoch [960/10000] Avg train loss: 0.048314\n",
      "Epoch [961/10000] Avg train loss: 0.048264\n",
      "Epoch [962/10000] Avg train loss: 0.048213\n",
      "Epoch [963/10000] Avg train loss: 0.048163\n",
      "Epoch [964/10000] Avg train loss: 0.048113\n",
      "Epoch [965/10000] Avg train loss: 0.048063\n",
      "Epoch [966/10000] Avg train loss: 0.048014\n",
      "Epoch [967/10000] Avg train loss: 0.047964\n",
      "Epoch [968/10000] Avg train loss: 0.047915\n",
      "Epoch [969/10000] Avg train loss: 0.047865\n",
      "Epoch [970/10000] Avg train loss: 0.047816\n",
      "Epoch [971/10000] Avg train loss: 0.047766\n",
      "Epoch [972/10000] Avg train loss: 0.047717\n",
      "Epoch [973/10000] Avg train loss: 0.047668\n",
      "Epoch [974/10000] Avg train loss: 0.047619\n",
      "Epoch [975/10000] Avg train loss: 0.047571\n",
      "Epoch [976/10000] Avg train loss: 0.047522\n",
      "Epoch [977/10000] Avg train loss: 0.047473\n",
      "Epoch [978/10000] Avg train loss: 0.047425\n",
      "Epoch [979/10000] Avg train loss: 0.047376\n",
      "Epoch [980/10000] Avg train loss: 0.047328\n",
      "Epoch [981/10000] Avg train loss: 0.047280\n",
      "Epoch [982/10000] Avg train loss: 0.047231\n",
      "Epoch [983/10000] Avg train loss: 0.047183\n",
      "Epoch [984/10000] Avg train loss: 0.047135\n",
      "Epoch [985/10000] Avg train loss: 0.047088\n",
      "Epoch [986/10000] Avg train loss: 0.047040\n",
      "Epoch [987/10000] Avg train loss: 0.046992\n",
      "Epoch [988/10000] Avg train loss: 0.046945\n",
      "Epoch [989/10000] Avg train loss: 0.046897\n",
      "Epoch [990/10000] Avg train loss: 0.046850\n",
      "Epoch [991/10000] Avg train loss: 0.046802\n",
      "Epoch [992/10000] Avg train loss: 0.046755\n",
      "Epoch [993/10000] Avg train loss: 0.046708\n",
      "Epoch [994/10000] Avg train loss: 0.046661\n",
      "Epoch [995/10000] Avg train loss: 0.046614\n",
      "Epoch [996/10000] Avg train loss: 0.046568\n",
      "Epoch [997/10000] Avg train loss: 0.046521\n",
      "Epoch [998/10000] Avg train loss: 0.046474\n",
      "Epoch [999/10000] Avg train loss: 0.046428\n",
      "Epoch [1000/10000] Avg train loss: 0.046381\n",
      "Epoch [1001/10000] Avg train loss: 0.046335\n",
      "Epoch [1002/10000] Avg train loss: 0.046289\n",
      "Epoch [1003/10000] Avg train loss: 0.046243\n",
      "Epoch [1004/10000] Avg train loss: 0.046196\n",
      "Epoch [1005/10000] Avg train loss: 0.046151\n",
      "Epoch [1006/10000] Avg train loss: 0.046105\n",
      "Epoch [1007/10000] Avg train loss: 0.046059\n",
      "Epoch [1008/10000] Avg train loss: 0.046013\n",
      "Epoch [1009/10000] Avg train loss: 0.045968\n",
      "Epoch [1010/10000] Avg train loss: 0.045922\n",
      "Epoch [1011/10000] Avg train loss: 0.045877\n",
      "Epoch [1012/10000] Avg train loss: 0.045831\n",
      "Epoch [1013/10000] Avg train loss: 0.045786\n",
      "Epoch [1014/10000] Avg train loss: 0.045741\n",
      "Epoch [1015/10000] Avg train loss: 0.045696\n",
      "Epoch [1016/10000] Avg train loss: 0.045651\n",
      "Epoch [1017/10000] Avg train loss: 0.045606\n",
      "Epoch [1018/10000] Avg train loss: 0.045561\n",
      "Epoch [1019/10000] Avg train loss: 0.045516\n",
      "Epoch [1020/10000] Avg train loss: 0.045472\n",
      "Epoch [1021/10000] Avg train loss: 0.045427\n",
      "Epoch [1022/10000] Avg train loss: 0.045383\n",
      "Epoch [1023/10000] Avg train loss: 0.045338\n",
      "Epoch [1024/10000] Avg train loss: 0.045294\n",
      "Epoch [1025/10000] Avg train loss: 0.045250\n",
      "Epoch [1026/10000] Avg train loss: 0.045206\n",
      "Epoch [1027/10000] Avg train loss: 0.045162\n",
      "Epoch [1028/10000] Avg train loss: 0.045118\n",
      "Epoch [1029/10000] Avg train loss: 0.045074\n",
      "Epoch [1030/10000] Avg train loss: 0.045030\n",
      "Epoch [1031/10000] Avg train loss: 0.044987\n",
      "Epoch [1032/10000] Avg train loss: 0.044943\n",
      "Epoch [1033/10000] Avg train loss: 0.044900\n",
      "Epoch [1034/10000] Avg train loss: 0.044856\n",
      "Epoch [1035/10000] Avg train loss: 0.044813\n",
      "Epoch [1036/10000] Avg train loss: 0.044770\n",
      "Epoch [1037/10000] Avg train loss: 0.044726\n",
      "Epoch [1038/10000] Avg train loss: 0.044683\n",
      "Epoch [1039/10000] Avg train loss: 0.044640\n",
      "Epoch [1040/10000] Avg train loss: 0.044597\n",
      "Epoch [1041/10000] Avg train loss: 0.044555\n",
      "Epoch [1042/10000] Avg train loss: 0.044512\n",
      "Epoch [1043/10000] Avg train loss: 0.044469\n",
      "Epoch [1044/10000] Avg train loss: 0.044426\n",
      "Epoch [1045/10000] Avg train loss: 0.044384\n",
      "Epoch [1046/10000] Avg train loss: 0.044342\n",
      "Epoch [1047/10000] Avg train loss: 0.044299\n",
      "Epoch [1048/10000] Avg train loss: 0.044257\n",
      "Epoch [1049/10000] Avg train loss: 0.044215\n",
      "Epoch [1050/10000] Avg train loss: 0.044173\n",
      "Epoch [1051/10000] Avg train loss: 0.044131\n",
      "Epoch [1052/10000] Avg train loss: 0.044089\n",
      "Epoch [1053/10000] Avg train loss: 0.044047\n",
      "Epoch [1054/10000] Avg train loss: 0.044005\n",
      "Epoch [1055/10000] Avg train loss: 0.043963\n",
      "Epoch [1056/10000] Avg train loss: 0.043922\n",
      "Epoch [1057/10000] Avg train loss: 0.043880\n",
      "Epoch [1058/10000] Avg train loss: 0.043839\n",
      "Epoch [1059/10000] Avg train loss: 0.043797\n",
      "Epoch [1060/10000] Avg train loss: 0.043756\n",
      "Epoch [1061/10000] Avg train loss: 0.043715\n",
      "Epoch [1062/10000] Avg train loss: 0.043674\n",
      "Epoch [1063/10000] Avg train loss: 0.043632\n",
      "Epoch [1064/10000] Avg train loss: 0.043591\n",
      "Epoch [1065/10000] Avg train loss: 0.043550\n",
      "Epoch [1066/10000] Avg train loss: 0.043510\n",
      "Epoch [1067/10000] Avg train loss: 0.043469\n",
      "Epoch [1068/10000] Avg train loss: 0.043428\n",
      "Epoch [1069/10000] Avg train loss: 0.043388\n",
      "Epoch [1070/10000] Avg train loss: 0.043347\n",
      "Epoch [1071/10000] Avg train loss: 0.043306\n",
      "Epoch [1072/10000] Avg train loss: 0.043266\n",
      "Epoch [1073/10000] Avg train loss: 0.043226\n",
      "Epoch [1074/10000] Avg train loss: 0.043186\n",
      "Epoch [1075/10000] Avg train loss: 0.043145\n",
      "Epoch [1076/10000] Avg train loss: 0.043105\n",
      "Epoch [1077/10000] Avg train loss: 0.043065\n",
      "Epoch [1078/10000] Avg train loss: 0.043025\n",
      "Epoch [1079/10000] Avg train loss: 0.042985\n",
      "Epoch [1080/10000] Avg train loss: 0.042946\n",
      "Epoch [1081/10000] Avg train loss: 0.042906\n",
      "Epoch [1082/10000] Avg train loss: 0.042866\n",
      "Epoch [1083/10000] Avg train loss: 0.042827\n",
      "Epoch [1084/10000] Avg train loss: 0.042787\n",
      "Epoch [1085/10000] Avg train loss: 0.042748\n",
      "Epoch [1086/10000] Avg train loss: 0.042708\n",
      "Epoch [1087/10000] Avg train loss: 0.042669\n",
      "Epoch [1088/10000] Avg train loss: 0.042630\n",
      "Epoch [1089/10000] Avg train loss: 0.042591\n",
      "Epoch [1090/10000] Avg train loss: 0.042552\n",
      "Epoch [1091/10000] Avg train loss: 0.042513\n",
      "Epoch [1092/10000] Avg train loss: 0.042474\n",
      "Epoch [1093/10000] Avg train loss: 0.042435\n",
      "Epoch [1094/10000] Avg train loss: 0.042396\n",
      "Epoch [1095/10000] Avg train loss: 0.042357\n",
      "Epoch [1096/10000] Avg train loss: 0.042319\n",
      "Epoch [1097/10000] Avg train loss: 0.042280\n",
      "Epoch [1098/10000] Avg train loss: 0.042242\n",
      "Epoch [1099/10000] Avg train loss: 0.042203\n",
      "Epoch [1100/10000] Avg train loss: 0.042165\n",
      "Epoch [1101/10000] Avg train loss: 0.042126\n",
      "Epoch [1102/10000] Avg train loss: 0.042088\n",
      "Epoch [1103/10000] Avg train loss: 0.042050\n",
      "Epoch [1104/10000] Avg train loss: 0.042012\n",
      "Epoch [1105/10000] Avg train loss: 0.041974\n",
      "Epoch [1106/10000] Avg train loss: 0.041936\n",
      "Epoch [1107/10000] Avg train loss: 0.041898\n",
      "Epoch [1108/10000] Avg train loss: 0.041860\n",
      "Epoch [1109/10000] Avg train loss: 0.041823\n",
      "Epoch [1110/10000] Avg train loss: 0.041785\n",
      "Epoch [1111/10000] Avg train loss: 0.041747\n",
      "Epoch [1112/10000] Avg train loss: 0.041710\n",
      "Epoch [1113/10000] Avg train loss: 0.041672\n",
      "Epoch [1114/10000] Avg train loss: 0.041635\n",
      "Epoch [1115/10000] Avg train loss: 0.041598\n",
      "Epoch [1116/10000] Avg train loss: 0.041560\n",
      "Epoch [1117/10000] Avg train loss: 0.041523\n",
      "Epoch [1118/10000] Avg train loss: 0.041486\n",
      "Epoch [1119/10000] Avg train loss: 0.041449\n",
      "Epoch [1120/10000] Avg train loss: 0.041412\n",
      "Epoch [1121/10000] Avg train loss: 0.041375\n",
      "Epoch [1122/10000] Avg train loss: 0.041338\n",
      "Epoch [1123/10000] Avg train loss: 0.041301\n",
      "Epoch [1124/10000] Avg train loss: 0.041264\n",
      "Epoch [1125/10000] Avg train loss: 0.041228\n",
      "Epoch [1126/10000] Avg train loss: 0.041191\n",
      "Epoch [1127/10000] Avg train loss: 0.041155\n",
      "Epoch [1128/10000] Avg train loss: 0.041118\n",
      "Epoch [1129/10000] Avg train loss: 0.041082\n",
      "Epoch [1130/10000] Avg train loss: 0.041045\n",
      "Epoch [1131/10000] Avg train loss: 0.041009\n",
      "Epoch [1132/10000] Avg train loss: 0.040973\n",
      "Epoch [1133/10000] Avg train loss: 0.040937\n",
      "Epoch [1134/10000] Avg train loss: 0.040901\n",
      "Epoch [1135/10000] Avg train loss: 0.040865\n",
      "Epoch [1136/10000] Avg train loss: 0.040829\n",
      "Epoch [1137/10000] Avg train loss: 0.040793\n",
      "Epoch [1138/10000] Avg train loss: 0.040757\n",
      "Epoch [1139/10000] Avg train loss: 0.040721\n",
      "Epoch [1140/10000] Avg train loss: 0.040685\n",
      "Epoch [1141/10000] Avg train loss: 0.040650\n",
      "Epoch [1142/10000] Avg train loss: 0.040614\n",
      "Epoch [1143/10000] Avg train loss: 0.040579\n",
      "Epoch [1144/10000] Avg train loss: 0.040543\n",
      "Epoch [1145/10000] Avg train loss: 0.040508\n",
      "Epoch [1146/10000] Avg train loss: 0.040472\n",
      "Epoch [1147/10000] Avg train loss: 0.040437\n",
      "Epoch [1148/10000] Avg train loss: 0.040402\n",
      "Epoch [1149/10000] Avg train loss: 0.040367\n",
      "Epoch [1150/10000] Avg train loss: 0.040332\n",
      "Epoch [1151/10000] Avg train loss: 0.040296\n",
      "Epoch [1152/10000] Avg train loss: 0.040262\n",
      "Epoch [1153/10000] Avg train loss: 0.040227\n",
      "Epoch [1154/10000] Avg train loss: 0.040192\n",
      "Epoch [1155/10000] Avg train loss: 0.040157\n",
      "Epoch [1156/10000] Avg train loss: 0.040122\n",
      "Epoch [1157/10000] Avg train loss: 0.040088\n",
      "Epoch [1158/10000] Avg train loss: 0.040053\n",
      "Epoch [1159/10000] Avg train loss: 0.040018\n",
      "Epoch [1160/10000] Avg train loss: 0.039984\n",
      "Epoch [1161/10000] Avg train loss: 0.039949\n",
      "Epoch [1162/10000] Avg train loss: 0.039915\n",
      "Epoch [1163/10000] Avg train loss: 0.039881\n",
      "Epoch [1164/10000] Avg train loss: 0.039846\n",
      "Epoch [1165/10000] Avg train loss: 0.039812\n",
      "Epoch [1166/10000] Avg train loss: 0.039778\n",
      "Epoch [1167/10000] Avg train loss: 0.039744\n",
      "Epoch [1168/10000] Avg train loss: 0.039710\n",
      "Epoch [1169/10000] Avg train loss: 0.039676\n",
      "Epoch [1170/10000] Avg train loss: 0.039642\n",
      "Epoch [1171/10000] Avg train loss: 0.039608\n",
      "Epoch [1172/10000] Avg train loss: 0.039574\n",
      "Epoch [1173/10000] Avg train loss: 0.039541\n",
      "Epoch [1174/10000] Avg train loss: 0.039507\n",
      "Epoch [1175/10000] Avg train loss: 0.039473\n",
      "Epoch [1176/10000] Avg train loss: 0.039440\n",
      "Epoch [1177/10000] Avg train loss: 0.039406\n",
      "Epoch [1178/10000] Avg train loss: 0.039373\n",
      "Epoch [1179/10000] Avg train loss: 0.039339\n",
      "Epoch [1180/10000] Avg train loss: 0.039306\n",
      "Epoch [1181/10000] Avg train loss: 0.039273\n",
      "Epoch [1182/10000] Avg train loss: 0.039240\n",
      "Epoch [1183/10000] Avg train loss: 0.039206\n",
      "Epoch [1184/10000] Avg train loss: 0.039173\n",
      "Epoch [1185/10000] Avg train loss: 0.039140\n",
      "Epoch [1186/10000] Avg train loss: 0.039107\n",
      "Epoch [1187/10000] Avg train loss: 0.039074\n",
      "Epoch [1188/10000] Avg train loss: 0.039041\n",
      "Epoch [1189/10000] Avg train loss: 0.039009\n",
      "Epoch [1190/10000] Avg train loss: 0.038976\n",
      "Epoch [1191/10000] Avg train loss: 0.038943\n",
      "Epoch [1192/10000] Avg train loss: 0.038910\n",
      "Epoch [1193/10000] Avg train loss: 0.038878\n",
      "Epoch [1194/10000] Avg train loss: 0.038845\n",
      "Epoch [1195/10000] Avg train loss: 0.038813\n",
      "Epoch [1196/10000] Avg train loss: 0.038780\n",
      "Epoch [1197/10000] Avg train loss: 0.038748\n",
      "Epoch [1198/10000] Avg train loss: 0.038716\n",
      "Epoch [1199/10000] Avg train loss: 0.038683\n",
      "Epoch [1200/10000] Avg train loss: 0.038651\n",
      "Epoch [1201/10000] Avg train loss: 0.038619\n",
      "Epoch [1202/10000] Avg train loss: 0.038587\n",
      "Epoch [1203/10000] Avg train loss: 0.038555\n",
      "Epoch [1204/10000] Avg train loss: 0.038523\n",
      "Epoch [1205/10000] Avg train loss: 0.038491\n",
      "Epoch [1206/10000] Avg train loss: 0.038459\n",
      "Epoch [1207/10000] Avg train loss: 0.038427\n",
      "Epoch [1208/10000] Avg train loss: 0.038395\n",
      "Epoch [1209/10000] Avg train loss: 0.038363\n",
      "Epoch [1210/10000] Avg train loss: 0.038332\n",
      "Epoch [1211/10000] Avg train loss: 0.038300\n",
      "Epoch [1212/10000] Avg train loss: 0.038268\n",
      "Epoch [1213/10000] Avg train loss: 0.038237\n",
      "Epoch [1214/10000] Avg train loss: 0.038205\n",
      "Epoch [1215/10000] Avg train loss: 0.038174\n",
      "Epoch [1216/10000] Avg train loss: 0.038142\n",
      "Epoch [1217/10000] Avg train loss: 0.038111\n",
      "Epoch [1218/10000] Avg train loss: 0.038080\n",
      "Epoch [1219/10000] Avg train loss: 0.038049\n",
      "Epoch [1220/10000] Avg train loss: 0.038017\n",
      "Epoch [1221/10000] Avg train loss: 0.037986\n",
      "Epoch [1222/10000] Avg train loss: 0.037955\n",
      "Epoch [1223/10000] Avg train loss: 0.037924\n",
      "Epoch [1224/10000] Avg train loss: 0.037893\n",
      "Epoch [1225/10000] Avg train loss: 0.037862\n",
      "Epoch [1226/10000] Avg train loss: 0.037831\n",
      "Epoch [1227/10000] Avg train loss: 0.037801\n",
      "Epoch [1228/10000] Avg train loss: 0.037770\n",
      "Epoch [1229/10000] Avg train loss: 0.037739\n",
      "Epoch [1230/10000] Avg train loss: 0.037708\n",
      "Epoch [1231/10000] Avg train loss: 0.037678\n",
      "Epoch [1232/10000] Avg train loss: 0.037647\n",
      "Epoch [1233/10000] Avg train loss: 0.037617\n",
      "Epoch [1234/10000] Avg train loss: 0.037586\n",
      "Epoch [1235/10000] Avg train loss: 0.037556\n",
      "Epoch [1236/10000] Avg train loss: 0.037525\n",
      "Epoch [1237/10000] Avg train loss: 0.037495\n",
      "Epoch [1238/10000] Avg train loss: 0.037465\n",
      "Epoch [1239/10000] Avg train loss: 0.037434\n",
      "Epoch [1240/10000] Avg train loss: 0.037404\n",
      "Epoch [1241/10000] Avg train loss: 0.037374\n",
      "Epoch [1242/10000] Avg train loss: 0.037344\n",
      "Epoch [1243/10000] Avg train loss: 0.037314\n",
      "Epoch [1244/10000] Avg train loss: 0.037284\n",
      "Epoch [1245/10000] Avg train loss: 0.037254\n",
      "Epoch [1246/10000] Avg train loss: 0.037224\n",
      "Epoch [1247/10000] Avg train loss: 0.037194\n",
      "Epoch [1248/10000] Avg train loss: 0.037164\n",
      "Epoch [1249/10000] Avg train loss: 0.037135\n",
      "Epoch [1250/10000] Avg train loss: 0.037105\n",
      "Epoch [1251/10000] Avg train loss: 0.037075\n",
      "Epoch [1252/10000] Avg train loss: 0.037046\n",
      "Epoch [1253/10000] Avg train loss: 0.037016\n",
      "Epoch [1254/10000] Avg train loss: 0.036987\n",
      "Epoch [1255/10000] Avg train loss: 0.036957\n",
      "Epoch [1256/10000] Avg train loss: 0.036928\n",
      "Epoch [1257/10000] Avg train loss: 0.036898\n",
      "Epoch [1258/10000] Avg train loss: 0.036869\n",
      "Epoch [1259/10000] Avg train loss: 0.036840\n",
      "Epoch [1260/10000] Avg train loss: 0.036811\n",
      "Epoch [1261/10000] Avg train loss: 0.036781\n",
      "Epoch [1262/10000] Avg train loss: 0.036752\n",
      "Epoch [1263/10000] Avg train loss: 0.036723\n",
      "Epoch [1264/10000] Avg train loss: 0.036694\n",
      "Epoch [1265/10000] Avg train loss: 0.036665\n",
      "Epoch [1266/10000] Avg train loss: 0.036636\n",
      "Epoch [1267/10000] Avg train loss: 0.036607\n",
      "Epoch [1268/10000] Avg train loss: 0.036578\n",
      "Epoch [1269/10000] Avg train loss: 0.036549\n",
      "Epoch [1270/10000] Avg train loss: 0.036521\n",
      "Epoch [1271/10000] Avg train loss: 0.036492\n",
      "Epoch [1272/10000] Avg train loss: 0.036463\n",
      "Epoch [1273/10000] Avg train loss: 0.036435\n",
      "Epoch [1274/10000] Avg train loss: 0.036406\n",
      "Epoch [1275/10000] Avg train loss: 0.036377\n",
      "Epoch [1276/10000] Avg train loss: 0.036349\n",
      "Epoch [1277/10000] Avg train loss: 0.036320\n",
      "Epoch [1278/10000] Avg train loss: 0.036292\n",
      "Epoch [1279/10000] Avg train loss: 0.036264\n",
      "Epoch [1280/10000] Avg train loss: 0.036235\n",
      "Epoch [1281/10000] Avg train loss: 0.036207\n",
      "Epoch [1282/10000] Avg train loss: 0.036179\n",
      "Epoch [1283/10000] Avg train loss: 0.036151\n",
      "Epoch [1284/10000] Avg train loss: 0.036122\n",
      "Epoch [1285/10000] Avg train loss: 0.036094\n",
      "Epoch [1286/10000] Avg train loss: 0.036066\n",
      "Epoch [1287/10000] Avg train loss: 0.036038\n",
      "Epoch [1288/10000] Avg train loss: 0.036010\n",
      "Epoch [1289/10000] Avg train loss: 0.035982\n",
      "Epoch [1290/10000] Avg train loss: 0.035954\n",
      "Epoch [1291/10000] Avg train loss: 0.035927\n",
      "Epoch [1292/10000] Avg train loss: 0.035899\n",
      "Epoch [1293/10000] Avg train loss: 0.035871\n",
      "Epoch [1294/10000] Avg train loss: 0.035843\n",
      "Epoch [1295/10000] Avg train loss: 0.035816\n",
      "Epoch [1296/10000] Avg train loss: 0.035788\n",
      "Epoch [1297/10000] Avg train loss: 0.035760\n",
      "Epoch [1298/10000] Avg train loss: 0.035733\n",
      "Epoch [1299/10000] Avg train loss: 0.035705\n",
      "Epoch [1300/10000] Avg train loss: 0.035678\n",
      "Epoch [1301/10000] Avg train loss: 0.035650\n",
      "Epoch [1302/10000] Avg train loss: 0.035623\n",
      "Epoch [1303/10000] Avg train loss: 0.035596\n",
      "Epoch [1304/10000] Avg train loss: 0.035568\n",
      "Epoch [1305/10000] Avg train loss: 0.035541\n",
      "Epoch [1306/10000] Avg train loss: 0.035514\n",
      "Epoch [1307/10000] Avg train loss: 0.035487\n",
      "Epoch [1308/10000] Avg train loss: 0.035460\n",
      "Epoch [1309/10000] Avg train loss: 0.035433\n",
      "Epoch [1310/10000] Avg train loss: 0.035406\n",
      "Epoch [1311/10000] Avg train loss: 0.035379\n",
      "Epoch [1312/10000] Avg train loss: 0.035352\n",
      "Epoch [1313/10000] Avg train loss: 0.035325\n",
      "Epoch [1314/10000] Avg train loss: 0.035298\n",
      "Epoch [1315/10000] Avg train loss: 0.035271\n",
      "Epoch [1316/10000] Avg train loss: 0.035244\n",
      "Epoch [1317/10000] Avg train loss: 0.035217\n",
      "Epoch [1318/10000] Avg train loss: 0.035191\n",
      "Epoch [1319/10000] Avg train loss: 0.035164\n",
      "Epoch [1320/10000] Avg train loss: 0.035137\n",
      "Epoch [1321/10000] Avg train loss: 0.035111\n",
      "Epoch [1322/10000] Avg train loss: 0.035084\n",
      "Epoch [1323/10000] Avg train loss: 0.035058\n",
      "Epoch [1324/10000] Avg train loss: 0.035031\n",
      "Epoch [1325/10000] Avg train loss: 0.035005\n",
      "Epoch [1326/10000] Avg train loss: 0.034978\n",
      "Epoch [1327/10000] Avg train loss: 0.034952\n",
      "Epoch [1328/10000] Avg train loss: 0.034926\n",
      "Epoch [1329/10000] Avg train loss: 0.034899\n",
      "Epoch [1330/10000] Avg train loss: 0.034873\n",
      "Epoch [1331/10000] Avg train loss: 0.034847\n",
      "Epoch [1332/10000] Avg train loss: 0.034821\n",
      "Epoch [1333/10000] Avg train loss: 0.034795\n",
      "Epoch [1334/10000] Avg train loss: 0.034769\n",
      "Epoch [1335/10000] Avg train loss: 0.034743\n",
      "Epoch [1336/10000] Avg train loss: 0.034717\n",
      "Epoch [1337/10000] Avg train loss: 0.034691\n",
      "Epoch [1338/10000] Avg train loss: 0.034665\n",
      "Epoch [1339/10000] Avg train loss: 0.034639\n",
      "Epoch [1340/10000] Avg train loss: 0.034613\n",
      "Epoch [1341/10000] Avg train loss: 0.034587\n",
      "Epoch [1342/10000] Avg train loss: 0.034561\n",
      "Epoch [1343/10000] Avg train loss: 0.034536\n",
      "Epoch [1344/10000] Avg train loss: 0.034510\n",
      "Epoch [1345/10000] Avg train loss: 0.034484\n",
      "Epoch [1346/10000] Avg train loss: 0.034459\n",
      "Epoch [1347/10000] Avg train loss: 0.034433\n",
      "Epoch [1348/10000] Avg train loss: 0.034407\n",
      "Epoch [1349/10000] Avg train loss: 0.034382\n",
      "Epoch [1350/10000] Avg train loss: 0.034356\n",
      "Epoch [1351/10000] Avg train loss: 0.034331\n",
      "Epoch [1352/10000] Avg train loss: 0.034306\n",
      "Epoch [1353/10000] Avg train loss: 0.034280\n",
      "Epoch [1354/10000] Avg train loss: 0.034255\n",
      "Epoch [1355/10000] Avg train loss: 0.034230\n",
      "Epoch [1356/10000] Avg train loss: 0.034204\n",
      "Epoch [1357/10000] Avg train loss: 0.034179\n",
      "Epoch [1358/10000] Avg train loss: 0.034154\n",
      "Epoch [1359/10000] Avg train loss: 0.034129\n",
      "Epoch [1360/10000] Avg train loss: 0.034104\n",
      "Epoch [1361/10000] Avg train loss: 0.034079\n",
      "Epoch [1362/10000] Avg train loss: 0.034054\n",
      "Epoch [1363/10000] Avg train loss: 0.034029\n",
      "Epoch [1364/10000] Avg train loss: 0.034004\n",
      "Epoch [1365/10000] Avg train loss: 0.033979\n",
      "Epoch [1366/10000] Avg train loss: 0.033954\n",
      "Epoch [1367/10000] Avg train loss: 0.033929\n",
      "Epoch [1368/10000] Avg train loss: 0.033904\n",
      "Epoch [1369/10000] Avg train loss: 0.033880\n",
      "Epoch [1370/10000] Avg train loss: 0.033855\n",
      "Epoch [1371/10000] Avg train loss: 0.033830\n",
      "Epoch [1372/10000] Avg train loss: 0.033806\n",
      "Epoch [1373/10000] Avg train loss: 0.033781\n",
      "Epoch [1374/10000] Avg train loss: 0.033756\n",
      "Epoch [1375/10000] Avg train loss: 0.033732\n",
      "Epoch [1376/10000] Avg train loss: 0.033707\n",
      "Epoch [1377/10000] Avg train loss: 0.033683\n",
      "Epoch [1378/10000] Avg train loss: 0.033658\n",
      "Epoch [1379/10000] Avg train loss: 0.033634\n",
      "Epoch [1380/10000] Avg train loss: 0.033610\n",
      "Epoch [1381/10000] Avg train loss: 0.033585\n",
      "Epoch [1382/10000] Avg train loss: 0.033561\n",
      "Epoch [1383/10000] Avg train loss: 0.033537\n",
      "Epoch [1384/10000] Avg train loss: 0.033512\n",
      "Epoch [1385/10000] Avg train loss: 0.033488\n",
      "Epoch [1386/10000] Avg train loss: 0.033464\n",
      "Epoch [1387/10000] Avg train loss: 0.033440\n",
      "Epoch [1388/10000] Avg train loss: 0.033416\n",
      "Epoch [1389/10000] Avg train loss: 0.033392\n",
      "Epoch [1390/10000] Avg train loss: 0.033368\n",
      "Epoch [1391/10000] Avg train loss: 0.033344\n",
      "Epoch [1392/10000] Avg train loss: 0.033320\n",
      "Epoch [1393/10000] Avg train loss: 0.033296\n",
      "Epoch [1394/10000] Avg train loss: 0.033272\n",
      "Epoch [1395/10000] Avg train loss: 0.033248\n",
      "Epoch [1396/10000] Avg train loss: 0.033224\n",
      "Epoch [1397/10000] Avg train loss: 0.033201\n",
      "Epoch [1398/10000] Avg train loss: 0.033177\n",
      "Epoch [1399/10000] Avg train loss: 0.033153\n",
      "Epoch [1400/10000] Avg train loss: 0.033129\n",
      "Epoch [1401/10000] Avg train loss: 0.033106\n",
      "Epoch [1402/10000] Avg train loss: 0.033082\n",
      "Epoch [1403/10000] Avg train loss: 0.033059\n",
      "Epoch [1404/10000] Avg train loss: 0.033035\n",
      "Epoch [1405/10000] Avg train loss: 0.033012\n",
      "Epoch [1406/10000] Avg train loss: 0.032988\n",
      "Epoch [1407/10000] Avg train loss: 0.032965\n",
      "Epoch [1408/10000] Avg train loss: 0.032941\n",
      "Epoch [1409/10000] Avg train loss: 0.032918\n",
      "Epoch [1410/10000] Avg train loss: 0.032895\n",
      "Epoch [1411/10000] Avg train loss: 0.032871\n",
      "Epoch [1412/10000] Avg train loss: 0.032848\n",
      "Epoch [1413/10000] Avg train loss: 0.032825\n",
      "Epoch [1414/10000] Avg train loss: 0.032801\n",
      "Epoch [1415/10000] Avg train loss: 0.032778\n",
      "Epoch [1416/10000] Avg train loss: 0.032755\n",
      "Epoch [1417/10000] Avg train loss: 0.032732\n",
      "Epoch [1418/10000] Avg train loss: 0.032709\n",
      "Epoch [1419/10000] Avg train loss: 0.032686\n",
      "Epoch [1420/10000] Avg train loss: 0.032663\n",
      "Epoch [1421/10000] Avg train loss: 0.032640\n",
      "Epoch [1422/10000] Avg train loss: 0.032617\n",
      "Epoch [1423/10000] Avg train loss: 0.032594\n",
      "Epoch [1424/10000] Avg train loss: 0.032571\n",
      "Epoch [1425/10000] Avg train loss: 0.032548\n",
      "Epoch [1426/10000] Avg train loss: 0.032525\n",
      "Epoch [1427/10000] Avg train loss: 0.032503\n",
      "Epoch [1428/10000] Avg train loss: 0.032480\n",
      "Epoch [1429/10000] Avg train loss: 0.032457\n",
      "Epoch [1430/10000] Avg train loss: 0.032434\n",
      "Epoch [1431/10000] Avg train loss: 0.032412\n",
      "Epoch [1432/10000] Avg train loss: 0.032389\n",
      "Epoch [1433/10000] Avg train loss: 0.032367\n",
      "Epoch [1434/10000] Avg train loss: 0.032344\n",
      "Epoch [1435/10000] Avg train loss: 0.032321\n",
      "Epoch [1436/10000] Avg train loss: 0.032299\n",
      "Epoch [1437/10000] Avg train loss: 0.032276\n",
      "Epoch [1438/10000] Avg train loss: 0.032254\n",
      "Epoch [1439/10000] Avg train loss: 0.032232\n",
      "Epoch [1440/10000] Avg train loss: 0.032209\n",
      "Epoch [1441/10000] Avg train loss: 0.032187\n",
      "Epoch [1442/10000] Avg train loss: 0.032165\n",
      "Epoch [1443/10000] Avg train loss: 0.032142\n",
      "Epoch [1444/10000] Avg train loss: 0.032120\n",
      "Epoch [1445/10000] Avg train loss: 0.032098\n",
      "Epoch [1446/10000] Avg train loss: 0.032076\n",
      "Epoch [1447/10000] Avg train loss: 0.032053\n",
      "Epoch [1448/10000] Avg train loss: 0.032031\n",
      "Epoch [1449/10000] Avg train loss: 0.032009\n",
      "Epoch [1450/10000] Avg train loss: 0.031987\n",
      "Epoch [1451/10000] Avg train loss: 0.031965\n",
      "Epoch [1452/10000] Avg train loss: 0.031943\n",
      "Epoch [1453/10000] Avg train loss: 0.031921\n",
      "Epoch [1454/10000] Avg train loss: 0.031899\n",
      "Epoch [1455/10000] Avg train loss: 0.031877\n",
      "Epoch [1456/10000] Avg train loss: 0.031855\n",
      "Epoch [1457/10000] Avg train loss: 0.031833\n",
      "Epoch [1458/10000] Avg train loss: 0.031812\n",
      "Epoch [1459/10000] Avg train loss: 0.031790\n",
      "Epoch [1460/10000] Avg train loss: 0.031768\n",
      "Epoch [1461/10000] Avg train loss: 0.031746\n",
      "Epoch [1462/10000] Avg train loss: 0.031725\n",
      "Epoch [1463/10000] Avg train loss: 0.031703\n",
      "Epoch [1464/10000] Avg train loss: 0.031681\n",
      "Epoch [1465/10000] Avg train loss: 0.031660\n",
      "Epoch [1466/10000] Avg train loss: 0.031638\n",
      "Epoch [1467/10000] Avg train loss: 0.031616\n",
      "Epoch [1468/10000] Avg train loss: 0.031595\n",
      "Epoch [1469/10000] Avg train loss: 0.031573\n",
      "Epoch [1470/10000] Avg train loss: 0.031552\n",
      "Epoch [1471/10000] Avg train loss: 0.031530\n",
      "Epoch [1472/10000] Avg train loss: 0.031509\n",
      "Epoch [1473/10000] Avg train loss: 0.031488\n",
      "Epoch [1474/10000] Avg train loss: 0.031466\n",
      "Epoch [1475/10000] Avg train loss: 0.031445\n",
      "Epoch [1476/10000] Avg train loss: 0.031424\n",
      "Epoch [1477/10000] Avg train loss: 0.031402\n",
      "Epoch [1478/10000] Avg train loss: 0.031381\n",
      "Epoch [1479/10000] Avg train loss: 0.031360\n",
      "Epoch [1480/10000] Avg train loss: 0.031339\n",
      "Epoch [1481/10000] Avg train loss: 0.031318\n",
      "Epoch [1482/10000] Avg train loss: 0.031296\n",
      "Epoch [1483/10000] Avg train loss: 0.031275\n",
      "Epoch [1484/10000] Avg train loss: 0.031254\n",
      "Epoch [1485/10000] Avg train loss: 0.031233\n",
      "Epoch [1486/10000] Avg train loss: 0.031212\n",
      "Epoch [1487/10000] Avg train loss: 0.031191\n",
      "Epoch [1488/10000] Avg train loss: 0.031170\n",
      "Epoch [1489/10000] Avg train loss: 0.031149\n",
      "Epoch [1490/10000] Avg train loss: 0.031128\n",
      "Epoch [1491/10000] Avg train loss: 0.031107\n",
      "Epoch [1492/10000] Avg train loss: 0.031087\n",
      "Epoch [1493/10000] Avg train loss: 0.031066\n",
      "Epoch [1494/10000] Avg train loss: 0.031045\n",
      "Epoch [1495/10000] Avg train loss: 0.031024\n",
      "Epoch [1496/10000] Avg train loss: 0.031004\n",
      "Epoch [1497/10000] Avg train loss: 0.030983\n",
      "Epoch [1498/10000] Avg train loss: 0.030962\n",
      "Epoch [1499/10000] Avg train loss: 0.030941\n",
      "Epoch [1500/10000] Avg train loss: 0.030921\n",
      "Epoch [1501/10000] Avg train loss: 0.030900\n",
      "Epoch [1502/10000] Avg train loss: 0.030880\n",
      "Epoch [1503/10000] Avg train loss: 0.030859\n",
      "Epoch [1504/10000] Avg train loss: 0.030839\n",
      "Epoch [1505/10000] Avg train loss: 0.030818\n",
      "Epoch [1506/10000] Avg train loss: 0.030798\n",
      "Epoch [1507/10000] Avg train loss: 0.030777\n",
      "Epoch [1508/10000] Avg train loss: 0.030757\n",
      "Epoch [1509/10000] Avg train loss: 0.030736\n",
      "Epoch [1510/10000] Avg train loss: 0.030716\n",
      "Epoch [1511/10000] Avg train loss: 0.030696\n",
      "Epoch [1512/10000] Avg train loss: 0.030675\n",
      "Epoch [1513/10000] Avg train loss: 0.030655\n",
      "Epoch [1514/10000] Avg train loss: 0.030635\n",
      "Epoch [1515/10000] Avg train loss: 0.030615\n",
      "Epoch [1516/10000] Avg train loss: 0.030594\n",
      "Epoch [1517/10000] Avg train loss: 0.030574\n",
      "Epoch [1518/10000] Avg train loss: 0.030554\n",
      "Epoch [1519/10000] Avg train loss: 0.030534\n",
      "Epoch [1520/10000] Avg train loss: 0.030514\n",
      "Epoch [1521/10000] Avg train loss: 0.030494\n",
      "Epoch [1522/10000] Avg train loss: 0.030474\n",
      "Epoch [1523/10000] Avg train loss: 0.030454\n",
      "Epoch [1524/10000] Avg train loss: 0.030434\n",
      "Epoch [1525/10000] Avg train loss: 0.030414\n",
      "Epoch [1526/10000] Avg train loss: 0.030394\n",
      "Epoch [1527/10000] Avg train loss: 0.030374\n",
      "Epoch [1528/10000] Avg train loss: 0.030354\n",
      "Epoch [1529/10000] Avg train loss: 0.030334\n",
      "Epoch [1530/10000] Avg train loss: 0.030315\n",
      "Epoch [1531/10000] Avg train loss: 0.030295\n",
      "Epoch [1532/10000] Avg train loss: 0.030275\n",
      "Epoch [1533/10000] Avg train loss: 0.030255\n",
      "Epoch [1534/10000] Avg train loss: 0.030236\n",
      "Epoch [1535/10000] Avg train loss: 0.030216\n",
      "Epoch [1536/10000] Avg train loss: 0.030196\n",
      "Epoch [1537/10000] Avg train loss: 0.030176\n",
      "Epoch [1538/10000] Avg train loss: 0.030157\n",
      "Epoch [1539/10000] Avg train loss: 0.030137\n",
      "Epoch [1540/10000] Avg train loss: 0.030118\n",
      "Epoch [1541/10000] Avg train loss: 0.030098\n",
      "Epoch [1542/10000] Avg train loss: 0.030079\n",
      "Epoch [1543/10000] Avg train loss: 0.030059\n",
      "Epoch [1544/10000] Avg train loss: 0.030040\n",
      "Epoch [1545/10000] Avg train loss: 0.030020\n",
      "Epoch [1546/10000] Avg train loss: 0.030001\n",
      "Epoch [1547/10000] Avg train loss: 0.029981\n",
      "Epoch [1548/10000] Avg train loss: 0.029962\n",
      "Epoch [1549/10000] Avg train loss: 0.029943\n",
      "Epoch [1550/10000] Avg train loss: 0.029923\n",
      "Epoch [1551/10000] Avg train loss: 0.029904\n",
      "Epoch [1552/10000] Avg train loss: 0.029885\n",
      "Epoch [1553/10000] Avg train loss: 0.029866\n",
      "Epoch [1554/10000] Avg train loss: 0.029846\n",
      "Epoch [1555/10000] Avg train loss: 0.029827\n",
      "Epoch [1556/10000] Avg train loss: 0.029808\n",
      "Epoch [1557/10000] Avg train loss: 0.029789\n",
      "Epoch [1558/10000] Avg train loss: 0.029770\n",
      "Epoch [1559/10000] Avg train loss: 0.029751\n",
      "Epoch [1560/10000] Avg train loss: 0.029732\n",
      "Epoch [1561/10000] Avg train loss: 0.029713\n",
      "Epoch [1562/10000] Avg train loss: 0.029694\n",
      "Epoch [1563/10000] Avg train loss: 0.029675\n",
      "Epoch [1564/10000] Avg train loss: 0.029656\n",
      "Epoch [1565/10000] Avg train loss: 0.029637\n",
      "Epoch [1566/10000] Avg train loss: 0.029618\n",
      "Epoch [1567/10000] Avg train loss: 0.029599\n",
      "Epoch [1568/10000] Avg train loss: 0.029580\n",
      "Epoch [1569/10000] Avg train loss: 0.029561\n",
      "Epoch [1570/10000] Avg train loss: 0.029542\n",
      "Epoch [1571/10000] Avg train loss: 0.029523\n",
      "Epoch [1572/10000] Avg train loss: 0.029505\n",
      "Epoch [1573/10000] Avg train loss: 0.029486\n",
      "Epoch [1574/10000] Avg train loss: 0.029467\n",
      "Epoch [1575/10000] Avg train loss: 0.029448\n",
      "Epoch [1576/10000] Avg train loss: 0.029430\n",
      "Epoch [1577/10000] Avg train loss: 0.029411\n",
      "Epoch [1578/10000] Avg train loss: 0.029392\n",
      "Epoch [1579/10000] Avg train loss: 0.029374\n",
      "Epoch [1580/10000] Avg train loss: 0.029355\n",
      "Epoch [1581/10000] Avg train loss: 0.029337\n",
      "Epoch [1582/10000] Avg train loss: 0.029318\n",
      "Epoch [1583/10000] Avg train loss: 0.029300\n",
      "Epoch [1584/10000] Avg train loss: 0.029281\n",
      "Epoch [1585/10000] Avg train loss: 0.029263\n",
      "Epoch [1586/10000] Avg train loss: 0.029244\n",
      "Epoch [1587/10000] Avg train loss: 0.029226\n",
      "Epoch [1588/10000] Avg train loss: 0.029207\n",
      "Epoch [1589/10000] Avg train loss: 0.029189\n",
      "Epoch [1590/10000] Avg train loss: 0.029171\n",
      "Epoch [1591/10000] Avg train loss: 0.029152\n",
      "Epoch [1592/10000] Avg train loss: 0.029134\n",
      "Epoch [1593/10000] Avg train loss: 0.029116\n",
      "Epoch [1594/10000] Avg train loss: 0.029097\n",
      "Epoch [1595/10000] Avg train loss: 0.029079\n",
      "Epoch [1596/10000] Avg train loss: 0.029061\n",
      "Epoch [1597/10000] Avg train loss: 0.029043\n",
      "Epoch [1598/10000] Avg train loss: 0.029025\n",
      "Epoch [1599/10000] Avg train loss: 0.029006\n",
      "Epoch [1600/10000] Avg train loss: 0.028988\n",
      "Epoch [1601/10000] Avg train loss: 0.028970\n",
      "Epoch [1602/10000] Avg train loss: 0.028952\n",
      "Epoch [1603/10000] Avg train loss: 0.028934\n",
      "Epoch [1604/10000] Avg train loss: 0.028916\n",
      "Epoch [1605/10000] Avg train loss: 0.028898\n",
      "Epoch [1606/10000] Avg train loss: 0.028880\n",
      "Epoch [1607/10000] Avg train loss: 0.028862\n",
      "Epoch [1608/10000] Avg train loss: 0.028844\n",
      "Epoch [1609/10000] Avg train loss: 0.028826\n",
      "Epoch [1610/10000] Avg train loss: 0.028808\n",
      "Epoch [1611/10000] Avg train loss: 0.028790\n",
      "Epoch [1612/10000] Avg train loss: 0.028772\n",
      "Epoch [1613/10000] Avg train loss: 0.028755\n",
      "Epoch [1614/10000] Avg train loss: 0.028737\n",
      "Epoch [1615/10000] Avg train loss: 0.028719\n",
      "Epoch [1616/10000] Avg train loss: 0.028701\n",
      "Epoch [1617/10000] Avg train loss: 0.028684\n",
      "Epoch [1618/10000] Avg train loss: 0.028666\n",
      "Epoch [1619/10000] Avg train loss: 0.028648\n",
      "Epoch [1620/10000] Avg train loss: 0.028630\n",
      "Epoch [1621/10000] Avg train loss: 0.028613\n",
      "Epoch [1622/10000] Avg train loss: 0.028595\n",
      "Epoch [1623/10000] Avg train loss: 0.028577\n",
      "Epoch [1624/10000] Avg train loss: 0.028560\n",
      "Epoch [1625/10000] Avg train loss: 0.028542\n",
      "Epoch [1626/10000] Avg train loss: 0.028525\n",
      "Epoch [1627/10000] Avg train loss: 0.028507\n",
      "Epoch [1628/10000] Avg train loss: 0.028490\n",
      "Epoch [1629/10000] Avg train loss: 0.028472\n",
      "Epoch [1630/10000] Avg train loss: 0.028455\n",
      "Epoch [1631/10000] Avg train loss: 0.028437\n",
      "Epoch [1632/10000] Avg train loss: 0.028420\n",
      "Epoch [1633/10000] Avg train loss: 0.028402\n",
      "Epoch [1634/10000] Avg train loss: 0.028385\n",
      "Epoch [1635/10000] Avg train loss: 0.028368\n",
      "Epoch [1636/10000] Avg train loss: 0.028350\n",
      "Epoch [1637/10000] Avg train loss: 0.028333\n",
      "Epoch [1638/10000] Avg train loss: 0.028316\n",
      "Epoch [1639/10000] Avg train loss: 0.028299\n",
      "Epoch [1640/10000] Avg train loss: 0.028281\n",
      "Epoch [1641/10000] Avg train loss: 0.028264\n",
      "Epoch [1642/10000] Avg train loss: 0.028247\n",
      "Epoch [1643/10000] Avg train loss: 0.028230\n",
      "Epoch [1644/10000] Avg train loss: 0.028212\n",
      "Epoch [1645/10000] Avg train loss: 0.028195\n",
      "Epoch [1646/10000] Avg train loss: 0.028178\n",
      "Epoch [1647/10000] Avg train loss: 0.028161\n",
      "Epoch [1648/10000] Avg train loss: 0.028144\n",
      "Epoch [1649/10000] Avg train loss: 0.028127\n",
      "Epoch [1650/10000] Avg train loss: 0.028110\n",
      "Epoch [1651/10000] Avg train loss: 0.028093\n",
      "Epoch [1652/10000] Avg train loss: 0.028076\n",
      "Epoch [1653/10000] Avg train loss: 0.028059\n",
      "Epoch [1654/10000] Avg train loss: 0.028042\n",
      "Epoch [1655/10000] Avg train loss: 0.028025\n",
      "Epoch [1656/10000] Avg train loss: 0.028008\n",
      "Epoch [1657/10000] Avg train loss: 0.027991\n",
      "Epoch [1658/10000] Avg train loss: 0.027974\n",
      "Epoch [1659/10000] Avg train loss: 0.027957\n",
      "Epoch [1660/10000] Avg train loss: 0.027941\n",
      "Epoch [1661/10000] Avg train loss: 0.027924\n",
      "Epoch [1662/10000] Avg train loss: 0.027907\n",
      "Epoch [1663/10000] Avg train loss: 0.027890\n",
      "Epoch [1664/10000] Avg train loss: 0.027873\n",
      "Epoch [1665/10000] Avg train loss: 0.027857\n",
      "Epoch [1666/10000] Avg train loss: 0.027840\n",
      "Epoch [1667/10000] Avg train loss: 0.027823\n",
      "Epoch [1668/10000] Avg train loss: 0.027807\n",
      "Epoch [1669/10000] Avg train loss: 0.027790\n",
      "Epoch [1670/10000] Avg train loss: 0.027773\n",
      "Epoch [1671/10000] Avg train loss: 0.027757\n",
      "Epoch [1672/10000] Avg train loss: 0.027740\n",
      "Epoch [1673/10000] Avg train loss: 0.027723\n",
      "Epoch [1674/10000] Avg train loss: 0.027707\n",
      "Epoch [1675/10000] Avg train loss: 0.027690\n",
      "Epoch [1676/10000] Avg train loss: 0.027674\n",
      "Epoch [1677/10000] Avg train loss: 0.027657\n",
      "Epoch [1678/10000] Avg train loss: 0.027641\n",
      "Epoch [1679/10000] Avg train loss: 0.027624\n",
      "Epoch [1680/10000] Avg train loss: 0.027608\n",
      "Epoch [1681/10000] Avg train loss: 0.027591\n",
      "Epoch [1682/10000] Avg train loss: 0.027575\n",
      "Epoch [1683/10000] Avg train loss: 0.027559\n",
      "Epoch [1684/10000] Avg train loss: 0.027542\n",
      "Epoch [1685/10000] Avg train loss: 0.027526\n",
      "Epoch [1686/10000] Avg train loss: 0.027510\n",
      "Epoch [1687/10000] Avg train loss: 0.027493\n",
      "Epoch [1688/10000] Avg train loss: 0.027477\n",
      "Epoch [1689/10000] Avg train loss: 0.027461\n",
      "Epoch [1690/10000] Avg train loss: 0.027445\n",
      "Epoch [1691/10000] Avg train loss: 0.027428\n",
      "Epoch [1692/10000] Avg train loss: 0.027412\n",
      "Epoch [1693/10000] Avg train loss: 0.027396\n",
      "Epoch [1694/10000] Avg train loss: 0.027380\n",
      "Epoch [1695/10000] Avg train loss: 0.027364\n",
      "Epoch [1696/10000] Avg train loss: 0.027347\n",
      "Epoch [1697/10000] Avg train loss: 0.027331\n",
      "Epoch [1698/10000] Avg train loss: 0.027315\n",
      "Epoch [1699/10000] Avg train loss: 0.027299\n",
      "Epoch [1700/10000] Avg train loss: 0.027283\n",
      "Epoch [1701/10000] Avg train loss: 0.027267\n",
      "Epoch [1702/10000] Avg train loss: 0.027251\n",
      "Epoch [1703/10000] Avg train loss: 0.027235\n",
      "Epoch [1704/10000] Avg train loss: 0.027219\n",
      "Epoch [1705/10000] Avg train loss: 0.027203\n",
      "Epoch [1706/10000] Avg train loss: 0.027187\n",
      "Epoch [1707/10000] Avg train loss: 0.027171\n",
      "Epoch [1708/10000] Avg train loss: 0.027155\n",
      "Epoch [1709/10000] Avg train loss: 0.027139\n",
      "Epoch [1710/10000] Avg train loss: 0.027124\n",
      "Epoch [1711/10000] Avg train loss: 0.027108\n",
      "Epoch [1712/10000] Avg train loss: 0.027092\n",
      "Epoch [1713/10000] Avg train loss: 0.027076\n",
      "Epoch [1714/10000] Avg train loss: 0.027060\n",
      "Epoch [1715/10000] Avg train loss: 0.027044\n",
      "Epoch [1716/10000] Avg train loss: 0.027029\n",
      "Epoch [1717/10000] Avg train loss: 0.027013\n",
      "Epoch [1718/10000] Avg train loss: 0.026997\n",
      "Epoch [1719/10000] Avg train loss: 0.026982\n",
      "Epoch [1720/10000] Avg train loss: 0.026966\n",
      "Epoch [1721/10000] Avg train loss: 0.026950\n",
      "Epoch [1722/10000] Avg train loss: 0.026935\n",
      "Epoch [1723/10000] Avg train loss: 0.026919\n",
      "Epoch [1724/10000] Avg train loss: 0.026903\n",
      "Epoch [1725/10000] Avg train loss: 0.026888\n",
      "Epoch [1726/10000] Avg train loss: 0.026872\n",
      "Epoch [1727/10000] Avg train loss: 0.026857\n",
      "Epoch [1728/10000] Avg train loss: 0.026841\n",
      "Epoch [1729/10000] Avg train loss: 0.026825\n",
      "Epoch [1730/10000] Avg train loss: 0.026810\n",
      "Epoch [1731/10000] Avg train loss: 0.026794\n",
      "Epoch [1732/10000] Avg train loss: 0.026779\n",
      "Epoch [1733/10000] Avg train loss: 0.026764\n",
      "Epoch [1734/10000] Avg train loss: 0.026748\n",
      "Epoch [1735/10000] Avg train loss: 0.026733\n",
      "Epoch [1736/10000] Avg train loss: 0.026717\n",
      "Epoch [1737/10000] Avg train loss: 0.026702\n",
      "Epoch [1738/10000] Avg train loss: 0.026687\n",
      "Epoch [1739/10000] Avg train loss: 0.026671\n",
      "Epoch [1740/10000] Avg train loss: 0.026656\n",
      "Epoch [1741/10000] Avg train loss: 0.026641\n",
      "Epoch [1742/10000] Avg train loss: 0.026625\n",
      "Epoch [1743/10000] Avg train loss: 0.026610\n",
      "Epoch [1744/10000] Avg train loss: 0.026595\n",
      "Epoch [1745/10000] Avg train loss: 0.026580\n",
      "Epoch [1746/10000] Avg train loss: 0.026564\n",
      "Epoch [1747/10000] Avg train loss: 0.026549\n",
      "Epoch [1748/10000] Avg train loss: 0.026534\n",
      "Epoch [1749/10000] Avg train loss: 0.026519\n",
      "Epoch [1750/10000] Avg train loss: 0.026504\n",
      "Epoch [1751/10000] Avg train loss: 0.026488\n",
      "Epoch [1752/10000] Avg train loss: 0.026473\n",
      "Epoch [1753/10000] Avg train loss: 0.026458\n",
      "Epoch [1754/10000] Avg train loss: 0.026443\n",
      "Epoch [1755/10000] Avg train loss: 0.026428\n",
      "Epoch [1756/10000] Avg train loss: 0.026413\n",
      "Epoch [1757/10000] Avg train loss: 0.026398\n",
      "Epoch [1758/10000] Avg train loss: 0.026383\n",
      "Epoch [1759/10000] Avg train loss: 0.026368\n",
      "Epoch [1760/10000] Avg train loss: 0.026353\n",
      "Epoch [1761/10000] Avg train loss: 0.026338\n",
      "Epoch [1762/10000] Avg train loss: 0.026323\n",
      "Epoch [1763/10000] Avg train loss: 0.026308\n",
      "Epoch [1764/10000] Avg train loss: 0.026293\n",
      "Epoch [1765/10000] Avg train loss: 0.026278\n",
      "Epoch [1766/10000] Avg train loss: 0.026263\n",
      "Epoch [1767/10000] Avg train loss: 0.026249\n",
      "Epoch [1768/10000] Avg train loss: 0.026234\n",
      "Epoch [1769/10000] Avg train loss: 0.026219\n",
      "Epoch [1770/10000] Avg train loss: 0.026204\n",
      "Epoch [1771/10000] Avg train loss: 0.026189\n",
      "Epoch [1772/10000] Avg train loss: 0.026175\n",
      "Epoch [1773/10000] Avg train loss: 0.026160\n",
      "Epoch [1774/10000] Avg train loss: 0.026145\n",
      "Epoch [1775/10000] Avg train loss: 0.026130\n",
      "Epoch [1776/10000] Avg train loss: 0.026116\n",
      "Epoch [1777/10000] Avg train loss: 0.026101\n",
      "Epoch [1778/10000] Avg train loss: 0.026086\n",
      "Epoch [1779/10000] Avg train loss: 0.026072\n",
      "Epoch [1780/10000] Avg train loss: 0.026057\n",
      "Epoch [1781/10000] Avg train loss: 0.026042\n",
      "Epoch [1782/10000] Avg train loss: 0.026028\n",
      "Epoch [1783/10000] Avg train loss: 0.026013\n",
      "Epoch [1784/10000] Avg train loss: 0.025998\n",
      "Epoch [1785/10000] Avg train loss: 0.025984\n",
      "Epoch [1786/10000] Avg train loss: 0.025969\n",
      "Epoch [1787/10000] Avg train loss: 0.025955\n",
      "Epoch [1788/10000] Avg train loss: 0.025940\n",
      "Epoch [1789/10000] Avg train loss: 0.025926\n",
      "Epoch [1790/10000] Avg train loss: 0.025911\n",
      "Epoch [1791/10000] Avg train loss: 0.025897\n",
      "Epoch [1792/10000] Avg train loss: 0.025882\n",
      "Epoch [1793/10000] Avg train loss: 0.025868\n",
      "Epoch [1794/10000] Avg train loss: 0.025854\n",
      "Epoch [1795/10000] Avg train loss: 0.025839\n",
      "Epoch [1796/10000] Avg train loss: 0.025825\n",
      "Epoch [1797/10000] Avg train loss: 0.025810\n",
      "Epoch [1798/10000] Avg train loss: 0.025796\n",
      "Epoch [1799/10000] Avg train loss: 0.025782\n",
      "Epoch [1800/10000] Avg train loss: 0.025767\n",
      "Epoch [1801/10000] Avg train loss: 0.025753\n",
      "Epoch [1802/10000] Avg train loss: 0.025739\n",
      "Epoch [1803/10000] Avg train loss: 0.025724\n",
      "Epoch [1804/10000] Avg train loss: 0.025710\n",
      "Epoch [1805/10000] Avg train loss: 0.025696\n",
      "Epoch [1806/10000] Avg train loss: 0.025682\n",
      "Epoch [1807/10000] Avg train loss: 0.025668\n",
      "Epoch [1808/10000] Avg train loss: 0.025653\n",
      "Epoch [1809/10000] Avg train loss: 0.025639\n",
      "Epoch [1810/10000] Avg train loss: 0.025625\n",
      "Epoch [1811/10000] Avg train loss: 0.025611\n",
      "Epoch [1812/10000] Avg train loss: 0.025597\n",
      "Epoch [1813/10000] Avg train loss: 0.025583\n",
      "Epoch [1814/10000] Avg train loss: 0.025569\n",
      "Epoch [1815/10000] Avg train loss: 0.025554\n",
      "Epoch [1816/10000] Avg train loss: 0.025540\n",
      "Epoch [1817/10000] Avg train loss: 0.025526\n",
      "Epoch [1818/10000] Avg train loss: 0.025512\n",
      "Epoch [1819/10000] Avg train loss: 0.025498\n",
      "Epoch [1820/10000] Avg train loss: 0.025484\n",
      "Epoch [1821/10000] Avg train loss: 0.025470\n",
      "Epoch [1822/10000] Avg train loss: 0.025456\n",
      "Epoch [1823/10000] Avg train loss: 0.025442\n",
      "Epoch [1824/10000] Avg train loss: 0.025428\n",
      "Epoch [1825/10000] Avg train loss: 0.025414\n",
      "Epoch [1826/10000] Avg train loss: 0.025400\n",
      "Epoch [1827/10000] Avg train loss: 0.025387\n",
      "Epoch [1828/10000] Avg train loss: 0.025373\n",
      "Epoch [1829/10000] Avg train loss: 0.025359\n",
      "Epoch [1830/10000] Avg train loss: 0.025345\n",
      "Epoch [1831/10000] Avg train loss: 0.025331\n",
      "Epoch [1832/10000] Avg train loss: 0.025317\n",
      "Epoch [1833/10000] Avg train loss: 0.025303\n",
      "Epoch [1834/10000] Avg train loss: 0.025290\n",
      "Epoch [1835/10000] Avg train loss: 0.025276\n",
      "Epoch [1836/10000] Avg train loss: 0.025262\n",
      "Epoch [1837/10000] Avg train loss: 0.025248\n",
      "Epoch [1838/10000] Avg train loss: 0.025235\n",
      "Epoch [1839/10000] Avg train loss: 0.025221\n",
      "Epoch [1840/10000] Avg train loss: 0.025207\n",
      "Epoch [1841/10000] Avg train loss: 0.025194\n",
      "Epoch [1842/10000] Avg train loss: 0.025180\n",
      "Epoch [1843/10000] Avg train loss: 0.025166\n",
      "Epoch [1844/10000] Avg train loss: 0.025153\n",
      "Epoch [1845/10000] Avg train loss: 0.025139\n",
      "Epoch [1846/10000] Avg train loss: 0.025125\n",
      "Epoch [1847/10000] Avg train loss: 0.025112\n",
      "Epoch [1848/10000] Avg train loss: 0.025098\n",
      "Epoch [1849/10000] Avg train loss: 0.025085\n",
      "Epoch [1850/10000] Avg train loss: 0.025071\n",
      "Epoch [1851/10000] Avg train loss: 0.025057\n",
      "Epoch [1852/10000] Avg train loss: 0.025044\n",
      "Epoch [1853/10000] Avg train loss: 0.025030\n",
      "Epoch [1854/10000] Avg train loss: 0.025017\n",
      "Epoch [1855/10000] Avg train loss: 0.025003\n",
      "Epoch [1856/10000] Avg train loss: 0.024990\n",
      "Epoch [1857/10000] Avg train loss: 0.024976\n",
      "Epoch [1858/10000] Avg train loss: 0.024963\n",
      "Epoch [1859/10000] Avg train loss: 0.024950\n",
      "Epoch [1860/10000] Avg train loss: 0.024936\n",
      "Epoch [1861/10000] Avg train loss: 0.024923\n",
      "Epoch [1862/10000] Avg train loss: 0.024909\n",
      "Epoch [1863/10000] Avg train loss: 0.024896\n",
      "Epoch [1864/10000] Avg train loss: 0.024883\n",
      "Epoch [1865/10000] Avg train loss: 0.024869\n",
      "Epoch [1866/10000] Avg train loss: 0.024856\n",
      "Epoch [1867/10000] Avg train loss: 0.024843\n",
      "Epoch [1868/10000] Avg train loss: 0.024829\n",
      "Epoch [1869/10000] Avg train loss: 0.024816\n",
      "Epoch [1870/10000] Avg train loss: 0.024803\n",
      "Epoch [1871/10000] Avg train loss: 0.024790\n",
      "Epoch [1872/10000] Avg train loss: 0.024776\n",
      "Epoch [1873/10000] Avg train loss: 0.024763\n",
      "Epoch [1874/10000] Avg train loss: 0.024750\n",
      "Epoch [1875/10000] Avg train loss: 0.024737\n",
      "Epoch [1876/10000] Avg train loss: 0.024723\n",
      "Epoch [1877/10000] Avg train loss: 0.024710\n",
      "Epoch [1878/10000] Avg train loss: 0.024697\n",
      "Epoch [1879/10000] Avg train loss: 0.024684\n",
      "Epoch [1880/10000] Avg train loss: 0.024671\n",
      "Epoch [1881/10000] Avg train loss: 0.024658\n",
      "Epoch [1882/10000] Avg train loss: 0.024645\n",
      "Epoch [1883/10000] Avg train loss: 0.024632\n",
      "Epoch [1884/10000] Avg train loss: 0.024619\n",
      "Epoch [1885/10000] Avg train loss: 0.024605\n",
      "Epoch [1886/10000] Avg train loss: 0.024592\n",
      "Epoch [1887/10000] Avg train loss: 0.024579\n",
      "Epoch [1888/10000] Avg train loss: 0.024566\n",
      "Epoch [1889/10000] Avg train loss: 0.024553\n",
      "Epoch [1890/10000] Avg train loss: 0.024540\n",
      "Epoch [1891/10000] Avg train loss: 0.024527\n",
      "Epoch [1892/10000] Avg train loss: 0.024514\n",
      "Epoch [1893/10000] Avg train loss: 0.024501\n",
      "Epoch [1894/10000] Avg train loss: 0.024489\n",
      "Epoch [1895/10000] Avg train loss: 0.024476\n",
      "Epoch [1896/10000] Avg train loss: 0.024463\n",
      "Epoch [1897/10000] Avg train loss: 0.024450\n",
      "Epoch [1898/10000] Avg train loss: 0.024437\n",
      "Epoch [1899/10000] Avg train loss: 0.024424\n",
      "Epoch [1900/10000] Avg train loss: 0.024411\n",
      "Epoch [1901/10000] Avg train loss: 0.024398\n",
      "Epoch [1902/10000] Avg train loss: 0.024386\n",
      "Epoch [1903/10000] Avg train loss: 0.024373\n",
      "Epoch [1904/10000] Avg train loss: 0.024360\n",
      "Epoch [1905/10000] Avg train loss: 0.024347\n",
      "Epoch [1906/10000] Avg train loss: 0.024334\n",
      "Epoch [1907/10000] Avg train loss: 0.024322\n",
      "Epoch [1908/10000] Avg train loss: 0.024309\n",
      "Epoch [1909/10000] Avg train loss: 0.024296\n",
      "Epoch [1910/10000] Avg train loss: 0.024283\n",
      "Epoch [1911/10000] Avg train loss: 0.024271\n",
      "Epoch [1912/10000] Avg train loss: 0.024258\n",
      "Epoch [1913/10000] Avg train loss: 0.024245\n",
      "Epoch [1914/10000] Avg train loss: 0.024233\n",
      "Epoch [1915/10000] Avg train loss: 0.024220\n",
      "Epoch [1916/10000] Avg train loss: 0.024207\n",
      "Epoch [1917/10000] Avg train loss: 0.024195\n",
      "Epoch [1918/10000] Avg train loss: 0.024182\n",
      "Epoch [1919/10000] Avg train loss: 0.024169\n",
      "Epoch [1920/10000] Avg train loss: 0.024157\n",
      "Epoch [1921/10000] Avg train loss: 0.024144\n",
      "Epoch [1922/10000] Avg train loss: 0.024132\n",
      "Epoch [1923/10000] Avg train loss: 0.024119\n",
      "Epoch [1924/10000] Avg train loss: 0.024107\n",
      "Epoch [1925/10000] Avg train loss: 0.024094\n",
      "Epoch [1926/10000] Avg train loss: 0.024082\n",
      "Epoch [1927/10000] Avg train loss: 0.024069\n",
      "Epoch [1928/10000] Avg train loss: 0.024057\n",
      "Epoch [1929/10000] Avg train loss: 0.024044\n",
      "Epoch [1930/10000] Avg train loss: 0.024032\n",
      "Epoch [1931/10000] Avg train loss: 0.024019\n",
      "Epoch [1932/10000] Avg train loss: 0.024007\n",
      "Epoch [1933/10000] Avg train loss: 0.023994\n",
      "Epoch [1934/10000] Avg train loss: 0.023982\n",
      "Epoch [1935/10000] Avg train loss: 0.023970\n",
      "Epoch [1936/10000] Avg train loss: 0.023957\n",
      "Epoch [1937/10000] Avg train loss: 0.023945\n",
      "Epoch [1938/10000] Avg train loss: 0.023933\n",
      "Epoch [1939/10000] Avg train loss: 0.023920\n",
      "Epoch [1940/10000] Avg train loss: 0.023908\n",
      "Epoch [1941/10000] Avg train loss: 0.023896\n",
      "Epoch [1942/10000] Avg train loss: 0.023883\n",
      "Epoch [1943/10000] Avg train loss: 0.023871\n",
      "Epoch [1944/10000] Avg train loss: 0.023859\n",
      "Epoch [1945/10000] Avg train loss: 0.023846\n",
      "Epoch [1946/10000] Avg train loss: 0.023834\n",
      "Epoch [1947/10000] Avg train loss: 0.023822\n",
      "Epoch [1948/10000] Avg train loss: 0.023810\n",
      "Epoch [1949/10000] Avg train loss: 0.023797\n",
      "Epoch [1950/10000] Avg train loss: 0.023785\n",
      "Epoch [1951/10000] Avg train loss: 0.023773\n",
      "Epoch [1952/10000] Avg train loss: 0.023761\n",
      "Epoch [1953/10000] Avg train loss: 0.023749\n",
      "Epoch [1954/10000] Avg train loss: 0.023737\n",
      "Epoch [1955/10000] Avg train loss: 0.023724\n",
      "Epoch [1956/10000] Avg train loss: 0.023712\n",
      "Epoch [1957/10000] Avg train loss: 0.023700\n",
      "Epoch [1958/10000] Avg train loss: 0.023688\n",
      "Epoch [1959/10000] Avg train loss: 0.023676\n",
      "Epoch [1960/10000] Avg train loss: 0.023664\n",
      "Epoch [1961/10000] Avg train loss: 0.023652\n",
      "Epoch [1962/10000] Avg train loss: 0.023640\n",
      "Epoch [1963/10000] Avg train loss: 0.023628\n",
      "Epoch [1964/10000] Avg train loss: 0.023616\n",
      "Epoch [1965/10000] Avg train loss: 0.023604\n",
      "Epoch [1966/10000] Avg train loss: 0.023592\n",
      "Epoch [1967/10000] Avg train loss: 0.023580\n",
      "Epoch [1968/10000] Avg train loss: 0.023568\n",
      "Epoch [1969/10000] Avg train loss: 0.023556\n",
      "Epoch [1970/10000] Avg train loss: 0.023544\n",
      "Epoch [1971/10000] Avg train loss: 0.023532\n",
      "Epoch [1972/10000] Avg train loss: 0.023520\n",
      "Epoch [1973/10000] Avg train loss: 0.023508\n",
      "Epoch [1974/10000] Avg train loss: 0.023496\n",
      "Epoch [1975/10000] Avg train loss: 0.023484\n",
      "Epoch [1976/10000] Avg train loss: 0.023472\n",
      "Epoch [1977/10000] Avg train loss: 0.023460\n",
      "Epoch [1978/10000] Avg train loss: 0.023449\n",
      "Epoch [1979/10000] Avg train loss: 0.023437\n",
      "Epoch [1980/10000] Avg train loss: 0.023425\n",
      "Epoch [1981/10000] Avg train loss: 0.023413\n",
      "Epoch [1982/10000] Avg train loss: 0.023401\n",
      "Epoch [1983/10000] Avg train loss: 0.023389\n",
      "Epoch [1984/10000] Avg train loss: 0.023378\n",
      "Epoch [1985/10000] Avg train loss: 0.023366\n",
      "Epoch [1986/10000] Avg train loss: 0.023354\n",
      "Epoch [1987/10000] Avg train loss: 0.023342\n",
      "Epoch [1988/10000] Avg train loss: 0.023331\n",
      "Epoch [1989/10000] Avg train loss: 0.023319\n",
      "Epoch [1990/10000] Avg train loss: 0.023307\n",
      "Epoch [1991/10000] Avg train loss: 0.023295\n",
      "Epoch [1992/10000] Avg train loss: 0.023284\n",
      "Epoch [1993/10000] Avg train loss: 0.023272\n",
      "Epoch [1994/10000] Avg train loss: 0.023260\n",
      "Epoch [1995/10000] Avg train loss: 0.023249\n",
      "Epoch [1996/10000] Avg train loss: 0.023237\n",
      "Epoch [1997/10000] Avg train loss: 0.023225\n",
      "Epoch [1998/10000] Avg train loss: 0.023214\n",
      "Epoch [1999/10000] Avg train loss: 0.023202\n",
      "Epoch [2000/10000] Avg train loss: 0.023191\n",
      "Epoch [2001/10000] Avg train loss: 0.023179\n",
      "Epoch [2002/10000] Avg train loss: 0.023167\n",
      "Epoch [2003/10000] Avg train loss: 0.023156\n",
      "Epoch [2004/10000] Avg train loss: 0.023144\n",
      "Epoch [2005/10000] Avg train loss: 0.023133\n",
      "Epoch [2006/10000] Avg train loss: 0.023121\n",
      "Epoch [2007/10000] Avg train loss: 0.023110\n",
      "Epoch [2008/10000] Avg train loss: 0.023098\n",
      "Epoch [2009/10000] Avg train loss: 0.023087\n",
      "Epoch [2010/10000] Avg train loss: 0.023075\n",
      "Epoch [2011/10000] Avg train loss: 0.023064\n",
      "Epoch [2012/10000] Avg train loss: 0.023052\n",
      "Epoch [2013/10000] Avg train loss: 0.023041\n",
      "Epoch [2014/10000] Avg train loss: 0.023029\n",
      "Epoch [2015/10000] Avg train loss: 0.023018\n",
      "Epoch [2016/10000] Avg train loss: 0.023007\n",
      "Epoch [2017/10000] Avg train loss: 0.022995\n",
      "Epoch [2018/10000] Avg train loss: 0.022984\n",
      "Epoch [2019/10000] Avg train loss: 0.022972\n",
      "Epoch [2020/10000] Avg train loss: 0.022961\n",
      "Epoch [2021/10000] Avg train loss: 0.022950\n",
      "Epoch [2022/10000] Avg train loss: 0.022938\n",
      "Epoch [2023/10000] Avg train loss: 0.022927\n",
      "Epoch [2024/10000] Avg train loss: 0.022916\n",
      "Epoch [2025/10000] Avg train loss: 0.022904\n",
      "Epoch [2026/10000] Avg train loss: 0.022893\n",
      "Epoch [2027/10000] Avg train loss: 0.022882\n",
      "Epoch [2028/10000] Avg train loss: 0.022870\n",
      "Epoch [2029/10000] Avg train loss: 0.022859\n",
      "Epoch [2030/10000] Avg train loss: 0.022848\n",
      "Epoch [2031/10000] Avg train loss: 0.022837\n",
      "Epoch [2032/10000] Avg train loss: 0.022825\n",
      "Epoch [2033/10000] Avg train loss: 0.022814\n",
      "Epoch [2034/10000] Avg train loss: 0.022803\n",
      "Epoch [2035/10000] Avg train loss: 0.022792\n",
      "Epoch [2036/10000] Avg train loss: 0.022781\n",
      "Epoch [2037/10000] Avg train loss: 0.022769\n",
      "Epoch [2038/10000] Avg train loss: 0.022758\n",
      "Epoch [2039/10000] Avg train loss: 0.022747\n",
      "Epoch [2040/10000] Avg train loss: 0.022736\n",
      "Epoch [2041/10000] Avg train loss: 0.022725\n",
      "Epoch [2042/10000] Avg train loss: 0.022714\n",
      "Epoch [2043/10000] Avg train loss: 0.022703\n",
      "Epoch [2044/10000] Avg train loss: 0.022691\n",
      "Epoch [2045/10000] Avg train loss: 0.022680\n",
      "Epoch [2046/10000] Avg train loss: 0.022669\n",
      "Epoch [2047/10000] Avg train loss: 0.022658\n",
      "Epoch [2048/10000] Avg train loss: 0.022647\n",
      "Epoch [2049/10000] Avg train loss: 0.022636\n",
      "Epoch [2050/10000] Avg train loss: 0.022625\n",
      "Epoch [2051/10000] Avg train loss: 0.022614\n",
      "Epoch [2052/10000] Avg train loss: 0.022603\n",
      "Epoch [2053/10000] Avg train loss: 0.022592\n",
      "Epoch [2054/10000] Avg train loss: 0.022581\n",
      "Epoch [2055/10000] Avg train loss: 0.022570\n",
      "Epoch [2056/10000] Avg train loss: 0.022559\n",
      "Epoch [2057/10000] Avg train loss: 0.022548\n",
      "Epoch [2058/10000] Avg train loss: 0.022537\n",
      "Epoch [2059/10000] Avg train loss: 0.022526\n",
      "Epoch [2060/10000] Avg train loss: 0.022515\n",
      "Epoch [2061/10000] Avg train loss: 0.022504\n",
      "Epoch [2062/10000] Avg train loss: 0.022493\n",
      "Epoch [2063/10000] Avg train loss: 0.022482\n",
      "Epoch [2064/10000] Avg train loss: 0.022472\n",
      "Epoch [2065/10000] Avg train loss: 0.022461\n",
      "Epoch [2066/10000] Avg train loss: 0.022450\n",
      "Epoch [2067/10000] Avg train loss: 0.022439\n",
      "Epoch [2068/10000] Avg train loss: 0.022428\n",
      "Epoch [2069/10000] Avg train loss: 0.022417\n",
      "Epoch [2070/10000] Avg train loss: 0.022406\n",
      "Epoch [2071/10000] Avg train loss: 0.022396\n",
      "Epoch [2072/10000] Avg train loss: 0.022385\n",
      "Epoch [2073/10000] Avg train loss: 0.022374\n",
      "Epoch [2074/10000] Avg train loss: 0.022363\n",
      "Epoch [2075/10000] Avg train loss: 0.022352\n",
      "Epoch [2076/10000] Avg train loss: 0.022342\n",
      "Epoch [2077/10000] Avg train loss: 0.022331\n",
      "Epoch [2078/10000] Avg train loss: 0.022320\n",
      "Epoch [2079/10000] Avg train loss: 0.022309\n",
      "Epoch [2080/10000] Avg train loss: 0.022299\n",
      "Epoch [2081/10000] Avg train loss: 0.022288\n",
      "Epoch [2082/10000] Avg train loss: 0.022277\n",
      "Epoch [2083/10000] Avg train loss: 0.022267\n",
      "Epoch [2084/10000] Avg train loss: 0.022256\n",
      "Epoch [2085/10000] Avg train loss: 0.022245\n",
      "Epoch [2086/10000] Avg train loss: 0.022235\n",
      "Epoch [2087/10000] Avg train loss: 0.022224\n",
      "Epoch [2088/10000] Avg train loss: 0.022213\n",
      "Epoch [2089/10000] Avg train loss: 0.022203\n",
      "Epoch [2090/10000] Avg train loss: 0.022192\n",
      "Epoch [2091/10000] Avg train loss: 0.022181\n",
      "Epoch [2092/10000] Avg train loss: 0.022171\n",
      "Epoch [2093/10000] Avg train loss: 0.022160\n",
      "Epoch [2094/10000] Avg train loss: 0.022150\n",
      "Epoch [2095/10000] Avg train loss: 0.022139\n",
      "Epoch [2096/10000] Avg train loss: 0.022128\n",
      "Epoch [2097/10000] Avg train loss: 0.022118\n",
      "Epoch [2098/10000] Avg train loss: 0.022107\n",
      "Epoch [2099/10000] Avg train loss: 0.022097\n",
      "Epoch [2100/10000] Avg train loss: 0.022086\n",
      "Epoch [2101/10000] Avg train loss: 0.022076\n",
      "Epoch [2102/10000] Avg train loss: 0.022065\n",
      "Epoch [2103/10000] Avg train loss: 0.022055\n",
      "Epoch [2104/10000] Avg train loss: 0.022044\n",
      "Epoch [2105/10000] Avg train loss: 0.022034\n",
      "Epoch [2106/10000] Avg train loss: 0.022023\n",
      "Epoch [2107/10000] Avg train loss: 0.022013\n",
      "Epoch [2108/10000] Avg train loss: 0.022002\n",
      "Epoch [2109/10000] Avg train loss: 0.021992\n",
      "Epoch [2110/10000] Avg train loss: 0.021982\n",
      "Epoch [2111/10000] Avg train loss: 0.021971\n",
      "Epoch [2112/10000] Avg train loss: 0.021961\n",
      "Epoch [2113/10000] Avg train loss: 0.021950\n",
      "Epoch [2114/10000] Avg train loss: 0.021940\n",
      "Epoch [2115/10000] Avg train loss: 0.021930\n",
      "Epoch [2116/10000] Avg train loss: 0.021919\n",
      "Epoch [2117/10000] Avg train loss: 0.021909\n",
      "Epoch [2118/10000] Avg train loss: 0.021899\n",
      "Epoch [2119/10000] Avg train loss: 0.021888\n",
      "Epoch [2120/10000] Avg train loss: 0.021878\n",
      "Epoch [2121/10000] Avg train loss: 0.021868\n",
      "Epoch [2122/10000] Avg train loss: 0.021857\n",
      "Epoch [2123/10000] Avg train loss: 0.021847\n",
      "Epoch [2124/10000] Avg train loss: 0.021837\n",
      "Epoch [2125/10000] Avg train loss: 0.021826\n",
      "Epoch [2126/10000] Avg train loss: 0.021816\n",
      "Epoch [2127/10000] Avg train loss: 0.021806\n",
      "Epoch [2128/10000] Avg train loss: 0.021796\n",
      "Epoch [2129/10000] Avg train loss: 0.021785\n",
      "Epoch [2130/10000] Avg train loss: 0.021775\n",
      "Epoch [2131/10000] Avg train loss: 0.021765\n",
      "Epoch [2132/10000] Avg train loss: 0.021755\n",
      "Epoch [2133/10000] Avg train loss: 0.021745\n",
      "Epoch [2134/10000] Avg train loss: 0.021734\n",
      "Epoch [2135/10000] Avg train loss: 0.021724\n",
      "Epoch [2136/10000] Avg train loss: 0.021714\n",
      "Epoch [2137/10000] Avg train loss: 0.021704\n",
      "Epoch [2138/10000] Avg train loss: 0.021694\n",
      "Epoch [2139/10000] Avg train loss: 0.021684\n",
      "Epoch [2140/10000] Avg train loss: 0.021673\n",
      "Epoch [2141/10000] Avg train loss: 0.021663\n",
      "Epoch [2142/10000] Avg train loss: 0.021653\n",
      "Epoch [2143/10000] Avg train loss: 0.021643\n",
      "Epoch [2144/10000] Avg train loss: 0.021633\n",
      "Epoch [2145/10000] Avg train loss: 0.021623\n",
      "Epoch [2146/10000] Avg train loss: 0.021613\n",
      "Epoch [2147/10000] Avg train loss: 0.021603\n",
      "Epoch [2148/10000] Avg train loss: 0.021593\n",
      "Epoch [2149/10000] Avg train loss: 0.021583\n",
      "Epoch [2150/10000] Avg train loss: 0.021573\n",
      "Epoch [2151/10000] Avg train loss: 0.021563\n",
      "Epoch [2152/10000] Avg train loss: 0.021553\n",
      "Epoch [2153/10000] Avg train loss: 0.021543\n",
      "Epoch [2154/10000] Avg train loss: 0.021533\n",
      "Epoch [2155/10000] Avg train loss: 0.021523\n",
      "Epoch [2156/10000] Avg train loss: 0.021513\n",
      "Epoch [2157/10000] Avg train loss: 0.021503\n",
      "Epoch [2158/10000] Avg train loss: 0.021493\n",
      "Epoch [2159/10000] Avg train loss: 0.021483\n",
      "Epoch [2160/10000] Avg train loss: 0.021473\n",
      "Epoch [2161/10000] Avg train loss: 0.021463\n",
      "Epoch [2162/10000] Avg train loss: 0.021453\n",
      "Epoch [2163/10000] Avg train loss: 0.021443\n",
      "Epoch [2164/10000] Avg train loss: 0.021433\n",
      "Epoch [2165/10000] Avg train loss: 0.021423\n",
      "Epoch [2166/10000] Avg train loss: 0.021413\n",
      "Epoch [2167/10000] Avg train loss: 0.021403\n",
      "Epoch [2168/10000] Avg train loss: 0.021394\n",
      "Epoch [2169/10000] Avg train loss: 0.021384\n",
      "Epoch [2170/10000] Avg train loss: 0.021374\n",
      "Epoch [2171/10000] Avg train loss: 0.021364\n",
      "Epoch [2172/10000] Avg train loss: 0.021354\n",
      "Epoch [2173/10000] Avg train loss: 0.021344\n",
      "Epoch [2174/10000] Avg train loss: 0.021335\n",
      "Epoch [2175/10000] Avg train loss: 0.021325\n",
      "Epoch [2176/10000] Avg train loss: 0.021315\n",
      "Epoch [2177/10000] Avg train loss: 0.021305\n",
      "Epoch [2178/10000] Avg train loss: 0.021295\n",
      "Epoch [2179/10000] Avg train loss: 0.021286\n",
      "Epoch [2180/10000] Avg train loss: 0.021276\n",
      "Epoch [2181/10000] Avg train loss: 0.021266\n",
      "Epoch [2182/10000] Avg train loss: 0.021256\n",
      "Epoch [2183/10000] Avg train loss: 0.021247\n",
      "Epoch [2184/10000] Avg train loss: 0.021237\n",
      "Epoch [2185/10000] Avg train loss: 0.021227\n",
      "Epoch [2186/10000] Avg train loss: 0.021217\n",
      "Epoch [2187/10000] Avg train loss: 0.021208\n",
      "Epoch [2188/10000] Avg train loss: 0.021198\n",
      "Epoch [2189/10000] Avg train loss: 0.021188\n",
      "Epoch [2190/10000] Avg train loss: 0.021179\n",
      "Epoch [2191/10000] Avg train loss: 0.021169\n",
      "Epoch [2192/10000] Avg train loss: 0.021159\n",
      "Epoch [2193/10000] Avg train loss: 0.021150\n",
      "Epoch [2194/10000] Avg train loss: 0.021140\n",
      "Epoch [2195/10000] Avg train loss: 0.021130\n",
      "Epoch [2196/10000] Avg train loss: 0.021121\n",
      "Epoch [2197/10000] Avg train loss: 0.021111\n",
      "Epoch [2198/10000] Avg train loss: 0.021102\n",
      "Epoch [2199/10000] Avg train loss: 0.021092\n",
      "Epoch [2200/10000] Avg train loss: 0.021082\n",
      "Epoch [2201/10000] Avg train loss: 0.021073\n",
      "Epoch [2202/10000] Avg train loss: 0.021063\n",
      "Epoch [2203/10000] Avg train loss: 0.021054\n",
      "Epoch [2204/10000] Avg train loss: 0.021044\n",
      "Epoch [2205/10000] Avg train loss: 0.021035\n",
      "Epoch [2206/10000] Avg train loss: 0.021025\n",
      "Epoch [2207/10000] Avg train loss: 0.021016\n",
      "Epoch [2208/10000] Avg train loss: 0.021006\n",
      "Epoch [2209/10000] Avg train loss: 0.020996\n",
      "Epoch [2210/10000] Avg train loss: 0.020987\n",
      "Epoch [2211/10000] Avg train loss: 0.020978\n",
      "Epoch [2212/10000] Avg train loss: 0.020968\n",
      "Epoch [2213/10000] Avg train loss: 0.020959\n",
      "Epoch [2214/10000] Avg train loss: 0.020949\n",
      "Epoch [2215/10000] Avg train loss: 0.020940\n",
      "Epoch [2216/10000] Avg train loss: 0.020930\n",
      "Epoch [2217/10000] Avg train loss: 0.020921\n",
      "Epoch [2218/10000] Avg train loss: 0.020911\n",
      "Epoch [2219/10000] Avg train loss: 0.020902\n",
      "Epoch [2220/10000] Avg train loss: 0.020892\n",
      "Epoch [2221/10000] Avg train loss: 0.020883\n",
      "Epoch [2222/10000] Avg train loss: 0.020874\n",
      "Epoch [2223/10000] Avg train loss: 0.020864\n",
      "Epoch [2224/10000] Avg train loss: 0.020855\n",
      "Epoch [2225/10000] Avg train loss: 0.020846\n",
      "Epoch [2226/10000] Avg train loss: 0.020836\n",
      "Epoch [2227/10000] Avg train loss: 0.020827\n",
      "Epoch [2228/10000] Avg train loss: 0.020817\n",
      "Epoch [2229/10000] Avg train loss: 0.020808\n",
      "Epoch [2230/10000] Avg train loss: 0.020799\n",
      "Epoch [2231/10000] Avg train loss: 0.020789\n",
      "Epoch [2232/10000] Avg train loss: 0.020780\n",
      "Epoch [2233/10000] Avg train loss: 0.020771\n",
      "Epoch [2234/10000] Avg train loss: 0.020762\n",
      "Epoch [2235/10000] Avg train loss: 0.020752\n",
      "Epoch [2236/10000] Avg train loss: 0.020743\n",
      "Epoch [2237/10000] Avg train loss: 0.020734\n",
      "Epoch [2238/10000] Avg train loss: 0.020724\n",
      "Epoch [2239/10000] Avg train loss: 0.020715\n",
      "Epoch [2240/10000] Avg train loss: 0.020706\n",
      "Epoch [2241/10000] Avg train loss: 0.020697\n",
      "Epoch [2242/10000] Avg train loss: 0.020687\n",
      "Epoch [2243/10000] Avg train loss: 0.020678\n",
      "Epoch [2244/10000] Avg train loss: 0.020669\n",
      "Epoch [2245/10000] Avg train loss: 0.020660\n",
      "Epoch [2246/10000] Avg train loss: 0.020651\n",
      "Epoch [2247/10000] Avg train loss: 0.020641\n",
      "Epoch [2248/10000] Avg train loss: 0.020632\n",
      "Epoch [2249/10000] Avg train loss: 0.020623\n",
      "Epoch [2250/10000] Avg train loss: 0.020614\n",
      "Epoch [2251/10000] Avg train loss: 0.020605\n",
      "Epoch [2252/10000] Avg train loss: 0.020596\n",
      "Epoch [2253/10000] Avg train loss: 0.020586\n",
      "Epoch [2254/10000] Avg train loss: 0.020577\n",
      "Epoch [2255/10000] Avg train loss: 0.020568\n",
      "Epoch [2256/10000] Avg train loss: 0.020559\n",
      "Epoch [2257/10000] Avg train loss: 0.020550\n",
      "Epoch [2258/10000] Avg train loss: 0.020541\n",
      "Epoch [2259/10000] Avg train loss: 0.020532\n",
      "Epoch [2260/10000] Avg train loss: 0.020523\n",
      "Epoch [2261/10000] Avg train loss: 0.020514\n",
      "Epoch [2262/10000] Avg train loss: 0.020505\n",
      "Epoch [2263/10000] Avg train loss: 0.020495\n",
      "Epoch [2264/10000] Avg train loss: 0.020486\n",
      "Epoch [2265/10000] Avg train loss: 0.020477\n",
      "Epoch [2266/10000] Avg train loss: 0.020468\n",
      "Epoch [2267/10000] Avg train loss: 0.020459\n",
      "Epoch [2268/10000] Avg train loss: 0.020450\n",
      "Epoch [2269/10000] Avg train loss: 0.020441\n",
      "Epoch [2270/10000] Avg train loss: 0.020432\n",
      "Epoch [2271/10000] Avg train loss: 0.020423\n",
      "Epoch [2272/10000] Avg train loss: 0.020414\n",
      "Epoch [2273/10000] Avg train loss: 0.020405\n",
      "Epoch [2274/10000] Avg train loss: 0.020396\n",
      "Epoch [2275/10000] Avg train loss: 0.020387\n",
      "Epoch [2276/10000] Avg train loss: 0.020378\n",
      "Epoch [2277/10000] Avg train loss: 0.020369\n",
      "Epoch [2278/10000] Avg train loss: 0.020361\n",
      "Epoch [2279/10000] Avg train loss: 0.020352\n",
      "Epoch [2280/10000] Avg train loss: 0.020343\n",
      "Epoch [2281/10000] Avg train loss: 0.020334\n",
      "Epoch [2282/10000] Avg train loss: 0.020325\n",
      "Epoch [2283/10000] Avg train loss: 0.020316\n",
      "Epoch [2284/10000] Avg train loss: 0.020307\n",
      "Epoch [2285/10000] Avg train loss: 0.020298\n",
      "Epoch [2286/10000] Avg train loss: 0.020289\n",
      "Epoch [2287/10000] Avg train loss: 0.020280\n",
      "Epoch [2288/10000] Avg train loss: 0.020272\n",
      "Epoch [2289/10000] Avg train loss: 0.020263\n",
      "Epoch [2290/10000] Avg train loss: 0.020254\n",
      "Epoch [2291/10000] Avg train loss: 0.020245\n",
      "Epoch [2292/10000] Avg train loss: 0.020236\n",
      "Epoch [2293/10000] Avg train loss: 0.020227\n",
      "Epoch [2294/10000] Avg train loss: 0.020219\n",
      "Epoch [2295/10000] Avg train loss: 0.020210\n",
      "Epoch [2296/10000] Avg train loss: 0.020201\n",
      "Epoch [2297/10000] Avg train loss: 0.020192\n",
      "Epoch [2298/10000] Avg train loss: 0.020183\n",
      "Epoch [2299/10000] Avg train loss: 0.020175\n",
      "Epoch [2300/10000] Avg train loss: 0.020166\n",
      "Epoch [2301/10000] Avg train loss: 0.020157\n",
      "Epoch [2302/10000] Avg train loss: 0.020148\n",
      "Epoch [2303/10000] Avg train loss: 0.020139\n",
      "Epoch [2304/10000] Avg train loss: 0.020131\n",
      "Epoch [2305/10000] Avg train loss: 0.020122\n",
      "Epoch [2306/10000] Avg train loss: 0.020113\n",
      "Epoch [2307/10000] Avg train loss: 0.020105\n",
      "Epoch [2308/10000] Avg train loss: 0.020096\n",
      "Epoch [2309/10000] Avg train loss: 0.020087\n",
      "Epoch [2310/10000] Avg train loss: 0.020078\n",
      "Epoch [2311/10000] Avg train loss: 0.020070\n",
      "Epoch [2312/10000] Avg train loss: 0.020061\n",
      "Epoch [2313/10000] Avg train loss: 0.020052\n",
      "Epoch [2314/10000] Avg train loss: 0.020044\n",
      "Epoch [2315/10000] Avg train loss: 0.020035\n",
      "Epoch [2316/10000] Avg train loss: 0.020026\n",
      "Epoch [2317/10000] Avg train loss: 0.020018\n",
      "Epoch [2318/10000] Avg train loss: 0.020009\n",
      "Epoch [2319/10000] Avg train loss: 0.020001\n",
      "Epoch [2320/10000] Avg train loss: 0.019992\n",
      "Epoch [2321/10000] Avg train loss: 0.019983\n",
      "Epoch [2322/10000] Avg train loss: 0.019975\n",
      "Epoch [2323/10000] Avg train loss: 0.019966\n",
      "Epoch [2324/10000] Avg train loss: 0.019958\n",
      "Epoch [2325/10000] Avg train loss: 0.019949\n",
      "Epoch [2326/10000] Avg train loss: 0.019940\n",
      "Epoch [2327/10000] Avg train loss: 0.019932\n",
      "Epoch [2328/10000] Avg train loss: 0.019923\n",
      "Epoch [2329/10000] Avg train loss: 0.019915\n",
      "Epoch [2330/10000] Avg train loss: 0.019906\n",
      "Epoch [2331/10000] Avg train loss: 0.019898\n",
      "Epoch [2332/10000] Avg train loss: 0.019889\n",
      "Epoch [2333/10000] Avg train loss: 0.019881\n",
      "Epoch [2334/10000] Avg train loss: 0.019872\n",
      "Epoch [2335/10000] Avg train loss: 0.019863\n",
      "Epoch [2336/10000] Avg train loss: 0.019855\n",
      "Epoch [2337/10000] Avg train loss: 0.019846\n",
      "Epoch [2338/10000] Avg train loss: 0.019838\n",
      "Epoch [2339/10000] Avg train loss: 0.019830\n",
      "Epoch [2340/10000] Avg train loss: 0.019821\n",
      "Epoch [2341/10000] Avg train loss: 0.019813\n",
      "Epoch [2342/10000] Avg train loss: 0.019804\n",
      "Epoch [2343/10000] Avg train loss: 0.019796\n",
      "Epoch [2344/10000] Avg train loss: 0.019787\n",
      "Epoch [2345/10000] Avg train loss: 0.019779\n",
      "Epoch [2346/10000] Avg train loss: 0.019770\n",
      "Epoch [2347/10000] Avg train loss: 0.019762\n",
      "Epoch [2348/10000] Avg train loss: 0.019754\n",
      "Epoch [2349/10000] Avg train loss: 0.019745\n",
      "Epoch [2350/10000] Avg train loss: 0.019737\n",
      "Epoch [2351/10000] Avg train loss: 0.019728\n",
      "Epoch [2352/10000] Avg train loss: 0.019720\n",
      "Epoch [2353/10000] Avg train loss: 0.019712\n",
      "Epoch [2354/10000] Avg train loss: 0.019703\n",
      "Epoch [2355/10000] Avg train loss: 0.019695\n",
      "Epoch [2356/10000] Avg train loss: 0.019686\n",
      "Epoch [2357/10000] Avg train loss: 0.019678\n",
      "Epoch [2358/10000] Avg train loss: 0.019670\n",
      "Epoch [2359/10000] Avg train loss: 0.019661\n",
      "Epoch [2360/10000] Avg train loss: 0.019653\n",
      "Epoch [2361/10000] Avg train loss: 0.019645\n",
      "Epoch [2362/10000] Avg train loss: 0.019636\n",
      "Epoch [2363/10000] Avg train loss: 0.019628\n",
      "Epoch [2364/10000] Avg train loss: 0.019620\n",
      "Epoch [2365/10000] Avg train loss: 0.019612\n",
      "Epoch [2366/10000] Avg train loss: 0.019603\n",
      "Epoch [2367/10000] Avg train loss: 0.019595\n",
      "Epoch [2368/10000] Avg train loss: 0.019587\n",
      "Epoch [2369/10000] Avg train loss: 0.019578\n",
      "Epoch [2370/10000] Avg train loss: 0.019570\n",
      "Epoch [2371/10000] Avg train loss: 0.019562\n",
      "Epoch [2372/10000] Avg train loss: 0.019554\n",
      "Epoch [2373/10000] Avg train loss: 0.019545\n",
      "Epoch [2374/10000] Avg train loss: 0.019537\n",
      "Epoch [2375/10000] Avg train loss: 0.019529\n",
      "Epoch [2376/10000] Avg train loss: 0.019521\n",
      "Epoch [2377/10000] Avg train loss: 0.019513\n",
      "Epoch [2378/10000] Avg train loss: 0.019504\n",
      "Epoch [2379/10000] Avg train loss: 0.019496\n",
      "Epoch [2380/10000] Avg train loss: 0.019488\n",
      "Epoch [2381/10000] Avg train loss: 0.019480\n",
      "Epoch [2382/10000] Avg train loss: 0.019472\n",
      "Epoch [2383/10000] Avg train loss: 0.019463\n",
      "Epoch [2384/10000] Avg train loss: 0.019455\n",
      "Epoch [2385/10000] Avg train loss: 0.019447\n",
      "Epoch [2386/10000] Avg train loss: 0.019439\n",
      "Epoch [2387/10000] Avg train loss: 0.019431\n",
      "Epoch [2388/10000] Avg train loss: 0.019423\n",
      "Epoch [2389/10000] Avg train loss: 0.019415\n",
      "Epoch [2390/10000] Avg train loss: 0.019406\n",
      "Epoch [2391/10000] Avg train loss: 0.019398\n",
      "Epoch [2392/10000] Avg train loss: 0.019390\n",
      "Epoch [2393/10000] Avg train loss: 0.019382\n",
      "Epoch [2394/10000] Avg train loss: 0.019374\n",
      "Epoch [2395/10000] Avg train loss: 0.019366\n",
      "Epoch [2396/10000] Avg train loss: 0.019358\n",
      "Epoch [2397/10000] Avg train loss: 0.019350\n",
      "Epoch [2398/10000] Avg train loss: 0.019342\n",
      "Epoch [2399/10000] Avg train loss: 0.019334\n",
      "Epoch [2400/10000] Avg train loss: 0.019326\n",
      "Epoch [2401/10000] Avg train loss: 0.019317\n",
      "Epoch [2402/10000] Avg train loss: 0.019309\n",
      "Epoch [2403/10000] Avg train loss: 0.019301\n",
      "Epoch [2404/10000] Avg train loss: 0.019293\n",
      "Epoch [2405/10000] Avg train loss: 0.019285\n",
      "Epoch [2406/10000] Avg train loss: 0.019277\n",
      "Epoch [2407/10000] Avg train loss: 0.019269\n",
      "Epoch [2408/10000] Avg train loss: 0.019261\n",
      "Epoch [2409/10000] Avg train loss: 0.019253\n",
      "Epoch [2410/10000] Avg train loss: 0.019245\n",
      "Epoch [2411/10000] Avg train loss: 0.019237\n",
      "Epoch [2412/10000] Avg train loss: 0.019229\n",
      "Epoch [2413/10000] Avg train loss: 0.019221\n",
      "Epoch [2414/10000] Avg train loss: 0.019213\n",
      "Epoch [2415/10000] Avg train loss: 0.019205\n",
      "Epoch [2416/10000] Avg train loss: 0.019198\n",
      "Epoch [2417/10000] Avg train loss: 0.019190\n",
      "Epoch [2418/10000] Avg train loss: 0.019182\n",
      "Epoch [2419/10000] Avg train loss: 0.019174\n",
      "Epoch [2420/10000] Avg train loss: 0.019166\n",
      "Epoch [2421/10000] Avg train loss: 0.019158\n",
      "Epoch [2422/10000] Avg train loss: 0.019150\n",
      "Epoch [2423/10000] Avg train loss: 0.019142\n",
      "Epoch [2424/10000] Avg train loss: 0.019134\n",
      "Epoch [2425/10000] Avg train loss: 0.019126\n",
      "Epoch [2426/10000] Avg train loss: 0.019118\n",
      "Epoch [2427/10000] Avg train loss: 0.019111\n",
      "Epoch [2428/10000] Avg train loss: 0.019103\n",
      "Epoch [2429/10000] Avg train loss: 0.019095\n",
      "Epoch [2430/10000] Avg train loss: 0.019087\n",
      "Epoch [2431/10000] Avg train loss: 0.019079\n",
      "Epoch [2432/10000] Avg train loss: 0.019071\n",
      "Epoch [2433/10000] Avg train loss: 0.019063\n",
      "Epoch [2434/10000] Avg train loss: 0.019056\n",
      "Epoch [2435/10000] Avg train loss: 0.019048\n",
      "Epoch [2436/10000] Avg train loss: 0.019040\n",
      "Epoch [2437/10000] Avg train loss: 0.019032\n",
      "Epoch [2438/10000] Avg train loss: 0.019024\n",
      "Epoch [2439/10000] Avg train loss: 0.019017\n",
      "Epoch [2440/10000] Avg train loss: 0.019009\n",
      "Epoch [2441/10000] Avg train loss: 0.019001\n",
      "Epoch [2442/10000] Avg train loss: 0.018993\n",
      "Epoch [2443/10000] Avg train loss: 0.018985\n",
      "Epoch [2444/10000] Avg train loss: 0.018978\n",
      "Epoch [2445/10000] Avg train loss: 0.018970\n",
      "Epoch [2446/10000] Avg train loss: 0.018962\n",
      "Epoch [2447/10000] Avg train loss: 0.018954\n",
      "Epoch [2448/10000] Avg train loss: 0.018947\n",
      "Epoch [2449/10000] Avg train loss: 0.018939\n",
      "Epoch [2450/10000] Avg train loss: 0.018931\n",
      "Epoch [2451/10000] Avg train loss: 0.018923\n",
      "Epoch [2452/10000] Avg train loss: 0.018916\n",
      "Epoch [2453/10000] Avg train loss: 0.018908\n",
      "Epoch [2454/10000] Avg train loss: 0.018900\n",
      "Epoch [2455/10000] Avg train loss: 0.018893\n",
      "Epoch [2456/10000] Avg train loss: 0.018885\n",
      "Epoch [2457/10000] Avg train loss: 0.018877\n",
      "Epoch [2458/10000] Avg train loss: 0.018870\n",
      "Epoch [2459/10000] Avg train loss: 0.018862\n",
      "Epoch [2460/10000] Avg train loss: 0.018854\n",
      "Epoch [2461/10000] Avg train loss: 0.018847\n",
      "Epoch [2462/10000] Avg train loss: 0.018839\n",
      "Epoch [2463/10000] Avg train loss: 0.018831\n",
      "Epoch [2464/10000] Avg train loss: 0.018824\n",
      "Epoch [2465/10000] Avg train loss: 0.018816\n",
      "Epoch [2466/10000] Avg train loss: 0.018808\n",
      "Epoch [2467/10000] Avg train loss: 0.018801\n",
      "Epoch [2468/10000] Avg train loss: 0.018793\n",
      "Epoch [2469/10000] Avg train loss: 0.018785\n",
      "Epoch [2470/10000] Avg train loss: 0.018778\n",
      "Epoch [2471/10000] Avg train loss: 0.018770\n",
      "Epoch [2472/10000] Avg train loss: 0.018763\n",
      "Epoch [2473/10000] Avg train loss: 0.018755\n",
      "Epoch [2474/10000] Avg train loss: 0.018747\n",
      "Epoch [2475/10000] Avg train loss: 0.018740\n",
      "Epoch [2476/10000] Avg train loss: 0.018732\n",
      "Epoch [2477/10000] Avg train loss: 0.018725\n",
      "Epoch [2478/10000] Avg train loss: 0.018717\n",
      "Epoch [2479/10000] Avg train loss: 0.018710\n",
      "Epoch [2480/10000] Avg train loss: 0.018702\n",
      "Epoch [2481/10000] Avg train loss: 0.018695\n",
      "Epoch [2482/10000] Avg train loss: 0.018687\n",
      "Epoch [2483/10000] Avg train loss: 0.018680\n",
      "Epoch [2484/10000] Avg train loss: 0.018672\n",
      "Epoch [2485/10000] Avg train loss: 0.018664\n",
      "Epoch [2486/10000] Avg train loss: 0.018657\n",
      "Epoch [2487/10000] Avg train loss: 0.018649\n",
      "Epoch [2488/10000] Avg train loss: 0.018642\n",
      "Epoch [2489/10000] Avg train loss: 0.018634\n",
      "Epoch [2490/10000] Avg train loss: 0.018627\n",
      "Epoch [2491/10000] Avg train loss: 0.018620\n",
      "Epoch [2492/10000] Avg train loss: 0.018612\n",
      "Epoch [2493/10000] Avg train loss: 0.018605\n",
      "Epoch [2494/10000] Avg train loss: 0.018597\n",
      "Epoch [2495/10000] Avg train loss: 0.018590\n",
      "Epoch [2496/10000] Avg train loss: 0.018582\n",
      "Epoch [2497/10000] Avg train loss: 0.018575\n",
      "Epoch [2498/10000] Avg train loss: 0.018567\n",
      "Epoch [2499/10000] Avg train loss: 0.018560\n",
      "Epoch [2500/10000] Avg train loss: 0.018553\n",
      "Epoch [2501/10000] Avg train loss: 0.018545\n",
      "Epoch [2502/10000] Avg train loss: 0.018538\n",
      "Epoch [2503/10000] Avg train loss: 0.018530\n",
      "Epoch [2504/10000] Avg train loss: 0.018523\n",
      "Epoch [2505/10000] Avg train loss: 0.018515\n",
      "Epoch [2506/10000] Avg train loss: 0.018508\n",
      "Epoch [2507/10000] Avg train loss: 0.018501\n",
      "Epoch [2508/10000] Avg train loss: 0.018493\n",
      "Epoch [2509/10000] Avg train loss: 0.018486\n",
      "Epoch [2510/10000] Avg train loss: 0.018479\n",
      "Epoch [2511/10000] Avg train loss: 0.018471\n",
      "Epoch [2512/10000] Avg train loss: 0.018464\n",
      "Epoch [2513/10000] Avg train loss: 0.018457\n",
      "Epoch [2514/10000] Avg train loss: 0.018449\n",
      "Epoch [2515/10000] Avg train loss: 0.018442\n",
      "Epoch [2516/10000] Avg train loss: 0.018435\n",
      "Epoch [2517/10000] Avg train loss: 0.018427\n",
      "Epoch [2518/10000] Avg train loss: 0.018420\n",
      "Epoch [2519/10000] Avg train loss: 0.018413\n",
      "Epoch [2520/10000] Avg train loss: 0.018405\n",
      "Epoch [2521/10000] Avg train loss: 0.018398\n",
      "Epoch [2522/10000] Avg train loss: 0.018391\n",
      "Epoch [2523/10000] Avg train loss: 0.018383\n",
      "Epoch [2524/10000] Avg train loss: 0.018376\n",
      "Epoch [2525/10000] Avg train loss: 0.018369\n",
      "Epoch [2526/10000] Avg train loss: 0.018362\n",
      "Epoch [2527/10000] Avg train loss: 0.018354\n",
      "Epoch [2528/10000] Avg train loss: 0.018347\n",
      "Epoch [2529/10000] Avg train loss: 0.018340\n",
      "Epoch [2530/10000] Avg train loss: 0.018333\n",
      "Epoch [2531/10000] Avg train loss: 0.018325\n",
      "Epoch [2532/10000] Avg train loss: 0.018318\n",
      "Epoch [2533/10000] Avg train loss: 0.018311\n",
      "Epoch [2534/10000] Avg train loss: 0.018304\n",
      "Epoch [2535/10000] Avg train loss: 0.018296\n",
      "Epoch [2536/10000] Avg train loss: 0.018289\n",
      "Epoch [2537/10000] Avg train loss: 0.018282\n",
      "Epoch [2538/10000] Avg train loss: 0.018275\n",
      "Epoch [2539/10000] Avg train loss: 0.018268\n",
      "Epoch [2540/10000] Avg train loss: 0.018260\n",
      "Epoch [2541/10000] Avg train loss: 0.018253\n",
      "Epoch [2542/10000] Avg train loss: 0.018246\n",
      "Epoch [2543/10000] Avg train loss: 0.018239\n",
      "Epoch [2544/10000] Avg train loss: 0.018232\n",
      "Epoch [2545/10000] Avg train loss: 0.018224\n",
      "Epoch [2546/10000] Avg train loss: 0.018217\n",
      "Epoch [2547/10000] Avg train loss: 0.018210\n",
      "Epoch [2548/10000] Avg train loss: 0.018203\n",
      "Epoch [2549/10000] Avg train loss: 0.018196\n",
      "Epoch [2550/10000] Avg train loss: 0.018189\n",
      "Epoch [2551/10000] Avg train loss: 0.018182\n",
      "Epoch [2552/10000] Avg train loss: 0.018174\n",
      "Epoch [2553/10000] Avg train loss: 0.018167\n",
      "Epoch [2554/10000] Avg train loss: 0.018160\n",
      "Epoch [2555/10000] Avg train loss: 0.018153\n",
      "Epoch [2556/10000] Avg train loss: 0.018146\n",
      "Epoch [2557/10000] Avg train loss: 0.018139\n",
      "Epoch [2558/10000] Avg train loss: 0.018132\n",
      "Epoch [2559/10000] Avg train loss: 0.018125\n",
      "Epoch [2560/10000] Avg train loss: 0.018118\n",
      "Epoch [2561/10000] Avg train loss: 0.018111\n",
      "Epoch [2562/10000] Avg train loss: 0.018104\n",
      "Epoch [2563/10000] Avg train loss: 0.018096\n",
      "Epoch [2564/10000] Avg train loss: 0.018089\n",
      "Epoch [2565/10000] Avg train loss: 0.018082\n",
      "Epoch [2566/10000] Avg train loss: 0.018075\n",
      "Epoch [2567/10000] Avg train loss: 0.018068\n",
      "Epoch [2568/10000] Avg train loss: 0.018061\n",
      "Epoch [2569/10000] Avg train loss: 0.018054\n",
      "Epoch [2570/10000] Avg train loss: 0.018047\n",
      "Epoch [2571/10000] Avg train loss: 0.018040\n",
      "Epoch [2572/10000] Avg train loss: 0.018033\n",
      "Epoch [2573/10000] Avg train loss: 0.018026\n",
      "Epoch [2574/10000] Avg train loss: 0.018019\n",
      "Epoch [2575/10000] Avg train loss: 0.018012\n",
      "Epoch [2576/10000] Avg train loss: 0.018005\n",
      "Epoch [2577/10000] Avg train loss: 0.017998\n",
      "Epoch [2578/10000] Avg train loss: 0.017991\n",
      "Epoch [2579/10000] Avg train loss: 0.017984\n",
      "Epoch [2580/10000] Avg train loss: 0.017977\n",
      "Epoch [2581/10000] Avg train loss: 0.017970\n",
      "Epoch [2582/10000] Avg train loss: 0.017963\n",
      "Epoch [2583/10000] Avg train loss: 0.017956\n",
      "Epoch [2584/10000] Avg train loss: 0.017949\n",
      "Epoch [2585/10000] Avg train loss: 0.017942\n",
      "Epoch [2586/10000] Avg train loss: 0.017936\n",
      "Epoch [2587/10000] Avg train loss: 0.017929\n",
      "Epoch [2588/10000] Avg train loss: 0.017922\n",
      "Epoch [2589/10000] Avg train loss: 0.017915\n",
      "Epoch [2590/10000] Avg train loss: 0.017908\n",
      "Epoch [2591/10000] Avg train loss: 0.017901\n",
      "Epoch [2592/10000] Avg train loss: 0.017894\n",
      "Epoch [2593/10000] Avg train loss: 0.017887\n",
      "Epoch [2594/10000] Avg train loss: 0.017880\n",
      "Epoch [2595/10000] Avg train loss: 0.017873\n",
      "Epoch [2596/10000] Avg train loss: 0.017866\n",
      "Epoch [2597/10000] Avg train loss: 0.017860\n",
      "Epoch [2598/10000] Avg train loss: 0.017853\n",
      "Epoch [2599/10000] Avg train loss: 0.017846\n",
      "Epoch [2600/10000] Avg train loss: 0.017839\n",
      "Epoch [2601/10000] Avg train loss: 0.017832\n",
      "Epoch [2602/10000] Avg train loss: 0.017825\n",
      "Epoch [2603/10000] Avg train loss: 0.017818\n",
      "Epoch [2604/10000] Avg train loss: 0.017812\n",
      "Epoch [2605/10000] Avg train loss: 0.017805\n",
      "Epoch [2606/10000] Avg train loss: 0.017798\n",
      "Epoch [2607/10000] Avg train loss: 0.017791\n",
      "Epoch [2608/10000] Avg train loss: 0.017784\n",
      "Epoch [2609/10000] Avg train loss: 0.017777\n",
      "Epoch [2610/10000] Avg train loss: 0.017771\n",
      "Epoch [2611/10000] Avg train loss: 0.017764\n",
      "Epoch [2612/10000] Avg train loss: 0.017757\n",
      "Epoch [2613/10000] Avg train loss: 0.017750\n",
      "Epoch [2614/10000] Avg train loss: 0.017743\n",
      "Epoch [2615/10000] Avg train loss: 0.017737\n",
      "Epoch [2616/10000] Avg train loss: 0.017730\n",
      "Epoch [2617/10000] Avg train loss: 0.017723\n",
      "Epoch [2618/10000] Avg train loss: 0.017716\n",
      "Epoch [2619/10000] Avg train loss: 0.017710\n",
      "Epoch [2620/10000] Avg train loss: 0.017703\n",
      "Epoch [2621/10000] Avg train loss: 0.017696\n",
      "Epoch [2622/10000] Avg train loss: 0.017689\n",
      "Epoch [2623/10000] Avg train loss: 0.017683\n",
      "Epoch [2624/10000] Avg train loss: 0.017676\n",
      "Epoch [2625/10000] Avg train loss: 0.017669\n",
      "Epoch [2626/10000] Avg train loss: 0.017662\n",
      "Epoch [2627/10000] Avg train loss: 0.017656\n",
      "Epoch [2628/10000] Avg train loss: 0.017649\n",
      "Epoch [2629/10000] Avg train loss: 0.017642\n",
      "Epoch [2630/10000] Avg train loss: 0.017635\n",
      "Epoch [2631/10000] Avg train loss: 0.017629\n",
      "Epoch [2632/10000] Avg train loss: 0.017622\n",
      "Epoch [2633/10000] Avg train loss: 0.017615\n",
      "Epoch [2634/10000] Avg train loss: 0.017609\n",
      "Epoch [2635/10000] Avg train loss: 0.017602\n",
      "Epoch [2636/10000] Avg train loss: 0.017595\n",
      "Epoch [2637/10000] Avg train loss: 0.017589\n",
      "Epoch [2638/10000] Avg train loss: 0.017582\n",
      "Epoch [2639/10000] Avg train loss: 0.017575\n",
      "Epoch [2640/10000] Avg train loss: 0.017569\n",
      "Epoch [2641/10000] Avg train loss: 0.017562\n",
      "Epoch [2642/10000] Avg train loss: 0.017555\n",
      "Epoch [2643/10000] Avg train loss: 0.017549\n",
      "Epoch [2644/10000] Avg train loss: 0.017542\n",
      "Epoch [2645/10000] Avg train loss: 0.017535\n",
      "Epoch [2646/10000] Avg train loss: 0.017529\n",
      "Epoch [2647/10000] Avg train loss: 0.017522\n",
      "Epoch [2648/10000] Avg train loss: 0.017516\n",
      "Epoch [2649/10000] Avg train loss: 0.017509\n",
      "Epoch [2650/10000] Avg train loss: 0.017502\n",
      "Epoch [2651/10000] Avg train loss: 0.017496\n",
      "Epoch [2652/10000] Avg train loss: 0.017489\n",
      "Epoch [2653/10000] Avg train loss: 0.017483\n",
      "Epoch [2654/10000] Avg train loss: 0.017476\n",
      "Epoch [2655/10000] Avg train loss: 0.017469\n",
      "Epoch [2656/10000] Avg train loss: 0.017463\n",
      "Epoch [2657/10000] Avg train loss: 0.017456\n",
      "Epoch [2658/10000] Avg train loss: 0.017450\n",
      "Epoch [2659/10000] Avg train loss: 0.017443\n",
      "Epoch [2660/10000] Avg train loss: 0.017437\n",
      "Epoch [2661/10000] Avg train loss: 0.017430\n",
      "Epoch [2662/10000] Avg train loss: 0.017423\n",
      "Epoch [2663/10000] Avg train loss: 0.017417\n",
      "Epoch [2664/10000] Avg train loss: 0.017410\n",
      "Epoch [2665/10000] Avg train loss: 0.017404\n",
      "Epoch [2666/10000] Avg train loss: 0.017397\n",
      "Epoch [2667/10000] Avg train loss: 0.017391\n",
      "Epoch [2668/10000] Avg train loss: 0.017384\n",
      "Epoch [2669/10000] Avg train loss: 0.017378\n",
      "Epoch [2670/10000] Avg train loss: 0.017371\n",
      "Epoch [2671/10000] Avg train loss: 0.017365\n",
      "Epoch [2672/10000] Avg train loss: 0.017358\n",
      "Epoch [2673/10000] Avg train loss: 0.017352\n",
      "Epoch [2674/10000] Avg train loss: 0.017345\n",
      "Epoch [2675/10000] Avg train loss: 0.017339\n",
      "Epoch [2676/10000] Avg train loss: 0.017332\n",
      "Epoch [2677/10000] Avg train loss: 0.017326\n",
      "Epoch [2678/10000] Avg train loss: 0.017319\n",
      "Epoch [2679/10000] Avg train loss: 0.017313\n",
      "Epoch [2680/10000] Avg train loss: 0.017306\n",
      "Epoch [2681/10000] Avg train loss: 0.017300\n",
      "Epoch [2682/10000] Avg train loss: 0.017294\n",
      "Epoch [2683/10000] Avg train loss: 0.017287\n",
      "Epoch [2684/10000] Avg train loss: 0.017281\n",
      "Epoch [2685/10000] Avg train loss: 0.017274\n",
      "Epoch [2686/10000] Avg train loss: 0.017268\n",
      "Epoch [2687/10000] Avg train loss: 0.017261\n",
      "Epoch [2688/10000] Avg train loss: 0.017255\n",
      "Epoch [2689/10000] Avg train loss: 0.017249\n",
      "Epoch [2690/10000] Avg train loss: 0.017242\n",
      "Epoch [2691/10000] Avg train loss: 0.017236\n",
      "Epoch [2692/10000] Avg train loss: 0.017229\n",
      "Epoch [2693/10000] Avg train loss: 0.017223\n",
      "Epoch [2694/10000] Avg train loss: 0.017217\n",
      "Epoch [2695/10000] Avg train loss: 0.017210\n",
      "Epoch [2696/10000] Avg train loss: 0.017204\n",
      "Epoch [2697/10000] Avg train loss: 0.017197\n",
      "Epoch [2698/10000] Avg train loss: 0.017191\n",
      "Epoch [2699/10000] Avg train loss: 0.017185\n",
      "Epoch [2700/10000] Avg train loss: 0.017178\n",
      "Epoch [2701/10000] Avg train loss: 0.017172\n",
      "Epoch [2702/10000] Avg train loss: 0.017166\n",
      "Epoch [2703/10000] Avg train loss: 0.017159\n",
      "Epoch [2704/10000] Avg train loss: 0.017153\n",
      "Epoch [2705/10000] Avg train loss: 0.017146\n",
      "Epoch [2706/10000] Avg train loss: 0.017140\n",
      "Epoch [2707/10000] Avg train loss: 0.017134\n",
      "Epoch [2708/10000] Avg train loss: 0.017127\n",
      "Epoch [2709/10000] Avg train loss: 0.017121\n",
      "Epoch [2710/10000] Avg train loss: 0.017115\n",
      "Epoch [2711/10000] Avg train loss: 0.017109\n",
      "Epoch [2712/10000] Avg train loss: 0.017102\n",
      "Epoch [2713/10000] Avg train loss: 0.017096\n",
      "Epoch [2714/10000] Avg train loss: 0.017090\n",
      "Epoch [2715/10000] Avg train loss: 0.017083\n",
      "Epoch [2716/10000] Avg train loss: 0.017077\n",
      "Epoch [2717/10000] Avg train loss: 0.017071\n",
      "Epoch [2718/10000] Avg train loss: 0.017064\n",
      "Epoch [2719/10000] Avg train loss: 0.017058\n",
      "Epoch [2720/10000] Avg train loss: 0.017052\n",
      "Epoch [2721/10000] Avg train loss: 0.017046\n",
      "Epoch [2722/10000] Avg train loss: 0.017039\n",
      "Epoch [2723/10000] Avg train loss: 0.017033\n",
      "Epoch [2724/10000] Avg train loss: 0.017027\n",
      "Epoch [2725/10000] Avg train loss: 0.017021\n",
      "Epoch [2726/10000] Avg train loss: 0.017014\n",
      "Epoch [2727/10000] Avg train loss: 0.017008\n",
      "Epoch [2728/10000] Avg train loss: 0.017002\n",
      "Epoch [2729/10000] Avg train loss: 0.016996\n",
      "Epoch [2730/10000] Avg train loss: 0.016989\n",
      "Epoch [2731/10000] Avg train loss: 0.016983\n",
      "Epoch [2732/10000] Avg train loss: 0.016977\n",
      "Epoch [2733/10000] Avg train loss: 0.016971\n",
      "Epoch [2734/10000] Avg train loss: 0.016965\n",
      "Epoch [2735/10000] Avg train loss: 0.016958\n",
      "Epoch [2736/10000] Avg train loss: 0.016952\n",
      "Epoch [2737/10000] Avg train loss: 0.016946\n",
      "Epoch [2738/10000] Avg train loss: 0.016940\n",
      "Epoch [2739/10000] Avg train loss: 0.016934\n",
      "Epoch [2740/10000] Avg train loss: 0.016927\n",
      "Epoch [2741/10000] Avg train loss: 0.016921\n",
      "Epoch [2742/10000] Avg train loss: 0.016915\n",
      "Epoch [2743/10000] Avg train loss: 0.016909\n",
      "Epoch [2744/10000] Avg train loss: 0.016903\n",
      "Epoch [2745/10000] Avg train loss: 0.016897\n",
      "Epoch [2746/10000] Avg train loss: 0.016890\n",
      "Epoch [2747/10000] Avg train loss: 0.016884\n",
      "Epoch [2748/10000] Avg train loss: 0.016878\n",
      "Epoch [2749/10000] Avg train loss: 0.016872\n",
      "Epoch [2750/10000] Avg train loss: 0.016866\n",
      "Epoch [2751/10000] Avg train loss: 0.016860\n",
      "Epoch [2752/10000] Avg train loss: 0.016854\n",
      "Epoch [2753/10000] Avg train loss: 0.016848\n",
      "Epoch [2754/10000] Avg train loss: 0.016841\n",
      "Epoch [2755/10000] Avg train loss: 0.016835\n",
      "Epoch [2756/10000] Avg train loss: 0.016829\n",
      "Epoch [2757/10000] Avg train loss: 0.016823\n",
      "Epoch [2758/10000] Avg train loss: 0.016817\n",
      "Epoch [2759/10000] Avg train loss: 0.016811\n",
      "Epoch [2760/10000] Avg train loss: 0.016805\n",
      "Epoch [2761/10000] Avg train loss: 0.016799\n",
      "Epoch [2762/10000] Avg train loss: 0.016793\n",
      "Epoch [2763/10000] Avg train loss: 0.016787\n",
      "Epoch [2764/10000] Avg train loss: 0.016780\n",
      "Epoch [2765/10000] Avg train loss: 0.016774\n",
      "Epoch [2766/10000] Avg train loss: 0.016768\n",
      "Epoch [2767/10000] Avg train loss: 0.016762\n",
      "Epoch [2768/10000] Avg train loss: 0.016756\n",
      "Epoch [2769/10000] Avg train loss: 0.016750\n",
      "Epoch [2770/10000] Avg train loss: 0.016744\n",
      "Epoch [2771/10000] Avg train loss: 0.016738\n",
      "Epoch [2772/10000] Avg train loss: 0.016732\n",
      "Epoch [2773/10000] Avg train loss: 0.016726\n",
      "Epoch [2774/10000] Avg train loss: 0.016720\n",
      "Epoch [2775/10000] Avg train loss: 0.016714\n",
      "Epoch [2776/10000] Avg train loss: 0.016708\n",
      "Epoch [2777/10000] Avg train loss: 0.016702\n",
      "Epoch [2778/10000] Avg train loss: 0.016696\n",
      "Epoch [2779/10000] Avg train loss: 0.016690\n",
      "Epoch [2780/10000] Avg train loss: 0.016684\n",
      "Epoch [2781/10000] Avg train loss: 0.016678\n",
      "Epoch [2782/10000] Avg train loss: 0.016672\n",
      "Epoch [2783/10000] Avg train loss: 0.016666\n",
      "Epoch [2784/10000] Avg train loss: 0.016660\n",
      "Epoch [2785/10000] Avg train loss: 0.016654\n",
      "Epoch [2786/10000] Avg train loss: 0.016648\n",
      "Epoch [2787/10000] Avg train loss: 0.016642\n",
      "Epoch [2788/10000] Avg train loss: 0.016636\n",
      "Epoch [2789/10000] Avg train loss: 0.016630\n",
      "Epoch [2790/10000] Avg train loss: 0.016624\n",
      "Epoch [2791/10000] Avg train loss: 0.016618\n",
      "Epoch [2792/10000] Avg train loss: 0.016612\n",
      "Epoch [2793/10000] Avg train loss: 0.016606\n",
      "Epoch [2794/10000] Avg train loss: 0.016600\n",
      "Epoch [2795/10000] Avg train loss: 0.016594\n",
      "Epoch [2796/10000] Avg train loss: 0.016588\n",
      "Epoch [2797/10000] Avg train loss: 0.016583\n",
      "Epoch [2798/10000] Avg train loss: 0.016577\n",
      "Epoch [2799/10000] Avg train loss: 0.016571\n",
      "Epoch [2800/10000] Avg train loss: 0.016565\n",
      "Epoch [2801/10000] Avg train loss: 0.016559\n",
      "Epoch [2802/10000] Avg train loss: 0.016553\n",
      "Epoch [2803/10000] Avg train loss: 0.016547\n",
      "Epoch [2804/10000] Avg train loss: 0.016541\n",
      "Epoch [2805/10000] Avg train loss: 0.016535\n",
      "Epoch [2806/10000] Avg train loss: 0.016529\n",
      "Epoch [2807/10000] Avg train loss: 0.016523\n",
      "Epoch [2808/10000] Avg train loss: 0.016518\n",
      "Epoch [2809/10000] Avg train loss: 0.016512\n",
      "Epoch [2810/10000] Avg train loss: 0.016506\n",
      "Epoch [2811/10000] Avg train loss: 0.016500\n",
      "Epoch [2812/10000] Avg train loss: 0.016494\n",
      "Epoch [2813/10000] Avg train loss: 0.016488\n",
      "Epoch [2814/10000] Avg train loss: 0.016482\n",
      "Epoch [2815/10000] Avg train loss: 0.016476\n",
      "Epoch [2816/10000] Avg train loss: 0.016471\n",
      "Epoch [2817/10000] Avg train loss: 0.016465\n",
      "Epoch [2818/10000] Avg train loss: 0.016459\n",
      "Epoch [2819/10000] Avg train loss: 0.016453\n",
      "Epoch [2820/10000] Avg train loss: 0.016447\n",
      "Epoch [2821/10000] Avg train loss: 0.016441\n",
      "Epoch [2822/10000] Avg train loss: 0.016436\n",
      "Epoch [2823/10000] Avg train loss: 0.016430\n",
      "Epoch [2824/10000] Avg train loss: 0.016424\n",
      "Epoch [2825/10000] Avg train loss: 0.016418\n",
      "Epoch [2826/10000] Avg train loss: 0.016412\n",
      "Epoch [2827/10000] Avg train loss: 0.016407\n",
      "Epoch [2828/10000] Avg train loss: 0.016401\n",
      "Epoch [2829/10000] Avg train loss: 0.016395\n",
      "Epoch [2830/10000] Avg train loss: 0.016389\n",
      "Epoch [2831/10000] Avg train loss: 0.016383\n",
      "Epoch [2832/10000] Avg train loss: 0.016378\n",
      "Epoch [2833/10000] Avg train loss: 0.016372\n",
      "Epoch [2834/10000] Avg train loss: 0.016366\n",
      "Epoch [2835/10000] Avg train loss: 0.016360\n",
      "Epoch [2836/10000] Avg train loss: 0.016354\n",
      "Epoch [2837/10000] Avg train loss: 0.016349\n",
      "Epoch [2838/10000] Avg train loss: 0.016343\n",
      "Epoch [2839/10000] Avg train loss: 0.016337\n",
      "Epoch [2840/10000] Avg train loss: 0.016331\n",
      "Epoch [2841/10000] Avg train loss: 0.016326\n",
      "Epoch [2842/10000] Avg train loss: 0.016320\n",
      "Epoch [2843/10000] Avg train loss: 0.016314\n",
      "Epoch [2844/10000] Avg train loss: 0.016308\n",
      "Epoch [2845/10000] Avg train loss: 0.016303\n",
      "Epoch [2846/10000] Avg train loss: 0.016297\n",
      "Epoch [2847/10000] Avg train loss: 0.016291\n",
      "Epoch [2848/10000] Avg train loss: 0.016286\n",
      "Epoch [2849/10000] Avg train loss: 0.016280\n",
      "Epoch [2850/10000] Avg train loss: 0.016274\n",
      "Epoch [2851/10000] Avg train loss: 0.016268\n",
      "Epoch [2852/10000] Avg train loss: 0.016263\n",
      "Epoch [2853/10000] Avg train loss: 0.016257\n",
      "Epoch [2854/10000] Avg train loss: 0.016251\n",
      "Epoch [2855/10000] Avg train loss: 0.016246\n",
      "Epoch [2856/10000] Avg train loss: 0.016240\n",
      "Epoch [2857/10000] Avg train loss: 0.016234\n",
      "Epoch [2858/10000] Avg train loss: 0.016229\n",
      "Epoch [2859/10000] Avg train loss: 0.016223\n",
      "Epoch [2860/10000] Avg train loss: 0.016217\n",
      "Epoch [2861/10000] Avg train loss: 0.016212\n",
      "Epoch [2862/10000] Avg train loss: 0.016206\n",
      "Epoch [2863/10000] Avg train loss: 0.016200\n",
      "Epoch [2864/10000] Avg train loss: 0.016195\n",
      "Epoch [2865/10000] Avg train loss: 0.016189\n",
      "Epoch [2866/10000] Avg train loss: 0.016183\n",
      "Epoch [2867/10000] Avg train loss: 0.016178\n",
      "Epoch [2868/10000] Avg train loss: 0.016172\n",
      "Epoch [2869/10000] Avg train loss: 0.016166\n",
      "Epoch [2870/10000] Avg train loss: 0.016161\n",
      "Epoch [2871/10000] Avg train loss: 0.016155\n",
      "Epoch [2872/10000] Avg train loss: 0.016149\n",
      "Epoch [2873/10000] Avg train loss: 0.016144\n",
      "Epoch [2874/10000] Avg train loss: 0.016138\n",
      "Epoch [2875/10000] Avg train loss: 0.016133\n",
      "Epoch [2876/10000] Avg train loss: 0.016127\n",
      "Epoch [2877/10000] Avg train loss: 0.016121\n",
      "Epoch [2878/10000] Avg train loss: 0.016116\n",
      "Epoch [2879/10000] Avg train loss: 0.016110\n",
      "Epoch [2880/10000] Avg train loss: 0.016105\n",
      "Epoch [2881/10000] Avg train loss: 0.016099\n",
      "Epoch [2882/10000] Avg train loss: 0.016093\n",
      "Epoch [2883/10000] Avg train loss: 0.016088\n",
      "Epoch [2884/10000] Avg train loss: 0.016082\n",
      "Epoch [2885/10000] Avg train loss: 0.016077\n",
      "Epoch [2886/10000] Avg train loss: 0.016071\n",
      "Epoch [2887/10000] Avg train loss: 0.016066\n",
      "Epoch [2888/10000] Avg train loss: 0.016060\n",
      "Epoch [2889/10000] Avg train loss: 0.016054\n",
      "Epoch [2890/10000] Avg train loss: 0.016049\n",
      "Epoch [2891/10000] Avg train loss: 0.016043\n",
      "Epoch [2892/10000] Avg train loss: 0.016038\n",
      "Epoch [2893/10000] Avg train loss: 0.016032\n",
      "Epoch [2894/10000] Avg train loss: 0.016027\n",
      "Epoch [2895/10000] Avg train loss: 0.016021\n",
      "Epoch [2896/10000] Avg train loss: 0.016016\n",
      "Epoch [2897/10000] Avg train loss: 0.016010\n",
      "Epoch [2898/10000] Avg train loss: 0.016005\n",
      "Epoch [2899/10000] Avg train loss: 0.015999\n",
      "Epoch [2900/10000] Avg train loss: 0.015994\n",
      "Epoch [2901/10000] Avg train loss: 0.015988\n",
      "Epoch [2902/10000] Avg train loss: 0.015983\n",
      "Epoch [2903/10000] Avg train loss: 0.015977\n",
      "Epoch [2904/10000] Avg train loss: 0.015972\n",
      "Epoch [2905/10000] Avg train loss: 0.015966\n",
      "Epoch [2906/10000] Avg train loss: 0.015961\n",
      "Epoch [2907/10000] Avg train loss: 0.015955\n",
      "Epoch [2908/10000] Avg train loss: 0.015950\n",
      "Epoch [2909/10000] Avg train loss: 0.015944\n",
      "Epoch [2910/10000] Avg train loss: 0.015939\n",
      "Epoch [2911/10000] Avg train loss: 0.015933\n",
      "Epoch [2912/10000] Avg train loss: 0.015928\n",
      "Epoch [2913/10000] Avg train loss: 0.015922\n",
      "Epoch [2914/10000] Avg train loss: 0.015917\n",
      "Epoch [2915/10000] Avg train loss: 0.015911\n",
      "Epoch [2916/10000] Avg train loss: 0.015906\n",
      "Epoch [2917/10000] Avg train loss: 0.015900\n",
      "Epoch [2918/10000] Avg train loss: 0.015895\n",
      "Epoch [2919/10000] Avg train loss: 0.015889\n",
      "Epoch [2920/10000] Avg train loss: 0.015884\n",
      "Epoch [2921/10000] Avg train loss: 0.015879\n",
      "Epoch [2922/10000] Avg train loss: 0.015873\n",
      "Epoch [2923/10000] Avg train loss: 0.015868\n",
      "Epoch [2924/10000] Avg train loss: 0.015862\n",
      "Epoch [2925/10000] Avg train loss: 0.015857\n",
      "Epoch [2926/10000] Avg train loss: 0.015851\n",
      "Epoch [2927/10000] Avg train loss: 0.015846\n",
      "Epoch [2928/10000] Avg train loss: 0.015841\n",
      "Epoch [2929/10000] Avg train loss: 0.015835\n",
      "Epoch [2930/10000] Avg train loss: 0.015830\n",
      "Epoch [2931/10000] Avg train loss: 0.015824\n",
      "Epoch [2932/10000] Avg train loss: 0.015819\n",
      "Epoch [2933/10000] Avg train loss: 0.015814\n",
      "Epoch [2934/10000] Avg train loss: 0.015808\n",
      "Epoch [2935/10000] Avg train loss: 0.015803\n",
      "Epoch [2936/10000] Avg train loss: 0.015797\n",
      "Epoch [2937/10000] Avg train loss: 0.015792\n",
      "Epoch [2938/10000] Avg train loss: 0.015787\n",
      "Epoch [2939/10000] Avg train loss: 0.015781\n",
      "Epoch [2940/10000] Avg train loss: 0.015776\n",
      "Epoch [2941/10000] Avg train loss: 0.015771\n",
      "Epoch [2942/10000] Avg train loss: 0.015765\n",
      "Epoch [2943/10000] Avg train loss: 0.015760\n",
      "Epoch [2944/10000] Avg train loss: 0.015755\n",
      "Epoch [2945/10000] Avg train loss: 0.015749\n",
      "Epoch [2946/10000] Avg train loss: 0.015744\n",
      "Epoch [2947/10000] Avg train loss: 0.015738\n",
      "Epoch [2948/10000] Avg train loss: 0.015733\n",
      "Epoch [2949/10000] Avg train loss: 0.015728\n",
      "Epoch [2950/10000] Avg train loss: 0.015722\n",
      "Epoch [2951/10000] Avg train loss: 0.015717\n",
      "Epoch [2952/10000] Avg train loss: 0.015712\n",
      "Epoch [2953/10000] Avg train loss: 0.015706\n",
      "Epoch [2954/10000] Avg train loss: 0.015701\n",
      "Epoch [2955/10000] Avg train loss: 0.015696\n",
      "Epoch [2956/10000] Avg train loss: 0.015691\n",
      "Epoch [2957/10000] Avg train loss: 0.015685\n",
      "Epoch [2958/10000] Avg train loss: 0.015680\n",
      "Epoch [2959/10000] Avg train loss: 0.015675\n",
      "Epoch [2960/10000] Avg train loss: 0.015669\n",
      "Epoch [2961/10000] Avg train loss: 0.015664\n",
      "Epoch [2962/10000] Avg train loss: 0.015659\n",
      "Epoch [2963/10000] Avg train loss: 0.015653\n",
      "Epoch [2964/10000] Avg train loss: 0.015648\n",
      "Epoch [2965/10000] Avg train loss: 0.015643\n",
      "Epoch [2966/10000] Avg train loss: 0.015638\n",
      "Epoch [2967/10000] Avg train loss: 0.015632\n",
      "Epoch [2968/10000] Avg train loss: 0.015627\n",
      "Epoch [2969/10000] Avg train loss: 0.015622\n",
      "Epoch [2970/10000] Avg train loss: 0.015617\n",
      "Epoch [2971/10000] Avg train loss: 0.015611\n",
      "Epoch [2972/10000] Avg train loss: 0.015606\n",
      "Epoch [2973/10000] Avg train loss: 0.015601\n",
      "Epoch [2974/10000] Avg train loss: 0.015596\n",
      "Epoch [2975/10000] Avg train loss: 0.015590\n",
      "Epoch [2976/10000] Avg train loss: 0.015585\n",
      "Epoch [2977/10000] Avg train loss: 0.015580\n",
      "Epoch [2978/10000] Avg train loss: 0.015575\n",
      "Epoch [2979/10000] Avg train loss: 0.015569\n",
      "Epoch [2980/10000] Avg train loss: 0.015564\n",
      "Epoch [2981/10000] Avg train loss: 0.015559\n",
      "Epoch [2982/10000] Avg train loss: 0.015554\n",
      "Epoch [2983/10000] Avg train loss: 0.015549\n",
      "Epoch [2984/10000] Avg train loss: 0.015543\n",
      "Epoch [2985/10000] Avg train loss: 0.015538\n",
      "Epoch [2986/10000] Avg train loss: 0.015533\n",
      "Epoch [2987/10000] Avg train loss: 0.015528\n",
      "Epoch [2988/10000] Avg train loss: 0.015523\n",
      "Epoch [2989/10000] Avg train loss: 0.015517\n",
      "Epoch [2990/10000] Avg train loss: 0.015512\n",
      "Epoch [2991/10000] Avg train loss: 0.015507\n",
      "Epoch [2992/10000] Avg train loss: 0.015502\n",
      "Epoch [2993/10000] Avg train loss: 0.015497\n",
      "Epoch [2994/10000] Avg train loss: 0.015491\n",
      "Epoch [2995/10000] Avg train loss: 0.015486\n",
      "Epoch [2996/10000] Avg train loss: 0.015481\n",
      "Epoch [2997/10000] Avg train loss: 0.015476\n",
      "Epoch [2998/10000] Avg train loss: 0.015471\n",
      "Epoch [2999/10000] Avg train loss: 0.015466\n",
      "Epoch [3000/10000] Avg train loss: 0.015460\n",
      "Epoch [3001/10000] Avg train loss: 0.015455\n",
      "Epoch [3002/10000] Avg train loss: 0.015450\n",
      "Epoch [3003/10000] Avg train loss: 0.015445\n",
      "Epoch [3004/10000] Avg train loss: 0.015440\n",
      "Epoch [3005/10000] Avg train loss: 0.015435\n",
      "Epoch [3006/10000] Avg train loss: 0.015430\n",
      "Epoch [3007/10000] Avg train loss: 0.015424\n",
      "Epoch [3008/10000] Avg train loss: 0.015419\n",
      "Epoch [3009/10000] Avg train loss: 0.015414\n",
      "Epoch [3010/10000] Avg train loss: 0.015409\n",
      "Epoch [3011/10000] Avg train loss: 0.015404\n",
      "Epoch [3012/10000] Avg train loss: 0.015399\n",
      "Epoch [3013/10000] Avg train loss: 0.015394\n",
      "Epoch [3014/10000] Avg train loss: 0.015389\n",
      "Epoch [3015/10000] Avg train loss: 0.015384\n",
      "Epoch [3016/10000] Avg train loss: 0.015378\n",
      "Epoch [3017/10000] Avg train loss: 0.015373\n",
      "Epoch [3018/10000] Avg train loss: 0.015368\n",
      "Epoch [3019/10000] Avg train loss: 0.015363\n",
      "Epoch [3020/10000] Avg train loss: 0.015358\n",
      "Epoch [3021/10000] Avg train loss: 0.015353\n",
      "Epoch [3022/10000] Avg train loss: 0.015348\n",
      "Epoch [3023/10000] Avg train loss: 0.015343\n",
      "Epoch [3024/10000] Avg train loss: 0.015338\n",
      "Epoch [3025/10000] Avg train loss: 0.015333\n",
      "Epoch [3026/10000] Avg train loss: 0.015328\n",
      "Epoch [3027/10000] Avg train loss: 0.015323\n",
      "Epoch [3028/10000] Avg train loss: 0.015317\n",
      "Epoch [3029/10000] Avg train loss: 0.015312\n",
      "Epoch [3030/10000] Avg train loss: 0.015307\n",
      "Epoch [3031/10000] Avg train loss: 0.015302\n",
      "Epoch [3032/10000] Avg train loss: 0.015297\n",
      "Epoch [3033/10000] Avg train loss: 0.015292\n",
      "Epoch [3034/10000] Avg train loss: 0.015287\n",
      "Epoch [3035/10000] Avg train loss: 0.015282\n",
      "Epoch [3036/10000] Avg train loss: 0.015277\n",
      "Epoch [3037/10000] Avg train loss: 0.015272\n",
      "Epoch [3038/10000] Avg train loss: 0.015267\n",
      "Epoch [3039/10000] Avg train loss: 0.015262\n",
      "Epoch [3040/10000] Avg train loss: 0.015257\n",
      "Epoch [3041/10000] Avg train loss: 0.015252\n",
      "Epoch [3042/10000] Avg train loss: 0.015247\n",
      "Epoch [3043/10000] Avg train loss: 0.015242\n",
      "Epoch [3044/10000] Avg train loss: 0.015237\n",
      "Epoch [3045/10000] Avg train loss: 0.015232\n",
      "Epoch [3046/10000] Avg train loss: 0.015227\n",
      "Epoch [3047/10000] Avg train loss: 0.015222\n",
      "Epoch [3048/10000] Avg train loss: 0.015217\n",
      "Epoch [3049/10000] Avg train loss: 0.015212\n",
      "Epoch [3050/10000] Avg train loss: 0.015207\n",
      "Epoch [3051/10000] Avg train loss: 0.015202\n",
      "Epoch [3052/10000] Avg train loss: 0.015197\n",
      "Epoch [3053/10000] Avg train loss: 0.015192\n",
      "Epoch [3054/10000] Avg train loss: 0.015187\n",
      "Epoch [3055/10000] Avg train loss: 0.015182\n",
      "Epoch [3056/10000] Avg train loss: 0.015177\n",
      "Epoch [3057/10000] Avg train loss: 0.015172\n",
      "Epoch [3058/10000] Avg train loss: 0.015167\n",
      "Epoch [3059/10000] Avg train loss: 0.015162\n",
      "Epoch [3060/10000] Avg train loss: 0.015157\n",
      "Epoch [3061/10000] Avg train loss: 0.015152\n",
      "Epoch [3062/10000] Avg train loss: 0.015147\n",
      "Epoch [3063/10000] Avg train loss: 0.015142\n",
      "Epoch [3064/10000] Avg train loss: 0.015137\n",
      "Epoch [3065/10000] Avg train loss: 0.015133\n",
      "Epoch [3066/10000] Avg train loss: 0.015128\n",
      "Epoch [3067/10000] Avg train loss: 0.015123\n",
      "Epoch [3068/10000] Avg train loss: 0.015118\n",
      "Epoch [3069/10000] Avg train loss: 0.015113\n",
      "Epoch [3070/10000] Avg train loss: 0.015108\n",
      "Epoch [3071/10000] Avg train loss: 0.015103\n",
      "Epoch [3072/10000] Avg train loss: 0.015098\n",
      "Epoch [3073/10000] Avg train loss: 0.015093\n",
      "Epoch [3074/10000] Avg train loss: 0.015088\n",
      "Epoch [3075/10000] Avg train loss: 0.015083\n",
      "Epoch [3076/10000] Avg train loss: 0.015078\n",
      "Epoch [3077/10000] Avg train loss: 0.015074\n",
      "Epoch [3078/10000] Avg train loss: 0.015069\n",
      "Epoch [3079/10000] Avg train loss: 0.015064\n",
      "Epoch [3080/10000] Avg train loss: 0.015059\n",
      "Epoch [3081/10000] Avg train loss: 0.015054\n",
      "Epoch [3082/10000] Avg train loss: 0.015049\n",
      "Epoch [3083/10000] Avg train loss: 0.015044\n",
      "Epoch [3084/10000] Avg train loss: 0.015039\n",
      "Epoch [3085/10000] Avg train loss: 0.015034\n",
      "Epoch [3086/10000] Avg train loss: 0.015030\n",
      "Epoch [3087/10000] Avg train loss: 0.015025\n",
      "Epoch [3088/10000] Avg train loss: 0.015020\n",
      "Epoch [3089/10000] Avg train loss: 0.015015\n",
      "Epoch [3090/10000] Avg train loss: 0.015010\n",
      "Epoch [3091/10000] Avg train loss: 0.015005\n",
      "Epoch [3092/10000] Avg train loss: 0.015000\n",
      "Epoch [3093/10000] Avg train loss: 0.014996\n",
      "Epoch [3094/10000] Avg train loss: 0.014991\n",
      "Epoch [3095/10000] Avg train loss: 0.014986\n",
      "Epoch [3096/10000] Avg train loss: 0.014981\n",
      "Epoch [3097/10000] Avg train loss: 0.014976\n",
      "Epoch [3098/10000] Avg train loss: 0.014971\n",
      "Epoch [3099/10000] Avg train loss: 0.014967\n",
      "Epoch [3100/10000] Avg train loss: 0.014962\n",
      "Epoch [3101/10000] Avg train loss: 0.014957\n",
      "Epoch [3102/10000] Avg train loss: 0.014952\n",
      "Epoch [3103/10000] Avg train loss: 0.014947\n",
      "Epoch [3104/10000] Avg train loss: 0.014942\n",
      "Epoch [3105/10000] Avg train loss: 0.014938\n",
      "Epoch [3106/10000] Avg train loss: 0.014933\n",
      "Epoch [3107/10000] Avg train loss: 0.014928\n",
      "Epoch [3108/10000] Avg train loss: 0.014923\n",
      "Epoch [3109/10000] Avg train loss: 0.014918\n",
      "Epoch [3110/10000] Avg train loss: 0.014914\n",
      "Epoch [3111/10000] Avg train loss: 0.014909\n",
      "Epoch [3112/10000] Avg train loss: 0.014904\n",
      "Epoch [3113/10000] Avg train loss: 0.014899\n",
      "Epoch [3114/10000] Avg train loss: 0.014894\n",
      "Epoch [3115/10000] Avg train loss: 0.014890\n",
      "Epoch [3116/10000] Avg train loss: 0.014885\n",
      "Epoch [3117/10000] Avg train loss: 0.014880\n",
      "Epoch [3118/10000] Avg train loss: 0.014875\n",
      "Epoch [3119/10000] Avg train loss: 0.014871\n",
      "Epoch [3120/10000] Avg train loss: 0.014866\n",
      "Epoch [3121/10000] Avg train loss: 0.014861\n",
      "Epoch [3122/10000] Avg train loss: 0.014856\n",
      "Epoch [3123/10000] Avg train loss: 0.014852\n",
      "Epoch [3124/10000] Avg train loss: 0.014847\n",
      "Epoch [3125/10000] Avg train loss: 0.014842\n",
      "Epoch [3126/10000] Avg train loss: 0.014837\n",
      "Epoch [3127/10000] Avg train loss: 0.014833\n",
      "Epoch [3128/10000] Avg train loss: 0.014828\n",
      "Epoch [3129/10000] Avg train loss: 0.014823\n",
      "Epoch [3130/10000] Avg train loss: 0.014818\n",
      "Epoch [3131/10000] Avg train loss: 0.014814\n",
      "Epoch [3132/10000] Avg train loss: 0.014809\n",
      "Epoch [3133/10000] Avg train loss: 0.014804\n",
      "Epoch [3134/10000] Avg train loss: 0.014799\n",
      "Epoch [3135/10000] Avg train loss: 0.014795\n",
      "Epoch [3136/10000] Avg train loss: 0.014790\n",
      "Epoch [3137/10000] Avg train loss: 0.014785\n",
      "Epoch [3138/10000] Avg train loss: 0.014781\n",
      "Epoch [3139/10000] Avg train loss: 0.014776\n",
      "Epoch [3140/10000] Avg train loss: 0.014771\n",
      "Epoch [3141/10000] Avg train loss: 0.014766\n",
      "Epoch [3142/10000] Avg train loss: 0.014762\n",
      "Epoch [3143/10000] Avg train loss: 0.014757\n",
      "Epoch [3144/10000] Avg train loss: 0.014752\n",
      "Epoch [3145/10000] Avg train loss: 0.014748\n",
      "Epoch [3146/10000] Avg train loss: 0.014743\n",
      "Epoch [3147/10000] Avg train loss: 0.014738\n",
      "Epoch [3148/10000] Avg train loss: 0.014734\n",
      "Epoch [3149/10000] Avg train loss: 0.014729\n",
      "Epoch [3150/10000] Avg train loss: 0.014724\n",
      "Epoch [3151/10000] Avg train loss: 0.014720\n",
      "Epoch [3152/10000] Avg train loss: 0.014715\n",
      "Epoch [3153/10000] Avg train loss: 0.014710\n",
      "Epoch [3154/10000] Avg train loss: 0.014706\n",
      "Epoch [3155/10000] Avg train loss: 0.014701\n",
      "Epoch [3156/10000] Avg train loss: 0.014696\n",
      "Epoch [3157/10000] Avg train loss: 0.014692\n",
      "Epoch [3158/10000] Avg train loss: 0.014687\n",
      "Epoch [3159/10000] Avg train loss: 0.014682\n",
      "Epoch [3160/10000] Avg train loss: 0.014678\n",
      "Epoch [3161/10000] Avg train loss: 0.014673\n",
      "Epoch [3162/10000] Avg train loss: 0.014668\n",
      "Epoch [3163/10000] Avg train loss: 0.014664\n",
      "Epoch [3164/10000] Avg train loss: 0.014659\n",
      "Epoch [3165/10000] Avg train loss: 0.014654\n",
      "Epoch [3166/10000] Avg train loss: 0.014650\n",
      "Epoch [3167/10000] Avg train loss: 0.014645\n",
      "Epoch [3168/10000] Avg train loss: 0.014641\n",
      "Epoch [3169/10000] Avg train loss: 0.014636\n",
      "Epoch [3170/10000] Avg train loss: 0.014631\n",
      "Epoch [3171/10000] Avg train loss: 0.014627\n",
      "Epoch [3172/10000] Avg train loss: 0.014622\n",
      "Epoch [3173/10000] Avg train loss: 0.014617\n",
      "Epoch [3174/10000] Avg train loss: 0.014613\n",
      "Epoch [3175/10000] Avg train loss: 0.014608\n",
      "Epoch [3176/10000] Avg train loss: 0.014604\n",
      "Epoch [3177/10000] Avg train loss: 0.014599\n",
      "Epoch [3178/10000] Avg train loss: 0.014594\n",
      "Epoch [3179/10000] Avg train loss: 0.014590\n",
      "Epoch [3180/10000] Avg train loss: 0.014585\n",
      "Epoch [3181/10000] Avg train loss: 0.014581\n",
      "Epoch [3182/10000] Avg train loss: 0.014576\n",
      "Epoch [3183/10000] Avg train loss: 0.014572\n",
      "Epoch [3184/10000] Avg train loss: 0.014567\n",
      "Epoch [3185/10000] Avg train loss: 0.014562\n",
      "Epoch [3186/10000] Avg train loss: 0.014558\n",
      "Epoch [3187/10000] Avg train loss: 0.014553\n",
      "Epoch [3188/10000] Avg train loss: 0.014549\n",
      "Epoch [3189/10000] Avg train loss: 0.014544\n",
      "Epoch [3190/10000] Avg train loss: 0.014540\n",
      "Epoch [3191/10000] Avg train loss: 0.014535\n",
      "Epoch [3192/10000] Avg train loss: 0.014530\n",
      "Epoch [3193/10000] Avg train loss: 0.014526\n",
      "Epoch [3194/10000] Avg train loss: 0.014521\n",
      "Epoch [3195/10000] Avg train loss: 0.014517\n",
      "Epoch [3196/10000] Avg train loss: 0.014512\n",
      "Epoch [3197/10000] Avg train loss: 0.014508\n",
      "Epoch [3198/10000] Avg train loss: 0.014503\n",
      "Epoch [3199/10000] Avg train loss: 0.014499\n",
      "Epoch [3200/10000] Avg train loss: 0.014494\n",
      "Epoch [3201/10000] Avg train loss: 0.014490\n",
      "Epoch [3202/10000] Avg train loss: 0.014485\n",
      "Epoch [3203/10000] Avg train loss: 0.014481\n",
      "Epoch [3204/10000] Avg train loss: 0.014476\n",
      "Epoch [3205/10000] Avg train loss: 0.014472\n",
      "Epoch [3206/10000] Avg train loss: 0.014467\n",
      "Epoch [3207/10000] Avg train loss: 0.014463\n",
      "Epoch [3208/10000] Avg train loss: 0.014458\n",
      "Epoch [3209/10000] Avg train loss: 0.014453\n",
      "Epoch [3210/10000] Avg train loss: 0.014449\n",
      "Epoch [3211/10000] Avg train loss: 0.014444\n",
      "Epoch [3212/10000] Avg train loss: 0.014440\n",
      "Epoch [3213/10000] Avg train loss: 0.014435\n",
      "Epoch [3214/10000] Avg train loss: 0.014431\n",
      "Epoch [3215/10000] Avg train loss: 0.014427\n",
      "Epoch [3216/10000] Avg train loss: 0.014422\n",
      "Epoch [3217/10000] Avg train loss: 0.014418\n",
      "Epoch [3218/10000] Avg train loss: 0.014413\n",
      "Epoch [3219/10000] Avg train loss: 0.014409\n",
      "Epoch [3220/10000] Avg train loss: 0.014404\n",
      "Epoch [3221/10000] Avg train loss: 0.014400\n",
      "Epoch [3222/10000] Avg train loss: 0.014395\n",
      "Epoch [3223/10000] Avg train loss: 0.014391\n",
      "Epoch [3224/10000] Avg train loss: 0.014386\n",
      "Epoch [3225/10000] Avg train loss: 0.014382\n",
      "Epoch [3226/10000] Avg train loss: 0.014377\n",
      "Epoch [3227/10000] Avg train loss: 0.014373\n",
      "Epoch [3228/10000] Avg train loss: 0.014368\n",
      "Epoch [3229/10000] Avg train loss: 0.014364\n",
      "Epoch [3230/10000] Avg train loss: 0.014360\n",
      "Epoch [3231/10000] Avg train loss: 0.014355\n",
      "Epoch [3232/10000] Avg train loss: 0.014351\n",
      "Epoch [3233/10000] Avg train loss: 0.014346\n",
      "Epoch [3234/10000] Avg train loss: 0.014342\n",
      "Epoch [3235/10000] Avg train loss: 0.014337\n",
      "Epoch [3236/10000] Avg train loss: 0.014333\n",
      "Epoch [3237/10000] Avg train loss: 0.014328\n",
      "Epoch [3238/10000] Avg train loss: 0.014324\n",
      "Epoch [3239/10000] Avg train loss: 0.014320\n",
      "Epoch [3240/10000] Avg train loss: 0.014315\n",
      "Epoch [3241/10000] Avg train loss: 0.014311\n",
      "Epoch [3242/10000] Avg train loss: 0.014306\n",
      "Epoch [3243/10000] Avg train loss: 0.014302\n",
      "Epoch [3244/10000] Avg train loss: 0.014298\n",
      "Epoch [3245/10000] Avg train loss: 0.014293\n",
      "Epoch [3246/10000] Avg train loss: 0.014289\n",
      "Epoch [3247/10000] Avg train loss: 0.014284\n",
      "Epoch [3248/10000] Avg train loss: 0.014280\n",
      "Epoch [3249/10000] Avg train loss: 0.014276\n",
      "Epoch [3250/10000] Avg train loss: 0.014271\n",
      "Epoch [3251/10000] Avg train loss: 0.014267\n",
      "Epoch [3252/10000] Avg train loss: 0.014262\n",
      "Epoch [3253/10000] Avg train loss: 0.014258\n",
      "Epoch [3254/10000] Avg train loss: 0.014254\n",
      "Epoch [3255/10000] Avg train loss: 0.014249\n",
      "Epoch [3256/10000] Avg train loss: 0.014245\n",
      "Epoch [3257/10000] Avg train loss: 0.014240\n",
      "Epoch [3258/10000] Avg train loss: 0.014236\n",
      "Epoch [3259/10000] Avg train loss: 0.014232\n",
      "Epoch [3260/10000] Avg train loss: 0.014227\n",
      "Epoch [3261/10000] Avg train loss: 0.014223\n",
      "Epoch [3262/10000] Avg train loss: 0.014219\n",
      "Epoch [3263/10000] Avg train loss: 0.014214\n",
      "Epoch [3264/10000] Avg train loss: 0.014210\n",
      "Epoch [3265/10000] Avg train loss: 0.014206\n",
      "Epoch [3266/10000] Avg train loss: 0.014201\n",
      "Epoch [3267/10000] Avg train loss: 0.014197\n",
      "Epoch [3268/10000] Avg train loss: 0.014193\n",
      "Epoch [3269/10000] Avg train loss: 0.014188\n",
      "Epoch [3270/10000] Avg train loss: 0.014184\n",
      "Epoch [3271/10000] Avg train loss: 0.014180\n",
      "Epoch [3272/10000] Avg train loss: 0.014175\n",
      "Epoch [3273/10000] Avg train loss: 0.014171\n",
      "Epoch [3274/10000] Avg train loss: 0.014167\n",
      "Epoch [3275/10000] Avg train loss: 0.014162\n",
      "Epoch [3276/10000] Avg train loss: 0.014158\n",
      "Epoch [3277/10000] Avg train loss: 0.014154\n",
      "Epoch [3278/10000] Avg train loss: 0.014149\n",
      "Epoch [3279/10000] Avg train loss: 0.014145\n",
      "Epoch [3280/10000] Avg train loss: 0.014141\n",
      "Epoch [3281/10000] Avg train loss: 0.014136\n",
      "Epoch [3282/10000] Avg train loss: 0.014132\n",
      "Epoch [3283/10000] Avg train loss: 0.014128\n",
      "Epoch [3284/10000] Avg train loss: 0.014123\n",
      "Epoch [3285/10000] Avg train loss: 0.014119\n",
      "Epoch [3286/10000] Avg train loss: 0.014115\n",
      "Epoch [3287/10000] Avg train loss: 0.014111\n",
      "Epoch [3288/10000] Avg train loss: 0.014106\n",
      "Epoch [3289/10000] Avg train loss: 0.014102\n",
      "Epoch [3290/10000] Avg train loss: 0.014098\n",
      "Epoch [3291/10000] Avg train loss: 0.014093\n",
      "Epoch [3292/10000] Avg train loss: 0.014089\n",
      "Epoch [3293/10000] Avg train loss: 0.014085\n",
      "Epoch [3294/10000] Avg train loss: 0.014081\n",
      "Epoch [3295/10000] Avg train loss: 0.014076\n",
      "Epoch [3296/10000] Avg train loss: 0.014072\n",
      "Epoch [3297/10000] Avg train loss: 0.014068\n",
      "Epoch [3298/10000] Avg train loss: 0.014063\n",
      "Epoch [3299/10000] Avg train loss: 0.014059\n",
      "Epoch [3300/10000] Avg train loss: 0.014055\n",
      "Epoch [3301/10000] Avg train loss: 0.014051\n",
      "Epoch [3302/10000] Avg train loss: 0.014046\n",
      "Epoch [3303/10000] Avg train loss: 0.014042\n",
      "Epoch [3304/10000] Avg train loss: 0.014038\n",
      "Epoch [3305/10000] Avg train loss: 0.014034\n",
      "Epoch [3306/10000] Avg train loss: 0.014029\n",
      "Epoch [3307/10000] Avg train loss: 0.014025\n",
      "Epoch [3308/10000] Avg train loss: 0.014021\n",
      "Epoch [3309/10000] Avg train loss: 0.014017\n",
      "Epoch [3310/10000] Avg train loss: 0.014012\n",
      "Epoch [3311/10000] Avg train loss: 0.014008\n",
      "Epoch [3312/10000] Avg train loss: 0.014004\n",
      "Epoch [3313/10000] Avg train loss: 0.014000\n",
      "Epoch [3314/10000] Avg train loss: 0.013996\n",
      "Epoch [3315/10000] Avg train loss: 0.013991\n",
      "Epoch [3316/10000] Avg train loss: 0.013987\n",
      "Epoch [3317/10000] Avg train loss: 0.013983\n",
      "Epoch [3318/10000] Avg train loss: 0.013979\n",
      "Epoch [3319/10000] Avg train loss: 0.013974\n",
      "Epoch [3320/10000] Avg train loss: 0.013970\n",
      "Epoch [3321/10000] Avg train loss: 0.013966\n",
      "Epoch [3322/10000] Avg train loss: 0.013962\n",
      "Epoch [3323/10000] Avg train loss: 0.013958\n",
      "Epoch [3324/10000] Avg train loss: 0.013953\n",
      "Epoch [3325/10000] Avg train loss: 0.013949\n",
      "Epoch [3326/10000] Avg train loss: 0.013945\n",
      "Epoch [3327/10000] Avg train loss: 0.013941\n",
      "Epoch [3328/10000] Avg train loss: 0.013937\n",
      "Epoch [3329/10000] Avg train loss: 0.013932\n",
      "Epoch [3330/10000] Avg train loss: 0.013928\n",
      "Epoch [3331/10000] Avg train loss: 0.013924\n",
      "Epoch [3332/10000] Avg train loss: 0.013920\n",
      "Epoch [3333/10000] Avg train loss: 0.013916\n",
      "Epoch [3334/10000] Avg train loss: 0.013912\n",
      "Epoch [3335/10000] Avg train loss: 0.013907\n",
      "Epoch [3336/10000] Avg train loss: 0.013903\n",
      "Epoch [3337/10000] Avg train loss: 0.013899\n",
      "Epoch [3338/10000] Avg train loss: 0.013895\n",
      "Epoch [3339/10000] Avg train loss: 0.013891\n",
      "Epoch [3340/10000] Avg train loss: 0.013887\n",
      "Epoch [3341/10000] Avg train loss: 0.013882\n",
      "Epoch [3342/10000] Avg train loss: 0.013878\n",
      "Epoch [3343/10000] Avg train loss: 0.013874\n",
      "Epoch [3344/10000] Avg train loss: 0.013870\n",
      "Epoch [3345/10000] Avg train loss: 0.013866\n",
      "Epoch [3346/10000] Avg train loss: 0.013862\n",
      "Epoch [3347/10000] Avg train loss: 0.013858\n",
      "Epoch [3348/10000] Avg train loss: 0.013853\n",
      "Epoch [3349/10000] Avg train loss: 0.013849\n",
      "Epoch [3350/10000] Avg train loss: 0.013845\n",
      "Epoch [3351/10000] Avg train loss: 0.013841\n",
      "Epoch [3352/10000] Avg train loss: 0.013837\n",
      "Epoch [3353/10000] Avg train loss: 0.013833\n",
      "Epoch [3354/10000] Avg train loss: 0.013829\n",
      "Epoch [3355/10000] Avg train loss: 0.013825\n",
      "Epoch [3356/10000] Avg train loss: 0.013820\n",
      "Epoch [3357/10000] Avg train loss: 0.013816\n",
      "Epoch [3358/10000] Avg train loss: 0.013812\n",
      "Epoch [3359/10000] Avg train loss: 0.013808\n",
      "Epoch [3360/10000] Avg train loss: 0.013804\n",
      "Epoch [3361/10000] Avg train loss: 0.013800\n",
      "Epoch [3362/10000] Avg train loss: 0.013796\n",
      "Epoch [3363/10000] Avg train loss: 0.013792\n",
      "Epoch [3364/10000] Avg train loss: 0.013788\n",
      "Epoch [3365/10000] Avg train loss: 0.013783\n",
      "Epoch [3366/10000] Avg train loss: 0.013779\n",
      "Epoch [3367/10000] Avg train loss: 0.013775\n",
      "Epoch [3368/10000] Avg train loss: 0.013771\n",
      "Epoch [3369/10000] Avg train loss: 0.013767\n",
      "Epoch [3370/10000] Avg train loss: 0.013763\n",
      "Epoch [3371/10000] Avg train loss: 0.013759\n",
      "Epoch [3372/10000] Avg train loss: 0.013755\n",
      "Epoch [3373/10000] Avg train loss: 0.013751\n",
      "Epoch [3374/10000] Avg train loss: 0.013747\n",
      "Epoch [3375/10000] Avg train loss: 0.013743\n",
      "Epoch [3376/10000] Avg train loss: 0.013739\n",
      "Epoch [3377/10000] Avg train loss: 0.013734\n",
      "Epoch [3378/10000] Avg train loss: 0.013730\n",
      "Epoch [3379/10000] Avg train loss: 0.013726\n",
      "Epoch [3380/10000] Avg train loss: 0.013722\n",
      "Epoch [3381/10000] Avg train loss: 0.013718\n",
      "Epoch [3382/10000] Avg train loss: 0.013714\n",
      "Epoch [3383/10000] Avg train loss: 0.013710\n",
      "Epoch [3384/10000] Avg train loss: 0.013706\n",
      "Epoch [3385/10000] Avg train loss: 0.013702\n",
      "Epoch [3386/10000] Avg train loss: 0.013698\n",
      "Epoch [3387/10000] Avg train loss: 0.013694\n",
      "Epoch [3388/10000] Avg train loss: 0.013690\n",
      "Epoch [3389/10000] Avg train loss: 0.013686\n",
      "Epoch [3390/10000] Avg train loss: 0.013682\n",
      "Epoch [3391/10000] Avg train loss: 0.013678\n",
      "Epoch [3392/10000] Avg train loss: 0.013674\n",
      "Epoch [3393/10000] Avg train loss: 0.013670\n",
      "Epoch [3394/10000] Avg train loss: 0.013666\n",
      "Epoch [3395/10000] Avg train loss: 0.013662\n",
      "Epoch [3396/10000] Avg train loss: 0.013658\n",
      "Epoch [3397/10000] Avg train loss: 0.013654\n",
      "Epoch [3398/10000] Avg train loss: 0.013650\n",
      "Epoch [3399/10000] Avg train loss: 0.013646\n",
      "Epoch [3400/10000] Avg train loss: 0.013642\n",
      "Epoch [3401/10000] Avg train loss: 0.013638\n",
      "Epoch [3402/10000] Avg train loss: 0.013634\n",
      "Epoch [3403/10000] Avg train loss: 0.013630\n",
      "Epoch [3404/10000] Avg train loss: 0.013626\n",
      "Epoch [3405/10000] Avg train loss: 0.013622\n",
      "Epoch [3406/10000] Avg train loss: 0.013618\n",
      "Epoch [3407/10000] Avg train loss: 0.013614\n",
      "Epoch [3408/10000] Avg train loss: 0.013610\n",
      "Epoch [3409/10000] Avg train loss: 0.013606\n",
      "Epoch [3410/10000] Avg train loss: 0.013602\n",
      "Epoch [3411/10000] Avg train loss: 0.013598\n",
      "Epoch [3412/10000] Avg train loss: 0.013594\n",
      "Epoch [3413/10000] Avg train loss: 0.013590\n",
      "Epoch [3414/10000] Avg train loss: 0.013586\n",
      "Epoch [3415/10000] Avg train loss: 0.013582\n",
      "Epoch [3416/10000] Avg train loss: 0.013578\n",
      "Epoch [3417/10000] Avg train loss: 0.013574\n",
      "Epoch [3418/10000] Avg train loss: 0.013570\n",
      "Epoch [3419/10000] Avg train loss: 0.013566\n",
      "Epoch [3420/10000] Avg train loss: 0.013562\n",
      "Epoch [3421/10000] Avg train loss: 0.013558\n",
      "Epoch [3422/10000] Avg train loss: 0.013554\n",
      "Epoch [3423/10000] Avg train loss: 0.013550\n",
      "Epoch [3424/10000] Avg train loss: 0.013546\n",
      "Epoch [3425/10000] Avg train loss: 0.013542\n",
      "Epoch [3426/10000] Avg train loss: 0.013538\n",
      "Epoch [3427/10000] Avg train loss: 0.013534\n",
      "Epoch [3428/10000] Avg train loss: 0.013530\n",
      "Epoch [3429/10000] Avg train loss: 0.013526\n",
      "Epoch [3430/10000] Avg train loss: 0.013522\n",
      "Epoch [3431/10000] Avg train loss: 0.013518\n",
      "Epoch [3432/10000] Avg train loss: 0.013514\n",
      "Epoch [3433/10000] Avg train loss: 0.013510\n",
      "Epoch [3434/10000] Avg train loss: 0.013506\n",
      "Epoch [3435/10000] Avg train loss: 0.013503\n",
      "Epoch [3436/10000] Avg train loss: 0.013499\n",
      "Epoch [3437/10000] Avg train loss: 0.013495\n",
      "Epoch [3438/10000] Avg train loss: 0.013491\n",
      "Epoch [3439/10000] Avg train loss: 0.013487\n",
      "Epoch [3440/10000] Avg train loss: 0.013483\n",
      "Epoch [3441/10000] Avg train loss: 0.013479\n",
      "Epoch [3442/10000] Avg train loss: 0.013475\n",
      "Epoch [3443/10000] Avg train loss: 0.013471\n",
      "Epoch [3444/10000] Avg train loss: 0.013467\n",
      "Epoch [3445/10000] Avg train loss: 0.013463\n",
      "Epoch [3446/10000] Avg train loss: 0.013459\n",
      "Epoch [3447/10000] Avg train loss: 0.013456\n",
      "Epoch [3448/10000] Avg train loss: 0.013452\n",
      "Epoch [3449/10000] Avg train loss: 0.013448\n",
      "Epoch [3450/10000] Avg train loss: 0.013444\n",
      "Epoch [3451/10000] Avg train loss: 0.013440\n",
      "Epoch [3452/10000] Avg train loss: 0.013436\n",
      "Epoch [3453/10000] Avg train loss: 0.013432\n",
      "Epoch [3454/10000] Avg train loss: 0.013428\n",
      "Epoch [3455/10000] Avg train loss: 0.013424\n",
      "Epoch [3456/10000] Avg train loss: 0.013421\n",
      "Epoch [3457/10000] Avg train loss: 0.013417\n",
      "Epoch [3458/10000] Avg train loss: 0.013413\n",
      "Epoch [3459/10000] Avg train loss: 0.013409\n",
      "Epoch [3460/10000] Avg train loss: 0.013405\n",
      "Epoch [3461/10000] Avg train loss: 0.013401\n",
      "Epoch [3462/10000] Avg train loss: 0.013397\n",
      "Epoch [3463/10000] Avg train loss: 0.013393\n",
      "Epoch [3464/10000] Avg train loss: 0.013390\n",
      "Epoch [3465/10000] Avg train loss: 0.013386\n",
      "Epoch [3466/10000] Avg train loss: 0.013382\n",
      "Epoch [3467/10000] Avg train loss: 0.013378\n",
      "Epoch [3468/10000] Avg train loss: 0.013374\n",
      "Epoch [3469/10000] Avg train loss: 0.013370\n",
      "Epoch [3470/10000] Avg train loss: 0.013366\n",
      "Epoch [3471/10000] Avg train loss: 0.013363\n",
      "Epoch [3472/10000] Avg train loss: 0.013359\n",
      "Epoch [3473/10000] Avg train loss: 0.013355\n",
      "Epoch [3474/10000] Avg train loss: 0.013351\n",
      "Epoch [3475/10000] Avg train loss: 0.013347\n",
      "Epoch [3476/10000] Avg train loss: 0.013343\n",
      "Epoch [3477/10000] Avg train loss: 0.013339\n",
      "Epoch [3478/10000] Avg train loss: 0.013336\n",
      "Epoch [3479/10000] Avg train loss: 0.013332\n",
      "Epoch [3480/10000] Avg train loss: 0.013328\n",
      "Epoch [3481/10000] Avg train loss: 0.013324\n",
      "Epoch [3482/10000] Avg train loss: 0.013320\n",
      "Epoch [3483/10000] Avg train loss: 0.013316\n",
      "Epoch [3484/10000] Avg train loss: 0.013313\n",
      "Epoch [3485/10000] Avg train loss: 0.013309\n",
      "Epoch [3486/10000] Avg train loss: 0.013305\n",
      "Epoch [3487/10000] Avg train loss: 0.013301\n",
      "Epoch [3488/10000] Avg train loss: 0.013297\n",
      "Epoch [3489/10000] Avg train loss: 0.013294\n",
      "Epoch [3490/10000] Avg train loss: 0.013290\n",
      "Epoch [3491/10000] Avg train loss: 0.013286\n",
      "Epoch [3492/10000] Avg train loss: 0.013282\n",
      "Epoch [3493/10000] Avg train loss: 0.013278\n",
      "Epoch [3494/10000] Avg train loss: 0.013275\n",
      "Epoch [3495/10000] Avg train loss: 0.013271\n",
      "Epoch [3496/10000] Avg train loss: 0.013267\n",
      "Epoch [3497/10000] Avg train loss: 0.013263\n",
      "Epoch [3498/10000] Avg train loss: 0.013259\n",
      "Epoch [3499/10000] Avg train loss: 0.013256\n",
      "Epoch [3500/10000] Avg train loss: 0.013252\n",
      "Epoch [3501/10000] Avg train loss: 0.013248\n",
      "Epoch [3502/10000] Avg train loss: 0.013244\n",
      "Epoch [3503/10000] Avg train loss: 0.013240\n",
      "Epoch [3504/10000] Avg train loss: 0.013237\n",
      "Epoch [3505/10000] Avg train loss: 0.013233\n",
      "Epoch [3506/10000] Avg train loss: 0.013229\n",
      "Epoch [3507/10000] Avg train loss: 0.013225\n",
      "Epoch [3508/10000] Avg train loss: 0.013222\n",
      "Epoch [3509/10000] Avg train loss: 0.013218\n",
      "Epoch [3510/10000] Avg train loss: 0.013214\n",
      "Epoch [3511/10000] Avg train loss: 0.013210\n",
      "Epoch [3512/10000] Avg train loss: 0.013207\n",
      "Epoch [3513/10000] Avg train loss: 0.013203\n",
      "Epoch [3514/10000] Avg train loss: 0.013199\n",
      "Epoch [3515/10000] Avg train loss: 0.013195\n",
      "Epoch [3516/10000] Avg train loss: 0.013191\n",
      "Epoch [3517/10000] Avg train loss: 0.013188\n",
      "Epoch [3518/10000] Avg train loss: 0.013184\n",
      "Epoch [3519/10000] Avg train loss: 0.013180\n",
      "Epoch [3520/10000] Avg train loss: 0.013176\n",
      "Epoch [3521/10000] Avg train loss: 0.013173\n",
      "Epoch [3522/10000] Avg train loss: 0.013169\n",
      "Epoch [3523/10000] Avg train loss: 0.013165\n",
      "Epoch [3524/10000] Avg train loss: 0.013162\n",
      "Epoch [3525/10000] Avg train loss: 0.013158\n",
      "Epoch [3526/10000] Avg train loss: 0.013154\n",
      "Epoch [3527/10000] Avg train loss: 0.013150\n",
      "Epoch [3528/10000] Avg train loss: 0.013147\n",
      "Epoch [3529/10000] Avg train loss: 0.013143\n",
      "Epoch [3530/10000] Avg train loss: 0.013139\n",
      "Epoch [3531/10000] Avg train loss: 0.013135\n",
      "Epoch [3532/10000] Avg train loss: 0.013132\n",
      "Epoch [3533/10000] Avg train loss: 0.013128\n",
      "Epoch [3534/10000] Avg train loss: 0.013124\n",
      "Epoch [3535/10000] Avg train loss: 0.013121\n",
      "Epoch [3536/10000] Avg train loss: 0.013117\n",
      "Epoch [3537/10000] Avg train loss: 0.013113\n",
      "Epoch [3538/10000] Avg train loss: 0.013109\n",
      "Epoch [3539/10000] Avg train loss: 0.013106\n",
      "Epoch [3540/10000] Avg train loss: 0.013102\n",
      "Epoch [3541/10000] Avg train loss: 0.013098\n",
      "Epoch [3542/10000] Avg train loss: 0.013095\n",
      "Epoch [3543/10000] Avg train loss: 0.013091\n",
      "Epoch [3544/10000] Avg train loss: 0.013087\n",
      "Epoch [3545/10000] Avg train loss: 0.013084\n",
      "Epoch [3546/10000] Avg train loss: 0.013080\n",
      "Epoch [3547/10000] Avg train loss: 0.013076\n",
      "Epoch [3548/10000] Avg train loss: 0.013073\n",
      "Epoch [3549/10000] Avg train loss: 0.013069\n",
      "Epoch [3550/10000] Avg train loss: 0.013065\n",
      "Epoch [3551/10000] Avg train loss: 0.013061\n",
      "Epoch [3552/10000] Avg train loss: 0.013058\n",
      "Epoch [3553/10000] Avg train loss: 0.013054\n",
      "Epoch [3554/10000] Avg train loss: 0.013050\n",
      "Epoch [3555/10000] Avg train loss: 0.013047\n",
      "Epoch [3556/10000] Avg train loss: 0.013043\n",
      "Epoch [3557/10000] Avg train loss: 0.013039\n",
      "Epoch [3558/10000] Avg train loss: 0.013036\n",
      "Epoch [3559/10000] Avg train loss: 0.013032\n",
      "Epoch [3560/10000] Avg train loss: 0.013028\n",
      "Epoch [3561/10000] Avg train loss: 0.013025\n",
      "Epoch [3562/10000] Avg train loss: 0.013021\n",
      "Epoch [3563/10000] Avg train loss: 0.013017\n",
      "Epoch [3564/10000] Avg train loss: 0.013014\n",
      "Epoch [3565/10000] Avg train loss: 0.013010\n",
      "Epoch [3566/10000] Avg train loss: 0.013007\n",
      "Epoch [3567/10000] Avg train loss: 0.013003\n",
      "Epoch [3568/10000] Avg train loss: 0.012999\n",
      "Epoch [3569/10000] Avg train loss: 0.012996\n",
      "Epoch [3570/10000] Avg train loss: 0.012992\n",
      "Epoch [3571/10000] Avg train loss: 0.012988\n",
      "Epoch [3572/10000] Avg train loss: 0.012985\n",
      "Epoch [3573/10000] Avg train loss: 0.012981\n",
      "Epoch [3574/10000] Avg train loss: 0.012977\n",
      "Epoch [3575/10000] Avg train loss: 0.012974\n",
      "Epoch [3576/10000] Avg train loss: 0.012970\n",
      "Epoch [3577/10000] Avg train loss: 0.012967\n",
      "Epoch [3578/10000] Avg train loss: 0.012963\n",
      "Epoch [3579/10000] Avg train loss: 0.012959\n",
      "Epoch [3580/10000] Avg train loss: 0.012956\n",
      "Epoch [3581/10000] Avg train loss: 0.012952\n",
      "Epoch [3582/10000] Avg train loss: 0.012948\n",
      "Epoch [3583/10000] Avg train loss: 0.012945\n",
      "Epoch [3584/10000] Avg train loss: 0.012941\n",
      "Epoch [3585/10000] Avg train loss: 0.012938\n",
      "Epoch [3586/10000] Avg train loss: 0.012934\n",
      "Epoch [3587/10000] Avg train loss: 0.012930\n",
      "Epoch [3588/10000] Avg train loss: 0.012927\n",
      "Epoch [3589/10000] Avg train loss: 0.012923\n",
      "Epoch [3590/10000] Avg train loss: 0.012920\n",
      "Epoch [3591/10000] Avg train loss: 0.012916\n",
      "Epoch [3592/10000] Avg train loss: 0.012912\n",
      "Epoch [3593/10000] Avg train loss: 0.012909\n",
      "Epoch [3594/10000] Avg train loss: 0.012905\n",
      "Epoch [3595/10000] Avg train loss: 0.012902\n",
      "Epoch [3596/10000] Avg train loss: 0.012898\n",
      "Epoch [3597/10000] Avg train loss: 0.012894\n",
      "Epoch [3598/10000] Avg train loss: 0.012891\n",
      "Epoch [3599/10000] Avg train loss: 0.012887\n",
      "Epoch [3600/10000] Avg train loss: 0.012884\n",
      "Epoch [3601/10000] Avg train loss: 0.012880\n",
      "Epoch [3602/10000] Avg train loss: 0.012877\n",
      "Epoch [3603/10000] Avg train loss: 0.012873\n",
      "Epoch [3604/10000] Avg train loss: 0.012869\n",
      "Epoch [3605/10000] Avg train loss: 0.012866\n",
      "Epoch [3606/10000] Avg train loss: 0.012862\n",
      "Epoch [3607/10000] Avg train loss: 0.012859\n",
      "Epoch [3608/10000] Avg train loss: 0.012855\n",
      "Epoch [3609/10000] Avg train loss: 0.012852\n",
      "Epoch [3610/10000] Avg train loss: 0.012848\n",
      "Epoch [3611/10000] Avg train loss: 0.012844\n",
      "Epoch [3612/10000] Avg train loss: 0.012841\n",
      "Epoch [3613/10000] Avg train loss: 0.012837\n",
      "Epoch [3614/10000] Avg train loss: 0.012834\n",
      "Epoch [3615/10000] Avg train loss: 0.012830\n",
      "Epoch [3616/10000] Avg train loss: 0.012827\n",
      "Epoch [3617/10000] Avg train loss: 0.012823\n",
      "Epoch [3618/10000] Avg train loss: 0.012820\n",
      "Epoch [3619/10000] Avg train loss: 0.012816\n",
      "Epoch [3620/10000] Avg train loss: 0.012813\n",
      "Epoch [3621/10000] Avg train loss: 0.012809\n",
      "Epoch [3622/10000] Avg train loss: 0.012805\n",
      "Epoch [3623/10000] Avg train loss: 0.012802\n",
      "Epoch [3624/10000] Avg train loss: 0.012798\n",
      "Epoch [3625/10000] Avg train loss: 0.012795\n",
      "Epoch [3626/10000] Avg train loss: 0.012791\n",
      "Epoch [3627/10000] Avg train loss: 0.012788\n",
      "Epoch [3628/10000] Avg train loss: 0.012784\n",
      "Epoch [3629/10000] Avg train loss: 0.012781\n",
      "Epoch [3630/10000] Avg train loss: 0.012777\n",
      "Epoch [3631/10000] Avg train loss: 0.012774\n",
      "Epoch [3632/10000] Avg train loss: 0.012770\n",
      "Epoch [3633/10000] Avg train loss: 0.012767\n",
      "Epoch [3634/10000] Avg train loss: 0.012763\n",
      "Epoch [3635/10000] Avg train loss: 0.012760\n",
      "Epoch [3636/10000] Avg train loss: 0.012756\n",
      "Epoch [3637/10000] Avg train loss: 0.012753\n",
      "Epoch [3638/10000] Avg train loss: 0.012749\n",
      "Epoch [3639/10000] Avg train loss: 0.012746\n",
      "Epoch [3640/10000] Avg train loss: 0.012742\n",
      "Epoch [3641/10000] Avg train loss: 0.012739\n",
      "Epoch [3642/10000] Avg train loss: 0.012735\n",
      "Epoch [3643/10000] Avg train loss: 0.012732\n",
      "Epoch [3644/10000] Avg train loss: 0.012728\n",
      "Epoch [3645/10000] Avg train loss: 0.012725\n",
      "Epoch [3646/10000] Avg train loss: 0.012721\n",
      "Epoch [3647/10000] Avg train loss: 0.012718\n",
      "Epoch [3648/10000] Avg train loss: 0.012714\n",
      "Epoch [3649/10000] Avg train loss: 0.012711\n",
      "Epoch [3650/10000] Avg train loss: 0.012707\n",
      "Epoch [3651/10000] Avg train loss: 0.012704\n",
      "Epoch [3652/10000] Avg train loss: 0.012700\n",
      "Epoch [3653/10000] Avg train loss: 0.012697\n",
      "Epoch [3654/10000] Avg train loss: 0.012693\n",
      "Epoch [3655/10000] Avg train loss: 0.012690\n",
      "Epoch [3656/10000] Avg train loss: 0.012686\n",
      "Epoch [3657/10000] Avg train loss: 0.012683\n",
      "Epoch [3658/10000] Avg train loss: 0.012679\n",
      "Epoch [3659/10000] Avg train loss: 0.012676\n",
      "Epoch [3660/10000] Avg train loss: 0.012672\n",
      "Epoch [3661/10000] Avg train loss: 0.012669\n",
      "Epoch [3662/10000] Avg train loss: 0.012666\n",
      "Epoch [3663/10000] Avg train loss: 0.012662\n",
      "Epoch [3664/10000] Avg train loss: 0.012659\n",
      "Epoch [3665/10000] Avg train loss: 0.012655\n",
      "Epoch [3666/10000] Avg train loss: 0.012652\n",
      "Epoch [3667/10000] Avg train loss: 0.012648\n",
      "Epoch [3668/10000] Avg train loss: 0.012645\n",
      "Epoch [3669/10000] Avg train loss: 0.012641\n",
      "Epoch [3670/10000] Avg train loss: 0.012638\n",
      "Epoch [3671/10000] Avg train loss: 0.012635\n",
      "Epoch [3672/10000] Avg train loss: 0.012631\n",
      "Epoch [3673/10000] Avg train loss: 0.012628\n",
      "Epoch [3674/10000] Avg train loss: 0.012624\n",
      "Epoch [3675/10000] Avg train loss: 0.012621\n",
      "Epoch [3676/10000] Avg train loss: 0.012617\n",
      "Epoch [3677/10000] Avg train loss: 0.012614\n",
      "Epoch [3678/10000] Avg train loss: 0.012610\n",
      "Epoch [3679/10000] Avg train loss: 0.012607\n",
      "Epoch [3680/10000] Avg train loss: 0.012604\n",
      "Epoch [3681/10000] Avg train loss: 0.012600\n",
      "Epoch [3682/10000] Avg train loss: 0.012597\n",
      "Epoch [3683/10000] Avg train loss: 0.012593\n",
      "Epoch [3684/10000] Avg train loss: 0.012590\n",
      "Epoch [3685/10000] Avg train loss: 0.012587\n",
      "Epoch [3686/10000] Avg train loss: 0.012583\n",
      "Epoch [3687/10000] Avg train loss: 0.012580\n",
      "Epoch [3688/10000] Avg train loss: 0.012576\n",
      "Epoch [3689/10000] Avg train loss: 0.012573\n",
      "Epoch [3690/10000] Avg train loss: 0.012569\n",
      "Epoch [3691/10000] Avg train loss: 0.012566\n",
      "Epoch [3692/10000] Avg train loss: 0.012563\n",
      "Epoch [3693/10000] Avg train loss: 0.012559\n",
      "Epoch [3694/10000] Avg train loss: 0.012556\n",
      "Epoch [3695/10000] Avg train loss: 0.012552\n",
      "Epoch [3696/10000] Avg train loss: 0.012549\n",
      "Epoch [3697/10000] Avg train loss: 0.012546\n",
      "Epoch [3698/10000] Avg train loss: 0.012542\n",
      "Epoch [3699/10000] Avg train loss: 0.012539\n",
      "Epoch [3700/10000] Avg train loss: 0.012535\n",
      "Epoch [3701/10000] Avg train loss: 0.012532\n",
      "Epoch [3702/10000] Avg train loss: 0.012529\n",
      "Epoch [3703/10000] Avg train loss: 0.012525\n",
      "Epoch [3704/10000] Avg train loss: 0.012522\n",
      "Epoch [3705/10000] Avg train loss: 0.012519\n",
      "Epoch [3706/10000] Avg train loss: 0.012515\n",
      "Epoch [3707/10000] Avg train loss: 0.012512\n",
      "Epoch [3708/10000] Avg train loss: 0.012508\n",
      "Epoch [3709/10000] Avg train loss: 0.012505\n",
      "Epoch [3710/10000] Avg train loss: 0.012502\n",
      "Epoch [3711/10000] Avg train loss: 0.012498\n",
      "Epoch [3712/10000] Avg train loss: 0.012495\n",
      "Epoch [3713/10000] Avg train loss: 0.012492\n",
      "Epoch [3714/10000] Avg train loss: 0.012488\n",
      "Epoch [3715/10000] Avg train loss: 0.012485\n",
      "Epoch [3716/10000] Avg train loss: 0.012482\n",
      "Epoch [3717/10000] Avg train loss: 0.012478\n",
      "Epoch [3718/10000] Avg train loss: 0.012475\n",
      "Epoch [3719/10000] Avg train loss: 0.012471\n",
      "Epoch [3720/10000] Avg train loss: 0.012468\n",
      "Epoch [3721/10000] Avg train loss: 0.012465\n",
      "Epoch [3722/10000] Avg train loss: 0.012461\n",
      "Epoch [3723/10000] Avg train loss: 0.012458\n",
      "Epoch [3724/10000] Avg train loss: 0.012455\n",
      "Epoch [3725/10000] Avg train loss: 0.012451\n",
      "Epoch [3726/10000] Avg train loss: 0.012448\n",
      "Epoch [3727/10000] Avg train loss: 0.012445\n",
      "Epoch [3728/10000] Avg train loss: 0.012441\n",
      "Epoch [3729/10000] Avg train loss: 0.012438\n",
      "Epoch [3730/10000] Avg train loss: 0.012435\n",
      "Epoch [3731/10000] Avg train loss: 0.012431\n",
      "Epoch [3732/10000] Avg train loss: 0.012428\n",
      "Epoch [3733/10000] Avg train loss: 0.012425\n",
      "Epoch [3734/10000] Avg train loss: 0.012421\n",
      "Epoch [3735/10000] Avg train loss: 0.012418\n",
      "Epoch [3736/10000] Avg train loss: 0.012415\n",
      "Epoch [3737/10000] Avg train loss: 0.012411\n",
      "Epoch [3738/10000] Avg train loss: 0.012408\n",
      "Epoch [3739/10000] Avg train loss: 0.012405\n",
      "Epoch [3740/10000] Avg train loss: 0.012401\n",
      "Epoch [3741/10000] Avg train loss: 0.012398\n",
      "Epoch [3742/10000] Avg train loss: 0.012395\n",
      "Epoch [3743/10000] Avg train loss: 0.012391\n",
      "Epoch [3744/10000] Avg train loss: 0.012388\n",
      "Epoch [3745/10000] Avg train loss: 0.012385\n",
      "Epoch [3746/10000] Avg train loss: 0.012382\n",
      "Epoch [3747/10000] Avg train loss: 0.012378\n",
      "Epoch [3748/10000] Avg train loss: 0.012375\n",
      "Epoch [3749/10000] Avg train loss: 0.012372\n",
      "Epoch [3750/10000] Avg train loss: 0.012368\n",
      "Epoch [3751/10000] Avg train loss: 0.012365\n",
      "Epoch [3752/10000] Avg train loss: 0.012362\n",
      "Epoch [3753/10000] Avg train loss: 0.012358\n",
      "Epoch [3754/10000] Avg train loss: 0.012355\n",
      "Epoch [3755/10000] Avg train loss: 0.012352\n",
      "Epoch [3756/10000] Avg train loss: 0.012349\n",
      "Epoch [3757/10000] Avg train loss: 0.012345\n",
      "Epoch [3758/10000] Avg train loss: 0.012342\n",
      "Epoch [3759/10000] Avg train loss: 0.012339\n",
      "Epoch [3760/10000] Avg train loss: 0.012335\n",
      "Epoch [3761/10000] Avg train loss: 0.012332\n",
      "Epoch [3762/10000] Avg train loss: 0.012329\n",
      "Epoch [3763/10000] Avg train loss: 0.012326\n",
      "Epoch [3764/10000] Avg train loss: 0.012322\n",
      "Epoch [3765/10000] Avg train loss: 0.012319\n",
      "Epoch [3766/10000] Avg train loss: 0.012316\n",
      "Epoch [3767/10000] Avg train loss: 0.012313\n",
      "Epoch [3768/10000] Avg train loss: 0.012309\n",
      "Epoch [3769/10000] Avg train loss: 0.012306\n",
      "Epoch [3770/10000] Avg train loss: 0.012303\n",
      "Epoch [3771/10000] Avg train loss: 0.012299\n",
      "Epoch [3772/10000] Avg train loss: 0.012296\n",
      "Epoch [3773/10000] Avg train loss: 0.012293\n",
      "Epoch [3774/10000] Avg train loss: 0.012290\n",
      "Epoch [3775/10000] Avg train loss: 0.012286\n",
      "Epoch [3776/10000] Avg train loss: 0.012283\n",
      "Epoch [3777/10000] Avg train loss: 0.012280\n",
      "Epoch [3778/10000] Avg train loss: 0.012277\n",
      "Epoch [3779/10000] Avg train loss: 0.012273\n",
      "Epoch [3780/10000] Avg train loss: 0.012270\n",
      "Epoch [3781/10000] Avg train loss: 0.012267\n",
      "Epoch [3782/10000] Avg train loss: 0.012264\n",
      "Epoch [3783/10000] Avg train loss: 0.012260\n",
      "Epoch [3784/10000] Avg train loss: 0.012257\n",
      "Epoch [3785/10000] Avg train loss: 0.012254\n",
      "Epoch [3786/10000] Avg train loss: 0.012251\n",
      "Epoch [3787/10000] Avg train loss: 0.012247\n",
      "Epoch [3788/10000] Avg train loss: 0.012244\n",
      "Epoch [3789/10000] Avg train loss: 0.012241\n",
      "Epoch [3790/10000] Avg train loss: 0.012238\n",
      "Epoch [3791/10000] Avg train loss: 0.012235\n",
      "Epoch [3792/10000] Avg train loss: 0.012231\n",
      "Epoch [3793/10000] Avg train loss: 0.012228\n",
      "Epoch [3794/10000] Avg train loss: 0.012225\n",
      "Epoch [3795/10000] Avg train loss: 0.012222\n",
      "Epoch [3796/10000] Avg train loss: 0.012218\n",
      "Epoch [3797/10000] Avg train loss: 0.012215\n",
      "Epoch [3798/10000] Avg train loss: 0.012212\n",
      "Epoch [3799/10000] Avg train loss: 0.012209\n",
      "Epoch [3800/10000] Avg train loss: 0.012206\n",
      "Epoch [3801/10000] Avg train loss: 0.012202\n",
      "Epoch [3802/10000] Avg train loss: 0.012199\n",
      "Epoch [3803/10000] Avg train loss: 0.012196\n",
      "Epoch [3804/10000] Avg train loss: 0.012193\n",
      "Epoch [3805/10000] Avg train loss: 0.012190\n",
      "Epoch [3806/10000] Avg train loss: 0.012186\n",
      "Epoch [3807/10000] Avg train loss: 0.012183\n",
      "Epoch [3808/10000] Avg train loss: 0.012180\n",
      "Epoch [3809/10000] Avg train loss: 0.012177\n",
      "Epoch [3810/10000] Avg train loss: 0.012174\n",
      "Epoch [3811/10000] Avg train loss: 0.012170\n",
      "Epoch [3812/10000] Avg train loss: 0.012167\n",
      "Epoch [3813/10000] Avg train loss: 0.012164\n",
      "Epoch [3814/10000] Avg train loss: 0.012161\n",
      "Epoch [3815/10000] Avg train loss: 0.012158\n",
      "Epoch [3816/10000] Avg train loss: 0.012154\n",
      "Epoch [3817/10000] Avg train loss: 0.012151\n",
      "Epoch [3818/10000] Avg train loss: 0.012148\n",
      "Epoch [3819/10000] Avg train loss: 0.012145\n",
      "Epoch [3820/10000] Avg train loss: 0.012142\n",
      "Epoch [3821/10000] Avg train loss: 0.012139\n",
      "Epoch [3822/10000] Avg train loss: 0.012135\n",
      "Epoch [3823/10000] Avg train loss: 0.012132\n",
      "Epoch [3824/10000] Avg train loss: 0.012129\n",
      "Epoch [3825/10000] Avg train loss: 0.012126\n",
      "Epoch [3826/10000] Avg train loss: 0.012123\n",
      "Epoch [3827/10000] Avg train loss: 0.012119\n",
      "Epoch [3828/10000] Avg train loss: 0.012116\n",
      "Epoch [3829/10000] Avg train loss: 0.012113\n",
      "Epoch [3830/10000] Avg train loss: 0.012110\n",
      "Epoch [3831/10000] Avg train loss: 0.012107\n",
      "Epoch [3832/10000] Avg train loss: 0.012104\n",
      "Epoch [3833/10000] Avg train loss: 0.012101\n",
      "Epoch [3834/10000] Avg train loss: 0.012097\n",
      "Epoch [3835/10000] Avg train loss: 0.012094\n",
      "Epoch [3836/10000] Avg train loss: 0.012091\n",
      "Epoch [3837/10000] Avg train loss: 0.012088\n",
      "Epoch [3838/10000] Avg train loss: 0.012085\n",
      "Epoch [3839/10000] Avg train loss: 0.012082\n",
      "Epoch [3840/10000] Avg train loss: 0.012078\n",
      "Epoch [3841/10000] Avg train loss: 0.012075\n",
      "Epoch [3842/10000] Avg train loss: 0.012072\n",
      "Epoch [3843/10000] Avg train loss: 0.012069\n",
      "Epoch [3844/10000] Avg train loss: 0.012066\n",
      "Epoch [3845/10000] Avg train loss: 0.012063\n",
      "Epoch [3846/10000] Avg train loss: 0.012060\n",
      "Epoch [3847/10000] Avg train loss: 0.012056\n",
      "Epoch [3848/10000] Avg train loss: 0.012053\n",
      "Epoch [3849/10000] Avg train loss: 0.012050\n",
      "Epoch [3850/10000] Avg train loss: 0.012047\n",
      "Epoch [3851/10000] Avg train loss: 0.012044\n",
      "Epoch [3852/10000] Avg train loss: 0.012041\n",
      "Epoch [3853/10000] Avg train loss: 0.012038\n",
      "Epoch [3854/10000] Avg train loss: 0.012035\n",
      "Epoch [3855/10000] Avg train loss: 0.012031\n",
      "Epoch [3856/10000] Avg train loss: 0.012028\n",
      "Epoch [3857/10000] Avg train loss: 0.012025\n",
      "Epoch [3858/10000] Avg train loss: 0.012022\n",
      "Epoch [3859/10000] Avg train loss: 0.012019\n",
      "Epoch [3860/10000] Avg train loss: 0.012016\n",
      "Epoch [3861/10000] Avg train loss: 0.012013\n",
      "Epoch [3862/10000] Avg train loss: 0.012010\n",
      "Epoch [3863/10000] Avg train loss: 0.012007\n",
      "Epoch [3864/10000] Avg train loss: 0.012003\n",
      "Epoch [3865/10000] Avg train loss: 0.012000\n",
      "Epoch [3866/10000] Avg train loss: 0.011997\n",
      "Epoch [3867/10000] Avg train loss: 0.011994\n",
      "Epoch [3868/10000] Avg train loss: 0.011991\n",
      "Epoch [3869/10000] Avg train loss: 0.011988\n",
      "Epoch [3870/10000] Avg train loss: 0.011985\n",
      "Epoch [3871/10000] Avg train loss: 0.011982\n",
      "Epoch [3872/10000] Avg train loss: 0.011979\n",
      "Epoch [3873/10000] Avg train loss: 0.011976\n",
      "Epoch [3874/10000] Avg train loss: 0.011972\n",
      "Epoch [3875/10000] Avg train loss: 0.011969\n",
      "Epoch [3876/10000] Avg train loss: 0.011966\n",
      "Epoch [3877/10000] Avg train loss: 0.011963\n",
      "Epoch [3878/10000] Avg train loss: 0.011960\n",
      "Epoch [3879/10000] Avg train loss: 0.011957\n",
      "Epoch [3880/10000] Avg train loss: 0.011954\n",
      "Epoch [3881/10000] Avg train loss: 0.011951\n",
      "Epoch [3882/10000] Avg train loss: 0.011948\n",
      "Epoch [3883/10000] Avg train loss: 0.011945\n",
      "Epoch [3884/10000] Avg train loss: 0.011942\n",
      "Epoch [3885/10000] Avg train loss: 0.011939\n",
      "Epoch [3886/10000] Avg train loss: 0.011935\n",
      "Epoch [3887/10000] Avg train loss: 0.011932\n",
      "Epoch [3888/10000] Avg train loss: 0.011929\n",
      "Epoch [3889/10000] Avg train loss: 0.011926\n",
      "Epoch [3890/10000] Avg train loss: 0.011923\n",
      "Epoch [3891/10000] Avg train loss: 0.011920\n",
      "Epoch [3892/10000] Avg train loss: 0.011917\n",
      "Epoch [3893/10000] Avg train loss: 0.011914\n",
      "Epoch [3894/10000] Avg train loss: 0.011911\n",
      "Epoch [3895/10000] Avg train loss: 0.011908\n",
      "Epoch [3896/10000] Avg train loss: 0.011905\n",
      "Epoch [3897/10000] Avg train loss: 0.011902\n",
      "Epoch [3898/10000] Avg train loss: 0.011899\n",
      "Epoch [3899/10000] Avg train loss: 0.011896\n",
      "Epoch [3900/10000] Avg train loss: 0.011893\n",
      "Epoch [3901/10000] Avg train loss: 0.011890\n",
      "Epoch [3902/10000] Avg train loss: 0.011887\n",
      "Epoch [3903/10000] Avg train loss: 0.011883\n",
      "Epoch [3904/10000] Avg train loss: 0.011880\n",
      "Epoch [3905/10000] Avg train loss: 0.011877\n",
      "Epoch [3906/10000] Avg train loss: 0.011874\n",
      "Epoch [3907/10000] Avg train loss: 0.011871\n",
      "Epoch [3908/10000] Avg train loss: 0.011868\n",
      "Epoch [3909/10000] Avg train loss: 0.011865\n",
      "Epoch [3910/10000] Avg train loss: 0.011862\n",
      "Epoch [3911/10000] Avg train loss: 0.011859\n",
      "Epoch [3912/10000] Avg train loss: 0.011856\n",
      "Epoch [3913/10000] Avg train loss: 0.011853\n",
      "Epoch [3914/10000] Avg train loss: 0.011850\n",
      "Epoch [3915/10000] Avg train loss: 0.011847\n",
      "Epoch [3916/10000] Avg train loss: 0.011844\n",
      "Epoch [3917/10000] Avg train loss: 0.011841\n",
      "Epoch [3918/10000] Avg train loss: 0.011838\n",
      "Epoch [3919/10000] Avg train loss: 0.011835\n",
      "Epoch [3920/10000] Avg train loss: 0.011832\n",
      "Epoch [3921/10000] Avg train loss: 0.011829\n",
      "Epoch [3922/10000] Avg train loss: 0.011826\n",
      "Epoch [3923/10000] Avg train loss: 0.011823\n",
      "Epoch [3924/10000] Avg train loss: 0.011820\n",
      "Epoch [3925/10000] Avg train loss: 0.011817\n",
      "Epoch [3926/10000] Avg train loss: 0.011814\n",
      "Epoch [3927/10000] Avg train loss: 0.011811\n",
      "Epoch [3928/10000] Avg train loss: 0.011808\n",
      "Epoch [3929/10000] Avg train loss: 0.011805\n",
      "Epoch [3930/10000] Avg train loss: 0.011802\n",
      "Epoch [3931/10000] Avg train loss: 0.011799\n",
      "Epoch [3932/10000] Avg train loss: 0.011796\n",
      "Epoch [3933/10000] Avg train loss: 0.011793\n",
      "Epoch [3934/10000] Avg train loss: 0.011790\n",
      "Epoch [3935/10000] Avg train loss: 0.011787\n",
      "Epoch [3936/10000] Avg train loss: 0.011784\n",
      "Epoch [3937/10000] Avg train loss: 0.011781\n",
      "Epoch [3938/10000] Avg train loss: 0.011778\n",
      "Epoch [3939/10000] Avg train loss: 0.011775\n",
      "Epoch [3940/10000] Avg train loss: 0.011772\n",
      "Epoch [3941/10000] Avg train loss: 0.011769\n",
      "Epoch [3942/10000] Avg train loss: 0.011766\n",
      "Epoch [3943/10000] Avg train loss: 0.011763\n",
      "Epoch [3944/10000] Avg train loss: 0.011760\n",
      "Epoch [3945/10000] Avg train loss: 0.011757\n",
      "Epoch [3946/10000] Avg train loss: 0.011754\n",
      "Epoch [3947/10000] Avg train loss: 0.011751\n",
      "Epoch [3948/10000] Avg train loss: 0.011748\n",
      "Epoch [3949/10000] Avg train loss: 0.011745\n",
      "Epoch [3950/10000] Avg train loss: 0.011742\n",
      "Epoch [3951/10000] Avg train loss: 0.011739\n",
      "Epoch [3952/10000] Avg train loss: 0.011736\n",
      "Epoch [3953/10000] Avg train loss: 0.011733\n",
      "Epoch [3954/10000] Avg train loss: 0.011730\n",
      "Epoch [3955/10000] Avg train loss: 0.011727\n",
      "Epoch [3956/10000] Avg train loss: 0.011724\n",
      "Epoch [3957/10000] Avg train loss: 0.011721\n",
      "Epoch [3958/10000] Avg train loss: 0.011718\n",
      "Epoch [3959/10000] Avg train loss: 0.011715\n",
      "Epoch [3960/10000] Avg train loss: 0.011712\n",
      "Epoch [3961/10000] Avg train loss: 0.011709\n",
      "Epoch [3962/10000] Avg train loss: 0.011707\n",
      "Epoch [3963/10000] Avg train loss: 0.011704\n",
      "Epoch [3964/10000] Avg train loss: 0.011701\n",
      "Epoch [3965/10000] Avg train loss: 0.011698\n",
      "Epoch [3966/10000] Avg train loss: 0.011695\n",
      "Epoch [3967/10000] Avg train loss: 0.011692\n",
      "Epoch [3968/10000] Avg train loss: 0.011689\n",
      "Epoch [3969/10000] Avg train loss: 0.011686\n",
      "Epoch [3970/10000] Avg train loss: 0.011683\n",
      "Epoch [3971/10000] Avg train loss: 0.011680\n",
      "Epoch [3972/10000] Avg train loss: 0.011677\n",
      "Epoch [3973/10000] Avg train loss: 0.011674\n",
      "Epoch [3974/10000] Avg train loss: 0.011671\n",
      "Epoch [3975/10000] Avg train loss: 0.011668\n",
      "Epoch [3976/10000] Avg train loss: 0.011665\n",
      "Epoch [3977/10000] Avg train loss: 0.011662\n",
      "Epoch [3978/10000] Avg train loss: 0.011659\n",
      "Epoch [3979/10000] Avg train loss: 0.011657\n",
      "Epoch [3980/10000] Avg train loss: 0.011654\n",
      "Epoch [3981/10000] Avg train loss: 0.011651\n",
      "Epoch [3982/10000] Avg train loss: 0.011648\n",
      "Epoch [3983/10000] Avg train loss: 0.011645\n",
      "Epoch [3984/10000] Avg train loss: 0.011642\n",
      "Epoch [3985/10000] Avg train loss: 0.011639\n",
      "Epoch [3986/10000] Avg train loss: 0.011636\n",
      "Epoch [3987/10000] Avg train loss: 0.011633\n",
      "Epoch [3988/10000] Avg train loss: 0.011630\n",
      "Epoch [3989/10000] Avg train loss: 0.011627\n",
      "Epoch [3990/10000] Avg train loss: 0.011624\n",
      "Epoch [3991/10000] Avg train loss: 0.011621\n",
      "Epoch [3992/10000] Avg train loss: 0.011619\n",
      "Epoch [3993/10000] Avg train loss: 0.011616\n",
      "Epoch [3994/10000] Avg train loss: 0.011613\n",
      "Epoch [3995/10000] Avg train loss: 0.011610\n",
      "Epoch [3996/10000] Avg train loss: 0.011607\n",
      "Epoch [3997/10000] Avg train loss: 0.011604\n",
      "Epoch [3998/10000] Avg train loss: 0.011601\n",
      "Epoch [3999/10000] Avg train loss: 0.011598\n",
      "Epoch [4000/10000] Avg train loss: 0.011595\n",
      "Epoch [4001/10000] Avg train loss: 0.011592\n",
      "Epoch [4002/10000] Avg train loss: 0.011590\n",
      "Epoch [4003/10000] Avg train loss: 0.011587\n",
      "Epoch [4004/10000] Avg train loss: 0.011584\n",
      "Epoch [4005/10000] Avg train loss: 0.011581\n",
      "Epoch [4006/10000] Avg train loss: 0.011578\n",
      "Epoch [4007/10000] Avg train loss: 0.011575\n",
      "Epoch [4008/10000] Avg train loss: 0.011572\n",
      "Epoch [4009/10000] Avg train loss: 0.011569\n",
      "Epoch [4010/10000] Avg train loss: 0.011566\n",
      "Epoch [4011/10000] Avg train loss: 0.011564\n",
      "Epoch [4012/10000] Avg train loss: 0.011561\n",
      "Epoch [4013/10000] Avg train loss: 0.011558\n",
      "Epoch [4014/10000] Avg train loss: 0.011555\n",
      "Epoch [4015/10000] Avg train loss: 0.011552\n",
      "Epoch [4016/10000] Avg train loss: 0.011549\n",
      "Epoch [4017/10000] Avg train loss: 0.011546\n",
      "Epoch [4018/10000] Avg train loss: 0.011543\n",
      "Epoch [4019/10000] Avg train loss: 0.011540\n",
      "Epoch [4020/10000] Avg train loss: 0.011538\n",
      "Epoch [4021/10000] Avg train loss: 0.011535\n",
      "Epoch [4022/10000] Avg train loss: 0.011532\n",
      "Epoch [4023/10000] Avg train loss: 0.011529\n",
      "Epoch [4024/10000] Avg train loss: 0.011526\n",
      "Epoch [4025/10000] Avg train loss: 0.011523\n",
      "Epoch [4026/10000] Avg train loss: 0.011520\n",
      "Epoch [4027/10000] Avg train loss: 0.011518\n",
      "Epoch [4028/10000] Avg train loss: 0.011515\n",
      "Epoch [4029/10000] Avg train loss: 0.011512\n",
      "Epoch [4030/10000] Avg train loss: 0.011509\n",
      "Epoch [4031/10000] Avg train loss: 0.011506\n",
      "Epoch [4032/10000] Avg train loss: 0.011503\n",
      "Epoch [4033/10000] Avg train loss: 0.011500\n",
      "Epoch [4034/10000] Avg train loss: 0.011498\n",
      "Epoch [4035/10000] Avg train loss: 0.011495\n",
      "Epoch [4036/10000] Avg train loss: 0.011492\n",
      "Epoch [4037/10000] Avg train loss: 0.011489\n",
      "Epoch [4038/10000] Avg train loss: 0.011486\n",
      "Epoch [4039/10000] Avg train loss: 0.011483\n",
      "Epoch [4040/10000] Avg train loss: 0.011481\n",
      "Epoch [4041/10000] Avg train loss: 0.011478\n",
      "Epoch [4042/10000] Avg train loss: 0.011475\n",
      "Epoch [4043/10000] Avg train loss: 0.011472\n",
      "Epoch [4044/10000] Avg train loss: 0.011469\n",
      "Epoch [4045/10000] Avg train loss: 0.011466\n",
      "Epoch [4046/10000] Avg train loss: 0.011463\n",
      "Epoch [4047/10000] Avg train loss: 0.011461\n",
      "Epoch [4048/10000] Avg train loss: 0.011458\n",
      "Epoch [4049/10000] Avg train loss: 0.011455\n",
      "Epoch [4050/10000] Avg train loss: 0.011452\n",
      "Epoch [4051/10000] Avg train loss: 0.011449\n",
      "Epoch [4052/10000] Avg train loss: 0.011447\n",
      "Epoch [4053/10000] Avg train loss: 0.011444\n",
      "Epoch [4054/10000] Avg train loss: 0.011441\n",
      "Epoch [4055/10000] Avg train loss: 0.011438\n",
      "Epoch [4056/10000] Avg train loss: 0.011435\n",
      "Epoch [4057/10000] Avg train loss: 0.011432\n",
      "Epoch [4058/10000] Avg train loss: 0.011430\n",
      "Epoch [4059/10000] Avg train loss: 0.011427\n",
      "Epoch [4060/10000] Avg train loss: 0.011424\n",
      "Epoch [4061/10000] Avg train loss: 0.011421\n",
      "Epoch [4062/10000] Avg train loss: 0.011418\n",
      "Epoch [4063/10000] Avg train loss: 0.011416\n",
      "Epoch [4064/10000] Avg train loss: 0.011413\n",
      "Epoch [4065/10000] Avg train loss: 0.011410\n",
      "Epoch [4066/10000] Avg train loss: 0.011407\n",
      "Epoch [4067/10000] Avg train loss: 0.011404\n",
      "Epoch [4068/10000] Avg train loss: 0.011401\n",
      "Epoch [4069/10000] Avg train loss: 0.011399\n",
      "Epoch [4070/10000] Avg train loss: 0.011396\n",
      "Epoch [4071/10000] Avg train loss: 0.011393\n",
      "Epoch [4072/10000] Avg train loss: 0.011390\n",
      "Epoch [4073/10000] Avg train loss: 0.011387\n",
      "Epoch [4074/10000] Avg train loss: 0.011385\n",
      "Epoch [4075/10000] Avg train loss: 0.011382\n",
      "Epoch [4076/10000] Avg train loss: 0.011379\n",
      "Epoch [4077/10000] Avg train loss: 0.011376\n",
      "Epoch [4078/10000] Avg train loss: 0.011374\n",
      "Epoch [4079/10000] Avg train loss: 0.011371\n",
      "Epoch [4080/10000] Avg train loss: 0.011368\n",
      "Epoch [4081/10000] Avg train loss: 0.011365\n",
      "Epoch [4082/10000] Avg train loss: 0.011362\n",
      "Epoch [4083/10000] Avg train loss: 0.011360\n",
      "Epoch [4084/10000] Avg train loss: 0.011357\n",
      "Epoch [4085/10000] Avg train loss: 0.011354\n",
      "Epoch [4086/10000] Avg train loss: 0.011351\n",
      "Epoch [4087/10000] Avg train loss: 0.011348\n",
      "Epoch [4088/10000] Avg train loss: 0.011346\n",
      "Epoch [4089/10000] Avg train loss: 0.011343\n",
      "Epoch [4090/10000] Avg train loss: 0.011340\n",
      "Epoch [4091/10000] Avg train loss: 0.011337\n",
      "Epoch [4092/10000] Avg train loss: 0.011335\n",
      "Epoch [4093/10000] Avg train loss: 0.011332\n",
      "Epoch [4094/10000] Avg train loss: 0.011329\n",
      "Epoch [4095/10000] Avg train loss: 0.011326\n",
      "Epoch [4096/10000] Avg train loss: 0.011324\n",
      "Epoch [4097/10000] Avg train loss: 0.011321\n",
      "Epoch [4098/10000] Avg train loss: 0.011318\n",
      "Epoch [4099/10000] Avg train loss: 0.011315\n",
      "Epoch [4100/10000] Avg train loss: 0.011313\n",
      "Epoch [4101/10000] Avg train loss: 0.011310\n",
      "Epoch [4102/10000] Avg train loss: 0.011307\n",
      "Epoch [4103/10000] Avg train loss: 0.011304\n",
      "Epoch [4104/10000] Avg train loss: 0.011301\n",
      "Epoch [4105/10000] Avg train loss: 0.011299\n",
      "Epoch [4106/10000] Avg train loss: 0.011296\n",
      "Epoch [4107/10000] Avg train loss: 0.011293\n",
      "Epoch [4108/10000] Avg train loss: 0.011290\n",
      "Epoch [4109/10000] Avg train loss: 0.011288\n",
      "Epoch [4110/10000] Avg train loss: 0.011285\n",
      "Epoch [4111/10000] Avg train loss: 0.011282\n",
      "Epoch [4112/10000] Avg train loss: 0.011279\n",
      "Epoch [4113/10000] Avg train loss: 0.011277\n",
      "Epoch [4114/10000] Avg train loss: 0.011274\n",
      "Epoch [4115/10000] Avg train loss: 0.011271\n",
      "Epoch [4116/10000] Avg train loss: 0.011269\n",
      "Epoch [4117/10000] Avg train loss: 0.011266\n",
      "Epoch [4118/10000] Avg train loss: 0.011263\n",
      "Epoch [4119/10000] Avg train loss: 0.011260\n",
      "Epoch [4120/10000] Avg train loss: 0.011258\n",
      "Epoch [4121/10000] Avg train loss: 0.011255\n",
      "Epoch [4122/10000] Avg train loss: 0.011252\n",
      "Epoch [4123/10000] Avg train loss: 0.011249\n",
      "Epoch [4124/10000] Avg train loss: 0.011247\n",
      "Epoch [4125/10000] Avg train loss: 0.011244\n",
      "Epoch [4126/10000] Avg train loss: 0.011241\n",
      "Epoch [4127/10000] Avg train loss: 0.011238\n",
      "Epoch [4128/10000] Avg train loss: 0.011236\n",
      "Epoch [4129/10000] Avg train loss: 0.011233\n",
      "Epoch [4130/10000] Avg train loss: 0.011230\n",
      "Epoch [4131/10000] Avg train loss: 0.011228\n",
      "Epoch [4132/10000] Avg train loss: 0.011225\n",
      "Epoch [4133/10000] Avg train loss: 0.011222\n",
      "Epoch [4134/10000] Avg train loss: 0.011219\n",
      "Epoch [4135/10000] Avg train loss: 0.011217\n",
      "Epoch [4136/10000] Avg train loss: 0.011214\n",
      "Epoch [4137/10000] Avg train loss: 0.011211\n",
      "Epoch [4138/10000] Avg train loss: 0.011209\n",
      "Epoch [4139/10000] Avg train loss: 0.011206\n",
      "Epoch [4140/10000] Avg train loss: 0.011203\n",
      "Epoch [4141/10000] Avg train loss: 0.011200\n",
      "Epoch [4142/10000] Avg train loss: 0.011198\n",
      "Epoch [4143/10000] Avg train loss: 0.011195\n",
      "Epoch [4144/10000] Avg train loss: 0.011192\n",
      "Epoch [4145/10000] Avg train loss: 0.011190\n",
      "Epoch [4146/10000] Avg train loss: 0.011187\n",
      "Epoch [4147/10000] Avg train loss: 0.011184\n",
      "Epoch [4148/10000] Avg train loss: 0.011182\n",
      "Epoch [4149/10000] Avg train loss: 0.011179\n",
      "Epoch [4150/10000] Avg train loss: 0.011176\n",
      "Epoch [4151/10000] Avg train loss: 0.011174\n",
      "Epoch [4152/10000] Avg train loss: 0.011171\n",
      "Epoch [4153/10000] Avg train loss: 0.011168\n",
      "Epoch [4154/10000] Avg train loss: 0.011165\n",
      "Epoch [4155/10000] Avg train loss: 0.011163\n",
      "Epoch [4156/10000] Avg train loss: 0.011160\n",
      "Epoch [4157/10000] Avg train loss: 0.011157\n",
      "Epoch [4158/10000] Avg train loss: 0.011155\n",
      "Epoch [4159/10000] Avg train loss: 0.011152\n",
      "Epoch [4160/10000] Avg train loss: 0.011149\n",
      "Epoch [4161/10000] Avg train loss: 0.011147\n",
      "Epoch [4162/10000] Avg train loss: 0.011144\n",
      "Epoch [4163/10000] Avg train loss: 0.011141\n",
      "Epoch [4164/10000] Avg train loss: 0.011139\n",
      "Epoch [4165/10000] Avg train loss: 0.011136\n",
      "Epoch [4166/10000] Avg train loss: 0.011133\n",
      "Epoch [4167/10000] Avg train loss: 0.011131\n",
      "Epoch [4168/10000] Avg train loss: 0.011128\n",
      "Epoch [4169/10000] Avg train loss: 0.011125\n",
      "Epoch [4170/10000] Avg train loss: 0.011123\n",
      "Epoch [4171/10000] Avg train loss: 0.011120\n",
      "Epoch [4172/10000] Avg train loss: 0.011117\n",
      "Epoch [4173/10000] Avg train loss: 0.011115\n",
      "Epoch [4174/10000] Avg train loss: 0.011112\n",
      "Epoch [4175/10000] Avg train loss: 0.011109\n",
      "Epoch [4176/10000] Avg train loss: 0.011107\n",
      "Epoch [4177/10000] Avg train loss: 0.011104\n",
      "Epoch [4178/10000] Avg train loss: 0.011101\n",
      "Epoch [4179/10000] Avg train loss: 0.011099\n",
      "Epoch [4180/10000] Avg train loss: 0.011096\n",
      "Epoch [4181/10000] Avg train loss: 0.011093\n",
      "Epoch [4182/10000] Avg train loss: 0.011091\n",
      "Epoch [4183/10000] Avg train loss: 0.011088\n",
      "Epoch [4184/10000] Avg train loss: 0.011085\n",
      "Epoch [4185/10000] Avg train loss: 0.011083\n",
      "Epoch [4186/10000] Avg train loss: 0.011080\n",
      "Epoch [4187/10000] Avg train loss: 0.011077\n",
      "Epoch [4188/10000] Avg train loss: 0.011075\n",
      "Epoch [4189/10000] Avg train loss: 0.011072\n",
      "Epoch [4190/10000] Avg train loss: 0.011070\n",
      "Epoch [4191/10000] Avg train loss: 0.011067\n",
      "Epoch [4192/10000] Avg train loss: 0.011064\n",
      "Epoch [4193/10000] Avg train loss: 0.011062\n",
      "Epoch [4194/10000] Avg train loss: 0.011059\n",
      "Epoch [4195/10000] Avg train loss: 0.011056\n",
      "Epoch [4196/10000] Avg train loss: 0.011054\n",
      "Epoch [4197/10000] Avg train loss: 0.011051\n",
      "Epoch [4198/10000] Avg train loss: 0.011048\n",
      "Epoch [4199/10000] Avg train loss: 0.011046\n",
      "Epoch [4200/10000] Avg train loss: 0.011043\n",
      "Epoch [4201/10000] Avg train loss: 0.011041\n",
      "Epoch [4202/10000] Avg train loss: 0.011038\n",
      "Epoch [4203/10000] Avg train loss: 0.011035\n",
      "Epoch [4204/10000] Avg train loss: 0.011033\n",
      "Epoch [4205/10000] Avg train loss: 0.011030\n",
      "Epoch [4206/10000] Avg train loss: 0.011027\n",
      "Epoch [4207/10000] Avg train loss: 0.011025\n",
      "Epoch [4208/10000] Avg train loss: 0.011022\n",
      "Epoch [4209/10000] Avg train loss: 0.011020\n",
      "Epoch [4210/10000] Avg train loss: 0.011017\n",
      "Epoch [4211/10000] Avg train loss: 0.011014\n",
      "Epoch [4212/10000] Avg train loss: 0.011012\n",
      "Epoch [4213/10000] Avg train loss: 0.011009\n",
      "Epoch [4214/10000] Avg train loss: 0.011006\n",
      "Epoch [4215/10000] Avg train loss: 0.011004\n",
      "Epoch [4216/10000] Avg train loss: 0.011001\n",
      "Epoch [4217/10000] Avg train loss: 0.010999\n",
      "Epoch [4218/10000] Avg train loss: 0.010996\n",
      "Epoch [4219/10000] Avg train loss: 0.010993\n",
      "Epoch [4220/10000] Avg train loss: 0.010991\n",
      "Epoch [4221/10000] Avg train loss: 0.010988\n",
      "Epoch [4222/10000] Avg train loss: 0.010986\n",
      "Epoch [4223/10000] Avg train loss: 0.010983\n",
      "Epoch [4224/10000] Avg train loss: 0.010980\n",
      "Epoch [4225/10000] Avg train loss: 0.010978\n",
      "Epoch [4226/10000] Avg train loss: 0.010975\n",
      "Epoch [4227/10000] Avg train loss: 0.010973\n",
      "Epoch [4228/10000] Avg train loss: 0.010970\n",
      "Epoch [4229/10000] Avg train loss: 0.010967\n",
      "Epoch [4230/10000] Avg train loss: 0.010965\n",
      "Epoch [4231/10000] Avg train loss: 0.010962\n",
      "Epoch [4232/10000] Avg train loss: 0.010960\n",
      "Epoch [4233/10000] Avg train loss: 0.010957\n",
      "Epoch [4234/10000] Avg train loss: 0.010954\n",
      "Epoch [4235/10000] Avg train loss: 0.010952\n",
      "Epoch [4236/10000] Avg train loss: 0.010949\n",
      "Epoch [4237/10000] Avg train loss: 0.010947\n",
      "Epoch [4238/10000] Avg train loss: 0.010944\n",
      "Epoch [4239/10000] Avg train loss: 0.010942\n",
      "Epoch [4240/10000] Avg train loss: 0.010939\n",
      "Epoch [4241/10000] Avg train loss: 0.010936\n",
      "Epoch [4242/10000] Avg train loss: 0.010934\n",
      "Epoch [4243/10000] Avg train loss: 0.010931\n",
      "Epoch [4244/10000] Avg train loss: 0.010929\n",
      "Epoch [4245/10000] Avg train loss: 0.010926\n",
      "Epoch [4246/10000] Avg train loss: 0.010924\n",
      "Epoch [4247/10000] Avg train loss: 0.010921\n",
      "Epoch [4248/10000] Avg train loss: 0.010918\n",
      "Epoch [4249/10000] Avg train loss: 0.010916\n",
      "Epoch [4250/10000] Avg train loss: 0.010913\n",
      "Epoch [4251/10000] Avg train loss: 0.010911\n",
      "Epoch [4252/10000] Avg train loss: 0.010908\n",
      "Epoch [4253/10000] Avg train loss: 0.010906\n",
      "Epoch [4254/10000] Avg train loss: 0.010903\n",
      "Epoch [4255/10000] Avg train loss: 0.010900\n",
      "Epoch [4256/10000] Avg train loss: 0.010898\n",
      "Epoch [4257/10000] Avg train loss: 0.010895\n",
      "Epoch [4258/10000] Avg train loss: 0.010893\n",
      "Epoch [4259/10000] Avg train loss: 0.010890\n",
      "Epoch [4260/10000] Avg train loss: 0.010888\n",
      "Epoch [4261/10000] Avg train loss: 0.010885\n",
      "Epoch [4262/10000] Avg train loss: 0.010883\n",
      "Epoch [4263/10000] Avg train loss: 0.010880\n",
      "Epoch [4264/10000] Avg train loss: 0.010877\n",
      "Epoch [4265/10000] Avg train loss: 0.010875\n",
      "Epoch [4266/10000] Avg train loss: 0.010872\n",
      "Epoch [4267/10000] Avg train loss: 0.010870\n",
      "Epoch [4268/10000] Avg train loss: 0.010867\n",
      "Epoch [4269/10000] Avg train loss: 0.010865\n",
      "Epoch [4270/10000] Avg train loss: 0.010862\n",
      "Epoch [4271/10000] Avg train loss: 0.010860\n",
      "Epoch [4272/10000] Avg train loss: 0.010857\n",
      "Epoch [4273/10000] Avg train loss: 0.010854\n",
      "Epoch [4274/10000] Avg train loss: 0.010852\n",
      "Epoch [4275/10000] Avg train loss: 0.010849\n",
      "Epoch [4276/10000] Avg train loss: 0.010847\n",
      "Epoch [4277/10000] Avg train loss: 0.010844\n",
      "Epoch [4278/10000] Avg train loss: 0.010842\n",
      "Epoch [4279/10000] Avg train loss: 0.010839\n",
      "Epoch [4280/10000] Avg train loss: 0.010837\n",
      "Epoch [4281/10000] Avg train loss: 0.010834\n",
      "Epoch [4282/10000] Avg train loss: 0.010832\n",
      "Epoch [4283/10000] Avg train loss: 0.010829\n",
      "Epoch [4284/10000] Avg train loss: 0.010827\n",
      "Epoch [4285/10000] Avg train loss: 0.010824\n",
      "Epoch [4286/10000] Avg train loss: 0.010822\n",
      "Epoch [4287/10000] Avg train loss: 0.010819\n",
      "Epoch [4288/10000] Avg train loss: 0.010817\n",
      "Epoch [4289/10000] Avg train loss: 0.010814\n",
      "Epoch [4290/10000] Avg train loss: 0.010811\n",
      "Epoch [4291/10000] Avg train loss: 0.010809\n",
      "Epoch [4292/10000] Avg train loss: 0.010806\n",
      "Epoch [4293/10000] Avg train loss: 0.010804\n",
      "Epoch [4294/10000] Avg train loss: 0.010801\n",
      "Epoch [4295/10000] Avg train loss: 0.010799\n",
      "Epoch [4296/10000] Avg train loss: 0.010796\n",
      "Epoch [4297/10000] Avg train loss: 0.010794\n",
      "Epoch [4298/10000] Avg train loss: 0.010791\n",
      "Epoch [4299/10000] Avg train loss: 0.010789\n",
      "Epoch [4300/10000] Avg train loss: 0.010786\n",
      "Epoch [4301/10000] Avg train loss: 0.010784\n",
      "Epoch [4302/10000] Avg train loss: 0.010781\n",
      "Epoch [4303/10000] Avg train loss: 0.010779\n",
      "Epoch [4304/10000] Avg train loss: 0.010776\n",
      "Epoch [4305/10000] Avg train loss: 0.010774\n",
      "Epoch [4306/10000] Avg train loss: 0.010771\n",
      "Epoch [4307/10000] Avg train loss: 0.010769\n",
      "Epoch [4308/10000] Avg train loss: 0.010766\n",
      "Epoch [4309/10000] Avg train loss: 0.010764\n",
      "Epoch [4310/10000] Avg train loss: 0.010761\n",
      "Epoch [4311/10000] Avg train loss: 0.010759\n",
      "Epoch [4312/10000] Avg train loss: 0.010756\n",
      "Epoch [4313/10000] Avg train loss: 0.010754\n",
      "Epoch [4314/10000] Avg train loss: 0.010751\n",
      "Epoch [4315/10000] Avg train loss: 0.010749\n",
      "Epoch [4316/10000] Avg train loss: 0.010746\n",
      "Epoch [4317/10000] Avg train loss: 0.010744\n",
      "Epoch [4318/10000] Avg train loss: 0.010741\n",
      "Epoch [4319/10000] Avg train loss: 0.010739\n",
      "Epoch [4320/10000] Avg train loss: 0.010736\n",
      "Epoch [4321/10000] Avg train loss: 0.010734\n",
      "Epoch [4322/10000] Avg train loss: 0.010731\n",
      "Epoch [4323/10000] Avg train loss: 0.010729\n",
      "Epoch [4324/10000] Avg train loss: 0.010726\n",
      "Epoch [4325/10000] Avg train loss: 0.010724\n",
      "Epoch [4326/10000] Avg train loss: 0.010722\n",
      "Epoch [4327/10000] Avg train loss: 0.010719\n",
      "Epoch [4328/10000] Avg train loss: 0.010717\n",
      "Epoch [4329/10000] Avg train loss: 0.010714\n",
      "Epoch [4330/10000] Avg train loss: 0.010712\n",
      "Epoch [4331/10000] Avg train loss: 0.010709\n",
      "Epoch [4332/10000] Avg train loss: 0.010707\n",
      "Epoch [4333/10000] Avg train loss: 0.010704\n",
      "Epoch [4334/10000] Avg train loss: 0.010702\n",
      "Epoch [4335/10000] Avg train loss: 0.010699\n",
      "Epoch [4336/10000] Avg train loss: 0.010697\n",
      "Epoch [4337/10000] Avg train loss: 0.010694\n",
      "Epoch [4338/10000] Avg train loss: 0.010692\n",
      "Epoch [4339/10000] Avg train loss: 0.010689\n",
      "Epoch [4340/10000] Avg train loss: 0.010687\n",
      "Epoch [4341/10000] Avg train loss: 0.010684\n",
      "Epoch [4342/10000] Avg train loss: 0.010682\n",
      "Epoch [4343/10000] Avg train loss: 0.010680\n",
      "Epoch [4344/10000] Avg train loss: 0.010677\n",
      "Epoch [4345/10000] Avg train loss: 0.010675\n",
      "Epoch [4346/10000] Avg train loss: 0.010672\n",
      "Epoch [4347/10000] Avg train loss: 0.010670\n",
      "Epoch [4348/10000] Avg train loss: 0.010667\n",
      "Epoch [4349/10000] Avg train loss: 0.010665\n",
      "Epoch [4350/10000] Avg train loss: 0.010662\n",
      "Epoch [4351/10000] Avg train loss: 0.010660\n",
      "Epoch [4352/10000] Avg train loss: 0.010657\n",
      "Epoch [4353/10000] Avg train loss: 0.010655\n",
      "Epoch [4354/10000] Avg train loss: 0.010653\n",
      "Epoch [4355/10000] Avg train loss: 0.010650\n",
      "Epoch [4356/10000] Avg train loss: 0.010648\n",
      "Epoch [4357/10000] Avg train loss: 0.010645\n",
      "Epoch [4358/10000] Avg train loss: 0.010643\n",
      "Epoch [4359/10000] Avg train loss: 0.010640\n",
      "Epoch [4360/10000] Avg train loss: 0.010638\n",
      "Epoch [4361/10000] Avg train loss: 0.010635\n",
      "Epoch [4362/10000] Avg train loss: 0.010633\n",
      "Epoch [4363/10000] Avg train loss: 0.010631\n",
      "Epoch [4364/10000] Avg train loss: 0.010628\n",
      "Epoch [4365/10000] Avg train loss: 0.010626\n",
      "Epoch [4366/10000] Avg train loss: 0.010623\n",
      "Epoch [4367/10000] Avg train loss: 0.010621\n",
      "Epoch [4368/10000] Avg train loss: 0.010618\n",
      "Epoch [4369/10000] Avg train loss: 0.010616\n",
      "Epoch [4370/10000] Avg train loss: 0.010614\n",
      "Epoch [4371/10000] Avg train loss: 0.010611\n",
      "Epoch [4372/10000] Avg train loss: 0.010609\n",
      "Epoch [4373/10000] Avg train loss: 0.010606\n",
      "Epoch [4374/10000] Avg train loss: 0.010604\n",
      "Epoch [4375/10000] Avg train loss: 0.010601\n",
      "Epoch [4376/10000] Avg train loss: 0.010599\n",
      "Epoch [4377/10000] Avg train loss: 0.010597\n",
      "Epoch [4378/10000] Avg train loss: 0.010594\n",
      "Epoch [4379/10000] Avg train loss: 0.010592\n",
      "Epoch [4380/10000] Avg train loss: 0.010589\n",
      "Epoch [4381/10000] Avg train loss: 0.010587\n",
      "Epoch [4382/10000] Avg train loss: 0.010584\n",
      "Epoch [4383/10000] Avg train loss: 0.010582\n",
      "Epoch [4384/10000] Avg train loss: 0.010580\n",
      "Epoch [4385/10000] Avg train loss: 0.010577\n",
      "Epoch [4386/10000] Avg train loss: 0.010575\n",
      "Epoch [4387/10000] Avg train loss: 0.010572\n",
      "Epoch [4388/10000] Avg train loss: 0.010570\n",
      "Epoch [4389/10000] Avg train loss: 0.010568\n",
      "Epoch [4390/10000] Avg train loss: 0.010565\n",
      "Epoch [4391/10000] Avg train loss: 0.010563\n",
      "Epoch [4392/10000] Avg train loss: 0.010560\n",
      "Epoch [4393/10000] Avg train loss: 0.010558\n",
      "Epoch [4394/10000] Avg train loss: 0.010556\n",
      "Epoch [4395/10000] Avg train loss: 0.010553\n",
      "Epoch [4396/10000] Avg train loss: 0.010551\n",
      "Epoch [4397/10000] Avg train loss: 0.010548\n",
      "Epoch [4398/10000] Avg train loss: 0.010546\n",
      "Epoch [4399/10000] Avg train loss: 0.010544\n",
      "Epoch [4400/10000] Avg train loss: 0.010541\n",
      "Epoch [4401/10000] Avg train loss: 0.010539\n",
      "Epoch [4402/10000] Avg train loss: 0.010536\n",
      "Epoch [4403/10000] Avg train loss: 0.010534\n",
      "Epoch [4404/10000] Avg train loss: 0.010532\n",
      "Epoch [4405/10000] Avg train loss: 0.010529\n",
      "Epoch [4406/10000] Avg train loss: 0.010527\n",
      "Epoch [4407/10000] Avg train loss: 0.010524\n",
      "Epoch [4408/10000] Avg train loss: 0.010522\n",
      "Epoch [4409/10000] Avg train loss: 0.010520\n",
      "Epoch [4410/10000] Avg train loss: 0.010517\n",
      "Epoch [4411/10000] Avg train loss: 0.010515\n",
      "Epoch [4412/10000] Avg train loss: 0.010513\n",
      "Epoch [4413/10000] Avg train loss: 0.010510\n",
      "Epoch [4414/10000] Avg train loss: 0.010508\n",
      "Epoch [4415/10000] Avg train loss: 0.010505\n",
      "Epoch [4416/10000] Avg train loss: 0.010503\n",
      "Epoch [4417/10000] Avg train loss: 0.010501\n",
      "Epoch [4418/10000] Avg train loss: 0.010498\n",
      "Epoch [4419/10000] Avg train loss: 0.010496\n",
      "Epoch [4420/10000] Avg train loss: 0.010493\n",
      "Epoch [4421/10000] Avg train loss: 0.010491\n",
      "Epoch [4422/10000] Avg train loss: 0.010489\n",
      "Epoch [4423/10000] Avg train loss: 0.010486\n",
      "Epoch [4424/10000] Avg train loss: 0.010484\n",
      "Epoch [4425/10000] Avg train loss: 0.010482\n",
      "Epoch [4426/10000] Avg train loss: 0.010479\n",
      "Epoch [4427/10000] Avg train loss: 0.010477\n",
      "Epoch [4428/10000] Avg train loss: 0.010475\n",
      "Epoch [4429/10000] Avg train loss: 0.010472\n",
      "Epoch [4430/10000] Avg train loss: 0.010470\n",
      "Epoch [4431/10000] Avg train loss: 0.010467\n",
      "Epoch [4432/10000] Avg train loss: 0.010465\n",
      "Epoch [4433/10000] Avg train loss: 0.010463\n",
      "Epoch [4434/10000] Avg train loss: 0.010460\n",
      "Epoch [4435/10000] Avg train loss: 0.010458\n",
      "Epoch [4436/10000] Avg train loss: 0.010456\n",
      "Epoch [4437/10000] Avg train loss: 0.010453\n",
      "Epoch [4438/10000] Avg train loss: 0.010451\n",
      "Epoch [4439/10000] Avg train loss: 0.010449\n",
      "Epoch [4440/10000] Avg train loss: 0.010446\n",
      "Epoch [4441/10000] Avg train loss: 0.010444\n",
      "Epoch [4442/10000] Avg train loss: 0.010442\n",
      "Epoch [4443/10000] Avg train loss: 0.010439\n",
      "Epoch [4444/10000] Avg train loss: 0.010437\n",
      "Epoch [4445/10000] Avg train loss: 0.010434\n",
      "Epoch [4446/10000] Avg train loss: 0.010432\n",
      "Epoch [4447/10000] Avg train loss: 0.010430\n",
      "Epoch [4448/10000] Avg train loss: 0.010427\n",
      "Epoch [4449/10000] Avg train loss: 0.010425\n",
      "Epoch [4450/10000] Avg train loss: 0.010423\n",
      "Epoch [4451/10000] Avg train loss: 0.010420\n",
      "Epoch [4452/10000] Avg train loss: 0.010418\n",
      "Epoch [4453/10000] Avg train loss: 0.010416\n",
      "Epoch [4454/10000] Avg train loss: 0.010413\n",
      "Epoch [4455/10000] Avg train loss: 0.010411\n",
      "Epoch [4456/10000] Avg train loss: 0.010409\n",
      "Epoch [4457/10000] Avg train loss: 0.010406\n",
      "Epoch [4458/10000] Avg train loss: 0.010404\n",
      "Epoch [4459/10000] Avg train loss: 0.010402\n",
      "Epoch [4460/10000] Avg train loss: 0.010399\n",
      "Epoch [4461/10000] Avg train loss: 0.010397\n",
      "Epoch [4462/10000] Avg train loss: 0.010395\n",
      "Epoch [4463/10000] Avg train loss: 0.010392\n",
      "Epoch [4464/10000] Avg train loss: 0.010390\n",
      "Epoch [4465/10000] Avg train loss: 0.010388\n",
      "Epoch [4466/10000] Avg train loss: 0.010385\n",
      "Epoch [4467/10000] Avg train loss: 0.010383\n",
      "Epoch [4468/10000] Avg train loss: 0.010381\n",
      "Epoch [4469/10000] Avg train loss: 0.010378\n",
      "Epoch [4470/10000] Avg train loss: 0.010376\n",
      "Epoch [4471/10000] Avg train loss: 0.010374\n",
      "Epoch [4472/10000] Avg train loss: 0.010371\n",
      "Epoch [4473/10000] Avg train loss: 0.010369\n",
      "Epoch [4474/10000] Avg train loss: 0.010367\n",
      "Epoch [4475/10000] Avg train loss: 0.010365\n",
      "Epoch [4476/10000] Avg train loss: 0.010362\n",
      "Epoch [4477/10000] Avg train loss: 0.010360\n",
      "Epoch [4478/10000] Avg train loss: 0.010358\n",
      "Epoch [4479/10000] Avg train loss: 0.010355\n",
      "Epoch [4480/10000] Avg train loss: 0.010353\n",
      "Epoch [4481/10000] Avg train loss: 0.010351\n",
      "Epoch [4482/10000] Avg train loss: 0.010348\n",
      "Epoch [4483/10000] Avg train loss: 0.010346\n",
      "Epoch [4484/10000] Avg train loss: 0.010344\n",
      "Epoch [4485/10000] Avg train loss: 0.010341\n",
      "Epoch [4486/10000] Avg train loss: 0.010339\n",
      "Epoch [4487/10000] Avg train loss: 0.010337\n",
      "Epoch [4488/10000] Avg train loss: 0.010335\n",
      "Epoch [4489/10000] Avg train loss: 0.010332\n",
      "Epoch [4490/10000] Avg train loss: 0.010330\n",
      "Epoch [4491/10000] Avg train loss: 0.010328\n",
      "Epoch [4492/10000] Avg train loss: 0.010325\n",
      "Epoch [4493/10000] Avg train loss: 0.010323\n",
      "Epoch [4494/10000] Avg train loss: 0.010321\n",
      "Epoch [4495/10000] Avg train loss: 0.010318\n",
      "Epoch [4496/10000] Avg train loss: 0.010316\n",
      "Epoch [4497/10000] Avg train loss: 0.010314\n",
      "Epoch [4498/10000] Avg train loss: 0.010312\n",
      "Epoch [4499/10000] Avg train loss: 0.010309\n",
      "Epoch [4500/10000] Avg train loss: 0.010307\n",
      "Epoch [4501/10000] Avg train loss: 0.010305\n",
      "Epoch [4502/10000] Avg train loss: 0.010302\n",
      "Epoch [4503/10000] Avg train loss: 0.010300\n",
      "Epoch [4504/10000] Avg train loss: 0.010298\n",
      "Epoch [4505/10000] Avg train loss: 0.010296\n",
      "Epoch [4506/10000] Avg train loss: 0.010293\n",
      "Epoch [4507/10000] Avg train loss: 0.010291\n",
      "Epoch [4508/10000] Avg train loss: 0.010289\n",
      "Epoch [4509/10000] Avg train loss: 0.010286\n",
      "Epoch [4510/10000] Avg train loss: 0.010284\n",
      "Epoch [4511/10000] Avg train loss: 0.010282\n",
      "Epoch [4512/10000] Avg train loss: 0.010280\n",
      "Epoch [4513/10000] Avg train loss: 0.010277\n",
      "Epoch [4514/10000] Avg train loss: 0.010275\n",
      "Epoch [4515/10000] Avg train loss: 0.010273\n",
      "Epoch [4516/10000] Avg train loss: 0.010270\n",
      "Epoch [4517/10000] Avg train loss: 0.010268\n",
      "Epoch [4518/10000] Avg train loss: 0.010266\n",
      "Epoch [4519/10000] Avg train loss: 0.010264\n",
      "Epoch [4520/10000] Avg train loss: 0.010261\n",
      "Epoch [4521/10000] Avg train loss: 0.010259\n",
      "Epoch [4522/10000] Avg train loss: 0.010257\n",
      "Epoch [4523/10000] Avg train loss: 0.010255\n",
      "Epoch [4524/10000] Avg train loss: 0.010252\n",
      "Epoch [4525/10000] Avg train loss: 0.010250\n",
      "Epoch [4526/10000] Avg train loss: 0.010248\n",
      "Epoch [4527/10000] Avg train loss: 0.010245\n",
      "Epoch [4528/10000] Avg train loss: 0.010243\n",
      "Epoch [4529/10000] Avg train loss: 0.010241\n",
      "Epoch [4530/10000] Avg train loss: 0.010239\n",
      "Epoch [4531/10000] Avg train loss: 0.010236\n",
      "Epoch [4532/10000] Avg train loss: 0.010234\n",
      "Epoch [4533/10000] Avg train loss: 0.010232\n",
      "Epoch [4534/10000] Avg train loss: 0.010230\n",
      "Epoch [4535/10000] Avg train loss: 0.010227\n",
      "Epoch [4536/10000] Avg train loss: 0.010225\n",
      "Epoch [4537/10000] Avg train loss: 0.010223\n",
      "Epoch [4538/10000] Avg train loss: 0.010221\n",
      "Epoch [4539/10000] Avg train loss: 0.010218\n",
      "Epoch [4540/10000] Avg train loss: 0.010216\n",
      "Epoch [4541/10000] Avg train loss: 0.010214\n",
      "Epoch [4542/10000] Avg train loss: 0.010212\n",
      "Epoch [4543/10000] Avg train loss: 0.010209\n",
      "Epoch [4544/10000] Avg train loss: 0.010207\n",
      "Epoch [4545/10000] Avg train loss: 0.010205\n",
      "Epoch [4546/10000] Avg train loss: 0.010203\n",
      "Epoch [4547/10000] Avg train loss: 0.010200\n",
      "Epoch [4548/10000] Avg train loss: 0.010198\n",
      "Epoch [4549/10000] Avg train loss: 0.010196\n",
      "Epoch [4550/10000] Avg train loss: 0.010194\n",
      "Epoch [4551/10000] Avg train loss: 0.010191\n",
      "Epoch [4552/10000] Avg train loss: 0.010189\n",
      "Epoch [4553/10000] Avg train loss: 0.010187\n",
      "Epoch [4554/10000] Avg train loss: 0.010185\n",
      "Epoch [4555/10000] Avg train loss: 0.010182\n",
      "Epoch [4556/10000] Avg train loss: 0.010180\n",
      "Epoch [4557/10000] Avg train loss: 0.010178\n",
      "Epoch [4558/10000] Avg train loss: 0.010176\n",
      "Epoch [4559/10000] Avg train loss: 0.010174\n",
      "Epoch [4560/10000] Avg train loss: 0.010171\n",
      "Epoch [4561/10000] Avg train loss: 0.010169\n",
      "Epoch [4562/10000] Avg train loss: 0.010167\n",
      "Epoch [4563/10000] Avg train loss: 0.010165\n",
      "Epoch [4564/10000] Avg train loss: 0.010162\n",
      "Epoch [4565/10000] Avg train loss: 0.010160\n",
      "Epoch [4566/10000] Avg train loss: 0.010158\n",
      "Epoch [4567/10000] Avg train loss: 0.010156\n",
      "Epoch [4568/10000] Avg train loss: 0.010154\n",
      "Epoch [4569/10000] Avg train loss: 0.010151\n",
      "Epoch [4570/10000] Avg train loss: 0.010149\n",
      "Epoch [4571/10000] Avg train loss: 0.010147\n",
      "Epoch [4572/10000] Avg train loss: 0.010145\n",
      "Epoch [4573/10000] Avg train loss: 0.010142\n",
      "Epoch [4574/10000] Avg train loss: 0.010140\n",
      "Epoch [4575/10000] Avg train loss: 0.010138\n",
      "Epoch [4576/10000] Avg train loss: 0.010136\n",
      "Epoch [4577/10000] Avg train loss: 0.010134\n",
      "Epoch [4578/10000] Avg train loss: 0.010131\n",
      "Epoch [4579/10000] Avg train loss: 0.010129\n",
      "Epoch [4580/10000] Avg train loss: 0.010127\n",
      "Epoch [4581/10000] Avg train loss: 0.010125\n",
      "Epoch [4582/10000] Avg train loss: 0.010122\n",
      "Epoch [4583/10000] Avg train loss: 0.010120\n",
      "Epoch [4584/10000] Avg train loss: 0.010118\n",
      "Epoch [4585/10000] Avg train loss: 0.010116\n",
      "Epoch [4586/10000] Avg train loss: 0.010114\n",
      "Epoch [4587/10000] Avg train loss: 0.010111\n",
      "Epoch [4588/10000] Avg train loss: 0.010109\n",
      "Epoch [4589/10000] Avg train loss: 0.010107\n",
      "Epoch [4590/10000] Avg train loss: 0.010105\n",
      "Epoch [4591/10000] Avg train loss: 0.010103\n",
      "Epoch [4592/10000] Avg train loss: 0.010100\n",
      "Epoch [4593/10000] Avg train loss: 0.010098\n",
      "Epoch [4594/10000] Avg train loss: 0.010096\n",
      "Epoch [4595/10000] Avg train loss: 0.010094\n",
      "Epoch [4596/10000] Avg train loss: 0.010092\n",
      "Epoch [4597/10000] Avg train loss: 0.010089\n",
      "Epoch [4598/10000] Avg train loss: 0.010087\n",
      "Epoch [4599/10000] Avg train loss: 0.010085\n",
      "Epoch [4600/10000] Avg train loss: 0.010083\n",
      "Epoch [4601/10000] Avg train loss: 0.010081\n",
      "Epoch [4602/10000] Avg train loss: 0.010079\n",
      "Epoch [4603/10000] Avg train loss: 0.010076\n",
      "Epoch [4604/10000] Avg train loss: 0.010074\n",
      "Epoch [4605/10000] Avg train loss: 0.010072\n",
      "Epoch [4606/10000] Avg train loss: 0.010070\n",
      "Epoch [4607/10000] Avg train loss: 0.010068\n",
      "Epoch [4608/10000] Avg train loss: 0.010065\n",
      "Epoch [4609/10000] Avg train loss: 0.010063\n",
      "Epoch [4610/10000] Avg train loss: 0.010061\n",
      "Epoch [4611/10000] Avg train loss: 0.010059\n",
      "Epoch [4612/10000] Avg train loss: 0.010057\n",
      "Epoch [4613/10000] Avg train loss: 0.010054\n",
      "Epoch [4614/10000] Avg train loss: 0.010052\n",
      "Epoch [4615/10000] Avg train loss: 0.010050\n",
      "Epoch [4616/10000] Avg train loss: 0.010048\n",
      "Epoch [4617/10000] Avg train loss: 0.010046\n",
      "Epoch [4618/10000] Avg train loss: 0.010044\n",
      "Epoch [4619/10000] Avg train loss: 0.010041\n",
      "Epoch [4620/10000] Avg train loss: 0.010039\n",
      "Epoch [4621/10000] Avg train loss: 0.010037\n",
      "Epoch [4622/10000] Avg train loss: 0.010035\n",
      "Epoch [4623/10000] Avg train loss: 0.010033\n",
      "Epoch [4624/10000] Avg train loss: 0.010031\n",
      "Epoch [4625/10000] Avg train loss: 0.010028\n",
      "Epoch [4626/10000] Avg train loss: 0.010026\n",
      "Epoch [4627/10000] Avg train loss: 0.010024\n",
      "Epoch [4628/10000] Avg train loss: 0.010022\n",
      "Epoch [4629/10000] Avg train loss: 0.010020\n",
      "Epoch [4630/10000] Avg train loss: 0.010018\n",
      "Epoch [4631/10000] Avg train loss: 0.010015\n",
      "Epoch [4632/10000] Avg train loss: 0.010013\n",
      "Epoch [4633/10000] Avg train loss: 0.010011\n",
      "Epoch [4634/10000] Avg train loss: 0.010009\n",
      "Epoch [4635/10000] Avg train loss: 0.010007\n",
      "Epoch [4636/10000] Avg train loss: 0.010005\n",
      "Epoch [4637/10000] Avg train loss: 0.010002\n",
      "Epoch [4638/10000] Avg train loss: 0.010000\n",
      "Epoch [4639/10000] Avg train loss: 0.009998\n",
      "Epoch [4640/10000] Avg train loss: 0.009996\n",
      "Epoch [4641/10000] Avg train loss: 0.009994\n",
      "Epoch [4642/10000] Avg train loss: 0.009992\n",
      "Epoch [4643/10000] Avg train loss: 0.009990\n",
      "Epoch [4644/10000] Avg train loss: 0.009987\n",
      "Epoch [4645/10000] Avg train loss: 0.009985\n",
      "Epoch [4646/10000] Avg train loss: 0.009983\n",
      "Epoch [4647/10000] Avg train loss: 0.009981\n",
      "Epoch [4648/10000] Avg train loss: 0.009979\n",
      "Epoch [4649/10000] Avg train loss: 0.009977\n",
      "Epoch [4650/10000] Avg train loss: 0.009974\n",
      "Epoch [4651/10000] Avg train loss: 0.009972\n",
      "Epoch [4652/10000] Avg train loss: 0.009970\n",
      "Epoch [4653/10000] Avg train loss: 0.009968\n",
      "Epoch [4654/10000] Avg train loss: 0.009966\n",
      "Epoch [4655/10000] Avg train loss: 0.009964\n",
      "Epoch [4656/10000] Avg train loss: 0.009962\n",
      "Epoch [4657/10000] Avg train loss: 0.009959\n",
      "Epoch [4658/10000] Avg train loss: 0.009957\n",
      "Epoch [4659/10000] Avg train loss: 0.009955\n",
      "Epoch [4660/10000] Avg train loss: 0.009953\n",
      "Epoch [4661/10000] Avg train loss: 0.009951\n",
      "Epoch [4662/10000] Avg train loss: 0.009949\n",
      "Epoch [4663/10000] Avg train loss: 0.009947\n",
      "Epoch [4664/10000] Avg train loss: 0.009945\n",
      "Epoch [4665/10000] Avg train loss: 0.009942\n",
      "Epoch [4666/10000] Avg train loss: 0.009940\n",
      "Epoch [4667/10000] Avg train loss: 0.009938\n",
      "Epoch [4668/10000] Avg train loss: 0.009936\n",
      "Epoch [4669/10000] Avg train loss: 0.009934\n",
      "Epoch [4670/10000] Avg train loss: 0.009932\n",
      "Epoch [4671/10000] Avg train loss: 0.009930\n",
      "Epoch [4672/10000] Avg train loss: 0.009927\n",
      "Epoch [4673/10000] Avg train loss: 0.009925\n",
      "Epoch [4674/10000] Avg train loss: 0.009923\n",
      "Epoch [4675/10000] Avg train loss: 0.009921\n",
      "Epoch [4676/10000] Avg train loss: 0.009919\n",
      "Epoch [4677/10000] Avg train loss: 0.009917\n",
      "Epoch [4678/10000] Avg train loss: 0.009915\n",
      "Epoch [4679/10000] Avg train loss: 0.009913\n",
      "Epoch [4680/10000] Avg train loss: 0.009911\n",
      "Epoch [4681/10000] Avg train loss: 0.009908\n",
      "Epoch [4682/10000] Avg train loss: 0.009906\n",
      "Epoch [4683/10000] Avg train loss: 0.009904\n",
      "Epoch [4684/10000] Avg train loss: 0.009902\n",
      "Epoch [4685/10000] Avg train loss: 0.009900\n",
      "Epoch [4686/10000] Avg train loss: 0.009898\n",
      "Epoch [4687/10000] Avg train loss: 0.009896\n",
      "Epoch [4688/10000] Avg train loss: 0.009894\n",
      "Epoch [4689/10000] Avg train loss: 0.009892\n",
      "Epoch [4690/10000] Avg train loss: 0.009889\n",
      "Epoch [4691/10000] Avg train loss: 0.009887\n",
      "Epoch [4692/10000] Avg train loss: 0.009885\n",
      "Epoch [4693/10000] Avg train loss: 0.009883\n",
      "Epoch [4694/10000] Avg train loss: 0.009881\n",
      "Epoch [4695/10000] Avg train loss: 0.009879\n",
      "Epoch [4696/10000] Avg train loss: 0.009877\n",
      "Epoch [4697/10000] Avg train loss: 0.009875\n",
      "Epoch [4698/10000] Avg train loss: 0.009873\n",
      "Epoch [4699/10000] Avg train loss: 0.009870\n",
      "Epoch [4700/10000] Avg train loss: 0.009868\n",
      "Epoch [4701/10000] Avg train loss: 0.009866\n",
      "Epoch [4702/10000] Avg train loss: 0.009864\n",
      "Epoch [4703/10000] Avg train loss: 0.009862\n",
      "Epoch [4704/10000] Avg train loss: 0.009860\n",
      "Epoch [4705/10000] Avg train loss: 0.009858\n",
      "Epoch [4706/10000] Avg train loss: 0.009856\n",
      "Epoch [4707/10000] Avg train loss: 0.009854\n",
      "Epoch [4708/10000] Avg train loss: 0.009852\n",
      "Epoch [4709/10000] Avg train loss: 0.009849\n",
      "Epoch [4710/10000] Avg train loss: 0.009847\n",
      "Epoch [4711/10000] Avg train loss: 0.009845\n",
      "Epoch [4712/10000] Avg train loss: 0.009843\n",
      "Epoch [4713/10000] Avg train loss: 0.009841\n",
      "Epoch [4714/10000] Avg train loss: 0.009839\n",
      "Epoch [4715/10000] Avg train loss: 0.009837\n",
      "Epoch [4716/10000] Avg train loss: 0.009835\n",
      "Epoch [4717/10000] Avg train loss: 0.009833\n",
      "Epoch [4718/10000] Avg train loss: 0.009831\n",
      "Epoch [4719/10000] Avg train loss: 0.009829\n",
      "Epoch [4720/10000] Avg train loss: 0.009827\n",
      "Epoch [4721/10000] Avg train loss: 0.009824\n",
      "Epoch [4722/10000] Avg train loss: 0.009822\n",
      "Epoch [4723/10000] Avg train loss: 0.009820\n",
      "Epoch [4724/10000] Avg train loss: 0.009818\n",
      "Epoch [4725/10000] Avg train loss: 0.009816\n",
      "Epoch [4726/10000] Avg train loss: 0.009814\n",
      "Epoch [4727/10000] Avg train loss: 0.009812\n",
      "Epoch [4728/10000] Avg train loss: 0.009810\n",
      "Epoch [4729/10000] Avg train loss: 0.009808\n",
      "Epoch [4730/10000] Avg train loss: 0.009806\n",
      "Epoch [4731/10000] Avg train loss: 0.009804\n",
      "Epoch [4732/10000] Avg train loss: 0.009802\n",
      "Epoch [4733/10000] Avg train loss: 0.009800\n",
      "Epoch [4734/10000] Avg train loss: 0.009797\n",
      "Epoch [4735/10000] Avg train loss: 0.009795\n",
      "Epoch [4736/10000] Avg train loss: 0.009793\n",
      "Epoch [4737/10000] Avg train loss: 0.009791\n",
      "Epoch [4738/10000] Avg train loss: 0.009789\n",
      "Epoch [4739/10000] Avg train loss: 0.009787\n",
      "Epoch [4740/10000] Avg train loss: 0.009785\n",
      "Epoch [4741/10000] Avg train loss: 0.009783\n",
      "Epoch [4742/10000] Avg train loss: 0.009781\n",
      "Epoch [4743/10000] Avg train loss: 0.009779\n",
      "Epoch [4744/10000] Avg train loss: 0.009777\n",
      "Epoch [4745/10000] Avg train loss: 0.009775\n",
      "Epoch [4746/10000] Avg train loss: 0.009773\n",
      "Epoch [4747/10000] Avg train loss: 0.009771\n",
      "Epoch [4748/10000] Avg train loss: 0.009769\n",
      "Epoch [4749/10000] Avg train loss: 0.009767\n",
      "Epoch [4750/10000] Avg train loss: 0.009764\n",
      "Epoch [4751/10000] Avg train loss: 0.009762\n",
      "Epoch [4752/10000] Avg train loss: 0.009760\n",
      "Epoch [4753/10000] Avg train loss: 0.009758\n",
      "Epoch [4754/10000] Avg train loss: 0.009756\n",
      "Epoch [4755/10000] Avg train loss: 0.009754\n",
      "Epoch [4756/10000] Avg train loss: 0.009752\n",
      "Epoch [4757/10000] Avg train loss: 0.009750\n",
      "Epoch [4758/10000] Avg train loss: 0.009748\n",
      "Epoch [4759/10000] Avg train loss: 0.009746\n",
      "Epoch [4760/10000] Avg train loss: 0.009744\n",
      "Epoch [4761/10000] Avg train loss: 0.009742\n",
      "Epoch [4762/10000] Avg train loss: 0.009740\n",
      "Epoch [4763/10000] Avg train loss: 0.009738\n",
      "Epoch [4764/10000] Avg train loss: 0.009736\n",
      "Epoch [4765/10000] Avg train loss: 0.009734\n",
      "Epoch [4766/10000] Avg train loss: 0.009732\n",
      "Epoch [4767/10000] Avg train loss: 0.009730\n",
      "Epoch [4768/10000] Avg train loss: 0.009728\n",
      "Epoch [4769/10000] Avg train loss: 0.009726\n",
      "Epoch [4770/10000] Avg train loss: 0.009724\n",
      "Epoch [4771/10000] Avg train loss: 0.009721\n",
      "Epoch [4772/10000] Avg train loss: 0.009719\n",
      "Epoch [4773/10000] Avg train loss: 0.009717\n",
      "Epoch [4774/10000] Avg train loss: 0.009715\n",
      "Epoch [4775/10000] Avg train loss: 0.009713\n",
      "Epoch [4776/10000] Avg train loss: 0.009711\n",
      "Epoch [4777/10000] Avg train loss: 0.009709\n",
      "Epoch [4778/10000] Avg train loss: 0.009707\n",
      "Epoch [4779/10000] Avg train loss: 0.009705\n",
      "Epoch [4780/10000] Avg train loss: 0.009703\n",
      "Epoch [4781/10000] Avg train loss: 0.009701\n",
      "Epoch [4782/10000] Avg train loss: 0.009699\n",
      "Epoch [4783/10000] Avg train loss: 0.009697\n",
      "Epoch [4784/10000] Avg train loss: 0.009695\n",
      "Epoch [4785/10000] Avg train loss: 0.009693\n",
      "Epoch [4786/10000] Avg train loss: 0.009691\n",
      "Epoch [4787/10000] Avg train loss: 0.009689\n",
      "Epoch [4788/10000] Avg train loss: 0.009687\n",
      "Epoch [4789/10000] Avg train loss: 0.009685\n",
      "Epoch [4790/10000] Avg train loss: 0.009683\n",
      "Epoch [4791/10000] Avg train loss: 0.009681\n",
      "Epoch [4792/10000] Avg train loss: 0.009679\n",
      "Epoch [4793/10000] Avg train loss: 0.009677\n",
      "Epoch [4794/10000] Avg train loss: 0.009675\n",
      "Epoch [4795/10000] Avg train loss: 0.009673\n",
      "Epoch [4796/10000] Avg train loss: 0.009671\n",
      "Epoch [4797/10000] Avg train loss: 0.009669\n",
      "Epoch [4798/10000] Avg train loss: 0.009667\n",
      "Epoch [4799/10000] Avg train loss: 0.009665\n",
      "Epoch [4800/10000] Avg train loss: 0.009663\n",
      "Epoch [4801/10000] Avg train loss: 0.009661\n",
      "Epoch [4802/10000] Avg train loss: 0.009659\n",
      "Epoch [4803/10000] Avg train loss: 0.009657\n",
      "Epoch [4804/10000] Avg train loss: 0.009655\n",
      "Epoch [4805/10000] Avg train loss: 0.009653\n",
      "Epoch [4806/10000] Avg train loss: 0.009651\n",
      "Epoch [4807/10000] Avg train loss: 0.009649\n",
      "Epoch [4808/10000] Avg train loss: 0.009647\n",
      "Epoch [4809/10000] Avg train loss: 0.009645\n",
      "Epoch [4810/10000] Avg train loss: 0.009643\n",
      "Epoch [4811/10000] Avg train loss: 0.009641\n",
      "Epoch [4812/10000] Avg train loss: 0.009639\n",
      "Epoch [4813/10000] Avg train loss: 0.009637\n",
      "Epoch [4814/10000] Avg train loss: 0.009635\n",
      "Epoch [4815/10000] Avg train loss: 0.009633\n",
      "Epoch [4816/10000] Avg train loss: 0.009631\n",
      "Epoch [4817/10000] Avg train loss: 0.009629\n",
      "Epoch [4818/10000] Avg train loss: 0.009627\n",
      "Epoch [4819/10000] Avg train loss: 0.009625\n",
      "Epoch [4820/10000] Avg train loss: 0.009623\n",
      "Epoch [4821/10000] Avg train loss: 0.009621\n",
      "Epoch [4822/10000] Avg train loss: 0.009619\n",
      "Epoch [4823/10000] Avg train loss: 0.009617\n",
      "Epoch [4824/10000] Avg train loss: 0.009615\n",
      "Epoch [4825/10000] Avg train loss: 0.009613\n",
      "Epoch [4826/10000] Avg train loss: 0.009611\n",
      "Epoch [4827/10000] Avg train loss: 0.009609\n",
      "Epoch [4828/10000] Avg train loss: 0.009607\n",
      "Epoch [4829/10000] Avg train loss: 0.009605\n",
      "Epoch [4830/10000] Avg train loss: 0.009603\n",
      "Epoch [4831/10000] Avg train loss: 0.009601\n",
      "Epoch [4832/10000] Avg train loss: 0.009599\n",
      "Epoch [4833/10000] Avg train loss: 0.009597\n",
      "Epoch [4834/10000] Avg train loss: 0.009595\n",
      "Epoch [4835/10000] Avg train loss: 0.009593\n",
      "Epoch [4836/10000] Avg train loss: 0.009591\n",
      "Epoch [4837/10000] Avg train loss: 0.009589\n",
      "Epoch [4838/10000] Avg train loss: 0.009587\n",
      "Epoch [4839/10000] Avg train loss: 0.009585\n",
      "Epoch [4840/10000] Avg train loss: 0.009583\n",
      "Epoch [4841/10000] Avg train loss: 0.009581\n",
      "Epoch [4842/10000] Avg train loss: 0.009579\n",
      "Epoch [4843/10000] Avg train loss: 0.009577\n",
      "Epoch [4844/10000] Avg train loss: 0.009575\n",
      "Epoch [4845/10000] Avg train loss: 0.009573\n",
      "Epoch [4846/10000] Avg train loss: 0.009571\n",
      "Epoch [4847/10000] Avg train loss: 0.009569\n",
      "Epoch [4848/10000] Avg train loss: 0.009567\n",
      "Epoch [4849/10000] Avg train loss: 0.009565\n",
      "Epoch [4850/10000] Avg train loss: 0.009563\n",
      "Epoch [4851/10000] Avg train loss: 0.009561\n",
      "Epoch [4852/10000] Avg train loss: 0.009559\n",
      "Epoch [4853/10000] Avg train loss: 0.009557\n",
      "Epoch [4854/10000] Avg train loss: 0.009555\n",
      "Epoch [4855/10000] Avg train loss: 0.009553\n",
      "Epoch [4856/10000] Avg train loss: 0.009551\n",
      "Epoch [4857/10000] Avg train loss: 0.009549\n",
      "Epoch [4858/10000] Avg train loss: 0.009547\n",
      "Epoch [4859/10000] Avg train loss: 0.009545\n",
      "Epoch [4860/10000] Avg train loss: 0.009543\n",
      "Epoch [4861/10000] Avg train loss: 0.009542\n",
      "Epoch [4862/10000] Avg train loss: 0.009540\n",
      "Epoch [4863/10000] Avg train loss: 0.009538\n",
      "Epoch [4864/10000] Avg train loss: 0.009536\n",
      "Epoch [4865/10000] Avg train loss: 0.009534\n",
      "Epoch [4866/10000] Avg train loss: 0.009532\n",
      "Epoch [4867/10000] Avg train loss: 0.009530\n",
      "Epoch [4868/10000] Avg train loss: 0.009528\n",
      "Epoch [4869/10000] Avg train loss: 0.009526\n",
      "Epoch [4870/10000] Avg train loss: 0.009524\n",
      "Epoch [4871/10000] Avg train loss: 0.009522\n",
      "Epoch [4872/10000] Avg train loss: 0.009520\n",
      "Epoch [4873/10000] Avg train loss: 0.009518\n",
      "Epoch [4874/10000] Avg train loss: 0.009516\n",
      "Epoch [4875/10000] Avg train loss: 0.009514\n",
      "Epoch [4876/10000] Avg train loss: 0.009512\n",
      "Epoch [4877/10000] Avg train loss: 0.009510\n",
      "Epoch [4878/10000] Avg train loss: 0.009508\n",
      "Epoch [4879/10000] Avg train loss: 0.009506\n",
      "Epoch [4880/10000] Avg train loss: 0.009504\n",
      "Epoch [4881/10000] Avg train loss: 0.009502\n",
      "Epoch [4882/10000] Avg train loss: 0.009500\n",
      "Epoch [4883/10000] Avg train loss: 0.009499\n",
      "Epoch [4884/10000] Avg train loss: 0.009497\n",
      "Epoch [4885/10000] Avg train loss: 0.009495\n",
      "Epoch [4886/10000] Avg train loss: 0.009493\n",
      "Epoch [4887/10000] Avg train loss: 0.009491\n",
      "Epoch [4888/10000] Avg train loss: 0.009489\n",
      "Epoch [4889/10000] Avg train loss: 0.009487\n",
      "Epoch [4890/10000] Avg train loss: 0.009485\n",
      "Epoch [4891/10000] Avg train loss: 0.009483\n",
      "Epoch [4892/10000] Avg train loss: 0.009481\n",
      "Epoch [4893/10000] Avg train loss: 0.009479\n",
      "Epoch [4894/10000] Avg train loss: 0.009477\n",
      "Epoch [4895/10000] Avg train loss: 0.009475\n",
      "Epoch [4896/10000] Avg train loss: 0.009473\n",
      "Epoch [4897/10000] Avg train loss: 0.009471\n",
      "Epoch [4898/10000] Avg train loss: 0.009469\n",
      "Epoch [4899/10000] Avg train loss: 0.009467\n",
      "Epoch [4900/10000] Avg train loss: 0.009466\n",
      "Epoch [4901/10000] Avg train loss: 0.009464\n",
      "Epoch [4902/10000] Avg train loss: 0.009462\n",
      "Epoch [4903/10000] Avg train loss: 0.009460\n",
      "Epoch [4904/10000] Avg train loss: 0.009458\n",
      "Epoch [4905/10000] Avg train loss: 0.009456\n",
      "Epoch [4906/10000] Avg train loss: 0.009454\n",
      "Epoch [4907/10000] Avg train loss: 0.009452\n",
      "Epoch [4908/10000] Avg train loss: 0.009450\n",
      "Epoch [4909/10000] Avg train loss: 0.009448\n",
      "Epoch [4910/10000] Avg train loss: 0.009446\n",
      "Epoch [4911/10000] Avg train loss: 0.009444\n",
      "Epoch [4912/10000] Avg train loss: 0.009442\n",
      "Epoch [4913/10000] Avg train loss: 0.009441\n",
      "Epoch [4914/10000] Avg train loss: 0.009439\n",
      "Epoch [4915/10000] Avg train loss: 0.009437\n",
      "Epoch [4916/10000] Avg train loss: 0.009435\n",
      "Epoch [4917/10000] Avg train loss: 0.009433\n",
      "Epoch [4918/10000] Avg train loss: 0.009431\n",
      "Epoch [4919/10000] Avg train loss: 0.009429\n",
      "Epoch [4920/10000] Avg train loss: 0.009427\n",
      "Epoch [4921/10000] Avg train loss: 0.009425\n",
      "Epoch [4922/10000] Avg train loss: 0.009423\n",
      "Epoch [4923/10000] Avg train loss: 0.009421\n",
      "Epoch [4924/10000] Avg train loss: 0.009419\n",
      "Epoch [4925/10000] Avg train loss: 0.009418\n",
      "Epoch [4926/10000] Avg train loss: 0.009416\n",
      "Epoch [4927/10000] Avg train loss: 0.009414\n",
      "Epoch [4928/10000] Avg train loss: 0.009412\n",
      "Epoch [4929/10000] Avg train loss: 0.009410\n",
      "Epoch [4930/10000] Avg train loss: 0.009408\n",
      "Epoch [4931/10000] Avg train loss: 0.009406\n",
      "Epoch [4932/10000] Avg train loss: 0.009404\n",
      "Epoch [4933/10000] Avg train loss: 0.009402\n",
      "Epoch [4934/10000] Avg train loss: 0.009400\n",
      "Epoch [4935/10000] Avg train loss: 0.009398\n",
      "Epoch [4936/10000] Avg train loss: 0.009397\n",
      "Epoch [4937/10000] Avg train loss: 0.009395\n",
      "Epoch [4938/10000] Avg train loss: 0.009393\n",
      "Epoch [4939/10000] Avg train loss: 0.009391\n",
      "Epoch [4940/10000] Avg train loss: 0.009389\n",
      "Epoch [4941/10000] Avg train loss: 0.009387\n",
      "Epoch [4942/10000] Avg train loss: 0.009385\n",
      "Epoch [4943/10000] Avg train loss: 0.009383\n",
      "Epoch [4944/10000] Avg train loss: 0.009381\n",
      "Epoch [4945/10000] Avg train loss: 0.009379\n",
      "Epoch [4946/10000] Avg train loss: 0.009378\n",
      "Epoch [4947/10000] Avg train loss: 0.009376\n",
      "Epoch [4948/10000] Avg train loss: 0.009374\n",
      "Epoch [4949/10000] Avg train loss: 0.009372\n",
      "Epoch [4950/10000] Avg train loss: 0.009370\n",
      "Epoch [4951/10000] Avg train loss: 0.009368\n",
      "Epoch [4952/10000] Avg train loss: 0.009366\n",
      "Epoch [4953/10000] Avg train loss: 0.009364\n",
      "Epoch [4954/10000] Avg train loss: 0.009362\n",
      "Epoch [4955/10000] Avg train loss: 0.009360\n",
      "Epoch [4956/10000] Avg train loss: 0.009359\n",
      "Epoch [4957/10000] Avg train loss: 0.009357\n",
      "Epoch [4958/10000] Avg train loss: 0.009355\n",
      "Epoch [4959/10000] Avg train loss: 0.009353\n",
      "Epoch [4960/10000] Avg train loss: 0.009351\n",
      "Epoch [4961/10000] Avg train loss: 0.009349\n",
      "Epoch [4962/10000] Avg train loss: 0.009347\n",
      "Epoch [4963/10000] Avg train loss: 0.009345\n",
      "Epoch [4964/10000] Avg train loss: 0.009344\n",
      "Epoch [4965/10000] Avg train loss: 0.009342\n",
      "Epoch [4966/10000] Avg train loss: 0.009340\n",
      "Epoch [4967/10000] Avg train loss: 0.009338\n",
      "Epoch [4968/10000] Avg train loss: 0.009336\n",
      "Epoch [4969/10000] Avg train loss: 0.009334\n",
      "Epoch [4970/10000] Avg train loss: 0.009332\n",
      "Epoch [4971/10000] Avg train loss: 0.009330\n",
      "Epoch [4972/10000] Avg train loss: 0.009328\n",
      "Epoch [4973/10000] Avg train loss: 0.009327\n",
      "Epoch [4974/10000] Avg train loss: 0.009325\n",
      "Epoch [4975/10000] Avg train loss: 0.009323\n",
      "Epoch [4976/10000] Avg train loss: 0.009321\n",
      "Epoch [4977/10000] Avg train loss: 0.009319\n",
      "Epoch [4978/10000] Avg train loss: 0.009317\n",
      "Epoch [4979/10000] Avg train loss: 0.009315\n",
      "Epoch [4980/10000] Avg train loss: 0.009314\n",
      "Epoch [4981/10000] Avg train loss: 0.009312\n",
      "Epoch [4982/10000] Avg train loss: 0.009310\n",
      "Epoch [4983/10000] Avg train loss: 0.009308\n",
      "Epoch [4984/10000] Avg train loss: 0.009306\n",
      "Epoch [4985/10000] Avg train loss: 0.009304\n",
      "Epoch [4986/10000] Avg train loss: 0.009302\n",
      "Epoch [4987/10000] Avg train loss: 0.009300\n",
      "Epoch [4988/10000] Avg train loss: 0.009299\n",
      "Epoch [4989/10000] Avg train loss: 0.009297\n",
      "Epoch [4990/10000] Avg train loss: 0.009295\n",
      "Epoch [4991/10000] Avg train loss: 0.009293\n",
      "Epoch [4992/10000] Avg train loss: 0.009291\n",
      "Epoch [4993/10000] Avg train loss: 0.009289\n",
      "Epoch [4994/10000] Avg train loss: 0.009287\n",
      "Epoch [4995/10000] Avg train loss: 0.009286\n",
      "Epoch [4996/10000] Avg train loss: 0.009284\n",
      "Epoch [4997/10000] Avg train loss: 0.009282\n",
      "Epoch [4998/10000] Avg train loss: 0.009280\n",
      "Epoch [4999/10000] Avg train loss: 0.009278\n",
      "Epoch [5000/10000] Avg train loss: 0.009276\n",
      "Epoch [5001/10000] Avg train loss: 0.009274\n",
      "Epoch [5002/10000] Avg train loss: 0.009273\n",
      "Epoch [5003/10000] Avg train loss: 0.009271\n",
      "Epoch [5004/10000] Avg train loss: 0.009269\n",
      "Epoch [5005/10000] Avg train loss: 0.009267\n",
      "Epoch [5006/10000] Avg train loss: 0.009265\n",
      "Epoch [5007/10000] Avg train loss: 0.009263\n",
      "Epoch [5008/10000] Avg train loss: 0.009261\n",
      "Epoch [5009/10000] Avg train loss: 0.009260\n",
      "Epoch [5010/10000] Avg train loss: 0.009258\n",
      "Epoch [5011/10000] Avg train loss: 0.009256\n",
      "Epoch [5012/10000] Avg train loss: 0.009254\n",
      "Epoch [5013/10000] Avg train loss: 0.009252\n",
      "Epoch [5014/10000] Avg train loss: 0.009250\n",
      "Epoch [5015/10000] Avg train loss: 0.009249\n",
      "Epoch [5016/10000] Avg train loss: 0.009247\n",
      "Epoch [5017/10000] Avg train loss: 0.009245\n",
      "Epoch [5018/10000] Avg train loss: 0.009243\n",
      "Epoch [5019/10000] Avg train loss: 0.009241\n",
      "Epoch [5020/10000] Avg train loss: 0.009239\n",
      "Epoch [5021/10000] Avg train loss: 0.009237\n",
      "Epoch [5022/10000] Avg train loss: 0.009236\n",
      "Epoch [5023/10000] Avg train loss: 0.009234\n",
      "Epoch [5024/10000] Avg train loss: 0.009232\n",
      "Epoch [5025/10000] Avg train loss: 0.009230\n",
      "Epoch [5026/10000] Avg train loss: 0.009228\n",
      "Epoch [5027/10000] Avg train loss: 0.009226\n",
      "Epoch [5028/10000] Avg train loss: 0.009225\n",
      "Epoch [5029/10000] Avg train loss: 0.009223\n",
      "Epoch [5030/10000] Avg train loss: 0.009221\n",
      "Epoch [5031/10000] Avg train loss: 0.009219\n",
      "Epoch [5032/10000] Avg train loss: 0.009217\n",
      "Epoch [5033/10000] Avg train loss: 0.009215\n",
      "Epoch [5034/10000] Avg train loss: 0.009214\n",
      "Epoch [5035/10000] Avg train loss: 0.009212\n",
      "Epoch [5036/10000] Avg train loss: 0.009210\n",
      "Epoch [5037/10000] Avg train loss: 0.009208\n",
      "Epoch [5038/10000] Avg train loss: 0.009206\n",
      "Epoch [5039/10000] Avg train loss: 0.009204\n",
      "Epoch [5040/10000] Avg train loss: 0.009203\n",
      "Epoch [5041/10000] Avg train loss: 0.009201\n",
      "Epoch [5042/10000] Avg train loss: 0.009199\n",
      "Epoch [5043/10000] Avg train loss: 0.009197\n",
      "Epoch [5044/10000] Avg train loss: 0.009195\n",
      "Epoch [5045/10000] Avg train loss: 0.009194\n",
      "Epoch [5046/10000] Avg train loss: 0.009192\n",
      "Epoch [5047/10000] Avg train loss: 0.009190\n",
      "Epoch [5048/10000] Avg train loss: 0.009188\n",
      "Epoch [5049/10000] Avg train loss: 0.009186\n",
      "Epoch [5050/10000] Avg train loss: 0.009184\n",
      "Epoch [5051/10000] Avg train loss: 0.009183\n",
      "Epoch [5052/10000] Avg train loss: 0.009181\n",
      "Epoch [5053/10000] Avg train loss: 0.009179\n",
      "Epoch [5054/10000] Avg train loss: 0.009177\n",
      "Epoch [5055/10000] Avg train loss: 0.009175\n",
      "Epoch [5056/10000] Avg train loss: 0.009174\n",
      "Epoch [5057/10000] Avg train loss: 0.009172\n",
      "Epoch [5058/10000] Avg train loss: 0.009170\n",
      "Epoch [5059/10000] Avg train loss: 0.009168\n",
      "Epoch [5060/10000] Avg train loss: 0.009166\n",
      "Epoch [5061/10000] Avg train loss: 0.009164\n",
      "Epoch [5062/10000] Avg train loss: 0.009163\n",
      "Epoch [5063/10000] Avg train loss: 0.009161\n",
      "Epoch [5064/10000] Avg train loss: 0.009159\n",
      "Epoch [5065/10000] Avg train loss: 0.009157\n",
      "Epoch [5066/10000] Avg train loss: 0.009155\n",
      "Epoch [5067/10000] Avg train loss: 0.009154\n",
      "Epoch [5068/10000] Avg train loss: 0.009152\n",
      "Epoch [5069/10000] Avg train loss: 0.009150\n",
      "Epoch [5070/10000] Avg train loss: 0.009148\n",
      "Epoch [5071/10000] Avg train loss: 0.009146\n",
      "Epoch [5072/10000] Avg train loss: 0.009145\n",
      "Epoch [5073/10000] Avg train loss: 0.009143\n",
      "Epoch [5074/10000] Avg train loss: 0.009141\n",
      "Epoch [5075/10000] Avg train loss: 0.009139\n",
      "Epoch [5076/10000] Avg train loss: 0.009137\n",
      "Epoch [5077/10000] Avg train loss: 0.009136\n",
      "Epoch [5078/10000] Avg train loss: 0.009134\n",
      "Epoch [5079/10000] Avg train loss: 0.009132\n",
      "Epoch [5080/10000] Avg train loss: 0.009130\n",
      "Epoch [5081/10000] Avg train loss: 0.009128\n",
      "Epoch [5082/10000] Avg train loss: 0.009127\n",
      "Epoch [5083/10000] Avg train loss: 0.009125\n",
      "Epoch [5084/10000] Avg train loss: 0.009123\n",
      "Epoch [5085/10000] Avg train loss: 0.009121\n",
      "Epoch [5086/10000] Avg train loss: 0.009119\n",
      "Epoch [5087/10000] Avg train loss: 0.009118\n",
      "Epoch [5088/10000] Avg train loss: 0.009116\n",
      "Epoch [5089/10000] Avg train loss: 0.009114\n",
      "Epoch [5090/10000] Avg train loss: 0.009112\n",
      "Epoch [5091/10000] Avg train loss: 0.009110\n",
      "Epoch [5092/10000] Avg train loss: 0.009109\n",
      "Epoch [5093/10000] Avg train loss: 0.009107\n",
      "Epoch [5094/10000] Avg train loss: 0.009105\n",
      "Epoch [5095/10000] Avg train loss: 0.009103\n",
      "Epoch [5096/10000] Avg train loss: 0.009102\n",
      "Epoch [5097/10000] Avg train loss: 0.009100\n",
      "Epoch [5098/10000] Avg train loss: 0.009098\n",
      "Epoch [5099/10000] Avg train loss: 0.009096\n",
      "Epoch [5100/10000] Avg train loss: 0.009094\n",
      "Epoch [5101/10000] Avg train loss: 0.009093\n",
      "Epoch [5102/10000] Avg train loss: 0.009091\n",
      "Epoch [5103/10000] Avg train loss: 0.009089\n",
      "Epoch [5104/10000] Avg train loss: 0.009087\n",
      "Epoch [5105/10000] Avg train loss: 0.009085\n",
      "Epoch [5106/10000] Avg train loss: 0.009084\n",
      "Epoch [5107/10000] Avg train loss: 0.009082\n",
      "Epoch [5108/10000] Avg train loss: 0.009080\n",
      "Epoch [5109/10000] Avg train loss: 0.009078\n",
      "Epoch [5110/10000] Avg train loss: 0.009077\n",
      "Epoch [5111/10000] Avg train loss: 0.009075\n",
      "Epoch [5112/10000] Avg train loss: 0.009073\n",
      "Epoch [5113/10000] Avg train loss: 0.009071\n",
      "Epoch [5114/10000] Avg train loss: 0.009069\n",
      "Epoch [5115/10000] Avg train loss: 0.009068\n",
      "Epoch [5116/10000] Avg train loss: 0.009066\n",
      "Epoch [5117/10000] Avg train loss: 0.009064\n",
      "Epoch [5118/10000] Avg train loss: 0.009062\n",
      "Epoch [5119/10000] Avg train loss: 0.009061\n",
      "Epoch [5120/10000] Avg train loss: 0.009059\n",
      "Epoch [5121/10000] Avg train loss: 0.009057\n",
      "Epoch [5122/10000] Avg train loss: 0.009055\n",
      "Epoch [5123/10000] Avg train loss: 0.009054\n",
      "Epoch [5124/10000] Avg train loss: 0.009052\n",
      "Epoch [5125/10000] Avg train loss: 0.009050\n",
      "Epoch [5126/10000] Avg train loss: 0.009048\n",
      "Epoch [5127/10000] Avg train loss: 0.009046\n",
      "Epoch [5128/10000] Avg train loss: 0.009045\n",
      "Epoch [5129/10000] Avg train loss: 0.009043\n",
      "Epoch [5130/10000] Avg train loss: 0.009041\n",
      "Epoch [5131/10000] Avg train loss: 0.009039\n",
      "Epoch [5132/10000] Avg train loss: 0.009038\n",
      "Epoch [5133/10000] Avg train loss: 0.009036\n",
      "Epoch [5134/10000] Avg train loss: 0.009034\n",
      "Epoch [5135/10000] Avg train loss: 0.009032\n",
      "Epoch [5136/10000] Avg train loss: 0.009031\n",
      "Epoch [5137/10000] Avg train loss: 0.009029\n",
      "Epoch [5138/10000] Avg train loss: 0.009027\n",
      "Epoch [5139/10000] Avg train loss: 0.009025\n",
      "Epoch [5140/10000] Avg train loss: 0.009024\n",
      "Epoch [5141/10000] Avg train loss: 0.009022\n",
      "Epoch [5142/10000] Avg train loss: 0.009020\n",
      "Epoch [5143/10000] Avg train loss: 0.009018\n",
      "Epoch [5144/10000] Avg train loss: 0.009017\n",
      "Epoch [5145/10000] Avg train loss: 0.009015\n",
      "Epoch [5146/10000] Avg train loss: 0.009013\n",
      "Epoch [5147/10000] Avg train loss: 0.009011\n",
      "Epoch [5148/10000] Avg train loss: 0.009010\n",
      "Epoch [5149/10000] Avg train loss: 0.009008\n",
      "Epoch [5150/10000] Avg train loss: 0.009006\n",
      "Epoch [5151/10000] Avg train loss: 0.009004\n",
      "Epoch [5152/10000] Avg train loss: 0.009003\n",
      "Epoch [5153/10000] Avg train loss: 0.009001\n",
      "Epoch [5154/10000] Avg train loss: 0.008999\n",
      "Epoch [5155/10000] Avg train loss: 0.008997\n",
      "Epoch [5156/10000] Avg train loss: 0.008996\n",
      "Epoch [5157/10000] Avg train loss: 0.008994\n",
      "Epoch [5158/10000] Avg train loss: 0.008992\n",
      "Epoch [5159/10000] Avg train loss: 0.008990\n",
      "Epoch [5160/10000] Avg train loss: 0.008989\n",
      "Epoch [5161/10000] Avg train loss: 0.008987\n",
      "Epoch [5162/10000] Avg train loss: 0.008985\n",
      "Epoch [5163/10000] Avg train loss: 0.008983\n",
      "Epoch [5164/10000] Avg train loss: 0.008982\n",
      "Epoch [5165/10000] Avg train loss: 0.008980\n",
      "Epoch [5166/10000] Avg train loss: 0.008978\n",
      "Epoch [5167/10000] Avg train loss: 0.008976\n",
      "Epoch [5168/10000] Avg train loss: 0.008975\n",
      "Epoch [5169/10000] Avg train loss: 0.008973\n",
      "Epoch [5170/10000] Avg train loss: 0.008971\n",
      "Epoch [5171/10000] Avg train loss: 0.008969\n",
      "Epoch [5172/10000] Avg train loss: 0.008968\n",
      "Epoch [5173/10000] Avg train loss: 0.008966\n",
      "Epoch [5174/10000] Avg train loss: 0.008964\n",
      "Epoch [5175/10000] Avg train loss: 0.008963\n",
      "Epoch [5176/10000] Avg train loss: 0.008961\n",
      "Epoch [5177/10000] Avg train loss: 0.008959\n",
      "Epoch [5178/10000] Avg train loss: 0.008957\n",
      "Epoch [5179/10000] Avg train loss: 0.008956\n",
      "Epoch [5180/10000] Avg train loss: 0.008954\n",
      "Epoch [5181/10000] Avg train loss: 0.008952\n",
      "Epoch [5182/10000] Avg train loss: 0.008950\n",
      "Epoch [5183/10000] Avg train loss: 0.008949\n",
      "Epoch [5184/10000] Avg train loss: 0.008947\n",
      "Epoch [5185/10000] Avg train loss: 0.008945\n",
      "Epoch [5186/10000] Avg train loss: 0.008944\n",
      "Epoch [5187/10000] Avg train loss: 0.008942\n",
      "Epoch [5188/10000] Avg train loss: 0.008940\n",
      "Epoch [5189/10000] Avg train loss: 0.008938\n",
      "Epoch [5190/10000] Avg train loss: 0.008937\n",
      "Epoch [5191/10000] Avg train loss: 0.008935\n",
      "Epoch [5192/10000] Avg train loss: 0.008933\n",
      "Epoch [5193/10000] Avg train loss: 0.008931\n",
      "Epoch [5194/10000] Avg train loss: 0.008930\n",
      "Epoch [5195/10000] Avg train loss: 0.008928\n",
      "Epoch [5196/10000] Avg train loss: 0.008926\n",
      "Epoch [5197/10000] Avg train loss: 0.008925\n",
      "Epoch [5198/10000] Avg train loss: 0.008923\n",
      "Epoch [5199/10000] Avg train loss: 0.008921\n",
      "Epoch [5200/10000] Avg train loss: 0.008919\n",
      "Epoch [5201/10000] Avg train loss: 0.008918\n",
      "Epoch [5202/10000] Avg train loss: 0.008916\n",
      "Epoch [5203/10000] Avg train loss: 0.008914\n",
      "Epoch [5204/10000] Avg train loss: 0.008913\n",
      "Epoch [5205/10000] Avg train loss: 0.008911\n",
      "Epoch [5206/10000] Avg train loss: 0.008909\n",
      "Epoch [5207/10000] Avg train loss: 0.008907\n",
      "Epoch [5208/10000] Avg train loss: 0.008906\n",
      "Epoch [5209/10000] Avg train loss: 0.008904\n",
      "Epoch [5210/10000] Avg train loss: 0.008902\n",
      "Epoch [5211/10000] Avg train loss: 0.008901\n",
      "Epoch [5212/10000] Avg train loss: 0.008899\n",
      "Epoch [5213/10000] Avg train loss: 0.008897\n",
      "Epoch [5214/10000] Avg train loss: 0.008896\n",
      "Epoch [5215/10000] Avg train loss: 0.008894\n",
      "Epoch [5216/10000] Avg train loss: 0.008892\n",
      "Epoch [5217/10000] Avg train loss: 0.008890\n",
      "Epoch [5218/10000] Avg train loss: 0.008889\n",
      "Epoch [5219/10000] Avg train loss: 0.008887\n",
      "Epoch [5220/10000] Avg train loss: 0.008885\n",
      "Epoch [5221/10000] Avg train loss: 0.008884\n",
      "Epoch [5222/10000] Avg train loss: 0.008882\n",
      "Epoch [5223/10000] Avg train loss: 0.008880\n",
      "Epoch [5224/10000] Avg train loss: 0.008878\n",
      "Epoch [5225/10000] Avg train loss: 0.008877\n",
      "Epoch [5226/10000] Avg train loss: 0.008875\n",
      "Epoch [5227/10000] Avg train loss: 0.008873\n",
      "Epoch [5228/10000] Avg train loss: 0.008872\n",
      "Epoch [5229/10000] Avg train loss: 0.008870\n",
      "Epoch [5230/10000] Avg train loss: 0.008868\n",
      "Epoch [5231/10000] Avg train loss: 0.008867\n",
      "Epoch [5232/10000] Avg train loss: 0.008865\n",
      "Epoch [5233/10000] Avg train loss: 0.008863\n",
      "Epoch [5234/10000] Avg train loss: 0.008862\n",
      "Epoch [5235/10000] Avg train loss: 0.008860\n",
      "Epoch [5236/10000] Avg train loss: 0.008858\n",
      "Epoch [5237/10000] Avg train loss: 0.008856\n",
      "Epoch [5238/10000] Avg train loss: 0.008855\n",
      "Epoch [5239/10000] Avg train loss: 0.008853\n",
      "Epoch [5240/10000] Avg train loss: 0.008851\n",
      "Epoch [5241/10000] Avg train loss: 0.008850\n",
      "Epoch [5242/10000] Avg train loss: 0.008848\n",
      "Epoch [5243/10000] Avg train loss: 0.008846\n",
      "Epoch [5244/10000] Avg train loss: 0.008845\n",
      "Epoch [5245/10000] Avg train loss: 0.008843\n",
      "Epoch [5246/10000] Avg train loss: 0.008841\n",
      "Epoch [5247/10000] Avg train loss: 0.008840\n",
      "Epoch [5248/10000] Avg train loss: 0.008838\n",
      "Epoch [5249/10000] Avg train loss: 0.008836\n",
      "Epoch [5250/10000] Avg train loss: 0.008835\n",
      "Epoch [5251/10000] Avg train loss: 0.008833\n",
      "Epoch [5252/10000] Avg train loss: 0.008831\n",
      "Epoch [5253/10000] Avg train loss: 0.008829\n",
      "Epoch [5254/10000] Avg train loss: 0.008828\n",
      "Epoch [5255/10000] Avg train loss: 0.008826\n",
      "Epoch [5256/10000] Avg train loss: 0.008824\n",
      "Epoch [5257/10000] Avg train loss: 0.008823\n",
      "Epoch [5258/10000] Avg train loss: 0.008821\n",
      "Epoch [5259/10000] Avg train loss: 0.008819\n",
      "Epoch [5260/10000] Avg train loss: 0.008818\n",
      "Epoch [5261/10000] Avg train loss: 0.008816\n",
      "Epoch [5262/10000] Avg train loss: 0.008814\n",
      "Epoch [5263/10000] Avg train loss: 0.008813\n",
      "Epoch [5264/10000] Avg train loss: 0.008811\n",
      "Epoch [5265/10000] Avg train loss: 0.008809\n",
      "Epoch [5266/10000] Avg train loss: 0.008808\n",
      "Epoch [5267/10000] Avg train loss: 0.008806\n",
      "Epoch [5268/10000] Avg train loss: 0.008804\n",
      "Epoch [5269/10000] Avg train loss: 0.008803\n",
      "Epoch [5270/10000] Avg train loss: 0.008801\n",
      "Epoch [5271/10000] Avg train loss: 0.008799\n",
      "Epoch [5272/10000] Avg train loss: 0.008798\n",
      "Epoch [5273/10000] Avg train loss: 0.008796\n",
      "Epoch [5274/10000] Avg train loss: 0.008794\n",
      "Epoch [5275/10000] Avg train loss: 0.008793\n",
      "Epoch [5276/10000] Avg train loss: 0.008791\n",
      "Epoch [5277/10000] Avg train loss: 0.008789\n",
      "Epoch [5278/10000] Avg train loss: 0.008788\n",
      "Epoch [5279/10000] Avg train loss: 0.008786\n",
      "Epoch [5280/10000] Avg train loss: 0.008784\n",
      "Epoch [5281/10000] Avg train loss: 0.008783\n",
      "Epoch [5282/10000] Avg train loss: 0.008781\n",
      "Epoch [5283/10000] Avg train loss: 0.008779\n",
      "Epoch [5284/10000] Avg train loss: 0.008778\n",
      "Epoch [5285/10000] Avg train loss: 0.008776\n",
      "Epoch [5286/10000] Avg train loss: 0.008774\n",
      "Epoch [5287/10000] Avg train loss: 0.008773\n",
      "Epoch [5288/10000] Avg train loss: 0.008771\n",
      "Epoch [5289/10000] Avg train loss: 0.008769\n",
      "Epoch [5290/10000] Avg train loss: 0.008768\n",
      "Epoch [5291/10000] Avg train loss: 0.008766\n",
      "Epoch [5292/10000] Avg train loss: 0.008764\n",
      "Epoch [5293/10000] Avg train loss: 0.008763\n",
      "Epoch [5294/10000] Avg train loss: 0.008761\n",
      "Epoch [5295/10000] Avg train loss: 0.008759\n",
      "Epoch [5296/10000] Avg train loss: 0.008758\n",
      "Epoch [5297/10000] Avg train loss: 0.008756\n",
      "Epoch [5298/10000] Avg train loss: 0.008754\n",
      "Epoch [5299/10000] Avg train loss: 0.008753\n",
      "Epoch [5300/10000] Avg train loss: 0.008751\n",
      "Epoch [5301/10000] Avg train loss: 0.008750\n",
      "Epoch [5302/10000] Avg train loss: 0.008748\n",
      "Epoch [5303/10000] Avg train loss: 0.008746\n",
      "Epoch [5304/10000] Avg train loss: 0.008745\n",
      "Epoch [5305/10000] Avg train loss: 0.008743\n",
      "Epoch [5306/10000] Avg train loss: 0.008741\n",
      "Epoch [5307/10000] Avg train loss: 0.008740\n",
      "Epoch [5308/10000] Avg train loss: 0.008738\n",
      "Epoch [5309/10000] Avg train loss: 0.008736\n",
      "Epoch [5310/10000] Avg train loss: 0.008735\n",
      "Epoch [5311/10000] Avg train loss: 0.008733\n",
      "Epoch [5312/10000] Avg train loss: 0.008731\n",
      "Epoch [5313/10000] Avg train loss: 0.008730\n",
      "Epoch [5314/10000] Avg train loss: 0.008728\n",
      "Epoch [5315/10000] Avg train loss: 0.008726\n",
      "Epoch [5316/10000] Avg train loss: 0.008725\n",
      "Epoch [5317/10000] Avg train loss: 0.008723\n",
      "Epoch [5318/10000] Avg train loss: 0.008722\n",
      "Epoch [5319/10000] Avg train loss: 0.008720\n",
      "Epoch [5320/10000] Avg train loss: 0.008718\n",
      "Epoch [5321/10000] Avg train loss: 0.008717\n",
      "Epoch [5322/10000] Avg train loss: 0.008715\n",
      "Epoch [5323/10000] Avg train loss: 0.008713\n",
      "Epoch [5324/10000] Avg train loss: 0.008712\n",
      "Epoch [5325/10000] Avg train loss: 0.008710\n",
      "Epoch [5326/10000] Avg train loss: 0.008708\n",
      "Epoch [5327/10000] Avg train loss: 0.008707\n",
      "Epoch [5328/10000] Avg train loss: 0.008705\n",
      "Epoch [5329/10000] Avg train loss: 0.008704\n",
      "Epoch [5330/10000] Avg train loss: 0.008702\n",
      "Epoch [5331/10000] Avg train loss: 0.008700\n",
      "Epoch [5332/10000] Avg train loss: 0.008699\n",
      "Epoch [5333/10000] Avg train loss: 0.008697\n",
      "Epoch [5334/10000] Avg train loss: 0.008695\n",
      "Epoch [5335/10000] Avg train loss: 0.008694\n",
      "Epoch [5336/10000] Avg train loss: 0.008692\n",
      "Epoch [5337/10000] Avg train loss: 0.008691\n",
      "Epoch [5338/10000] Avg train loss: 0.008689\n",
      "Epoch [5339/10000] Avg train loss: 0.008687\n",
      "Epoch [5340/10000] Avg train loss: 0.008686\n",
      "Epoch [5341/10000] Avg train loss: 0.008684\n",
      "Epoch [5342/10000] Avg train loss: 0.008682\n",
      "Epoch [5343/10000] Avg train loss: 0.008681\n",
      "Epoch [5344/10000] Avg train loss: 0.008679\n",
      "Epoch [5345/10000] Avg train loss: 0.008678\n",
      "Epoch [5346/10000] Avg train loss: 0.008676\n",
      "Epoch [5347/10000] Avg train loss: 0.008674\n",
      "Epoch [5348/10000] Avg train loss: 0.008673\n",
      "Epoch [5349/10000] Avg train loss: 0.008671\n",
      "Epoch [5350/10000] Avg train loss: 0.008669\n",
      "Epoch [5351/10000] Avg train loss: 0.008668\n",
      "Epoch [5352/10000] Avg train loss: 0.008666\n",
      "Epoch [5353/10000] Avg train loss: 0.008665\n",
      "Epoch [5354/10000] Avg train loss: 0.008663\n",
      "Epoch [5355/10000] Avg train loss: 0.008661\n",
      "Epoch [5356/10000] Avg train loss: 0.008660\n",
      "Epoch [5357/10000] Avg train loss: 0.008658\n",
      "Epoch [5358/10000] Avg train loss: 0.008656\n",
      "Epoch [5359/10000] Avg train loss: 0.008655\n",
      "Epoch [5360/10000] Avg train loss: 0.008653\n",
      "Epoch [5361/10000] Avg train loss: 0.008652\n",
      "Epoch [5362/10000] Avg train loss: 0.008650\n",
      "Epoch [5363/10000] Avg train loss: 0.008648\n",
      "Epoch [5364/10000] Avg train loss: 0.008647\n",
      "Epoch [5365/10000] Avg train loss: 0.008645\n",
      "Epoch [5366/10000] Avg train loss: 0.008644\n",
      "Epoch [5367/10000] Avg train loss: 0.008642\n",
      "Epoch [5368/10000] Avg train loss: 0.008640\n",
      "Epoch [5369/10000] Avg train loss: 0.008639\n",
      "Epoch [5370/10000] Avg train loss: 0.008637\n",
      "Epoch [5371/10000] Avg train loss: 0.008635\n",
      "Epoch [5372/10000] Avg train loss: 0.008634\n",
      "Epoch [5373/10000] Avg train loss: 0.008632\n",
      "Epoch [5374/10000] Avg train loss: 0.008631\n",
      "Epoch [5375/10000] Avg train loss: 0.008629\n",
      "Epoch [5376/10000] Avg train loss: 0.008627\n",
      "Epoch [5377/10000] Avg train loss: 0.008626\n",
      "Epoch [5378/10000] Avg train loss: 0.008624\n",
      "Epoch [5379/10000] Avg train loss: 0.008623\n",
      "Epoch [5380/10000] Avg train loss: 0.008621\n",
      "Epoch [5381/10000] Avg train loss: 0.008619\n",
      "Epoch [5382/10000] Avg train loss: 0.008618\n",
      "Epoch [5383/10000] Avg train loss: 0.008616\n",
      "Epoch [5384/10000] Avg train loss: 0.008615\n",
      "Epoch [5385/10000] Avg train loss: 0.008613\n",
      "Epoch [5386/10000] Avg train loss: 0.008611\n",
      "Epoch [5387/10000] Avg train loss: 0.008610\n",
      "Epoch [5388/10000] Avg train loss: 0.008608\n",
      "Epoch [5389/10000] Avg train loss: 0.008607\n",
      "Epoch [5390/10000] Avg train loss: 0.008605\n",
      "Epoch [5391/10000] Avg train loss: 0.008603\n",
      "Epoch [5392/10000] Avg train loss: 0.008602\n",
      "Epoch [5393/10000] Avg train loss: 0.008600\n",
      "Epoch [5394/10000] Avg train loss: 0.008599\n",
      "Epoch [5395/10000] Avg train loss: 0.008597\n",
      "Epoch [5396/10000] Avg train loss: 0.008595\n",
      "Epoch [5397/10000] Avg train loss: 0.008594\n",
      "Epoch [5398/10000] Avg train loss: 0.008592\n",
      "Epoch [5399/10000] Avg train loss: 0.008591\n",
      "Epoch [5400/10000] Avg train loss: 0.008589\n",
      "Epoch [5401/10000] Avg train loss: 0.008588\n",
      "Epoch [5402/10000] Avg train loss: 0.008586\n",
      "Epoch [5403/10000] Avg train loss: 0.008584\n",
      "Epoch [5404/10000] Avg train loss: 0.008583\n",
      "Epoch [5405/10000] Avg train loss: 0.008581\n",
      "Epoch [5406/10000] Avg train loss: 0.008580\n",
      "Epoch [5407/10000] Avg train loss: 0.008578\n",
      "Epoch [5408/10000] Avg train loss: 0.008576\n",
      "Epoch [5409/10000] Avg train loss: 0.008575\n",
      "Epoch [5410/10000] Avg train loss: 0.008573\n",
      "Epoch [5411/10000] Avg train loss: 0.008572\n",
      "Epoch [5412/10000] Avg train loss: 0.008570\n",
      "Epoch [5413/10000] Avg train loss: 0.008568\n",
      "Epoch [5414/10000] Avg train loss: 0.008567\n",
      "Epoch [5415/10000] Avg train loss: 0.008565\n",
      "Epoch [5416/10000] Avg train loss: 0.008564\n",
      "Epoch [5417/10000] Avg train loss: 0.008562\n",
      "Epoch [5418/10000] Avg train loss: 0.008561\n",
      "Epoch [5419/10000] Avg train loss: 0.008559\n",
      "Epoch [5420/10000] Avg train loss: 0.008557\n",
      "Epoch [5421/10000] Avg train loss: 0.008556\n",
      "Epoch [5422/10000] Avg train loss: 0.008554\n",
      "Epoch [5423/10000] Avg train loss: 0.008553\n",
      "Epoch [5424/10000] Avg train loss: 0.008551\n",
      "Epoch [5425/10000] Avg train loss: 0.008550\n",
      "Epoch [5426/10000] Avg train loss: 0.008548\n",
      "Epoch [5427/10000] Avg train loss: 0.008546\n",
      "Epoch [5428/10000] Avg train loss: 0.008545\n",
      "Epoch [5429/10000] Avg train loss: 0.008543\n",
      "Epoch [5430/10000] Avg train loss: 0.008542\n",
      "Epoch [5431/10000] Avg train loss: 0.008540\n",
      "Epoch [5432/10000] Avg train loss: 0.008539\n",
      "Epoch [5433/10000] Avg train loss: 0.008537\n",
      "Epoch [5434/10000] Avg train loss: 0.008535\n",
      "Epoch [5435/10000] Avg train loss: 0.008534\n",
      "Epoch [5436/10000] Avg train loss: 0.008532\n",
      "Epoch [5437/10000] Avg train loss: 0.008531\n",
      "Epoch [5438/10000] Avg train loss: 0.008529\n",
      "Epoch [5439/10000] Avg train loss: 0.008528\n",
      "Epoch [5440/10000] Avg train loss: 0.008526\n",
      "Epoch [5441/10000] Avg train loss: 0.008524\n",
      "Epoch [5442/10000] Avg train loss: 0.008523\n",
      "Epoch [5443/10000] Avg train loss: 0.008521\n",
      "Epoch [5444/10000] Avg train loss: 0.008520\n",
      "Epoch [5445/10000] Avg train loss: 0.008518\n",
      "Epoch [5446/10000] Avg train loss: 0.008517\n",
      "Epoch [5447/10000] Avg train loss: 0.008515\n",
      "Epoch [5448/10000] Avg train loss: 0.008513\n",
      "Epoch [5449/10000] Avg train loss: 0.008512\n",
      "Epoch [5450/10000] Avg train loss: 0.008510\n",
      "Epoch [5451/10000] Avg train loss: 0.008509\n",
      "Epoch [5452/10000] Avg train loss: 0.008507\n",
      "Epoch [5453/10000] Avg train loss: 0.008506\n",
      "Epoch [5454/10000] Avg train loss: 0.008504\n",
      "Epoch [5455/10000] Avg train loss: 0.008503\n",
      "Epoch [5456/10000] Avg train loss: 0.008501\n",
      "Epoch [5457/10000] Avg train loss: 0.008499\n",
      "Epoch [5458/10000] Avg train loss: 0.008498\n",
      "Epoch [5459/10000] Avg train loss: 0.008496\n",
      "Epoch [5460/10000] Avg train loss: 0.008495\n",
      "Epoch [5461/10000] Avg train loss: 0.008493\n",
      "Epoch [5462/10000] Avg train loss: 0.008492\n",
      "Epoch [5463/10000] Avg train loss: 0.008490\n",
      "Epoch [5464/10000] Avg train loss: 0.008489\n",
      "Epoch [5465/10000] Avg train loss: 0.008487\n",
      "Epoch [5466/10000] Avg train loss: 0.008485\n",
      "Epoch [5467/10000] Avg train loss: 0.008484\n",
      "Epoch [5468/10000] Avg train loss: 0.008482\n",
      "Epoch [5469/10000] Avg train loss: 0.008481\n",
      "Epoch [5470/10000] Avg train loss: 0.008479\n",
      "Epoch [5471/10000] Avg train loss: 0.008478\n",
      "Epoch [5472/10000] Avg train loss: 0.008476\n",
      "Epoch [5473/10000] Avg train loss: 0.008475\n",
      "Epoch [5474/10000] Avg train loss: 0.008473\n",
      "Epoch [5475/10000] Avg train loss: 0.008471\n",
      "Epoch [5476/10000] Avg train loss: 0.008470\n",
      "Epoch [5477/10000] Avg train loss: 0.008468\n",
      "Epoch [5478/10000] Avg train loss: 0.008467\n",
      "Epoch [5479/10000] Avg train loss: 0.008465\n",
      "Epoch [5480/10000] Avg train loss: 0.008464\n",
      "Epoch [5481/10000] Avg train loss: 0.008462\n",
      "Epoch [5482/10000] Avg train loss: 0.008461\n",
      "Epoch [5483/10000] Avg train loss: 0.008459\n",
      "Epoch [5484/10000] Avg train loss: 0.008458\n",
      "Epoch [5485/10000] Avg train loss: 0.008456\n",
      "Epoch [5486/10000] Avg train loss: 0.008454\n",
      "Epoch [5487/10000] Avg train loss: 0.008453\n",
      "Epoch [5488/10000] Avg train loss: 0.008451\n",
      "Epoch [5489/10000] Avg train loss: 0.008450\n",
      "Epoch [5490/10000] Avg train loss: 0.008448\n",
      "Epoch [5491/10000] Avg train loss: 0.008447\n",
      "Epoch [5492/10000] Avg train loss: 0.008445\n",
      "Epoch [5493/10000] Avg train loss: 0.008444\n",
      "Epoch [5494/10000] Avg train loss: 0.008442\n",
      "Epoch [5495/10000] Avg train loss: 0.008441\n",
      "Epoch [5496/10000] Avg train loss: 0.008439\n",
      "Epoch [5497/10000] Avg train loss: 0.008438\n",
      "Epoch [5498/10000] Avg train loss: 0.008436\n",
      "Epoch [5499/10000] Avg train loss: 0.008434\n",
      "Epoch [5500/10000] Avg train loss: 0.008433\n",
      "Epoch [5501/10000] Avg train loss: 0.008431\n",
      "Epoch [5502/10000] Avg train loss: 0.008430\n",
      "Epoch [5503/10000] Avg train loss: 0.008428\n",
      "Epoch [5504/10000] Avg train loss: 0.008427\n",
      "Epoch [5505/10000] Avg train loss: 0.008425\n",
      "Epoch [5506/10000] Avg train loss: 0.008424\n",
      "Epoch [5507/10000] Avg train loss: 0.008422\n",
      "Epoch [5508/10000] Avg train loss: 0.008421\n",
      "Epoch [5509/10000] Avg train loss: 0.008419\n",
      "Epoch [5510/10000] Avg train loss: 0.008418\n",
      "Epoch [5511/10000] Avg train loss: 0.008416\n",
      "Epoch [5512/10000] Avg train loss: 0.008415\n",
      "Epoch [5513/10000] Avg train loss: 0.008413\n",
      "Epoch [5514/10000] Avg train loss: 0.008412\n",
      "Epoch [5515/10000] Avg train loss: 0.008410\n",
      "Epoch [5516/10000] Avg train loss: 0.008408\n",
      "Epoch [5517/10000] Avg train loss: 0.008407\n",
      "Epoch [5518/10000] Avg train loss: 0.008405\n",
      "Epoch [5519/10000] Avg train loss: 0.008404\n",
      "Epoch [5520/10000] Avg train loss: 0.008402\n",
      "Epoch [5521/10000] Avg train loss: 0.008401\n",
      "Epoch [5522/10000] Avg train loss: 0.008399\n",
      "Epoch [5523/10000] Avg train loss: 0.008398\n",
      "Epoch [5524/10000] Avg train loss: 0.008396\n",
      "Epoch [5525/10000] Avg train loss: 0.008395\n",
      "Epoch [5526/10000] Avg train loss: 0.008393\n",
      "Epoch [5527/10000] Avg train loss: 0.008392\n",
      "Epoch [5528/10000] Avg train loss: 0.008390\n",
      "Epoch [5529/10000] Avg train loss: 0.008389\n",
      "Epoch [5530/10000] Avg train loss: 0.008387\n",
      "Epoch [5531/10000] Avg train loss: 0.008386\n",
      "Epoch [5532/10000] Avg train loss: 0.008384\n",
      "Epoch [5533/10000] Avg train loss: 0.008383\n",
      "Epoch [5534/10000] Avg train loss: 0.008381\n",
      "Epoch [5535/10000] Avg train loss: 0.008380\n",
      "Epoch [5536/10000] Avg train loss: 0.008378\n",
      "Epoch [5537/10000] Avg train loss: 0.008377\n",
      "Epoch [5538/10000] Avg train loss: 0.008375\n",
      "Epoch [5539/10000] Avg train loss: 0.008374\n",
      "Epoch [5540/10000] Avg train loss: 0.008372\n",
      "Epoch [5541/10000] Avg train loss: 0.008371\n",
      "Epoch [5542/10000] Avg train loss: 0.008369\n",
      "Epoch [5543/10000] Avg train loss: 0.008368\n",
      "Epoch [5544/10000] Avg train loss: 0.008366\n",
      "Epoch [5545/10000] Avg train loss: 0.008365\n",
      "Epoch [5546/10000] Avg train loss: 0.008363\n",
      "Epoch [5547/10000] Avg train loss: 0.008362\n",
      "Epoch [5548/10000] Avg train loss: 0.008360\n",
      "Epoch [5549/10000] Avg train loss: 0.008358\n",
      "Epoch [5550/10000] Avg train loss: 0.008357\n",
      "Epoch [5551/10000] Avg train loss: 0.008355\n",
      "Epoch [5552/10000] Avg train loss: 0.008354\n",
      "Epoch [5553/10000] Avg train loss: 0.008352\n",
      "Epoch [5554/10000] Avg train loss: 0.008351\n",
      "Epoch [5555/10000] Avg train loss: 0.008349\n",
      "Epoch [5556/10000] Avg train loss: 0.008348\n",
      "Epoch [5557/10000] Avg train loss: 0.008346\n",
      "Epoch [5558/10000] Avg train loss: 0.008345\n",
      "Epoch [5559/10000] Avg train loss: 0.008343\n",
      "Epoch [5560/10000] Avg train loss: 0.008342\n",
      "Epoch [5561/10000] Avg train loss: 0.008340\n",
      "Epoch [5562/10000] Avg train loss: 0.008339\n",
      "Epoch [5563/10000] Avg train loss: 0.008337\n",
      "Epoch [5564/10000] Avg train loss: 0.008336\n",
      "Epoch [5565/10000] Avg train loss: 0.008334\n",
      "Epoch [5566/10000] Avg train loss: 0.008333\n",
      "Epoch [5567/10000] Avg train loss: 0.008331\n",
      "Epoch [5568/10000] Avg train loss: 0.008330\n",
      "Epoch [5569/10000] Avg train loss: 0.008328\n",
      "Epoch [5570/10000] Avg train loss: 0.008327\n",
      "Epoch [5571/10000] Avg train loss: 0.008325\n",
      "Epoch [5572/10000] Avg train loss: 0.008324\n",
      "Epoch [5573/10000] Avg train loss: 0.008322\n",
      "Epoch [5574/10000] Avg train loss: 0.008321\n",
      "Epoch [5575/10000] Avg train loss: 0.008320\n",
      "Epoch [5576/10000] Avg train loss: 0.008318\n",
      "Epoch [5577/10000] Avg train loss: 0.008317\n",
      "Epoch [5578/10000] Avg train loss: 0.008315\n",
      "Epoch [5579/10000] Avg train loss: 0.008314\n",
      "Epoch [5580/10000] Avg train loss: 0.008312\n",
      "Epoch [5581/10000] Avg train loss: 0.008311\n",
      "Epoch [5582/10000] Avg train loss: 0.008309\n",
      "Epoch [5583/10000] Avg train loss: 0.008308\n",
      "Epoch [5584/10000] Avg train loss: 0.008306\n",
      "Epoch [5585/10000] Avg train loss: 0.008305\n",
      "Epoch [5586/10000] Avg train loss: 0.008303\n",
      "Epoch [5587/10000] Avg train loss: 0.008302\n",
      "Epoch [5588/10000] Avg train loss: 0.008300\n",
      "Epoch [5589/10000] Avg train loss: 0.008299\n",
      "Epoch [5590/10000] Avg train loss: 0.008297\n",
      "Epoch [5591/10000] Avg train loss: 0.008296\n",
      "Epoch [5592/10000] Avg train loss: 0.008294\n",
      "Epoch [5593/10000] Avg train loss: 0.008293\n",
      "Epoch [5594/10000] Avg train loss: 0.008291\n",
      "Epoch [5595/10000] Avg train loss: 0.008290\n",
      "Epoch [5596/10000] Avg train loss: 0.008288\n",
      "Epoch [5597/10000] Avg train loss: 0.008287\n",
      "Epoch [5598/10000] Avg train loss: 0.008285\n",
      "Epoch [5599/10000] Avg train loss: 0.008284\n",
      "Epoch [5600/10000] Avg train loss: 0.008282\n",
      "Epoch [5601/10000] Avg train loss: 0.008281\n",
      "Epoch [5602/10000] Avg train loss: 0.008279\n",
      "Epoch [5603/10000] Avg train loss: 0.008278\n",
      "Epoch [5604/10000] Avg train loss: 0.008276\n",
      "Epoch [5605/10000] Avg train loss: 0.008275\n",
      "Epoch [5606/10000] Avg train loss: 0.008274\n",
      "Epoch [5607/10000] Avg train loss: 0.008272\n",
      "Epoch [5608/10000] Avg train loss: 0.008271\n",
      "Epoch [5609/10000] Avg train loss: 0.008269\n",
      "Epoch [5610/10000] Avg train loss: 0.008268\n",
      "Epoch [5611/10000] Avg train loss: 0.008266\n",
      "Epoch [5612/10000] Avg train loss: 0.008265\n",
      "Epoch [5613/10000] Avg train loss: 0.008263\n",
      "Epoch [5614/10000] Avg train loss: 0.008262\n",
      "Epoch [5615/10000] Avg train loss: 0.008260\n",
      "Epoch [5616/10000] Avg train loss: 0.008259\n",
      "Epoch [5617/10000] Avg train loss: 0.008257\n",
      "Epoch [5618/10000] Avg train loss: 0.008256\n",
      "Epoch [5619/10000] Avg train loss: 0.008254\n",
      "Epoch [5620/10000] Avg train loss: 0.008253\n",
      "Epoch [5621/10000] Avg train loss: 0.008251\n",
      "Epoch [5622/10000] Avg train loss: 0.008250\n",
      "Epoch [5623/10000] Avg train loss: 0.008248\n",
      "Epoch [5624/10000] Avg train loss: 0.008247\n",
      "Epoch [5625/10000] Avg train loss: 0.008246\n",
      "Epoch [5626/10000] Avg train loss: 0.008244\n",
      "Epoch [5627/10000] Avg train loss: 0.008243\n",
      "Epoch [5628/10000] Avg train loss: 0.008241\n",
      "Epoch [5629/10000] Avg train loss: 0.008240\n",
      "Epoch [5630/10000] Avg train loss: 0.008238\n",
      "Epoch [5631/10000] Avg train loss: 0.008237\n",
      "Epoch [5632/10000] Avg train loss: 0.008235\n",
      "Epoch [5633/10000] Avg train loss: 0.008234\n",
      "Epoch [5634/10000] Avg train loss: 0.008232\n",
      "Epoch [5635/10000] Avg train loss: 0.008231\n",
      "Epoch [5636/10000] Avg train loss: 0.008229\n",
      "Epoch [5637/10000] Avg train loss: 0.008228\n",
      "Epoch [5638/10000] Avg train loss: 0.008227\n",
      "Epoch [5639/10000] Avg train loss: 0.008225\n",
      "Epoch [5640/10000] Avg train loss: 0.008224\n",
      "Epoch [5641/10000] Avg train loss: 0.008222\n",
      "Epoch [5642/10000] Avg train loss: 0.008221\n",
      "Epoch [5643/10000] Avg train loss: 0.008219\n",
      "Epoch [5644/10000] Avg train loss: 0.008218\n",
      "Epoch [5645/10000] Avg train loss: 0.008216\n",
      "Epoch [5646/10000] Avg train loss: 0.008215\n",
      "Epoch [5647/10000] Avg train loss: 0.008213\n",
      "Epoch [5648/10000] Avg train loss: 0.008212\n",
      "Epoch [5649/10000] Avg train loss: 0.008211\n",
      "Epoch [5650/10000] Avg train loss: 0.008209\n",
      "Epoch [5651/10000] Avg train loss: 0.008208\n",
      "Epoch [5652/10000] Avg train loss: 0.008206\n",
      "Epoch [5653/10000] Avg train loss: 0.008205\n",
      "Epoch [5654/10000] Avg train loss: 0.008203\n",
      "Epoch [5655/10000] Avg train loss: 0.008202\n",
      "Epoch [5656/10000] Avg train loss: 0.008200\n",
      "Epoch [5657/10000] Avg train loss: 0.008199\n",
      "Epoch [5658/10000] Avg train loss: 0.008197\n",
      "Epoch [5659/10000] Avg train loss: 0.008196\n",
      "Epoch [5660/10000] Avg train loss: 0.008195\n",
      "Epoch [5661/10000] Avg train loss: 0.008193\n",
      "Epoch [5662/10000] Avg train loss: 0.008192\n",
      "Epoch [5663/10000] Avg train loss: 0.008190\n",
      "Epoch [5664/10000] Avg train loss: 0.008189\n",
      "Epoch [5665/10000] Avg train loss: 0.008187\n",
      "Epoch [5666/10000] Avg train loss: 0.008186\n",
      "Epoch [5667/10000] Avg train loss: 0.008184\n",
      "Epoch [5668/10000] Avg train loss: 0.008183\n",
      "Epoch [5669/10000] Avg train loss: 0.008182\n",
      "Epoch [5670/10000] Avg train loss: 0.008180\n",
      "Epoch [5671/10000] Avg train loss: 0.008179\n",
      "Epoch [5672/10000] Avg train loss: 0.008177\n",
      "Epoch [5673/10000] Avg train loss: 0.008176\n",
      "Epoch [5674/10000] Avg train loss: 0.008174\n",
      "Epoch [5675/10000] Avg train loss: 0.008173\n",
      "Epoch [5676/10000] Avg train loss: 0.008171\n",
      "Epoch [5677/10000] Avg train loss: 0.008170\n",
      "Epoch [5678/10000] Avg train loss: 0.008169\n",
      "Epoch [5679/10000] Avg train loss: 0.008167\n",
      "Epoch [5680/10000] Avg train loss: 0.008166\n",
      "Epoch [5681/10000] Avg train loss: 0.008164\n",
      "Epoch [5682/10000] Avg train loss: 0.008163\n",
      "Epoch [5683/10000] Avg train loss: 0.008161\n",
      "Epoch [5684/10000] Avg train loss: 0.008160\n",
      "Epoch [5685/10000] Avg train loss: 0.008159\n",
      "Epoch [5686/10000] Avg train loss: 0.008157\n",
      "Epoch [5687/10000] Avg train loss: 0.008156\n",
      "Epoch [5688/10000] Avg train loss: 0.008154\n",
      "Epoch [5689/10000] Avg train loss: 0.008153\n",
      "Epoch [5690/10000] Avg train loss: 0.008151\n",
      "Epoch [5691/10000] Avg train loss: 0.008150\n",
      "Epoch [5692/10000] Avg train loss: 0.008148\n",
      "Epoch [5693/10000] Avg train loss: 0.008147\n",
      "Epoch [5694/10000] Avg train loss: 0.008146\n",
      "Epoch [5695/10000] Avg train loss: 0.008144\n",
      "Epoch [5696/10000] Avg train loss: 0.008143\n",
      "Epoch [5697/10000] Avg train loss: 0.008141\n",
      "Epoch [5698/10000] Avg train loss: 0.008140\n",
      "Epoch [5699/10000] Avg train loss: 0.008138\n",
      "Epoch [5700/10000] Avg train loss: 0.008137\n",
      "Epoch [5701/10000] Avg train loss: 0.008136\n",
      "Epoch [5702/10000] Avg train loss: 0.008134\n",
      "Epoch [5703/10000] Avg train loss: 0.008133\n",
      "Epoch [5704/10000] Avg train loss: 0.008131\n",
      "Epoch [5705/10000] Avg train loss: 0.008130\n",
      "Epoch [5706/10000] Avg train loss: 0.008129\n",
      "Epoch [5707/10000] Avg train loss: 0.008127\n",
      "Epoch [5708/10000] Avg train loss: 0.008126\n",
      "Epoch [5709/10000] Avg train loss: 0.008124\n",
      "Epoch [5710/10000] Avg train loss: 0.008123\n",
      "Epoch [5711/10000] Avg train loss: 0.008121\n",
      "Epoch [5712/10000] Avg train loss: 0.008120\n",
      "Epoch [5713/10000] Avg train loss: 0.008119\n",
      "Epoch [5714/10000] Avg train loss: 0.008117\n",
      "Epoch [5715/10000] Avg train loss: 0.008116\n",
      "Epoch [5716/10000] Avg train loss: 0.008114\n",
      "Epoch [5717/10000] Avg train loss: 0.008113\n",
      "Epoch [5718/10000] Avg train loss: 0.008111\n",
      "Epoch [5719/10000] Avg train loss: 0.008110\n",
      "Epoch [5720/10000] Avg train loss: 0.008109\n",
      "Epoch [5721/10000] Avg train loss: 0.008107\n",
      "Epoch [5722/10000] Avg train loss: 0.008106\n",
      "Epoch [5723/10000] Avg train loss: 0.008104\n",
      "Epoch [5724/10000] Avg train loss: 0.008103\n",
      "Epoch [5725/10000] Avg train loss: 0.008102\n",
      "Epoch [5726/10000] Avg train loss: 0.008100\n",
      "Epoch [5727/10000] Avg train loss: 0.008099\n",
      "Epoch [5728/10000] Avg train loss: 0.008097\n",
      "Epoch [5729/10000] Avg train loss: 0.008096\n",
      "Epoch [5730/10000] Avg train loss: 0.008094\n",
      "Epoch [5731/10000] Avg train loss: 0.008093\n",
      "Epoch [5732/10000] Avg train loss: 0.008092\n",
      "Epoch [5733/10000] Avg train loss: 0.008090\n",
      "Epoch [5734/10000] Avg train loss: 0.008089\n",
      "Epoch [5735/10000] Avg train loss: 0.008087\n",
      "Epoch [5736/10000] Avg train loss: 0.008086\n",
      "Epoch [5737/10000] Avg train loss: 0.008085\n",
      "Epoch [5738/10000] Avg train loss: 0.008083\n",
      "Epoch [5739/10000] Avg train loss: 0.008082\n",
      "Epoch [5740/10000] Avg train loss: 0.008080\n",
      "Epoch [5741/10000] Avg train loss: 0.008079\n",
      "Epoch [5742/10000] Avg train loss: 0.008078\n",
      "Epoch [5743/10000] Avg train loss: 0.008076\n",
      "Epoch [5744/10000] Avg train loss: 0.008075\n",
      "Epoch [5745/10000] Avg train loss: 0.008073\n",
      "Epoch [5746/10000] Avg train loss: 0.008072\n",
      "Epoch [5747/10000] Avg train loss: 0.008071\n",
      "Epoch [5748/10000] Avg train loss: 0.008069\n",
      "Epoch [5749/10000] Avg train loss: 0.008068\n",
      "Epoch [5750/10000] Avg train loss: 0.008066\n",
      "Epoch [5751/10000] Avg train loss: 0.008065\n",
      "Epoch [5752/10000] Avg train loss: 0.008064\n",
      "Epoch [5753/10000] Avg train loss: 0.008062\n",
      "Epoch [5754/10000] Avg train loss: 0.008061\n",
      "Epoch [5755/10000] Avg train loss: 0.008059\n",
      "Epoch [5756/10000] Avg train loss: 0.008058\n",
      "Epoch [5757/10000] Avg train loss: 0.008056\n",
      "Epoch [5758/10000] Avg train loss: 0.008055\n",
      "Epoch [5759/10000] Avg train loss: 0.008054\n",
      "Epoch [5760/10000] Avg train loss: 0.008052\n",
      "Epoch [5761/10000] Avg train loss: 0.008051\n",
      "Epoch [5762/10000] Avg train loss: 0.008050\n",
      "Epoch [5763/10000] Avg train loss: 0.008048\n",
      "Epoch [5764/10000] Avg train loss: 0.008047\n",
      "Epoch [5765/10000] Avg train loss: 0.008045\n",
      "Epoch [5766/10000] Avg train loss: 0.008044\n",
      "Epoch [5767/10000] Avg train loss: 0.008043\n",
      "Epoch [5768/10000] Avg train loss: 0.008041\n",
      "Epoch [5769/10000] Avg train loss: 0.008040\n",
      "Epoch [5770/10000] Avg train loss: 0.008038\n",
      "Epoch [5771/10000] Avg train loss: 0.008037\n",
      "Epoch [5772/10000] Avg train loss: 0.008036\n",
      "Epoch [5773/10000] Avg train loss: 0.008034\n",
      "Epoch [5774/10000] Avg train loss: 0.008033\n",
      "Epoch [5775/10000] Avg train loss: 0.008031\n",
      "Epoch [5776/10000] Avg train loss: 0.008030\n",
      "Epoch [5777/10000] Avg train loss: 0.008029\n",
      "Epoch [5778/10000] Avg train loss: 0.008027\n",
      "Epoch [5779/10000] Avg train loss: 0.008026\n",
      "Epoch [5780/10000] Avg train loss: 0.008024\n",
      "Epoch [5781/10000] Avg train loss: 0.008023\n",
      "Epoch [5782/10000] Avg train loss: 0.008022\n",
      "Epoch [5783/10000] Avg train loss: 0.008020\n",
      "Epoch [5784/10000] Avg train loss: 0.008019\n",
      "Epoch [5785/10000] Avg train loss: 0.008018\n",
      "Epoch [5786/10000] Avg train loss: 0.008016\n",
      "Epoch [5787/10000] Avg train loss: 0.008015\n",
      "Epoch [5788/10000] Avg train loss: 0.008013\n",
      "Epoch [5789/10000] Avg train loss: 0.008012\n",
      "Epoch [5790/10000] Avg train loss: 0.008011\n",
      "Epoch [5791/10000] Avg train loss: 0.008009\n",
      "Epoch [5792/10000] Avg train loss: 0.008008\n",
      "Epoch [5793/10000] Avg train loss: 0.008006\n",
      "Epoch [5794/10000] Avg train loss: 0.008005\n",
      "Epoch [5795/10000] Avg train loss: 0.008004\n",
      "Epoch [5796/10000] Avg train loss: 0.008002\n",
      "Epoch [5797/10000] Avg train loss: 0.008001\n",
      "Epoch [5798/10000] Avg train loss: 0.008000\n",
      "Epoch [5799/10000] Avg train loss: 0.007998\n",
      "Epoch [5800/10000] Avg train loss: 0.007997\n",
      "Epoch [5801/10000] Avg train loss: 0.007995\n",
      "Epoch [5802/10000] Avg train loss: 0.007994\n",
      "Epoch [5803/10000] Avg train loss: 0.007993\n",
      "Epoch [5804/10000] Avg train loss: 0.007991\n",
      "Epoch [5805/10000] Avg train loss: 0.007990\n",
      "Epoch [5806/10000] Avg train loss: 0.007989\n",
      "Epoch [5807/10000] Avg train loss: 0.007987\n",
      "Epoch [5808/10000] Avg train loss: 0.007986\n",
      "Epoch [5809/10000] Avg train loss: 0.007984\n",
      "Epoch [5810/10000] Avg train loss: 0.007983\n",
      "Epoch [5811/10000] Avg train loss: 0.007982\n",
      "Epoch [5812/10000] Avg train loss: 0.007980\n",
      "Epoch [5813/10000] Avg train loss: 0.007979\n",
      "Epoch [5814/10000] Avg train loss: 0.007978\n",
      "Epoch [5815/10000] Avg train loss: 0.007976\n",
      "Epoch [5816/10000] Avg train loss: 0.007975\n",
      "Epoch [5817/10000] Avg train loss: 0.007973\n",
      "Epoch [5818/10000] Avg train loss: 0.007972\n",
      "Epoch [5819/10000] Avg train loss: 0.007971\n",
      "Epoch [5820/10000] Avg train loss: 0.007969\n",
      "Epoch [5821/10000] Avg train loss: 0.007968\n",
      "Epoch [5822/10000] Avg train loss: 0.007967\n",
      "Epoch [5823/10000] Avg train loss: 0.007965\n",
      "Epoch [5824/10000] Avg train loss: 0.007964\n",
      "Epoch [5825/10000] Avg train loss: 0.007962\n",
      "Epoch [5826/10000] Avg train loss: 0.007961\n",
      "Epoch [5827/10000] Avg train loss: 0.007960\n",
      "Epoch [5828/10000] Avg train loss: 0.007958\n",
      "Epoch [5829/10000] Avg train loss: 0.007957\n",
      "Epoch [5830/10000] Avg train loss: 0.007956\n",
      "Epoch [5831/10000] Avg train loss: 0.007954\n",
      "Epoch [5832/10000] Avg train loss: 0.007953\n",
      "Epoch [5833/10000] Avg train loss: 0.007952\n",
      "Epoch [5834/10000] Avg train loss: 0.007950\n",
      "Epoch [5835/10000] Avg train loss: 0.007949\n",
      "Epoch [5836/10000] Avg train loss: 0.007947\n",
      "Epoch [5837/10000] Avg train loss: 0.007946\n",
      "Epoch [5838/10000] Avg train loss: 0.007945\n",
      "Epoch [5839/10000] Avg train loss: 0.007943\n",
      "Epoch [5840/10000] Avg train loss: 0.007942\n",
      "Epoch [5841/10000] Avg train loss: 0.007941\n",
      "Epoch [5842/10000] Avg train loss: 0.007939\n",
      "Epoch [5843/10000] Avg train loss: 0.007938\n",
      "Epoch [5844/10000] Avg train loss: 0.007937\n",
      "Epoch [5845/10000] Avg train loss: 0.007935\n",
      "Epoch [5846/10000] Avg train loss: 0.007934\n",
      "Epoch [5847/10000] Avg train loss: 0.007932\n",
      "Epoch [5848/10000] Avg train loss: 0.007931\n",
      "Epoch [5849/10000] Avg train loss: 0.007930\n",
      "Epoch [5850/10000] Avg train loss: 0.007928\n",
      "Epoch [5851/10000] Avg train loss: 0.007927\n",
      "Epoch [5852/10000] Avg train loss: 0.007926\n",
      "Epoch [5853/10000] Avg train loss: 0.007924\n",
      "Epoch [5854/10000] Avg train loss: 0.007923\n",
      "Epoch [5855/10000] Avg train loss: 0.007922\n",
      "Epoch [5856/10000] Avg train loss: 0.007920\n",
      "Epoch [5857/10000] Avg train loss: 0.007919\n",
      "Epoch [5858/10000] Avg train loss: 0.007918\n",
      "Epoch [5859/10000] Avg train loss: 0.007916\n",
      "Epoch [5860/10000] Avg train loss: 0.007915\n",
      "Epoch [5861/10000] Avg train loss: 0.007914\n",
      "Epoch [5862/10000] Avg train loss: 0.007912\n",
      "Epoch [5863/10000] Avg train loss: 0.007911\n",
      "Epoch [5864/10000] Avg train loss: 0.007909\n",
      "Epoch [5865/10000] Avg train loss: 0.007908\n",
      "Epoch [5866/10000] Avg train loss: 0.007907\n",
      "Epoch [5867/10000] Avg train loss: 0.007905\n",
      "Epoch [5868/10000] Avg train loss: 0.007904\n",
      "Epoch [5869/10000] Avg train loss: 0.007903\n",
      "Epoch [5870/10000] Avg train loss: 0.007901\n",
      "Epoch [5871/10000] Avg train loss: 0.007900\n",
      "Epoch [5872/10000] Avg train loss: 0.007899\n",
      "Epoch [5873/10000] Avg train loss: 0.007897\n",
      "Epoch [5874/10000] Avg train loss: 0.007896\n",
      "Epoch [5875/10000] Avg train loss: 0.007895\n",
      "Epoch [5876/10000] Avg train loss: 0.007893\n",
      "Epoch [5877/10000] Avg train loss: 0.007892\n",
      "Epoch [5878/10000] Avg train loss: 0.007891\n",
      "Epoch [5879/10000] Avg train loss: 0.007889\n",
      "Epoch [5880/10000] Avg train loss: 0.007888\n",
      "Epoch [5881/10000] Avg train loss: 0.007887\n",
      "Epoch [5882/10000] Avg train loss: 0.007885\n",
      "Epoch [5883/10000] Avg train loss: 0.007884\n",
      "Epoch [5884/10000] Avg train loss: 0.007883\n",
      "Epoch [5885/10000] Avg train loss: 0.007881\n",
      "Epoch [5886/10000] Avg train loss: 0.007880\n",
      "Epoch [5887/10000] Avg train loss: 0.007879\n",
      "Epoch [5888/10000] Avg train loss: 0.007877\n",
      "Epoch [5889/10000] Avg train loss: 0.007876\n",
      "Epoch [5890/10000] Avg train loss: 0.007875\n",
      "Epoch [5891/10000] Avg train loss: 0.007873\n",
      "Epoch [5892/10000] Avg train loss: 0.007872\n",
      "Epoch [5893/10000] Avg train loss: 0.007871\n",
      "Epoch [5894/10000] Avg train loss: 0.007869\n",
      "Epoch [5895/10000] Avg train loss: 0.007868\n",
      "Epoch [5896/10000] Avg train loss: 0.007867\n",
      "Epoch [5897/10000] Avg train loss: 0.007865\n",
      "Epoch [5898/10000] Avg train loss: 0.007864\n",
      "Epoch [5899/10000] Avg train loss: 0.007863\n",
      "Epoch [5900/10000] Avg train loss: 0.007861\n",
      "Epoch [5901/10000] Avg train loss: 0.007860\n",
      "Epoch [5902/10000] Avg train loss: 0.007859\n",
      "Epoch [5903/10000] Avg train loss: 0.007857\n",
      "Epoch [5904/10000] Avg train loss: 0.007856\n",
      "Epoch [5905/10000] Avg train loss: 0.007855\n",
      "Epoch [5906/10000] Avg train loss: 0.007853\n",
      "Epoch [5907/10000] Avg train loss: 0.007852\n",
      "Epoch [5908/10000] Avg train loss: 0.007851\n",
      "Epoch [5909/10000] Avg train loss: 0.007849\n",
      "Epoch [5910/10000] Avg train loss: 0.007848\n",
      "Epoch [5911/10000] Avg train loss: 0.007847\n",
      "Epoch [5912/10000] Avg train loss: 0.007845\n",
      "Epoch [5913/10000] Avg train loss: 0.007844\n",
      "Epoch [5914/10000] Avg train loss: 0.007843\n",
      "Epoch [5915/10000] Avg train loss: 0.007841\n",
      "Epoch [5916/10000] Avg train loss: 0.007840\n",
      "Epoch [5917/10000] Avg train loss: 0.007839\n",
      "Epoch [5918/10000] Avg train loss: 0.007837\n",
      "Epoch [5919/10000] Avg train loss: 0.007836\n",
      "Epoch [5920/10000] Avg train loss: 0.007835\n",
      "Epoch [5921/10000] Avg train loss: 0.007833\n",
      "Epoch [5922/10000] Avg train loss: 0.007832\n",
      "Epoch [5923/10000] Avg train loss: 0.007831\n",
      "Epoch [5924/10000] Avg train loss: 0.007829\n",
      "Epoch [5925/10000] Avg train loss: 0.007828\n",
      "Epoch [5926/10000] Avg train loss: 0.007827\n",
      "Epoch [5927/10000] Avg train loss: 0.007825\n",
      "Epoch [5928/10000] Avg train loss: 0.007824\n",
      "Epoch [5929/10000] Avg train loss: 0.007823\n",
      "Epoch [5930/10000] Avg train loss: 0.007821\n",
      "Epoch [5931/10000] Avg train loss: 0.007820\n",
      "Epoch [5932/10000] Avg train loss: 0.007819\n",
      "Epoch [5933/10000] Avg train loss: 0.007818\n",
      "Epoch [5934/10000] Avg train loss: 0.007816\n",
      "Epoch [5935/10000] Avg train loss: 0.007815\n",
      "Epoch [5936/10000] Avg train loss: 0.007814\n",
      "Epoch [5937/10000] Avg train loss: 0.007812\n",
      "Epoch [5938/10000] Avg train loss: 0.007811\n",
      "Epoch [5939/10000] Avg train loss: 0.007810\n",
      "Epoch [5940/10000] Avg train loss: 0.007808\n",
      "Epoch [5941/10000] Avg train loss: 0.007807\n",
      "Epoch [5942/10000] Avg train loss: 0.007806\n",
      "Epoch [5943/10000] Avg train loss: 0.007804\n",
      "Epoch [5944/10000] Avg train loss: 0.007803\n",
      "Epoch [5945/10000] Avg train loss: 0.007802\n",
      "Epoch [5946/10000] Avg train loss: 0.007800\n",
      "Epoch [5947/10000] Avg train loss: 0.007799\n",
      "Epoch [5948/10000] Avg train loss: 0.007798\n",
      "Epoch [5949/10000] Avg train loss: 0.007796\n",
      "Epoch [5950/10000] Avg train loss: 0.007795\n",
      "Epoch [5951/10000] Avg train loss: 0.007794\n",
      "Epoch [5952/10000] Avg train loss: 0.007793\n",
      "Epoch [5953/10000] Avg train loss: 0.007791\n",
      "Epoch [5954/10000] Avg train loss: 0.007790\n",
      "Epoch [5955/10000] Avg train loss: 0.007789\n",
      "Epoch [5956/10000] Avg train loss: 0.007787\n",
      "Epoch [5957/10000] Avg train loss: 0.007786\n",
      "Epoch [5958/10000] Avg train loss: 0.007785\n",
      "Epoch [5959/10000] Avg train loss: 0.007783\n",
      "Epoch [5960/10000] Avg train loss: 0.007782\n",
      "Epoch [5961/10000] Avg train loss: 0.007781\n",
      "Epoch [5962/10000] Avg train loss: 0.007779\n",
      "Epoch [5963/10000] Avg train loss: 0.007778\n",
      "Epoch [5964/10000] Avg train loss: 0.007777\n",
      "Epoch [5965/10000] Avg train loss: 0.007776\n",
      "Epoch [5966/10000] Avg train loss: 0.007774\n",
      "Epoch [5967/10000] Avg train loss: 0.007773\n",
      "Epoch [5968/10000] Avg train loss: 0.007772\n",
      "Epoch [5969/10000] Avg train loss: 0.007770\n",
      "Epoch [5970/10000] Avg train loss: 0.007769\n",
      "Epoch [5971/10000] Avg train loss: 0.007768\n",
      "Epoch [5972/10000] Avg train loss: 0.007766\n",
      "Epoch [5973/10000] Avg train loss: 0.007765\n",
      "Epoch [5974/10000] Avg train loss: 0.007764\n",
      "Epoch [5975/10000] Avg train loss: 0.007763\n",
      "Epoch [5976/10000] Avg train loss: 0.007761\n",
      "Epoch [5977/10000] Avg train loss: 0.007760\n",
      "Epoch [5978/10000] Avg train loss: 0.007759\n",
      "Epoch [5979/10000] Avg train loss: 0.007757\n",
      "Epoch [5980/10000] Avg train loss: 0.007756\n",
      "Epoch [5981/10000] Avg train loss: 0.007755\n",
      "Epoch [5982/10000] Avg train loss: 0.007753\n",
      "Epoch [5983/10000] Avg train loss: 0.007752\n",
      "Epoch [5984/10000] Avg train loss: 0.007751\n",
      "Epoch [5985/10000] Avg train loss: 0.007750\n",
      "Epoch [5986/10000] Avg train loss: 0.007748\n",
      "Epoch [5987/10000] Avg train loss: 0.007747\n",
      "Epoch [5988/10000] Avg train loss: 0.007746\n",
      "Epoch [5989/10000] Avg train loss: 0.007744\n",
      "Epoch [5990/10000] Avg train loss: 0.007743\n",
      "Epoch [5991/10000] Avg train loss: 0.007742\n",
      "Epoch [5992/10000] Avg train loss: 0.007741\n",
      "Epoch [5993/10000] Avg train loss: 0.007739\n",
      "Epoch [5994/10000] Avg train loss: 0.007738\n",
      "Epoch [5995/10000] Avg train loss: 0.007737\n",
      "Epoch [5996/10000] Avg train loss: 0.007735\n",
      "Epoch [5997/10000] Avg train loss: 0.007734\n",
      "Epoch [5998/10000] Avg train loss: 0.007733\n",
      "Epoch [5999/10000] Avg train loss: 0.007731\n",
      "Epoch [6000/10000] Avg train loss: 0.007730\n",
      "Epoch [6001/10000] Avg train loss: 0.007729\n",
      "Epoch [6002/10000] Avg train loss: 0.007728\n",
      "Epoch [6003/10000] Avg train loss: 0.007726\n",
      "Epoch [6004/10000] Avg train loss: 0.007725\n",
      "Epoch [6005/10000] Avg train loss: 0.007724\n",
      "Epoch [6006/10000] Avg train loss: 0.007722\n",
      "Epoch [6007/10000] Avg train loss: 0.007721\n",
      "Epoch [6008/10000] Avg train loss: 0.007720\n",
      "Epoch [6009/10000] Avg train loss: 0.007719\n",
      "Epoch [6010/10000] Avg train loss: 0.007717\n",
      "Epoch [6011/10000] Avg train loss: 0.007716\n",
      "Epoch [6012/10000] Avg train loss: 0.007715\n",
      "Epoch [6013/10000] Avg train loss: 0.007713\n",
      "Epoch [6014/10000] Avg train loss: 0.007712\n",
      "Epoch [6015/10000] Avg train loss: 0.007711\n",
      "Epoch [6016/10000] Avg train loss: 0.007710\n",
      "Epoch [6017/10000] Avg train loss: 0.007708\n",
      "Epoch [6018/10000] Avg train loss: 0.007707\n",
      "Epoch [6019/10000] Avg train loss: 0.007706\n",
      "Epoch [6020/10000] Avg train loss: 0.007705\n",
      "Epoch [6021/10000] Avg train loss: 0.007703\n",
      "Epoch [6022/10000] Avg train loss: 0.007702\n",
      "Epoch [6023/10000] Avg train loss: 0.007701\n",
      "Epoch [6024/10000] Avg train loss: 0.007699\n",
      "Epoch [6025/10000] Avg train loss: 0.007698\n",
      "Epoch [6026/10000] Avg train loss: 0.007697\n",
      "Epoch [6027/10000] Avg train loss: 0.007696\n",
      "Epoch [6028/10000] Avg train loss: 0.007694\n",
      "Epoch [6029/10000] Avg train loss: 0.007693\n",
      "Epoch [6030/10000] Avg train loss: 0.007692\n",
      "Epoch [6031/10000] Avg train loss: 0.007690\n",
      "Epoch [6032/10000] Avg train loss: 0.007689\n",
      "Epoch [6033/10000] Avg train loss: 0.007688\n",
      "Epoch [6034/10000] Avg train loss: 0.007687\n",
      "Epoch [6035/10000] Avg train loss: 0.007685\n",
      "Epoch [6036/10000] Avg train loss: 0.007684\n",
      "Epoch [6037/10000] Avg train loss: 0.007683\n",
      "Epoch [6038/10000] Avg train loss: 0.007682\n",
      "Epoch [6039/10000] Avg train loss: 0.007680\n",
      "Epoch [6040/10000] Avg train loss: 0.007679\n",
      "Epoch [6041/10000] Avg train loss: 0.007678\n",
      "Epoch [6042/10000] Avg train loss: 0.007676\n",
      "Epoch [6043/10000] Avg train loss: 0.007675\n",
      "Epoch [6044/10000] Avg train loss: 0.007674\n",
      "Epoch [6045/10000] Avg train loss: 0.007673\n",
      "Epoch [6046/10000] Avg train loss: 0.007671\n",
      "Epoch [6047/10000] Avg train loss: 0.007670\n",
      "Epoch [6048/10000] Avg train loss: 0.007669\n",
      "Epoch [6049/10000] Avg train loss: 0.007668\n",
      "Epoch [6050/10000] Avg train loss: 0.007666\n",
      "Epoch [6051/10000] Avg train loss: 0.007665\n",
      "Epoch [6052/10000] Avg train loss: 0.007664\n",
      "Epoch [6053/10000] Avg train loss: 0.007663\n",
      "Epoch [6054/10000] Avg train loss: 0.007661\n",
      "Epoch [6055/10000] Avg train loss: 0.007660\n",
      "Epoch [6056/10000] Avg train loss: 0.007659\n",
      "Epoch [6057/10000] Avg train loss: 0.007657\n",
      "Epoch [6058/10000] Avg train loss: 0.007656\n",
      "Epoch [6059/10000] Avg train loss: 0.007655\n",
      "Epoch [6060/10000] Avg train loss: 0.007654\n",
      "Epoch [6061/10000] Avg train loss: 0.007652\n",
      "Epoch [6062/10000] Avg train loss: 0.007651\n",
      "Epoch [6063/10000] Avg train loss: 0.007650\n",
      "Epoch [6064/10000] Avg train loss: 0.007649\n",
      "Epoch [6065/10000] Avg train loss: 0.007647\n",
      "Epoch [6066/10000] Avg train loss: 0.007646\n",
      "Epoch [6067/10000] Avg train loss: 0.007645\n",
      "Epoch [6068/10000] Avg train loss: 0.007644\n",
      "Epoch [6069/10000] Avg train loss: 0.007642\n",
      "Epoch [6070/10000] Avg train loss: 0.007641\n",
      "Epoch [6071/10000] Avg train loss: 0.007640\n",
      "Epoch [6072/10000] Avg train loss: 0.007639\n",
      "Epoch [6073/10000] Avg train loss: 0.007637\n",
      "Epoch [6074/10000] Avg train loss: 0.007636\n",
      "Epoch [6075/10000] Avg train loss: 0.007635\n",
      "Epoch [6076/10000] Avg train loss: 0.007634\n",
      "Epoch [6077/10000] Avg train loss: 0.007632\n",
      "Epoch [6078/10000] Avg train loss: 0.007631\n",
      "Epoch [6079/10000] Avg train loss: 0.007630\n",
      "Epoch [6080/10000] Avg train loss: 0.007628\n",
      "Epoch [6081/10000] Avg train loss: 0.007627\n",
      "Epoch [6082/10000] Avg train loss: 0.007626\n",
      "Epoch [6083/10000] Avg train loss: 0.007625\n",
      "Epoch [6084/10000] Avg train loss: 0.007623\n",
      "Epoch [6085/10000] Avg train loss: 0.007622\n",
      "Epoch [6086/10000] Avg train loss: 0.007621\n",
      "Epoch [6087/10000] Avg train loss: 0.007620\n",
      "Epoch [6088/10000] Avg train loss: 0.007618\n",
      "Epoch [6089/10000] Avg train loss: 0.007617\n",
      "Epoch [6090/10000] Avg train loss: 0.007616\n",
      "Epoch [6091/10000] Avg train loss: 0.007615\n",
      "Epoch [6092/10000] Avg train loss: 0.007613\n",
      "Epoch [6093/10000] Avg train loss: 0.007612\n",
      "Epoch [6094/10000] Avg train loss: 0.007611\n",
      "Epoch [6095/10000] Avg train loss: 0.007610\n",
      "Epoch [6096/10000] Avg train loss: 0.007608\n",
      "Epoch [6097/10000] Avg train loss: 0.007607\n",
      "Epoch [6098/10000] Avg train loss: 0.007606\n",
      "Epoch [6099/10000] Avg train loss: 0.007605\n",
      "Epoch [6100/10000] Avg train loss: 0.007603\n",
      "Epoch [6101/10000] Avg train loss: 0.007602\n",
      "Epoch [6102/10000] Avg train loss: 0.007601\n",
      "Epoch [6103/10000] Avg train loss: 0.007600\n",
      "Epoch [6104/10000] Avg train loss: 0.007599\n",
      "Epoch [6105/10000] Avg train loss: 0.007597\n",
      "Epoch [6106/10000] Avg train loss: 0.007596\n",
      "Epoch [6107/10000] Avg train loss: 0.007595\n",
      "Epoch [6108/10000] Avg train loss: 0.007594\n",
      "Epoch [6109/10000] Avg train loss: 0.007592\n",
      "Epoch [6110/10000] Avg train loss: 0.007591\n",
      "Epoch [6111/10000] Avg train loss: 0.007590\n",
      "Epoch [6112/10000] Avg train loss: 0.007589\n",
      "Epoch [6113/10000] Avg train loss: 0.007587\n",
      "Epoch [6114/10000] Avg train loss: 0.007586\n",
      "Epoch [6115/10000] Avg train loss: 0.007585\n",
      "Epoch [6116/10000] Avg train loss: 0.007584\n",
      "Epoch [6117/10000] Avg train loss: 0.007582\n",
      "Epoch [6118/10000] Avg train loss: 0.007581\n",
      "Epoch [6119/10000] Avg train loss: 0.007580\n",
      "Epoch [6120/10000] Avg train loss: 0.007579\n",
      "Epoch [6121/10000] Avg train loss: 0.007577\n",
      "Epoch [6122/10000] Avg train loss: 0.007576\n",
      "Epoch [6123/10000] Avg train loss: 0.007575\n",
      "Epoch [6124/10000] Avg train loss: 0.007574\n",
      "Epoch [6125/10000] Avg train loss: 0.007572\n",
      "Epoch [6126/10000] Avg train loss: 0.007571\n",
      "Epoch [6127/10000] Avg train loss: 0.007570\n",
      "Epoch [6128/10000] Avg train loss: 0.007569\n",
      "Epoch [6129/10000] Avg train loss: 0.007568\n",
      "Epoch [6130/10000] Avg train loss: 0.007566\n",
      "Epoch [6131/10000] Avg train loss: 0.007565\n",
      "Epoch [6132/10000] Avg train loss: 0.007564\n",
      "Epoch [6133/10000] Avg train loss: 0.007563\n",
      "Epoch [6134/10000] Avg train loss: 0.007561\n",
      "Epoch [6135/10000] Avg train loss: 0.007560\n",
      "Epoch [6136/10000] Avg train loss: 0.007559\n",
      "Epoch [6137/10000] Avg train loss: 0.007558\n",
      "Epoch [6138/10000] Avg train loss: 0.007556\n",
      "Epoch [6139/10000] Avg train loss: 0.007555\n",
      "Epoch [6140/10000] Avg train loss: 0.007554\n",
      "Epoch [6141/10000] Avg train loss: 0.007553\n",
      "Epoch [6142/10000] Avg train loss: 0.007551\n",
      "Epoch [6143/10000] Avg train loss: 0.007550\n",
      "Epoch [6144/10000] Avg train loss: 0.007549\n",
      "Epoch [6145/10000] Avg train loss: 0.007548\n",
      "Epoch [6146/10000] Avg train loss: 0.007547\n",
      "Epoch [6147/10000] Avg train loss: 0.007545\n",
      "Epoch [6148/10000] Avg train loss: 0.007544\n",
      "Epoch [6149/10000] Avg train loss: 0.007543\n",
      "Epoch [6150/10000] Avg train loss: 0.007542\n",
      "Epoch [6151/10000] Avg train loss: 0.007540\n",
      "Epoch [6152/10000] Avg train loss: 0.007539\n",
      "Epoch [6153/10000] Avg train loss: 0.007538\n",
      "Epoch [6154/10000] Avg train loss: 0.007537\n",
      "Epoch [6155/10000] Avg train loss: 0.007536\n",
      "Epoch [6156/10000] Avg train loss: 0.007534\n",
      "Epoch [6157/10000] Avg train loss: 0.007533\n",
      "Epoch [6158/10000] Avg train loss: 0.007532\n",
      "Epoch [6159/10000] Avg train loss: 0.007531\n",
      "Epoch [6160/10000] Avg train loss: 0.007529\n",
      "Epoch [6161/10000] Avg train loss: 0.007528\n",
      "Epoch [6162/10000] Avg train loss: 0.007527\n",
      "Epoch [6163/10000] Avg train loss: 0.007526\n",
      "Epoch [6164/10000] Avg train loss: 0.007525\n",
      "Epoch [6165/10000] Avg train loss: 0.007523\n",
      "Epoch [6166/10000] Avg train loss: 0.007522\n",
      "Epoch [6167/10000] Avg train loss: 0.007521\n",
      "Epoch [6168/10000] Avg train loss: 0.007520\n",
      "Epoch [6169/10000] Avg train loss: 0.007518\n",
      "Epoch [6170/10000] Avg train loss: 0.007517\n",
      "Epoch [6171/10000] Avg train loss: 0.007516\n",
      "Epoch [6172/10000] Avg train loss: 0.007515\n",
      "Epoch [6173/10000] Avg train loss: 0.007514\n",
      "Epoch [6174/10000] Avg train loss: 0.007512\n",
      "Epoch [6175/10000] Avg train loss: 0.007511\n",
      "Epoch [6176/10000] Avg train loss: 0.007510\n",
      "Epoch [6177/10000] Avg train loss: 0.007509\n",
      "Epoch [6178/10000] Avg train loss: 0.007507\n",
      "Epoch [6179/10000] Avg train loss: 0.007506\n",
      "Epoch [6180/10000] Avg train loss: 0.007505\n",
      "Epoch [6181/10000] Avg train loss: 0.007504\n",
      "Epoch [6182/10000] Avg train loss: 0.007503\n",
      "Epoch [6183/10000] Avg train loss: 0.007501\n",
      "Epoch [6184/10000] Avg train loss: 0.007500\n",
      "Epoch [6185/10000] Avg train loss: 0.007499\n",
      "Epoch [6186/10000] Avg train loss: 0.007498\n",
      "Epoch [6187/10000] Avg train loss: 0.007497\n",
      "Epoch [6188/10000] Avg train loss: 0.007495\n",
      "Epoch [6189/10000] Avg train loss: 0.007494\n",
      "Epoch [6190/10000] Avg train loss: 0.007493\n",
      "Epoch [6191/10000] Avg train loss: 0.007492\n",
      "Epoch [6192/10000] Avg train loss: 0.007491\n",
      "Epoch [6193/10000] Avg train loss: 0.007489\n",
      "Epoch [6194/10000] Avg train loss: 0.007488\n",
      "Epoch [6195/10000] Avg train loss: 0.007487\n",
      "Epoch [6196/10000] Avg train loss: 0.007486\n",
      "Epoch [6197/10000] Avg train loss: 0.007484\n",
      "Epoch [6198/10000] Avg train loss: 0.007483\n",
      "Epoch [6199/10000] Avg train loss: 0.007482\n",
      "Epoch [6200/10000] Avg train loss: 0.007481\n",
      "Epoch [6201/10000] Avg train loss: 0.007480\n",
      "Epoch [6202/10000] Avg train loss: 0.007478\n",
      "Epoch [6203/10000] Avg train loss: 0.007477\n",
      "Epoch [6204/10000] Avg train loss: 0.007476\n",
      "Epoch [6205/10000] Avg train loss: 0.007475\n",
      "Epoch [6206/10000] Avg train loss: 0.007474\n",
      "Epoch [6207/10000] Avg train loss: 0.007472\n",
      "Epoch [6208/10000] Avg train loss: 0.007471\n",
      "Epoch [6209/10000] Avg train loss: 0.007470\n",
      "Epoch [6210/10000] Avg train loss: 0.007469\n",
      "Epoch [6211/10000] Avg train loss: 0.007468\n",
      "Epoch [6212/10000] Avg train loss: 0.007466\n",
      "Epoch [6213/10000] Avg train loss: 0.007465\n",
      "Epoch [6214/10000] Avg train loss: 0.007464\n",
      "Epoch [6215/10000] Avg train loss: 0.007463\n",
      "Epoch [6216/10000] Avg train loss: 0.007462\n",
      "Epoch [6217/10000] Avg train loss: 0.007460\n",
      "Epoch [6218/10000] Avg train loss: 0.007459\n",
      "Epoch [6219/10000] Avg train loss: 0.007458\n",
      "Epoch [6220/10000] Avg train loss: 0.007457\n",
      "Epoch [6221/10000] Avg train loss: 0.007456\n",
      "Epoch [6222/10000] Avg train loss: 0.007454\n",
      "Epoch [6223/10000] Avg train loss: 0.007453\n",
      "Epoch [6224/10000] Avg train loss: 0.007452\n",
      "Epoch [6225/10000] Avg train loss: 0.007451\n",
      "Epoch [6226/10000] Avg train loss: 0.007450\n",
      "Epoch [6227/10000] Avg train loss: 0.007448\n",
      "Epoch [6228/10000] Avg train loss: 0.007447\n",
      "Epoch [6229/10000] Avg train loss: 0.007446\n",
      "Epoch [6230/10000] Avg train loss: 0.007445\n",
      "Epoch [6231/10000] Avg train loss: 0.007444\n",
      "Epoch [6232/10000] Avg train loss: 0.007442\n",
      "Epoch [6233/10000] Avg train loss: 0.007441\n",
      "Epoch [6234/10000] Avg train loss: 0.007440\n",
      "Epoch [6235/10000] Avg train loss: 0.007439\n",
      "Epoch [6236/10000] Avg train loss: 0.007438\n",
      "Epoch [6237/10000] Avg train loss: 0.007436\n",
      "Epoch [6238/10000] Avg train loss: 0.007435\n",
      "Epoch [6239/10000] Avg train loss: 0.007434\n",
      "Epoch [6240/10000] Avg train loss: 0.007433\n",
      "Epoch [6241/10000] Avg train loss: 0.007432\n",
      "Epoch [6242/10000] Avg train loss: 0.007431\n",
      "Epoch [6243/10000] Avg train loss: 0.007429\n",
      "Epoch [6244/10000] Avg train loss: 0.007428\n",
      "Epoch [6245/10000] Avg train loss: 0.007427\n",
      "Epoch [6246/10000] Avg train loss: 0.007426\n",
      "Epoch [6247/10000] Avg train loss: 0.007425\n",
      "Epoch [6248/10000] Avg train loss: 0.007423\n",
      "Epoch [6249/10000] Avg train loss: 0.007422\n",
      "Epoch [6250/10000] Avg train loss: 0.007421\n",
      "Epoch [6251/10000] Avg train loss: 0.007420\n",
      "Epoch [6252/10000] Avg train loss: 0.007419\n",
      "Epoch [6253/10000] Avg train loss: 0.007417\n",
      "Epoch [6254/10000] Avg train loss: 0.007416\n",
      "Epoch [6255/10000] Avg train loss: 0.007415\n",
      "Epoch [6256/10000] Avg train loss: 0.007414\n",
      "Epoch [6257/10000] Avg train loss: 0.007413\n",
      "Epoch [6258/10000] Avg train loss: 0.007412\n",
      "Epoch [6259/10000] Avg train loss: 0.007410\n",
      "Epoch [6260/10000] Avg train loss: 0.007409\n",
      "Epoch [6261/10000] Avg train loss: 0.007408\n",
      "Epoch [6262/10000] Avg train loss: 0.007407\n",
      "Epoch [6263/10000] Avg train loss: 0.007406\n",
      "Epoch [6264/10000] Avg train loss: 0.007404\n",
      "Epoch [6265/10000] Avg train loss: 0.007403\n",
      "Epoch [6266/10000] Avg train loss: 0.007402\n",
      "Epoch [6267/10000] Avg train loss: 0.007401\n",
      "Epoch [6268/10000] Avg train loss: 0.007400\n",
      "Epoch [6269/10000] Avg train loss: 0.007399\n",
      "Epoch [6270/10000] Avg train loss: 0.007397\n",
      "Epoch [6271/10000] Avg train loss: 0.007396\n",
      "Epoch [6272/10000] Avg train loss: 0.007395\n",
      "Epoch [6273/10000] Avg train loss: 0.007394\n",
      "Epoch [6274/10000] Avg train loss: 0.007393\n",
      "Epoch [6275/10000] Avg train loss: 0.007391\n",
      "Epoch [6276/10000] Avg train loss: 0.007390\n",
      "Epoch [6277/10000] Avg train loss: 0.007389\n",
      "Epoch [6278/10000] Avg train loss: 0.007388\n",
      "Epoch [6279/10000] Avg train loss: 0.007387\n",
      "Epoch [6280/10000] Avg train loss: 0.007386\n",
      "Epoch [6281/10000] Avg train loss: 0.007384\n",
      "Epoch [6282/10000] Avg train loss: 0.007383\n",
      "Epoch [6283/10000] Avg train loss: 0.007382\n",
      "Epoch [6284/10000] Avg train loss: 0.007381\n",
      "Epoch [6285/10000] Avg train loss: 0.007380\n",
      "Epoch [6286/10000] Avg train loss: 0.007379\n",
      "Epoch [6287/10000] Avg train loss: 0.007377\n",
      "Epoch [6288/10000] Avg train loss: 0.007376\n",
      "Epoch [6289/10000] Avg train loss: 0.007375\n",
      "Epoch [6290/10000] Avg train loss: 0.007374\n",
      "Epoch [6291/10000] Avg train loss: 0.007373\n",
      "Epoch [6292/10000] Avg train loss: 0.007371\n",
      "Epoch [6293/10000] Avg train loss: 0.007370\n",
      "Epoch [6294/10000] Avg train loss: 0.007369\n",
      "Epoch [6295/10000] Avg train loss: 0.007368\n",
      "Epoch [6296/10000] Avg train loss: 0.007367\n",
      "Epoch [6297/10000] Avg train loss: 0.007366\n",
      "Epoch [6298/10000] Avg train loss: 0.007364\n",
      "Epoch [6299/10000] Avg train loss: 0.007363\n",
      "Epoch [6300/10000] Avg train loss: 0.007362\n",
      "Epoch [6301/10000] Avg train loss: 0.007361\n",
      "Epoch [6302/10000] Avg train loss: 0.007360\n",
      "Epoch [6303/10000] Avg train loss: 0.007359\n",
      "Epoch [6304/10000] Avg train loss: 0.007357\n",
      "Epoch [6305/10000] Avg train loss: 0.007356\n",
      "Epoch [6306/10000] Avg train loss: 0.007355\n",
      "Epoch [6307/10000] Avg train loss: 0.007354\n",
      "Epoch [6308/10000] Avg train loss: 0.007353\n",
      "Epoch [6309/10000] Avg train loss: 0.007352\n",
      "Epoch [6310/10000] Avg train loss: 0.007350\n",
      "Epoch [6311/10000] Avg train loss: 0.007349\n",
      "Epoch [6312/10000] Avg train loss: 0.007348\n",
      "Epoch [6313/10000] Avg train loss: 0.007347\n",
      "Epoch [6314/10000] Avg train loss: 0.007346\n",
      "Epoch [6315/10000] Avg train loss: 0.007345\n",
      "Epoch [6316/10000] Avg train loss: 0.007343\n",
      "Epoch [6317/10000] Avg train loss: 0.007342\n",
      "Epoch [6318/10000] Avg train loss: 0.007341\n",
      "Epoch [6319/10000] Avg train loss: 0.007340\n",
      "Epoch [6320/10000] Avg train loss: 0.007339\n",
      "Epoch [6321/10000] Avg train loss: 0.007338\n",
      "Epoch [6322/10000] Avg train loss: 0.007336\n",
      "Epoch [6323/10000] Avg train loss: 0.007335\n",
      "Epoch [6324/10000] Avg train loss: 0.007334\n",
      "Epoch [6325/10000] Avg train loss: 0.007333\n",
      "Epoch [6326/10000] Avg train loss: 0.007332\n",
      "Epoch [6327/10000] Avg train loss: 0.007331\n",
      "Epoch [6328/10000] Avg train loss: 0.007330\n",
      "Epoch [6329/10000] Avg train loss: 0.007328\n",
      "Epoch [6330/10000] Avg train loss: 0.007327\n",
      "Epoch [6331/10000] Avg train loss: 0.007326\n",
      "Epoch [6332/10000] Avg train loss: 0.007325\n",
      "Epoch [6333/10000] Avg train loss: 0.007324\n",
      "Epoch [6334/10000] Avg train loss: 0.007323\n",
      "Epoch [6335/10000] Avg train loss: 0.007321\n",
      "Epoch [6336/10000] Avg train loss: 0.007320\n",
      "Epoch [6337/10000] Avg train loss: 0.007319\n",
      "Epoch [6338/10000] Avg train loss: 0.007318\n",
      "Epoch [6339/10000] Avg train loss: 0.007317\n",
      "Epoch [6340/10000] Avg train loss: 0.007316\n",
      "Epoch [6341/10000] Avg train loss: 0.007315\n",
      "Epoch [6342/10000] Avg train loss: 0.007313\n",
      "Epoch [6343/10000] Avg train loss: 0.007312\n",
      "Epoch [6344/10000] Avg train loss: 0.007311\n",
      "Epoch [6345/10000] Avg train loss: 0.007310\n",
      "Epoch [6346/10000] Avg train loss: 0.007309\n",
      "Epoch [6347/10000] Avg train loss: 0.007308\n",
      "Epoch [6348/10000] Avg train loss: 0.007306\n",
      "Epoch [6349/10000] Avg train loss: 0.007305\n",
      "Epoch [6350/10000] Avg train loss: 0.007304\n",
      "Epoch [6351/10000] Avg train loss: 0.007303\n",
      "Epoch [6352/10000] Avg train loss: 0.007302\n",
      "Epoch [6353/10000] Avg train loss: 0.007301\n",
      "Epoch [6354/10000] Avg train loss: 0.007300\n",
      "Epoch [6355/10000] Avg train loss: 0.007298\n",
      "Epoch [6356/10000] Avg train loss: 0.007297\n",
      "Epoch [6357/10000] Avg train loss: 0.007296\n",
      "Epoch [6358/10000] Avg train loss: 0.007295\n",
      "Epoch [6359/10000] Avg train loss: 0.007294\n",
      "Epoch [6360/10000] Avg train loss: 0.007293\n",
      "Epoch [6361/10000] Avg train loss: 0.007292\n",
      "Epoch [6362/10000] Avg train loss: 0.007290\n",
      "Epoch [6363/10000] Avg train loss: 0.007289\n",
      "Epoch [6364/10000] Avg train loss: 0.007288\n",
      "Epoch [6365/10000] Avg train loss: 0.007287\n",
      "Epoch [6366/10000] Avg train loss: 0.007286\n",
      "Epoch [6367/10000] Avg train loss: 0.007285\n",
      "Epoch [6368/10000] Avg train loss: 0.007283\n",
      "Epoch [6369/10000] Avg train loss: 0.007282\n",
      "Epoch [6370/10000] Avg train loss: 0.007281\n",
      "Epoch [6371/10000] Avg train loss: 0.007280\n",
      "Epoch [6372/10000] Avg train loss: 0.007279\n",
      "Epoch [6373/10000] Avg train loss: 0.007278\n",
      "Epoch [6374/10000] Avg train loss: 0.007277\n",
      "Epoch [6375/10000] Avg train loss: 0.007275\n",
      "Epoch [6376/10000] Avg train loss: 0.007274\n",
      "Epoch [6377/10000] Avg train loss: 0.007273\n",
      "Epoch [6378/10000] Avg train loss: 0.007272\n",
      "Epoch [6379/10000] Avg train loss: 0.007271\n",
      "Epoch [6380/10000] Avg train loss: 0.007270\n",
      "Epoch [6381/10000] Avg train loss: 0.007269\n",
      "Epoch [6382/10000] Avg train loss: 0.007268\n",
      "Epoch [6383/10000] Avg train loss: 0.007266\n",
      "Epoch [6384/10000] Avg train loss: 0.007265\n",
      "Epoch [6385/10000] Avg train loss: 0.007264\n",
      "Epoch [6386/10000] Avg train loss: 0.007263\n",
      "Epoch [6387/10000] Avg train loss: 0.007262\n",
      "Epoch [6388/10000] Avg train loss: 0.007261\n",
      "Epoch [6389/10000] Avg train loss: 0.007260\n",
      "Epoch [6390/10000] Avg train loss: 0.007258\n",
      "Epoch [6391/10000] Avg train loss: 0.007257\n",
      "Epoch [6392/10000] Avg train loss: 0.007256\n",
      "Epoch [6393/10000] Avg train loss: 0.007255\n",
      "Epoch [6394/10000] Avg train loss: 0.007254\n",
      "Epoch [6395/10000] Avg train loss: 0.007253\n",
      "Epoch [6396/10000] Avg train loss: 0.007252\n",
      "Epoch [6397/10000] Avg train loss: 0.007250\n",
      "Epoch [6398/10000] Avg train loss: 0.007249\n",
      "Epoch [6399/10000] Avg train loss: 0.007248\n",
      "Epoch [6400/10000] Avg train loss: 0.007247\n",
      "Epoch [6401/10000] Avg train loss: 0.007246\n",
      "Epoch [6402/10000] Avg train loss: 0.007245\n",
      "Epoch [6403/10000] Avg train loss: 0.007244\n",
      "Epoch [6404/10000] Avg train loss: 0.007243\n",
      "Epoch [6405/10000] Avg train loss: 0.007241\n",
      "Epoch [6406/10000] Avg train loss: 0.007240\n",
      "Epoch [6407/10000] Avg train loss: 0.007239\n",
      "Epoch [6408/10000] Avg train loss: 0.007238\n",
      "Epoch [6409/10000] Avg train loss: 0.007237\n",
      "Epoch [6410/10000] Avg train loss: 0.007236\n",
      "Epoch [6411/10000] Avg train loss: 0.007235\n",
      "Epoch [6412/10000] Avg train loss: 0.007234\n",
      "Epoch [6413/10000] Avg train loss: 0.007232\n",
      "Epoch [6414/10000] Avg train loss: 0.007231\n",
      "Epoch [6415/10000] Avg train loss: 0.007230\n",
      "Epoch [6416/10000] Avg train loss: 0.007229\n",
      "Epoch [6417/10000] Avg train loss: 0.007228\n",
      "Epoch [6418/10000] Avg train loss: 0.007227\n",
      "Epoch [6419/10000] Avg train loss: 0.007226\n",
      "Epoch [6420/10000] Avg train loss: 0.007224\n",
      "Epoch [6421/10000] Avg train loss: 0.007223\n",
      "Epoch [6422/10000] Avg train loss: 0.007222\n",
      "Epoch [6423/10000] Avg train loss: 0.007221\n",
      "Epoch [6424/10000] Avg train loss: 0.007220\n",
      "Epoch [6425/10000] Avg train loss: 0.007219\n",
      "Epoch [6426/10000] Avg train loss: 0.007218\n",
      "Epoch [6427/10000] Avg train loss: 0.007217\n",
      "Epoch [6428/10000] Avg train loss: 0.007216\n",
      "Epoch [6429/10000] Avg train loss: 0.007214\n",
      "Epoch [6430/10000] Avg train loss: 0.007213\n",
      "Epoch [6431/10000] Avg train loss: 0.007212\n",
      "Epoch [6432/10000] Avg train loss: 0.007211\n",
      "Epoch [6433/10000] Avg train loss: 0.007210\n",
      "Epoch [6434/10000] Avg train loss: 0.007209\n",
      "Epoch [6435/10000] Avg train loss: 0.007208\n",
      "Epoch [6436/10000] Avg train loss: 0.007207\n",
      "Epoch [6437/10000] Avg train loss: 0.007205\n",
      "Epoch [6438/10000] Avg train loss: 0.007204\n",
      "Epoch [6439/10000] Avg train loss: 0.007203\n",
      "Epoch [6440/10000] Avg train loss: 0.007202\n",
      "Epoch [6441/10000] Avg train loss: 0.007201\n",
      "Epoch [6442/10000] Avg train loss: 0.007200\n",
      "Epoch [6443/10000] Avg train loss: 0.007199\n",
      "Epoch [6444/10000] Avg train loss: 0.007198\n",
      "Epoch [6445/10000] Avg train loss: 0.007196\n",
      "Epoch [6446/10000] Avg train loss: 0.007195\n",
      "Epoch [6447/10000] Avg train loss: 0.007194\n",
      "Epoch [6448/10000] Avg train loss: 0.007193\n",
      "Epoch [6449/10000] Avg train loss: 0.007192\n",
      "Epoch [6450/10000] Avg train loss: 0.007191\n",
      "Epoch [6451/10000] Avg train loss: 0.007190\n",
      "Epoch [6452/10000] Avg train loss: 0.007189\n",
      "Epoch [6453/10000] Avg train loss: 0.007188\n",
      "Epoch [6454/10000] Avg train loss: 0.007186\n",
      "Epoch [6455/10000] Avg train loss: 0.007185\n",
      "Epoch [6456/10000] Avg train loss: 0.007184\n",
      "Epoch [6457/10000] Avg train loss: 0.007183\n",
      "Epoch [6458/10000] Avg train loss: 0.007182\n",
      "Epoch [6459/10000] Avg train loss: 0.007181\n",
      "Epoch [6460/10000] Avg train loss: 0.007180\n",
      "Epoch [6461/10000] Avg train loss: 0.007179\n",
      "Epoch [6462/10000] Avg train loss: 0.007178\n",
      "Epoch [6463/10000] Avg train loss: 0.007176\n",
      "Epoch [6464/10000] Avg train loss: 0.007175\n",
      "Epoch [6465/10000] Avg train loss: 0.007174\n",
      "Epoch [6466/10000] Avg train loss: 0.007173\n",
      "Epoch [6467/10000] Avg train loss: 0.007172\n",
      "Epoch [6468/10000] Avg train loss: 0.007171\n",
      "Epoch [6469/10000] Avg train loss: 0.007170\n",
      "Epoch [6470/10000] Avg train loss: 0.007169\n",
      "Epoch [6471/10000] Avg train loss: 0.007168\n",
      "Epoch [6472/10000] Avg train loss: 0.007166\n",
      "Epoch [6473/10000] Avg train loss: 0.007165\n",
      "Epoch [6474/10000] Avg train loss: 0.007164\n",
      "Epoch [6475/10000] Avg train loss: 0.007163\n",
      "Epoch [6476/10000] Avg train loss: 0.007162\n",
      "Epoch [6477/10000] Avg train loss: 0.007161\n",
      "Epoch [6478/10000] Avg train loss: 0.007160\n",
      "Epoch [6479/10000] Avg train loss: 0.007159\n",
      "Epoch [6480/10000] Avg train loss: 0.007158\n",
      "Epoch [6481/10000] Avg train loss: 0.007156\n",
      "Epoch [6482/10000] Avg train loss: 0.007155\n",
      "Epoch [6483/10000] Avg train loss: 0.007154\n",
      "Epoch [6484/10000] Avg train loss: 0.007153\n",
      "Epoch [6485/10000] Avg train loss: 0.007152\n",
      "Epoch [6486/10000] Avg train loss: 0.007151\n",
      "Epoch [6487/10000] Avg train loss: 0.007150\n",
      "Epoch [6488/10000] Avg train loss: 0.007149\n",
      "Epoch [6489/10000] Avg train loss: 0.007148\n",
      "Epoch [6490/10000] Avg train loss: 0.007147\n",
      "Epoch [6491/10000] Avg train loss: 0.007145\n",
      "Epoch [6492/10000] Avg train loss: 0.007144\n",
      "Epoch [6493/10000] Avg train loss: 0.007143\n",
      "Epoch [6494/10000] Avg train loss: 0.007142\n",
      "Epoch [6495/10000] Avg train loss: 0.007141\n",
      "Epoch [6496/10000] Avg train loss: 0.007140\n",
      "Epoch [6497/10000] Avg train loss: 0.007139\n",
      "Epoch [6498/10000] Avg train loss: 0.007138\n",
      "Epoch [6499/10000] Avg train loss: 0.007137\n",
      "Epoch [6500/10000] Avg train loss: 0.007136\n",
      "Epoch [6501/10000] Avg train loss: 0.007134\n",
      "Epoch [6502/10000] Avg train loss: 0.007133\n",
      "Epoch [6503/10000] Avg train loss: 0.007132\n",
      "Epoch [6504/10000] Avg train loss: 0.007131\n",
      "Epoch [6505/10000] Avg train loss: 0.007130\n",
      "Epoch [6506/10000] Avg train loss: 0.007129\n",
      "Epoch [6507/10000] Avg train loss: 0.007128\n",
      "Epoch [6508/10000] Avg train loss: 0.007127\n",
      "Epoch [6509/10000] Avg train loss: 0.007126\n",
      "Epoch [6510/10000] Avg train loss: 0.007125\n",
      "Epoch [6511/10000] Avg train loss: 0.007124\n",
      "Epoch [6512/10000] Avg train loss: 0.007122\n",
      "Epoch [6513/10000] Avg train loss: 0.007121\n",
      "Epoch [6514/10000] Avg train loss: 0.007120\n",
      "Epoch [6515/10000] Avg train loss: 0.007119\n",
      "Epoch [6516/10000] Avg train loss: 0.007118\n",
      "Epoch [6517/10000] Avg train loss: 0.007117\n",
      "Epoch [6518/10000] Avg train loss: 0.007116\n",
      "Epoch [6519/10000] Avg train loss: 0.007115\n",
      "Epoch [6520/10000] Avg train loss: 0.007114\n",
      "Epoch [6521/10000] Avg train loss: 0.007113\n",
      "Epoch [6522/10000] Avg train loss: 0.007112\n",
      "Epoch [6523/10000] Avg train loss: 0.007110\n",
      "Epoch [6524/10000] Avg train loss: 0.007109\n",
      "Epoch [6525/10000] Avg train loss: 0.007108\n",
      "Epoch [6526/10000] Avg train loss: 0.007107\n",
      "Epoch [6527/10000] Avg train loss: 0.007106\n",
      "Epoch [6528/10000] Avg train loss: 0.007105\n",
      "Epoch [6529/10000] Avg train loss: 0.007104\n",
      "Epoch [6530/10000] Avg train loss: 0.007103\n",
      "Epoch [6531/10000] Avg train loss: 0.007102\n",
      "Epoch [6532/10000] Avg train loss: 0.007101\n",
      "Epoch [6533/10000] Avg train loss: 0.007100\n",
      "Epoch [6534/10000] Avg train loss: 0.007098\n",
      "Epoch [6535/10000] Avg train loss: 0.007097\n",
      "Epoch [6536/10000] Avg train loss: 0.007096\n",
      "Epoch [6537/10000] Avg train loss: 0.007095\n",
      "Epoch [6538/10000] Avg train loss: 0.007094\n",
      "Epoch [6539/10000] Avg train loss: 0.007093\n",
      "Epoch [6540/10000] Avg train loss: 0.007092\n",
      "Epoch [6541/10000] Avg train loss: 0.007091\n",
      "Epoch [6542/10000] Avg train loss: 0.007090\n",
      "Epoch [6543/10000] Avg train loss: 0.007089\n",
      "Epoch [6544/10000] Avg train loss: 0.007088\n",
      "Epoch [6545/10000] Avg train loss: 0.007087\n",
      "Epoch [6546/10000] Avg train loss: 0.007085\n",
      "Epoch [6547/10000] Avg train loss: 0.007084\n",
      "Epoch [6548/10000] Avg train loss: 0.007083\n",
      "Epoch [6549/10000] Avg train loss: 0.007082\n",
      "Epoch [6550/10000] Avg train loss: 0.007081\n",
      "Epoch [6551/10000] Avg train loss: 0.007080\n",
      "Epoch [6552/10000] Avg train loss: 0.007079\n",
      "Epoch [6553/10000] Avg train loss: 0.007078\n",
      "Epoch [6554/10000] Avg train loss: 0.007077\n",
      "Epoch [6555/10000] Avg train loss: 0.007076\n",
      "Epoch [6556/10000] Avg train loss: 0.007075\n",
      "Epoch [6557/10000] Avg train loss: 0.007074\n",
      "Epoch [6558/10000] Avg train loss: 0.007072\n",
      "Epoch [6559/10000] Avg train loss: 0.007071\n",
      "Epoch [6560/10000] Avg train loss: 0.007070\n",
      "Epoch [6561/10000] Avg train loss: 0.007069\n",
      "Epoch [6562/10000] Avg train loss: 0.007068\n",
      "Epoch [6563/10000] Avg train loss: 0.007067\n",
      "Epoch [6564/10000] Avg train loss: 0.007066\n",
      "Epoch [6565/10000] Avg train loss: 0.007065\n",
      "Epoch [6566/10000] Avg train loss: 0.007064\n",
      "Epoch [6567/10000] Avg train loss: 0.007063\n",
      "Epoch [6568/10000] Avg train loss: 0.007062\n",
      "Epoch [6569/10000] Avg train loss: 0.007061\n",
      "Epoch [6570/10000] Avg train loss: 0.007060\n",
      "Epoch [6571/10000] Avg train loss: 0.007058\n",
      "Epoch [6572/10000] Avg train loss: 0.007057\n",
      "Epoch [6573/10000] Avg train loss: 0.007056\n",
      "Epoch [6574/10000] Avg train loss: 0.007055\n",
      "Epoch [6575/10000] Avg train loss: 0.007054\n",
      "Epoch [6576/10000] Avg train loss: 0.007053\n",
      "Epoch [6577/10000] Avg train loss: 0.007052\n",
      "Epoch [6578/10000] Avg train loss: 0.007051\n",
      "Epoch [6579/10000] Avg train loss: 0.007050\n",
      "Epoch [6580/10000] Avg train loss: 0.007049\n",
      "Epoch [6581/10000] Avg train loss: 0.007048\n",
      "Epoch [6582/10000] Avg train loss: 0.007047\n",
      "Epoch [6583/10000] Avg train loss: 0.007046\n",
      "Epoch [6584/10000] Avg train loss: 0.007045\n",
      "Epoch [6585/10000] Avg train loss: 0.007043\n",
      "Epoch [6586/10000] Avg train loss: 0.007042\n",
      "Epoch [6587/10000] Avg train loss: 0.007041\n",
      "Epoch [6588/10000] Avg train loss: 0.007040\n",
      "Epoch [6589/10000] Avg train loss: 0.007039\n",
      "Epoch [6590/10000] Avg train loss: 0.007038\n",
      "Epoch [6591/10000] Avg train loss: 0.007037\n",
      "Epoch [6592/10000] Avg train loss: 0.007036\n",
      "Epoch [6593/10000] Avg train loss: 0.007035\n",
      "Epoch [6594/10000] Avg train loss: 0.007034\n",
      "Epoch [6595/10000] Avg train loss: 0.007033\n",
      "Epoch [6596/10000] Avg train loss: 0.007032\n",
      "Epoch [6597/10000] Avg train loss: 0.007031\n",
      "Epoch [6598/10000] Avg train loss: 0.007030\n",
      "Epoch [6599/10000] Avg train loss: 0.007029\n",
      "Epoch [6600/10000] Avg train loss: 0.007027\n",
      "Epoch [6601/10000] Avg train loss: 0.007026\n",
      "Epoch [6602/10000] Avg train loss: 0.007025\n",
      "Epoch [6603/10000] Avg train loss: 0.007024\n",
      "Epoch [6604/10000] Avg train loss: 0.007023\n",
      "Epoch [6605/10000] Avg train loss: 0.007022\n",
      "Epoch [6606/10000] Avg train loss: 0.007021\n",
      "Epoch [6607/10000] Avg train loss: 0.007020\n",
      "Epoch [6608/10000] Avg train loss: 0.007019\n",
      "Epoch [6609/10000] Avg train loss: 0.007018\n",
      "Epoch [6610/10000] Avg train loss: 0.007017\n",
      "Epoch [6611/10000] Avg train loss: 0.007016\n",
      "Epoch [6612/10000] Avg train loss: 0.007015\n",
      "Epoch [6613/10000] Avg train loss: 0.007014\n",
      "Epoch [6614/10000] Avg train loss: 0.007013\n",
      "Epoch [6615/10000] Avg train loss: 0.007012\n",
      "Epoch [6616/10000] Avg train loss: 0.007010\n",
      "Epoch [6617/10000] Avg train loss: 0.007009\n",
      "Epoch [6618/10000] Avg train loss: 0.007008\n",
      "Epoch [6619/10000] Avg train loss: 0.007007\n",
      "Epoch [6620/10000] Avg train loss: 0.007006\n",
      "Epoch [6621/10000] Avg train loss: 0.007005\n",
      "Epoch [6622/10000] Avg train loss: 0.007004\n",
      "Epoch [6623/10000] Avg train loss: 0.007003\n",
      "Epoch [6624/10000] Avg train loss: 0.007002\n",
      "Epoch [6625/10000] Avg train loss: 0.007001\n",
      "Epoch [6626/10000] Avg train loss: 0.007000\n",
      "Epoch [6627/10000] Avg train loss: 0.006999\n",
      "Epoch [6628/10000] Avg train loss: 0.006998\n",
      "Epoch [6629/10000] Avg train loss: 0.006997\n",
      "Epoch [6630/10000] Avg train loss: 0.006996\n",
      "Epoch [6631/10000] Avg train loss: 0.006995\n",
      "Epoch [6632/10000] Avg train loss: 0.006994\n",
      "Epoch [6633/10000] Avg train loss: 0.006993\n",
      "Epoch [6634/10000] Avg train loss: 0.006991\n",
      "Epoch [6635/10000] Avg train loss: 0.006990\n",
      "Epoch [6636/10000] Avg train loss: 0.006989\n",
      "Epoch [6637/10000] Avg train loss: 0.006988\n",
      "Epoch [6638/10000] Avg train loss: 0.006987\n",
      "Epoch [6639/10000] Avg train loss: 0.006986\n",
      "Epoch [6640/10000] Avg train loss: 0.006985\n",
      "Epoch [6641/10000] Avg train loss: 0.006984\n",
      "Epoch [6642/10000] Avg train loss: 0.006983\n",
      "Epoch [6643/10000] Avg train loss: 0.006982\n",
      "Epoch [6644/10000] Avg train loss: 0.006981\n",
      "Epoch [6645/10000] Avg train loss: 0.006980\n",
      "Epoch [6646/10000] Avg train loss: 0.006979\n",
      "Epoch [6647/10000] Avg train loss: 0.006978\n",
      "Epoch [6648/10000] Avg train loss: 0.006977\n",
      "Epoch [6649/10000] Avg train loss: 0.006976\n",
      "Epoch [6650/10000] Avg train loss: 0.006975\n",
      "Epoch [6651/10000] Avg train loss: 0.006974\n",
      "Epoch [6652/10000] Avg train loss: 0.006973\n",
      "Epoch [6653/10000] Avg train loss: 0.006971\n",
      "Epoch [6654/10000] Avg train loss: 0.006970\n",
      "Epoch [6655/10000] Avg train loss: 0.006969\n",
      "Epoch [6656/10000] Avg train loss: 0.006968\n",
      "Epoch [6657/10000] Avg train loss: 0.006967\n",
      "Epoch [6658/10000] Avg train loss: 0.006966\n",
      "Epoch [6659/10000] Avg train loss: 0.006965\n",
      "Epoch [6660/10000] Avg train loss: 0.006964\n",
      "Epoch [6661/10000] Avg train loss: 0.006963\n",
      "Epoch [6662/10000] Avg train loss: 0.006962\n",
      "Epoch [6663/10000] Avg train loss: 0.006961\n",
      "Epoch [6664/10000] Avg train loss: 0.006960\n",
      "Epoch [6665/10000] Avg train loss: 0.006959\n",
      "Epoch [6666/10000] Avg train loss: 0.006958\n",
      "Epoch [6667/10000] Avg train loss: 0.006957\n",
      "Epoch [6668/10000] Avg train loss: 0.006956\n",
      "Epoch [6669/10000] Avg train loss: 0.006955\n",
      "Epoch [6670/10000] Avg train loss: 0.006954\n",
      "Epoch [6671/10000] Avg train loss: 0.006953\n",
      "Epoch [6672/10000] Avg train loss: 0.006952\n",
      "Epoch [6673/10000] Avg train loss: 0.006951\n",
      "Epoch [6674/10000] Avg train loss: 0.006950\n",
      "Epoch [6675/10000] Avg train loss: 0.006949\n",
      "Epoch [6676/10000] Avg train loss: 0.006947\n",
      "Epoch [6677/10000] Avg train loss: 0.006946\n",
      "Epoch [6678/10000] Avg train loss: 0.006945\n",
      "Epoch [6679/10000] Avg train loss: 0.006944\n",
      "Epoch [6680/10000] Avg train loss: 0.006943\n",
      "Epoch [6681/10000] Avg train loss: 0.006942\n",
      "Epoch [6682/10000] Avg train loss: 0.006941\n",
      "Epoch [6683/10000] Avg train loss: 0.006940\n",
      "Epoch [6684/10000] Avg train loss: 0.006939\n",
      "Epoch [6685/10000] Avg train loss: 0.006938\n",
      "Epoch [6686/10000] Avg train loss: 0.006937\n",
      "Epoch [6687/10000] Avg train loss: 0.006936\n",
      "Epoch [6688/10000] Avg train loss: 0.006935\n",
      "Epoch [6689/10000] Avg train loss: 0.006934\n",
      "Epoch [6690/10000] Avg train loss: 0.006933\n",
      "Epoch [6691/10000] Avg train loss: 0.006932\n",
      "Epoch [6692/10000] Avg train loss: 0.006931\n",
      "Epoch [6693/10000] Avg train loss: 0.006930\n",
      "Epoch [6694/10000] Avg train loss: 0.006929\n",
      "Epoch [6695/10000] Avg train loss: 0.006928\n",
      "Epoch [6696/10000] Avg train loss: 0.006927\n",
      "Epoch [6697/10000] Avg train loss: 0.006926\n",
      "Epoch [6698/10000] Avg train loss: 0.006925\n",
      "Epoch [6699/10000] Avg train loss: 0.006924\n",
      "Epoch [6700/10000] Avg train loss: 0.006923\n",
      "Epoch [6701/10000] Avg train loss: 0.006922\n",
      "Epoch [6702/10000] Avg train loss: 0.006921\n",
      "Epoch [6703/10000] Avg train loss: 0.006919\n",
      "Epoch [6704/10000] Avg train loss: 0.006918\n",
      "Epoch [6705/10000] Avg train loss: 0.006917\n",
      "Epoch [6706/10000] Avg train loss: 0.006916\n",
      "Epoch [6707/10000] Avg train loss: 0.006915\n",
      "Epoch [6708/10000] Avg train loss: 0.006914\n",
      "Epoch [6709/10000] Avg train loss: 0.006913\n",
      "Epoch [6710/10000] Avg train loss: 0.006912\n",
      "Epoch [6711/10000] Avg train loss: 0.006911\n",
      "Epoch [6712/10000] Avg train loss: 0.006910\n",
      "Epoch [6713/10000] Avg train loss: 0.006909\n",
      "Epoch [6714/10000] Avg train loss: 0.006908\n",
      "Epoch [6715/10000] Avg train loss: 0.006907\n",
      "Epoch [6716/10000] Avg train loss: 0.006906\n",
      "Epoch [6717/10000] Avg train loss: 0.006905\n",
      "Epoch [6718/10000] Avg train loss: 0.006904\n",
      "Epoch [6719/10000] Avg train loss: 0.006903\n",
      "Epoch [6720/10000] Avg train loss: 0.006902\n",
      "Epoch [6721/10000] Avg train loss: 0.006901\n",
      "Epoch [6722/10000] Avg train loss: 0.006900\n",
      "Epoch [6723/10000] Avg train loss: 0.006899\n",
      "Epoch [6724/10000] Avg train loss: 0.006898\n",
      "Epoch [6725/10000] Avg train loss: 0.006897\n",
      "Epoch [6726/10000] Avg train loss: 0.006896\n",
      "Epoch [6727/10000] Avg train loss: 0.006895\n",
      "Epoch [6728/10000] Avg train loss: 0.006894\n",
      "Epoch [6729/10000] Avg train loss: 0.006893\n",
      "Epoch [6730/10000] Avg train loss: 0.006892\n",
      "Epoch [6731/10000] Avg train loss: 0.006891\n",
      "Epoch [6732/10000] Avg train loss: 0.006890\n",
      "Epoch [6733/10000] Avg train loss: 0.006889\n",
      "Epoch [6734/10000] Avg train loss: 0.006888\n",
      "Epoch [6735/10000] Avg train loss: 0.006887\n",
      "Epoch [6736/10000] Avg train loss: 0.006886\n",
      "Epoch [6737/10000] Avg train loss: 0.006885\n",
      "Epoch [6738/10000] Avg train loss: 0.006884\n",
      "Epoch [6739/10000] Avg train loss: 0.006883\n",
      "Epoch [6740/10000] Avg train loss: 0.006881\n",
      "Epoch [6741/10000] Avg train loss: 0.006880\n",
      "Epoch [6742/10000] Avg train loss: 0.006879\n",
      "Epoch [6743/10000] Avg train loss: 0.006878\n",
      "Epoch [6744/10000] Avg train loss: 0.006877\n",
      "Epoch [6745/10000] Avg train loss: 0.006876\n",
      "Epoch [6746/10000] Avg train loss: 0.006875\n",
      "Epoch [6747/10000] Avg train loss: 0.006874\n",
      "Epoch [6748/10000] Avg train loss: 0.006873\n",
      "Epoch [6749/10000] Avg train loss: 0.006872\n",
      "Epoch [6750/10000] Avg train loss: 0.006871\n",
      "Epoch [6751/10000] Avg train loss: 0.006870\n",
      "Epoch [6752/10000] Avg train loss: 0.006869\n",
      "Epoch [6753/10000] Avg train loss: 0.006868\n",
      "Epoch [6754/10000] Avg train loss: 0.006867\n",
      "Epoch [6755/10000] Avg train loss: 0.006866\n",
      "Epoch [6756/10000] Avg train loss: 0.006865\n",
      "Epoch [6757/10000] Avg train loss: 0.006864\n",
      "Epoch [6758/10000] Avg train loss: 0.006863\n",
      "Epoch [6759/10000] Avg train loss: 0.006862\n",
      "Epoch [6760/10000] Avg train loss: 0.006861\n",
      "Epoch [6761/10000] Avg train loss: 0.006860\n",
      "Epoch [6762/10000] Avg train loss: 0.006859\n",
      "Epoch [6763/10000] Avg train loss: 0.006858\n",
      "Epoch [6764/10000] Avg train loss: 0.006857\n",
      "Epoch [6765/10000] Avg train loss: 0.006856\n",
      "Epoch [6766/10000] Avg train loss: 0.006855\n",
      "Epoch [6767/10000] Avg train loss: 0.006854\n",
      "Epoch [6768/10000] Avg train loss: 0.006853\n",
      "Epoch [6769/10000] Avg train loss: 0.006852\n",
      "Epoch [6770/10000] Avg train loss: 0.006851\n",
      "Epoch [6771/10000] Avg train loss: 0.006850\n",
      "Epoch [6772/10000] Avg train loss: 0.006849\n",
      "Epoch [6773/10000] Avg train loss: 0.006848\n",
      "Epoch [6774/10000] Avg train loss: 0.006847\n",
      "Epoch [6775/10000] Avg train loss: 0.006846\n",
      "Epoch [6776/10000] Avg train loss: 0.006845\n",
      "Epoch [6777/10000] Avg train loss: 0.006844\n",
      "Epoch [6778/10000] Avg train loss: 0.006843\n",
      "Epoch [6779/10000] Avg train loss: 0.006842\n",
      "Epoch [6780/10000] Avg train loss: 0.006841\n",
      "Epoch [6781/10000] Avg train loss: 0.006840\n",
      "Epoch [6782/10000] Avg train loss: 0.006839\n",
      "Epoch [6783/10000] Avg train loss: 0.006838\n",
      "Epoch [6784/10000] Avg train loss: 0.006837\n",
      "Epoch [6785/10000] Avg train loss: 0.006836\n",
      "Epoch [6786/10000] Avg train loss: 0.006835\n",
      "Epoch [6787/10000] Avg train loss: 0.006834\n",
      "Epoch [6788/10000] Avg train loss: 0.006833\n",
      "Epoch [6789/10000] Avg train loss: 0.006832\n",
      "Epoch [6790/10000] Avg train loss: 0.006831\n",
      "Epoch [6791/10000] Avg train loss: 0.006830\n",
      "Epoch [6792/10000] Avg train loss: 0.006829\n",
      "Epoch [6793/10000] Avg train loss: 0.006828\n",
      "Epoch [6794/10000] Avg train loss: 0.006827\n",
      "Epoch [6795/10000] Avg train loss: 0.006826\n",
      "Epoch [6796/10000] Avg train loss: 0.006825\n",
      "Epoch [6797/10000] Avg train loss: 0.006824\n",
      "Epoch [6798/10000] Avg train loss: 0.006823\n",
      "Epoch [6799/10000] Avg train loss: 0.006822\n",
      "Epoch [6800/10000] Avg train loss: 0.006821\n",
      "Epoch [6801/10000] Avg train loss: 0.006820\n",
      "Epoch [6802/10000] Avg train loss: 0.006819\n",
      "Epoch [6803/10000] Avg train loss: 0.006818\n",
      "Epoch [6804/10000] Avg train loss: 0.006817\n",
      "Epoch [6805/10000] Avg train loss: 0.006816\n",
      "Epoch [6806/10000] Avg train loss: 0.006815\n",
      "Epoch [6807/10000] Avg train loss: 0.006814\n",
      "Epoch [6808/10000] Avg train loss: 0.006813\n",
      "Epoch [6809/10000] Avg train loss: 0.006812\n",
      "Epoch [6810/10000] Avg train loss: 0.006811\n",
      "Epoch [6811/10000] Avg train loss: 0.006810\n",
      "Epoch [6812/10000] Avg train loss: 0.006809\n",
      "Epoch [6813/10000] Avg train loss: 0.006808\n",
      "Epoch [6814/10000] Avg train loss: 0.006807\n",
      "Epoch [6815/10000] Avg train loss: 0.006806\n",
      "Epoch [6816/10000] Avg train loss: 0.006805\n",
      "Epoch [6817/10000] Avg train loss: 0.006804\n",
      "Epoch [6818/10000] Avg train loss: 0.006803\n",
      "Epoch [6819/10000] Avg train loss: 0.006802\n",
      "Epoch [6820/10000] Avg train loss: 0.006801\n",
      "Epoch [6821/10000] Avg train loss: 0.006800\n",
      "Epoch [6822/10000] Avg train loss: 0.006799\n",
      "Epoch [6823/10000] Avg train loss: 0.006798\n",
      "Epoch [6824/10000] Avg train loss: 0.006797\n",
      "Epoch [6825/10000] Avg train loss: 0.006796\n",
      "Epoch [6826/10000] Avg train loss: 0.006795\n",
      "Epoch [6827/10000] Avg train loss: 0.006794\n",
      "Epoch [6828/10000] Avg train loss: 0.006793\n",
      "Epoch [6829/10000] Avg train loss: 0.006792\n",
      "Epoch [6830/10000] Avg train loss: 0.006791\n",
      "Epoch [6831/10000] Avg train loss: 0.006790\n",
      "Epoch [6832/10000] Avg train loss: 0.006789\n",
      "Epoch [6833/10000] Avg train loss: 0.006788\n",
      "Epoch [6834/10000] Avg train loss: 0.006787\n",
      "Epoch [6835/10000] Avg train loss: 0.006786\n",
      "Epoch [6836/10000] Avg train loss: 0.006785\n",
      "Epoch [6837/10000] Avg train loss: 0.006784\n",
      "Epoch [6838/10000] Avg train loss: 0.006783\n",
      "Epoch [6839/10000] Avg train loss: 0.006782\n",
      "Epoch [6840/10000] Avg train loss: 0.006781\n",
      "Epoch [6841/10000] Avg train loss: 0.006780\n",
      "Epoch [6842/10000] Avg train loss: 0.006779\n",
      "Epoch [6843/10000] Avg train loss: 0.006778\n",
      "Epoch [6844/10000] Avg train loss: 0.006777\n",
      "Epoch [6845/10000] Avg train loss: 0.006776\n",
      "Epoch [6846/10000] Avg train loss: 0.006775\n",
      "Epoch [6847/10000] Avg train loss: 0.006774\n",
      "Epoch [6848/10000] Avg train loss: 0.006773\n",
      "Epoch [6849/10000] Avg train loss: 0.006772\n",
      "Epoch [6850/10000] Avg train loss: 0.006771\n",
      "Epoch [6851/10000] Avg train loss: 0.006770\n",
      "Epoch [6852/10000] Avg train loss: 0.006769\n",
      "Epoch [6853/10000] Avg train loss: 0.006768\n",
      "Epoch [6854/10000] Avg train loss: 0.006767\n",
      "Epoch [6855/10000] Avg train loss: 0.006766\n",
      "Epoch [6856/10000] Avg train loss: 0.006765\n",
      "Epoch [6857/10000] Avg train loss: 0.006764\n",
      "Epoch [6858/10000] Avg train loss: 0.006763\n",
      "Epoch [6859/10000] Avg train loss: 0.006762\n",
      "Epoch [6860/10000] Avg train loss: 0.006761\n",
      "Epoch [6861/10000] Avg train loss: 0.006760\n",
      "Epoch [6862/10000] Avg train loss: 0.006759\n",
      "Epoch [6863/10000] Avg train loss: 0.006758\n",
      "Epoch [6864/10000] Avg train loss: 0.006757\n",
      "Epoch [6865/10000] Avg train loss: 0.006756\n",
      "Epoch [6866/10000] Avg train loss: 0.006755\n",
      "Epoch [6867/10000] Avg train loss: 0.006754\n",
      "Epoch [6868/10000] Avg train loss: 0.006753\n",
      "Epoch [6869/10000] Avg train loss: 0.006752\n",
      "Epoch [6870/10000] Avg train loss: 0.006751\n",
      "Epoch [6871/10000] Avg train loss: 0.006750\n",
      "Epoch [6872/10000] Avg train loss: 0.006749\n",
      "Epoch [6873/10000] Avg train loss: 0.006748\n",
      "Epoch [6874/10000] Avg train loss: 0.006747\n",
      "Epoch [6875/10000] Avg train loss: 0.006746\n",
      "Epoch [6876/10000] Avg train loss: 0.006745\n",
      "Epoch [6877/10000] Avg train loss: 0.006744\n",
      "Epoch [6878/10000] Avg train loss: 0.006743\n",
      "Epoch [6879/10000] Avg train loss: 0.006742\n",
      "Epoch [6880/10000] Avg train loss: 0.006741\n",
      "Epoch [6881/10000] Avg train loss: 0.006740\n",
      "Epoch [6882/10000] Avg train loss: 0.006740\n",
      "Epoch [6883/10000] Avg train loss: 0.006739\n",
      "Epoch [6884/10000] Avg train loss: 0.006738\n",
      "Epoch [6885/10000] Avg train loss: 0.006737\n",
      "Epoch [6886/10000] Avg train loss: 0.006736\n",
      "Epoch [6887/10000] Avg train loss: 0.006735\n",
      "Epoch [6888/10000] Avg train loss: 0.006734\n",
      "Epoch [6889/10000] Avg train loss: 0.006733\n",
      "Epoch [6890/10000] Avg train loss: 0.006732\n",
      "Epoch [6891/10000] Avg train loss: 0.006731\n",
      "Epoch [6892/10000] Avg train loss: 0.006730\n",
      "Epoch [6893/10000] Avg train loss: 0.006729\n",
      "Epoch [6894/10000] Avg train loss: 0.006728\n",
      "Epoch [6895/10000] Avg train loss: 0.006727\n",
      "Epoch [6896/10000] Avg train loss: 0.006726\n",
      "Epoch [6897/10000] Avg train loss: 0.006725\n",
      "Epoch [6898/10000] Avg train loss: 0.006724\n",
      "Epoch [6899/10000] Avg train loss: 0.006723\n",
      "Epoch [6900/10000] Avg train loss: 0.006722\n",
      "Epoch [6901/10000] Avg train loss: 0.006721\n",
      "Epoch [6902/10000] Avg train loss: 0.006720\n",
      "Epoch [6903/10000] Avg train loss: 0.006719\n",
      "Epoch [6904/10000] Avg train loss: 0.006718\n",
      "Epoch [6905/10000] Avg train loss: 0.006717\n",
      "Epoch [6906/10000] Avg train loss: 0.006716\n",
      "Epoch [6907/10000] Avg train loss: 0.006715\n",
      "Epoch [6908/10000] Avg train loss: 0.006714\n",
      "Epoch [6909/10000] Avg train loss: 0.006713\n",
      "Epoch [6910/10000] Avg train loss: 0.006712\n",
      "Epoch [6911/10000] Avg train loss: 0.006711\n",
      "Epoch [6912/10000] Avg train loss: 0.006710\n",
      "Epoch [6913/10000] Avg train loss: 0.006709\n",
      "Epoch [6914/10000] Avg train loss: 0.006708\n",
      "Epoch [6915/10000] Avg train loss: 0.006707\n",
      "Epoch [6916/10000] Avg train loss: 0.006706\n",
      "Epoch [6917/10000] Avg train loss: 0.006705\n",
      "Epoch [6918/10000] Avg train loss: 0.006704\n",
      "Epoch [6919/10000] Avg train loss: 0.006703\n",
      "Epoch [6920/10000] Avg train loss: 0.006702\n",
      "Epoch [6921/10000] Avg train loss: 0.006702\n",
      "Epoch [6922/10000] Avg train loss: 0.006701\n",
      "Epoch [6923/10000] Avg train loss: 0.006700\n",
      "Epoch [6924/10000] Avg train loss: 0.006699\n",
      "Epoch [6925/10000] Avg train loss: 0.006698\n",
      "Epoch [6926/10000] Avg train loss: 0.006697\n",
      "Epoch [6927/10000] Avg train loss: 0.006696\n",
      "Epoch [6928/10000] Avg train loss: 0.006695\n",
      "Epoch [6929/10000] Avg train loss: 0.006694\n",
      "Epoch [6930/10000] Avg train loss: 0.006693\n",
      "Epoch [6931/10000] Avg train loss: 0.006692\n",
      "Epoch [6932/10000] Avg train loss: 0.006691\n",
      "Epoch [6933/10000] Avg train loss: 0.006690\n",
      "Epoch [6934/10000] Avg train loss: 0.006689\n",
      "Epoch [6935/10000] Avg train loss: 0.006688\n",
      "Epoch [6936/10000] Avg train loss: 0.006687\n",
      "Epoch [6937/10000] Avg train loss: 0.006686\n",
      "Epoch [6938/10000] Avg train loss: 0.006685\n",
      "Epoch [6939/10000] Avg train loss: 0.006684\n",
      "Epoch [6940/10000] Avg train loss: 0.006683\n",
      "Epoch [6941/10000] Avg train loss: 0.006682\n",
      "Epoch [6942/10000] Avg train loss: 0.006681\n",
      "Epoch [6943/10000] Avg train loss: 0.006680\n",
      "Epoch [6944/10000] Avg train loss: 0.006679\n",
      "Epoch [6945/10000] Avg train loss: 0.006678\n",
      "Epoch [6946/10000] Avg train loss: 0.006677\n",
      "Epoch [6947/10000] Avg train loss: 0.006676\n",
      "Epoch [6948/10000] Avg train loss: 0.006675\n",
      "Epoch [6949/10000] Avg train loss: 0.006675\n",
      "Epoch [6950/10000] Avg train loss: 0.006674\n",
      "Epoch [6951/10000] Avg train loss: 0.006673\n",
      "Epoch [6952/10000] Avg train loss: 0.006672\n",
      "Epoch [6953/10000] Avg train loss: 0.006671\n",
      "Epoch [6954/10000] Avg train loss: 0.006670\n",
      "Epoch [6955/10000] Avg train loss: 0.006669\n",
      "Epoch [6956/10000] Avg train loss: 0.006668\n",
      "Epoch [6957/10000] Avg train loss: 0.006667\n",
      "Epoch [6958/10000] Avg train loss: 0.006666\n",
      "Epoch [6959/10000] Avg train loss: 0.006665\n",
      "Epoch [6960/10000] Avg train loss: 0.006664\n",
      "Epoch [6961/10000] Avg train loss: 0.006663\n",
      "Epoch [6962/10000] Avg train loss: 0.006662\n",
      "Epoch [6963/10000] Avg train loss: 0.006661\n",
      "Epoch [6964/10000] Avg train loss: 0.006660\n",
      "Epoch [6965/10000] Avg train loss: 0.006659\n",
      "Epoch [6966/10000] Avg train loss: 0.006658\n",
      "Epoch [6967/10000] Avg train loss: 0.006657\n",
      "Epoch [6968/10000] Avg train loss: 0.006656\n",
      "Epoch [6969/10000] Avg train loss: 0.006655\n",
      "Epoch [6970/10000] Avg train loss: 0.006654\n",
      "Epoch [6971/10000] Avg train loss: 0.006653\n",
      "Epoch [6972/10000] Avg train loss: 0.006653\n",
      "Epoch [6973/10000] Avg train loss: 0.006652\n",
      "Epoch [6974/10000] Avg train loss: 0.006651\n",
      "Epoch [6975/10000] Avg train loss: 0.006650\n",
      "Epoch [6976/10000] Avg train loss: 0.006649\n",
      "Epoch [6977/10000] Avg train loss: 0.006648\n",
      "Epoch [6978/10000] Avg train loss: 0.006647\n",
      "Epoch [6979/10000] Avg train loss: 0.006646\n",
      "Epoch [6980/10000] Avg train loss: 0.006645\n",
      "Epoch [6981/10000] Avg train loss: 0.006644\n",
      "Epoch [6982/10000] Avg train loss: 0.006643\n",
      "Epoch [6983/10000] Avg train loss: 0.006642\n",
      "Epoch [6984/10000] Avg train loss: 0.006641\n",
      "Epoch [6985/10000] Avg train loss: 0.006640\n",
      "Epoch [6986/10000] Avg train loss: 0.006639\n",
      "Epoch [6987/10000] Avg train loss: 0.006638\n",
      "Epoch [6988/10000] Avg train loss: 0.006637\n",
      "Epoch [6989/10000] Avg train loss: 0.006636\n",
      "Epoch [6990/10000] Avg train loss: 0.006635\n",
      "Epoch [6991/10000] Avg train loss: 0.006634\n",
      "Epoch [6992/10000] Avg train loss: 0.006633\n",
      "Epoch [6993/10000] Avg train loss: 0.006633\n",
      "Epoch [6994/10000] Avg train loss: 0.006632\n",
      "Epoch [6995/10000] Avg train loss: 0.006631\n",
      "Epoch [6996/10000] Avg train loss: 0.006630\n",
      "Epoch [6997/10000] Avg train loss: 0.006629\n",
      "Epoch [6998/10000] Avg train loss: 0.006628\n",
      "Epoch [6999/10000] Avg train loss: 0.006627\n",
      "Epoch [7000/10000] Avg train loss: 0.006626\n",
      "Epoch [7001/10000] Avg train loss: 0.006625\n",
      "Epoch [7002/10000] Avg train loss: 0.006624\n",
      "Epoch [7003/10000] Avg train loss: 0.006623\n",
      "Epoch [7004/10000] Avg train loss: 0.006622\n",
      "Epoch [7005/10000] Avg train loss: 0.006621\n",
      "Epoch [7006/10000] Avg train loss: 0.006620\n",
      "Epoch [7007/10000] Avg train loss: 0.006619\n",
      "Epoch [7008/10000] Avg train loss: 0.006618\n",
      "Epoch [7009/10000] Avg train loss: 0.006617\n",
      "Epoch [7010/10000] Avg train loss: 0.006616\n",
      "Epoch [7011/10000] Avg train loss: 0.006615\n",
      "Epoch [7012/10000] Avg train loss: 0.006615\n",
      "Epoch [7013/10000] Avg train loss: 0.006614\n",
      "Epoch [7014/10000] Avg train loss: 0.006613\n",
      "Epoch [7015/10000] Avg train loss: 0.006612\n",
      "Epoch [7016/10000] Avg train loss: 0.006611\n",
      "Epoch [7017/10000] Avg train loss: 0.006610\n",
      "Epoch [7018/10000] Avg train loss: 0.006609\n",
      "Epoch [7019/10000] Avg train loss: 0.006608\n",
      "Epoch [7020/10000] Avg train loss: 0.006607\n",
      "Epoch [7021/10000] Avg train loss: 0.006606\n",
      "Epoch [7022/10000] Avg train loss: 0.006605\n",
      "Epoch [7023/10000] Avg train loss: 0.006604\n",
      "Epoch [7024/10000] Avg train loss: 0.006603\n",
      "Epoch [7025/10000] Avg train loss: 0.006602\n",
      "Epoch [7026/10000] Avg train loss: 0.006601\n",
      "Epoch [7027/10000] Avg train loss: 0.006600\n",
      "Epoch [7028/10000] Avg train loss: 0.006599\n",
      "Epoch [7029/10000] Avg train loss: 0.006599\n",
      "Epoch [7030/10000] Avg train loss: 0.006598\n",
      "Epoch [7031/10000] Avg train loss: 0.006597\n",
      "Epoch [7032/10000] Avg train loss: 0.006596\n",
      "Epoch [7033/10000] Avg train loss: 0.006595\n",
      "Epoch [7034/10000] Avg train loss: 0.006594\n",
      "Epoch [7035/10000] Avg train loss: 0.006593\n",
      "Epoch [7036/10000] Avg train loss: 0.006592\n",
      "Epoch [7037/10000] Avg train loss: 0.006591\n",
      "Epoch [7038/10000] Avg train loss: 0.006590\n",
      "Epoch [7039/10000] Avg train loss: 0.006589\n",
      "Epoch [7040/10000] Avg train loss: 0.006588\n",
      "Epoch [7041/10000] Avg train loss: 0.006587\n",
      "Epoch [7042/10000] Avg train loss: 0.006586\n",
      "Epoch [7043/10000] Avg train loss: 0.006585\n",
      "Epoch [7044/10000] Avg train loss: 0.006585\n",
      "Epoch [7045/10000] Avg train loss: 0.006584\n",
      "Epoch [7046/10000] Avg train loss: 0.006583\n",
      "Epoch [7047/10000] Avg train loss: 0.006582\n",
      "Epoch [7048/10000] Avg train loss: 0.006581\n",
      "Epoch [7049/10000] Avg train loss: 0.006580\n",
      "Epoch [7050/10000] Avg train loss: 0.006579\n",
      "Epoch [7051/10000] Avg train loss: 0.006578\n",
      "Epoch [7052/10000] Avg train loss: 0.006577\n",
      "Epoch [7053/10000] Avg train loss: 0.006576\n",
      "Epoch [7054/10000] Avg train loss: 0.006575\n",
      "Epoch [7055/10000] Avg train loss: 0.006574\n",
      "Epoch [7056/10000] Avg train loss: 0.006573\n",
      "Epoch [7057/10000] Avg train loss: 0.006572\n",
      "Epoch [7058/10000] Avg train loss: 0.006571\n",
      "Epoch [7059/10000] Avg train loss: 0.006571\n",
      "Epoch [7060/10000] Avg train loss: 0.006570\n",
      "Epoch [7061/10000] Avg train loss: 0.006569\n",
      "Epoch [7062/10000] Avg train loss: 0.006568\n",
      "Epoch [7063/10000] Avg train loss: 0.006567\n",
      "Epoch [7064/10000] Avg train loss: 0.006566\n",
      "Epoch [7065/10000] Avg train loss: 0.006565\n",
      "Epoch [7066/10000] Avg train loss: 0.006564\n",
      "Epoch [7067/10000] Avg train loss: 0.006563\n",
      "Epoch [7068/10000] Avg train loss: 0.006562\n",
      "Epoch [7069/10000] Avg train loss: 0.006561\n",
      "Epoch [7070/10000] Avg train loss: 0.006560\n",
      "Epoch [7071/10000] Avg train loss: 0.006559\n",
      "Epoch [7072/10000] Avg train loss: 0.006558\n",
      "Epoch [7073/10000] Avg train loss: 0.006558\n",
      "Epoch [7074/10000] Avg train loss: 0.006557\n",
      "Epoch [7075/10000] Avg train loss: 0.006556\n",
      "Epoch [7076/10000] Avg train loss: 0.006555\n",
      "Epoch [7077/10000] Avg train loss: 0.006554\n",
      "Epoch [7078/10000] Avg train loss: 0.006553\n",
      "Epoch [7079/10000] Avg train loss: 0.006552\n",
      "Epoch [7080/10000] Avg train loss: 0.006551\n",
      "Epoch [7081/10000] Avg train loss: 0.006550\n",
      "Epoch [7082/10000] Avg train loss: 0.006549\n",
      "Epoch [7083/10000] Avg train loss: 0.006548\n",
      "Epoch [7084/10000] Avg train loss: 0.006547\n",
      "Epoch [7085/10000] Avg train loss: 0.006546\n",
      "Epoch [7086/10000] Avg train loss: 0.006545\n",
      "Epoch [7087/10000] Avg train loss: 0.006545\n",
      "Epoch [7088/10000] Avg train loss: 0.006544\n",
      "Epoch [7089/10000] Avg train loss: 0.006543\n",
      "Epoch [7090/10000] Avg train loss: 0.006542\n",
      "Epoch [7091/10000] Avg train loss: 0.006541\n",
      "Epoch [7092/10000] Avg train loss: 0.006540\n",
      "Epoch [7093/10000] Avg train loss: 0.006539\n",
      "Epoch [7094/10000] Avg train loss: 0.006538\n",
      "Epoch [7095/10000] Avg train loss: 0.006537\n",
      "Epoch [7096/10000] Avg train loss: 0.006536\n",
      "Epoch [7097/10000] Avg train loss: 0.006535\n",
      "Epoch [7098/10000] Avg train loss: 0.006534\n",
      "Epoch [7099/10000] Avg train loss: 0.006533\n",
      "Epoch [7100/10000] Avg train loss: 0.006533\n",
      "Epoch [7101/10000] Avg train loss: 0.006532\n",
      "Epoch [7102/10000] Avg train loss: 0.006531\n",
      "Epoch [7103/10000] Avg train loss: 0.006530\n",
      "Epoch [7104/10000] Avg train loss: 0.006529\n",
      "Epoch [7105/10000] Avg train loss: 0.006528\n",
      "Epoch [7106/10000] Avg train loss: 0.006527\n",
      "Epoch [7107/10000] Avg train loss: 0.006526\n",
      "Epoch [7108/10000] Avg train loss: 0.006525\n",
      "Epoch [7109/10000] Avg train loss: 0.006524\n",
      "Epoch [7110/10000] Avg train loss: 0.006523\n",
      "Epoch [7111/10000] Avg train loss: 0.006522\n",
      "Epoch [7112/10000] Avg train loss: 0.006522\n",
      "Epoch [7113/10000] Avg train loss: 0.006521\n",
      "Epoch [7114/10000] Avg train loss: 0.006520\n",
      "Epoch [7115/10000] Avg train loss: 0.006519\n",
      "Epoch [7116/10000] Avg train loss: 0.006518\n",
      "Epoch [7117/10000] Avg train loss: 0.006517\n",
      "Epoch [7118/10000] Avg train loss: 0.006516\n",
      "Epoch [7119/10000] Avg train loss: 0.006515\n",
      "Epoch [7120/10000] Avg train loss: 0.006514\n",
      "Epoch [7121/10000] Avg train loss: 0.006513\n",
      "Epoch [7122/10000] Avg train loss: 0.006512\n",
      "Epoch [7123/10000] Avg train loss: 0.006511\n",
      "Epoch [7124/10000] Avg train loss: 0.006511\n",
      "Epoch [7125/10000] Avg train loss: 0.006510\n",
      "Epoch [7126/10000] Avg train loss: 0.006509\n",
      "Epoch [7127/10000] Avg train loss: 0.006508\n",
      "Epoch [7128/10000] Avg train loss: 0.006507\n",
      "Epoch [7129/10000] Avg train loss: 0.006506\n",
      "Epoch [7130/10000] Avg train loss: 0.006505\n",
      "Epoch [7131/10000] Avg train loss: 0.006504\n",
      "Epoch [7132/10000] Avg train loss: 0.006503\n",
      "Epoch [7133/10000] Avg train loss: 0.006502\n",
      "Epoch [7134/10000] Avg train loss: 0.006501\n",
      "Epoch [7135/10000] Avg train loss: 0.006501\n",
      "Epoch [7136/10000] Avg train loss: 0.006500\n",
      "Epoch [7137/10000] Avg train loss: 0.006499\n",
      "Epoch [7138/10000] Avg train loss: 0.006498\n",
      "Epoch [7139/10000] Avg train loss: 0.006497\n",
      "Epoch [7140/10000] Avg train loss: 0.006496\n",
      "Epoch [7141/10000] Avg train loss: 0.006495\n",
      "Epoch [7142/10000] Avg train loss: 0.006494\n",
      "Epoch [7143/10000] Avg train loss: 0.006493\n",
      "Epoch [7144/10000] Avg train loss: 0.006492\n",
      "Epoch [7145/10000] Avg train loss: 0.006491\n",
      "Epoch [7146/10000] Avg train loss: 0.006491\n",
      "Epoch [7147/10000] Avg train loss: 0.006490\n",
      "Epoch [7148/10000] Avg train loss: 0.006489\n",
      "Epoch [7149/10000] Avg train loss: 0.006488\n",
      "Epoch [7150/10000] Avg train loss: 0.006487\n",
      "Epoch [7151/10000] Avg train loss: 0.006486\n",
      "Epoch [7152/10000] Avg train loss: 0.006485\n",
      "Epoch [7153/10000] Avg train loss: 0.006484\n",
      "Epoch [7154/10000] Avg train loss: 0.006483\n",
      "Epoch [7155/10000] Avg train loss: 0.006482\n",
      "Epoch [7156/10000] Avg train loss: 0.006481\n",
      "Epoch [7157/10000] Avg train loss: 0.006481\n",
      "Epoch [7158/10000] Avg train loss: 0.006480\n",
      "Epoch [7159/10000] Avg train loss: 0.006479\n",
      "Epoch [7160/10000] Avg train loss: 0.006478\n",
      "Epoch [7161/10000] Avg train loss: 0.006477\n",
      "Epoch [7162/10000] Avg train loss: 0.006476\n",
      "Epoch [7163/10000] Avg train loss: 0.006475\n",
      "Epoch [7164/10000] Avg train loss: 0.006474\n",
      "Epoch [7165/10000] Avg train loss: 0.006473\n",
      "Epoch [7166/10000] Avg train loss: 0.006472\n",
      "Epoch [7167/10000] Avg train loss: 0.006472\n",
      "Epoch [7168/10000] Avg train loss: 0.006471\n",
      "Epoch [7169/10000] Avg train loss: 0.006470\n",
      "Epoch [7170/10000] Avg train loss: 0.006469\n",
      "Epoch [7171/10000] Avg train loss: 0.006468\n",
      "Epoch [7172/10000] Avg train loss: 0.006467\n",
      "Epoch [7173/10000] Avg train loss: 0.006466\n",
      "Epoch [7174/10000] Avg train loss: 0.006465\n",
      "Epoch [7175/10000] Avg train loss: 0.006464\n",
      "Epoch [7176/10000] Avg train loss: 0.006463\n",
      "Epoch [7177/10000] Avg train loss: 0.006462\n",
      "Epoch [7178/10000] Avg train loss: 0.006462\n",
      "Epoch [7179/10000] Avg train loss: 0.006461\n",
      "Epoch [7180/10000] Avg train loss: 0.006460\n",
      "Epoch [7181/10000] Avg train loss: 0.006459\n",
      "Epoch [7182/10000] Avg train loss: 0.006458\n",
      "Epoch [7183/10000] Avg train loss: 0.006457\n",
      "Epoch [7184/10000] Avg train loss: 0.006456\n",
      "Epoch [7185/10000] Avg train loss: 0.006455\n",
      "Epoch [7186/10000] Avg train loss: 0.006454\n",
      "Epoch [7187/10000] Avg train loss: 0.006453\n",
      "Epoch [7188/10000] Avg train loss: 0.006453\n",
      "Epoch [7189/10000] Avg train loss: 0.006452\n",
      "Epoch [7190/10000] Avg train loss: 0.006451\n",
      "Epoch [7191/10000] Avg train loss: 0.006450\n",
      "Epoch [7192/10000] Avg train loss: 0.006449\n",
      "Epoch [7193/10000] Avg train loss: 0.006448\n",
      "Epoch [7194/10000] Avg train loss: 0.006447\n",
      "Epoch [7195/10000] Avg train loss: 0.006446\n",
      "Epoch [7196/10000] Avg train loss: 0.006445\n",
      "Epoch [7197/10000] Avg train loss: 0.006445\n",
      "Epoch [7198/10000] Avg train loss: 0.006444\n",
      "Epoch [7199/10000] Avg train loss: 0.006443\n",
      "Epoch [7200/10000] Avg train loss: 0.006442\n",
      "Epoch [7201/10000] Avg train loss: 0.006441\n",
      "Epoch [7202/10000] Avg train loss: 0.006440\n",
      "Epoch [7203/10000] Avg train loss: 0.006439\n",
      "Epoch [7204/10000] Avg train loss: 0.006438\n",
      "Epoch [7205/10000] Avg train loss: 0.006437\n",
      "Epoch [7206/10000] Avg train loss: 0.006436\n",
      "Epoch [7207/10000] Avg train loss: 0.006436\n",
      "Epoch [7208/10000] Avg train loss: 0.006435\n",
      "Epoch [7209/10000] Avg train loss: 0.006434\n",
      "Epoch [7210/10000] Avg train loss: 0.006433\n",
      "Epoch [7211/10000] Avg train loss: 0.006432\n",
      "Epoch [7212/10000] Avg train loss: 0.006431\n",
      "Epoch [7213/10000] Avg train loss: 0.006430\n",
      "Epoch [7214/10000] Avg train loss: 0.006429\n",
      "Epoch [7215/10000] Avg train loss: 0.006428\n",
      "Epoch [7216/10000] Avg train loss: 0.006428\n",
      "Epoch [7217/10000] Avg train loss: 0.006427\n",
      "Epoch [7218/10000] Avg train loss: 0.006426\n",
      "Epoch [7219/10000] Avg train loss: 0.006425\n",
      "Epoch [7220/10000] Avg train loss: 0.006424\n",
      "Epoch [7221/10000] Avg train loss: 0.006423\n",
      "Epoch [7222/10000] Avg train loss: 0.006422\n",
      "Epoch [7223/10000] Avg train loss: 0.006421\n",
      "Epoch [7224/10000] Avg train loss: 0.006420\n",
      "Epoch [7225/10000] Avg train loss: 0.006420\n",
      "Epoch [7226/10000] Avg train loss: 0.006419\n",
      "Epoch [7227/10000] Avg train loss: 0.006418\n",
      "Epoch [7228/10000] Avg train loss: 0.006417\n",
      "Epoch [7229/10000] Avg train loss: 0.006416\n",
      "Epoch [7230/10000] Avg train loss: 0.006415\n",
      "Epoch [7231/10000] Avg train loss: 0.006414\n",
      "Epoch [7232/10000] Avg train loss: 0.006413\n",
      "Epoch [7233/10000] Avg train loss: 0.006412\n",
      "Epoch [7234/10000] Avg train loss: 0.006412\n",
      "Epoch [7235/10000] Avg train loss: 0.006411\n",
      "Epoch [7236/10000] Avg train loss: 0.006410\n",
      "Epoch [7237/10000] Avg train loss: 0.006409\n",
      "Epoch [7238/10000] Avg train loss: 0.006408\n",
      "Epoch [7239/10000] Avg train loss: 0.006407\n",
      "Epoch [7240/10000] Avg train loss: 0.006406\n",
      "Epoch [7241/10000] Avg train loss: 0.006405\n",
      "Epoch [7242/10000] Avg train loss: 0.006404\n",
      "Epoch [7243/10000] Avg train loss: 0.006404\n",
      "Epoch [7244/10000] Avg train loss: 0.006403\n",
      "Epoch [7245/10000] Avg train loss: 0.006402\n",
      "Epoch [7246/10000] Avg train loss: 0.006401\n",
      "Epoch [7247/10000] Avg train loss: 0.006400\n",
      "Epoch [7248/10000] Avg train loss: 0.006399\n",
      "Epoch [7249/10000] Avg train loss: 0.006398\n",
      "Epoch [7250/10000] Avg train loss: 0.006397\n",
      "Epoch [7251/10000] Avg train loss: 0.006397\n",
      "Epoch [7252/10000] Avg train loss: 0.006396\n",
      "Epoch [7253/10000] Avg train loss: 0.006395\n",
      "Epoch [7254/10000] Avg train loss: 0.006394\n",
      "Epoch [7255/10000] Avg train loss: 0.006393\n",
      "Epoch [7256/10000] Avg train loss: 0.006392\n",
      "Epoch [7257/10000] Avg train loss: 0.006391\n",
      "Epoch [7258/10000] Avg train loss: 0.006390\n",
      "Epoch [7259/10000] Avg train loss: 0.006389\n",
      "Epoch [7260/10000] Avg train loss: 0.006389\n",
      "Epoch [7261/10000] Avg train loss: 0.006388\n",
      "Epoch [7262/10000] Avg train loss: 0.006387\n",
      "Epoch [7263/10000] Avg train loss: 0.006386\n",
      "Epoch [7264/10000] Avg train loss: 0.006385\n",
      "Epoch [7265/10000] Avg train loss: 0.006384\n",
      "Epoch [7266/10000] Avg train loss: 0.006383\n",
      "Epoch [7267/10000] Avg train loss: 0.006382\n",
      "Epoch [7268/10000] Avg train loss: 0.006382\n",
      "Epoch [7269/10000] Avg train loss: 0.006381\n",
      "Epoch [7270/10000] Avg train loss: 0.006380\n",
      "Epoch [7271/10000] Avg train loss: 0.006379\n",
      "Epoch [7272/10000] Avg train loss: 0.006378\n",
      "Epoch [7273/10000] Avg train loss: 0.006377\n",
      "Epoch [7274/10000] Avg train loss: 0.006376\n",
      "Epoch [7275/10000] Avg train loss: 0.006375\n",
      "Epoch [7276/10000] Avg train loss: 0.006375\n",
      "Epoch [7277/10000] Avg train loss: 0.006374\n",
      "Epoch [7278/10000] Avg train loss: 0.006373\n",
      "Epoch [7279/10000] Avg train loss: 0.006372\n",
      "Epoch [7280/10000] Avg train loss: 0.006371\n",
      "Epoch [7281/10000] Avg train loss: 0.006370\n",
      "Epoch [7282/10000] Avg train loss: 0.006369\n",
      "Epoch [7283/10000] Avg train loss: 0.006368\n",
      "Epoch [7284/10000] Avg train loss: 0.006368\n",
      "Epoch [7285/10000] Avg train loss: 0.006367\n",
      "Epoch [7286/10000] Avg train loss: 0.006366\n",
      "Epoch [7287/10000] Avg train loss: 0.006365\n",
      "Epoch [7288/10000] Avg train loss: 0.006364\n",
      "Epoch [7289/10000] Avg train loss: 0.006363\n",
      "Epoch [7290/10000] Avg train loss: 0.006362\n",
      "Epoch [7291/10000] Avg train loss: 0.006361\n",
      "Epoch [7292/10000] Avg train loss: 0.006361\n",
      "Epoch [7293/10000] Avg train loss: 0.006360\n",
      "Epoch [7294/10000] Avg train loss: 0.006359\n",
      "Epoch [7295/10000] Avg train loss: 0.006358\n",
      "Epoch [7296/10000] Avg train loss: 0.006357\n",
      "Epoch [7297/10000] Avg train loss: 0.006356\n",
      "Epoch [7298/10000] Avg train loss: 0.006355\n",
      "Epoch [7299/10000] Avg train loss: 0.006354\n",
      "Epoch [7300/10000] Avg train loss: 0.006354\n",
      "Epoch [7301/10000] Avg train loss: 0.006353\n",
      "Epoch [7302/10000] Avg train loss: 0.006352\n",
      "Epoch [7303/10000] Avg train loss: 0.006351\n",
      "Epoch [7304/10000] Avg train loss: 0.006350\n",
      "Epoch [7305/10000] Avg train loss: 0.006349\n",
      "Epoch [7306/10000] Avg train loss: 0.006348\n",
      "Epoch [7307/10000] Avg train loss: 0.006348\n",
      "Epoch [7308/10000] Avg train loss: 0.006347\n",
      "Epoch [7309/10000] Avg train loss: 0.006346\n",
      "Epoch [7310/10000] Avg train loss: 0.006345\n",
      "Epoch [7311/10000] Avg train loss: 0.006344\n",
      "Epoch [7312/10000] Avg train loss: 0.006343\n",
      "Epoch [7313/10000] Avg train loss: 0.006342\n",
      "Epoch [7314/10000] Avg train loss: 0.006341\n",
      "Epoch [7315/10000] Avg train loss: 0.006341\n",
      "Epoch [7316/10000] Avg train loss: 0.006340\n",
      "Epoch [7317/10000] Avg train loss: 0.006339\n",
      "Epoch [7318/10000] Avg train loss: 0.006338\n",
      "Epoch [7319/10000] Avg train loss: 0.006337\n",
      "Epoch [7320/10000] Avg train loss: 0.006336\n",
      "Epoch [7321/10000] Avg train loss: 0.006335\n",
      "Epoch [7322/10000] Avg train loss: 0.006335\n",
      "Epoch [7323/10000] Avg train loss: 0.006334\n",
      "Epoch [7324/10000] Avg train loss: 0.006333\n",
      "Epoch [7325/10000] Avg train loss: 0.006332\n",
      "Epoch [7326/10000] Avg train loss: 0.006331\n",
      "Epoch [7327/10000] Avg train loss: 0.006330\n",
      "Epoch [7328/10000] Avg train loss: 0.006329\n",
      "Epoch [7329/10000] Avg train loss: 0.006328\n",
      "Epoch [7330/10000] Avg train loss: 0.006328\n",
      "Epoch [7331/10000] Avg train loss: 0.006327\n",
      "Epoch [7332/10000] Avg train loss: 0.006326\n",
      "Epoch [7333/10000] Avg train loss: 0.006325\n",
      "Epoch [7334/10000] Avg train loss: 0.006324\n",
      "Epoch [7335/10000] Avg train loss: 0.006323\n",
      "Epoch [7336/10000] Avg train loss: 0.006322\n",
      "Epoch [7337/10000] Avg train loss: 0.006322\n",
      "Epoch [7338/10000] Avg train loss: 0.006321\n",
      "Epoch [7339/10000] Avg train loss: 0.006320\n",
      "Epoch [7340/10000] Avg train loss: 0.006319\n",
      "Epoch [7341/10000] Avg train loss: 0.006318\n",
      "Epoch [7342/10000] Avg train loss: 0.006317\n",
      "Epoch [7343/10000] Avg train loss: 0.006316\n",
      "Epoch [7344/10000] Avg train loss: 0.006316\n",
      "Epoch [7345/10000] Avg train loss: 0.006315\n",
      "Epoch [7346/10000] Avg train loss: 0.006314\n",
      "Epoch [7347/10000] Avg train loss: 0.006313\n",
      "Epoch [7348/10000] Avg train loss: 0.006312\n",
      "Epoch [7349/10000] Avg train loss: 0.006311\n",
      "Epoch [7350/10000] Avg train loss: 0.006310\n",
      "Epoch [7351/10000] Avg train loss: 0.006310\n",
      "Epoch [7352/10000] Avg train loss: 0.006309\n",
      "Epoch [7353/10000] Avg train loss: 0.006308\n",
      "Epoch [7354/10000] Avg train loss: 0.006307\n",
      "Epoch [7355/10000] Avg train loss: 0.006306\n",
      "Epoch [7356/10000] Avg train loss: 0.006305\n",
      "Epoch [7357/10000] Avg train loss: 0.006304\n",
      "Epoch [7358/10000] Avg train loss: 0.006304\n",
      "Epoch [7359/10000] Avg train loss: 0.006303\n",
      "Epoch [7360/10000] Avg train loss: 0.006302\n",
      "Epoch [7361/10000] Avg train loss: 0.006301\n",
      "Epoch [7362/10000] Avg train loss: 0.006300\n",
      "Epoch [7363/10000] Avg train loss: 0.006299\n",
      "Epoch [7364/10000] Avg train loss: 0.006298\n",
      "Epoch [7365/10000] Avg train loss: 0.006298\n",
      "Epoch [7366/10000] Avg train loss: 0.006297\n",
      "Epoch [7367/10000] Avg train loss: 0.006296\n",
      "Epoch [7368/10000] Avg train loss: 0.006295\n",
      "Epoch [7369/10000] Avg train loss: 0.006294\n",
      "Epoch [7370/10000] Avg train loss: 0.006293\n",
      "Epoch [7371/10000] Avg train loss: 0.006292\n",
      "Epoch [7372/10000] Avg train loss: 0.006292\n",
      "Epoch [7373/10000] Avg train loss: 0.006291\n",
      "Epoch [7374/10000] Avg train loss: 0.006290\n",
      "Epoch [7375/10000] Avg train loss: 0.006289\n",
      "Epoch [7376/10000] Avg train loss: 0.006288\n",
      "Epoch [7377/10000] Avg train loss: 0.006287\n",
      "Epoch [7378/10000] Avg train loss: 0.006286\n",
      "Epoch [7379/10000] Avg train loss: 0.006286\n",
      "Epoch [7380/10000] Avg train loss: 0.006285\n",
      "Epoch [7381/10000] Avg train loss: 0.006284\n",
      "Epoch [7382/10000] Avg train loss: 0.006283\n",
      "Epoch [7383/10000] Avg train loss: 0.006282\n",
      "Epoch [7384/10000] Avg train loss: 0.006281\n",
      "Epoch [7385/10000] Avg train loss: 0.006280\n",
      "Epoch [7386/10000] Avg train loss: 0.006280\n",
      "Epoch [7387/10000] Avg train loss: 0.006279\n",
      "Epoch [7388/10000] Avg train loss: 0.006278\n",
      "Epoch [7389/10000] Avg train loss: 0.006277\n",
      "Epoch [7390/10000] Avg train loss: 0.006276\n",
      "Epoch [7391/10000] Avg train loss: 0.006275\n",
      "Epoch [7392/10000] Avg train loss: 0.006275\n",
      "Epoch [7393/10000] Avg train loss: 0.006274\n",
      "Epoch [7394/10000] Avg train loss: 0.006273\n",
      "Epoch [7395/10000] Avg train loss: 0.006272\n",
      "Epoch [7396/10000] Avg train loss: 0.006271\n",
      "Epoch [7397/10000] Avg train loss: 0.006270\n",
      "Epoch [7398/10000] Avg train loss: 0.006269\n",
      "Epoch [7399/10000] Avg train loss: 0.006269\n",
      "Epoch [7400/10000] Avg train loss: 0.006268\n",
      "Epoch [7401/10000] Avg train loss: 0.006267\n",
      "Epoch [7402/10000] Avg train loss: 0.006266\n",
      "Epoch [7403/10000] Avg train loss: 0.006265\n",
      "Epoch [7404/10000] Avg train loss: 0.006264\n",
      "Epoch [7405/10000] Avg train loss: 0.006264\n",
      "Epoch [7406/10000] Avg train loss: 0.006263\n",
      "Epoch [7407/10000] Avg train loss: 0.006262\n",
      "Epoch [7408/10000] Avg train loss: 0.006261\n",
      "Epoch [7409/10000] Avg train loss: 0.006260\n",
      "Epoch [7410/10000] Avg train loss: 0.006259\n",
      "Epoch [7411/10000] Avg train loss: 0.006258\n",
      "Epoch [7412/10000] Avg train loss: 0.006258\n",
      "Epoch [7413/10000] Avg train loss: 0.006257\n",
      "Epoch [7414/10000] Avg train loss: 0.006256\n",
      "Epoch [7415/10000] Avg train loss: 0.006255\n",
      "Epoch [7416/10000] Avg train loss: 0.006254\n",
      "Epoch [7417/10000] Avg train loss: 0.006253\n",
      "Epoch [7418/10000] Avg train loss: 0.006253\n",
      "Epoch [7419/10000] Avg train loss: 0.006252\n",
      "Epoch [7420/10000] Avg train loss: 0.006251\n",
      "Epoch [7421/10000] Avg train loss: 0.006250\n",
      "Epoch [7422/10000] Avg train loss: 0.006249\n",
      "Epoch [7423/10000] Avg train loss: 0.006248\n",
      "Epoch [7424/10000] Avg train loss: 0.006247\n",
      "Epoch [7425/10000] Avg train loss: 0.006247\n",
      "Epoch [7426/10000] Avg train loss: 0.006246\n",
      "Epoch [7427/10000] Avg train loss: 0.006245\n",
      "Epoch [7428/10000] Avg train loss: 0.006244\n",
      "Epoch [7429/10000] Avg train loss: 0.006243\n",
      "Epoch [7430/10000] Avg train loss: 0.006242\n",
      "Epoch [7431/10000] Avg train loss: 0.006242\n",
      "Epoch [7432/10000] Avg train loss: 0.006241\n",
      "Epoch [7433/10000] Avg train loss: 0.006240\n",
      "Epoch [7434/10000] Avg train loss: 0.006239\n",
      "Epoch [7435/10000] Avg train loss: 0.006238\n",
      "Epoch [7436/10000] Avg train loss: 0.006237\n",
      "Epoch [7437/10000] Avg train loss: 0.006237\n",
      "Epoch [7438/10000] Avg train loss: 0.006236\n",
      "Epoch [7439/10000] Avg train loss: 0.006235\n",
      "Epoch [7440/10000] Avg train loss: 0.006234\n",
      "Epoch [7441/10000] Avg train loss: 0.006233\n",
      "Epoch [7442/10000] Avg train loss: 0.006232\n",
      "Epoch [7443/10000] Avg train loss: 0.006232\n",
      "Epoch [7444/10000] Avg train loss: 0.006231\n",
      "Epoch [7445/10000] Avg train loss: 0.006230\n",
      "Epoch [7446/10000] Avg train loss: 0.006229\n",
      "Epoch [7447/10000] Avg train loss: 0.006228\n",
      "Epoch [7448/10000] Avg train loss: 0.006227\n",
      "Epoch [7449/10000] Avg train loss: 0.006227\n",
      "Epoch [7450/10000] Avg train loss: 0.006226\n",
      "Epoch [7451/10000] Avg train loss: 0.006225\n",
      "Epoch [7452/10000] Avg train loss: 0.006224\n",
      "Epoch [7453/10000] Avg train loss: 0.006223\n",
      "Epoch [7454/10000] Avg train loss: 0.006222\n",
      "Epoch [7455/10000] Avg train loss: 0.006221\n",
      "Epoch [7456/10000] Avg train loss: 0.006221\n",
      "Epoch [7457/10000] Avg train loss: 0.006220\n",
      "Epoch [7458/10000] Avg train loss: 0.006219\n",
      "Epoch [7459/10000] Avg train loss: 0.006218\n",
      "Epoch [7460/10000] Avg train loss: 0.006217\n",
      "Epoch [7461/10000] Avg train loss: 0.006216\n",
      "Epoch [7462/10000] Avg train loss: 0.006216\n",
      "Epoch [7463/10000] Avg train loss: 0.006215\n",
      "Epoch [7464/10000] Avg train loss: 0.006214\n",
      "Epoch [7465/10000] Avg train loss: 0.006213\n",
      "Epoch [7466/10000] Avg train loss: 0.006212\n",
      "Epoch [7467/10000] Avg train loss: 0.006211\n",
      "Epoch [7468/10000] Avg train loss: 0.006211\n",
      "Epoch [7469/10000] Avg train loss: 0.006210\n",
      "Epoch [7470/10000] Avg train loss: 0.006209\n",
      "Epoch [7471/10000] Avg train loss: 0.006208\n",
      "Epoch [7472/10000] Avg train loss: 0.006207\n",
      "Epoch [7473/10000] Avg train loss: 0.006207\n",
      "Epoch [7474/10000] Avg train loss: 0.006206\n",
      "Epoch [7475/10000] Avg train loss: 0.006205\n",
      "Epoch [7476/10000] Avg train loss: 0.006204\n",
      "Epoch [7477/10000] Avg train loss: 0.006203\n",
      "Epoch [7478/10000] Avg train loss: 0.006202\n",
      "Epoch [7479/10000] Avg train loss: 0.006202\n",
      "Epoch [7480/10000] Avg train loss: 0.006201\n",
      "Epoch [7481/10000] Avg train loss: 0.006200\n",
      "Epoch [7482/10000] Avg train loss: 0.006199\n",
      "Epoch [7483/10000] Avg train loss: 0.006198\n",
      "Epoch [7484/10000] Avg train loss: 0.006197\n",
      "Epoch [7485/10000] Avg train loss: 0.006197\n",
      "Epoch [7486/10000] Avg train loss: 0.006196\n",
      "Epoch [7487/10000] Avg train loss: 0.006195\n",
      "Epoch [7488/10000] Avg train loss: 0.006194\n",
      "Epoch [7489/10000] Avg train loss: 0.006193\n",
      "Epoch [7490/10000] Avg train loss: 0.006192\n",
      "Epoch [7491/10000] Avg train loss: 0.006192\n",
      "Epoch [7492/10000] Avg train loss: 0.006191\n",
      "Epoch [7493/10000] Avg train loss: 0.006190\n",
      "Epoch [7494/10000] Avg train loss: 0.006189\n",
      "Epoch [7495/10000] Avg train loss: 0.006188\n",
      "Epoch [7496/10000] Avg train loss: 0.006187\n",
      "Epoch [7497/10000] Avg train loss: 0.006187\n",
      "Epoch [7498/10000] Avg train loss: 0.006186\n",
      "Epoch [7499/10000] Avg train loss: 0.006185\n",
      "Epoch [7500/10000] Avg train loss: 0.006184\n",
      "Epoch [7501/10000] Avg train loss: 0.006183\n",
      "Epoch [7502/10000] Avg train loss: 0.006183\n",
      "Epoch [7503/10000] Avg train loss: 0.006182\n",
      "Epoch [7504/10000] Avg train loss: 0.006181\n",
      "Epoch [7505/10000] Avg train loss: 0.006180\n",
      "Epoch [7506/10000] Avg train loss: 0.006179\n",
      "Epoch [7507/10000] Avg train loss: 0.006178\n",
      "Epoch [7508/10000] Avg train loss: 0.006178\n",
      "Epoch [7509/10000] Avg train loss: 0.006177\n",
      "Epoch [7510/10000] Avg train loss: 0.006176\n",
      "Epoch [7511/10000] Avg train loss: 0.006175\n",
      "Epoch [7512/10000] Avg train loss: 0.006174\n",
      "Epoch [7513/10000] Avg train loss: 0.006173\n",
      "Epoch [7514/10000] Avg train loss: 0.006173\n",
      "Epoch [7515/10000] Avg train loss: 0.006172\n",
      "Epoch [7516/10000] Avg train loss: 0.006171\n",
      "Epoch [7517/10000] Avg train loss: 0.006170\n",
      "Epoch [7518/10000] Avg train loss: 0.006169\n",
      "Epoch [7519/10000] Avg train loss: 0.006169\n",
      "Epoch [7520/10000] Avg train loss: 0.006168\n",
      "Epoch [7521/10000] Avg train loss: 0.006167\n",
      "Epoch [7522/10000] Avg train loss: 0.006166\n",
      "Epoch [7523/10000] Avg train loss: 0.006165\n",
      "Epoch [7524/10000] Avg train loss: 0.006164\n",
      "Epoch [7525/10000] Avg train loss: 0.006164\n",
      "Epoch [7526/10000] Avg train loss: 0.006163\n",
      "Epoch [7527/10000] Avg train loss: 0.006162\n",
      "Epoch [7528/10000] Avg train loss: 0.006161\n",
      "Epoch [7529/10000] Avg train loss: 0.006160\n",
      "Epoch [7530/10000] Avg train loss: 0.006160\n",
      "Epoch [7531/10000] Avg train loss: 0.006159\n",
      "Epoch [7532/10000] Avg train loss: 0.006158\n",
      "Epoch [7533/10000] Avg train loss: 0.006157\n",
      "Epoch [7534/10000] Avg train loss: 0.006156\n",
      "Epoch [7535/10000] Avg train loss: 0.006155\n",
      "Epoch [7536/10000] Avg train loss: 0.006155\n",
      "Epoch [7537/10000] Avg train loss: 0.006154\n",
      "Epoch [7538/10000] Avg train loss: 0.006153\n",
      "Epoch [7539/10000] Avg train loss: 0.006152\n",
      "Epoch [7540/10000] Avg train loss: 0.006151\n",
      "Epoch [7541/10000] Avg train loss: 0.006151\n",
      "Epoch [7542/10000] Avg train loss: 0.006150\n",
      "Epoch [7543/10000] Avg train loss: 0.006149\n",
      "Epoch [7544/10000] Avg train loss: 0.006148\n",
      "Epoch [7545/10000] Avg train loss: 0.006147\n",
      "Epoch [7546/10000] Avg train loss: 0.006146\n",
      "Epoch [7547/10000] Avg train loss: 0.006146\n",
      "Epoch [7548/10000] Avg train loss: 0.006145\n",
      "Epoch [7549/10000] Avg train loss: 0.006144\n",
      "Epoch [7550/10000] Avg train loss: 0.006143\n",
      "Epoch [7551/10000] Avg train loss: 0.006142\n",
      "Epoch [7552/10000] Avg train loss: 0.006142\n",
      "Epoch [7553/10000] Avg train loss: 0.006141\n",
      "Epoch [7554/10000] Avg train loss: 0.006140\n",
      "Epoch [7555/10000] Avg train loss: 0.006139\n",
      "Epoch [7556/10000] Avg train loss: 0.006138\n",
      "Epoch [7557/10000] Avg train loss: 0.006138\n",
      "Epoch [7558/10000] Avg train loss: 0.006137\n",
      "Epoch [7559/10000] Avg train loss: 0.006136\n",
      "Epoch [7560/10000] Avg train loss: 0.006135\n",
      "Epoch [7561/10000] Avg train loss: 0.006134\n",
      "Epoch [7562/10000] Avg train loss: 0.006133\n",
      "Epoch [7563/10000] Avg train loss: 0.006133\n",
      "Epoch [7564/10000] Avg train loss: 0.006132\n",
      "Epoch [7565/10000] Avg train loss: 0.006131\n",
      "Epoch [7566/10000] Avg train loss: 0.006130\n",
      "Epoch [7567/10000] Avg train loss: 0.006129\n",
      "Epoch [7568/10000] Avg train loss: 0.006129\n",
      "Epoch [7569/10000] Avg train loss: 0.006128\n",
      "Epoch [7570/10000] Avg train loss: 0.006127\n",
      "Epoch [7571/10000] Avg train loss: 0.006126\n",
      "Epoch [7572/10000] Avg train loss: 0.006125\n",
      "Epoch [7573/10000] Avg train loss: 0.006125\n",
      "Epoch [7574/10000] Avg train loss: 0.006124\n",
      "Epoch [7575/10000] Avg train loss: 0.006123\n",
      "Epoch [7576/10000] Avg train loss: 0.006122\n",
      "Epoch [7577/10000] Avg train loss: 0.006121\n",
      "Epoch [7578/10000] Avg train loss: 0.006121\n",
      "Epoch [7579/10000] Avg train loss: 0.006120\n",
      "Epoch [7580/10000] Avg train loss: 0.006119\n",
      "Epoch [7581/10000] Avg train loss: 0.006118\n",
      "Epoch [7582/10000] Avg train loss: 0.006117\n",
      "Epoch [7583/10000] Avg train loss: 0.006116\n",
      "Epoch [7584/10000] Avg train loss: 0.006116\n",
      "Epoch [7585/10000] Avg train loss: 0.006115\n",
      "Epoch [7586/10000] Avg train loss: 0.006114\n",
      "Epoch [7587/10000] Avg train loss: 0.006113\n",
      "Epoch [7588/10000] Avg train loss: 0.006112\n",
      "Epoch [7589/10000] Avg train loss: 0.006112\n",
      "Epoch [7590/10000] Avg train loss: 0.006111\n",
      "Epoch [7591/10000] Avg train loss: 0.006110\n",
      "Epoch [7592/10000] Avg train loss: 0.006109\n",
      "Epoch [7593/10000] Avg train loss: 0.006108\n",
      "Epoch [7594/10000] Avg train loss: 0.006108\n",
      "Epoch [7595/10000] Avg train loss: 0.006107\n",
      "Epoch [7596/10000] Avg train loss: 0.006106\n",
      "Epoch [7597/10000] Avg train loss: 0.006105\n",
      "Epoch [7598/10000] Avg train loss: 0.006104\n",
      "Epoch [7599/10000] Avg train loss: 0.006104\n",
      "Epoch [7600/10000] Avg train loss: 0.006103\n",
      "Epoch [7601/10000] Avg train loss: 0.006102\n",
      "Epoch [7602/10000] Avg train loss: 0.006101\n",
      "Epoch [7603/10000] Avg train loss: 0.006100\n",
      "Epoch [7604/10000] Avg train loss: 0.006100\n",
      "Epoch [7605/10000] Avg train loss: 0.006099\n",
      "Epoch [7606/10000] Avg train loss: 0.006098\n",
      "Epoch [7607/10000] Avg train loss: 0.006097\n",
      "Epoch [7608/10000] Avg train loss: 0.006096\n",
      "Epoch [7609/10000] Avg train loss: 0.006096\n",
      "Epoch [7610/10000] Avg train loss: 0.006095\n",
      "Epoch [7611/10000] Avg train loss: 0.006094\n",
      "Epoch [7612/10000] Avg train loss: 0.006093\n",
      "Epoch [7613/10000] Avg train loss: 0.006092\n",
      "Epoch [7614/10000] Avg train loss: 0.006092\n",
      "Epoch [7615/10000] Avg train loss: 0.006091\n",
      "Epoch [7616/10000] Avg train loss: 0.006090\n",
      "Epoch [7617/10000] Avg train loss: 0.006089\n",
      "Epoch [7618/10000] Avg train loss: 0.006088\n",
      "Epoch [7619/10000] Avg train loss: 0.006088\n",
      "Epoch [7620/10000] Avg train loss: 0.006087\n",
      "Epoch [7621/10000] Avg train loss: 0.006086\n",
      "Epoch [7622/10000] Avg train loss: 0.006085\n",
      "Epoch [7623/10000] Avg train loss: 0.006084\n",
      "Epoch [7624/10000] Avg train loss: 0.006084\n",
      "Epoch [7625/10000] Avg train loss: 0.006083\n",
      "Epoch [7626/10000] Avg train loss: 0.006082\n",
      "Epoch [7627/10000] Avg train loss: 0.006081\n",
      "Epoch [7628/10000] Avg train loss: 0.006080\n",
      "Epoch [7629/10000] Avg train loss: 0.006080\n",
      "Epoch [7630/10000] Avg train loss: 0.006079\n",
      "Epoch [7631/10000] Avg train loss: 0.006078\n",
      "Epoch [7632/10000] Avg train loss: 0.006077\n",
      "Epoch [7633/10000] Avg train loss: 0.006076\n",
      "Epoch [7634/10000] Avg train loss: 0.006076\n",
      "Epoch [7635/10000] Avg train loss: 0.006075\n",
      "Epoch [7636/10000] Avg train loss: 0.006074\n",
      "Epoch [7637/10000] Avg train loss: 0.006073\n",
      "Epoch [7638/10000] Avg train loss: 0.006072\n",
      "Epoch [7639/10000] Avg train loss: 0.006072\n",
      "Epoch [7640/10000] Avg train loss: 0.006071\n",
      "Epoch [7641/10000] Avg train loss: 0.006070\n",
      "Epoch [7642/10000] Avg train loss: 0.006069\n",
      "Epoch [7643/10000] Avg train loss: 0.006068\n",
      "Epoch [7644/10000] Avg train loss: 0.006068\n",
      "Epoch [7645/10000] Avg train loss: 0.006067\n",
      "Epoch [7646/10000] Avg train loss: 0.006066\n",
      "Epoch [7647/10000] Avg train loss: 0.006065\n",
      "Epoch [7648/10000] Avg train loss: 0.006064\n",
      "Epoch [7649/10000] Avg train loss: 0.006064\n",
      "Epoch [7650/10000] Avg train loss: 0.006063\n",
      "Epoch [7651/10000] Avg train loss: 0.006062\n",
      "Epoch [7652/10000] Avg train loss: 0.006061\n",
      "Epoch [7653/10000] Avg train loss: 0.006061\n",
      "Epoch [7654/10000] Avg train loss: 0.006060\n",
      "Epoch [7655/10000] Avg train loss: 0.006059\n",
      "Epoch [7656/10000] Avg train loss: 0.006058\n",
      "Epoch [7657/10000] Avg train loss: 0.006057\n",
      "Epoch [7658/10000] Avg train loss: 0.006057\n",
      "Epoch [7659/10000] Avg train loss: 0.006056\n",
      "Epoch [7660/10000] Avg train loss: 0.006055\n",
      "Epoch [7661/10000] Avg train loss: 0.006054\n",
      "Epoch [7662/10000] Avg train loss: 0.006053\n",
      "Epoch [7663/10000] Avg train loss: 0.006053\n",
      "Epoch [7664/10000] Avg train loss: 0.006052\n",
      "Epoch [7665/10000] Avg train loss: 0.006051\n",
      "Epoch [7666/10000] Avg train loss: 0.006050\n",
      "Epoch [7667/10000] Avg train loss: 0.006049\n",
      "Epoch [7668/10000] Avg train loss: 0.006049\n",
      "Epoch [7669/10000] Avg train loss: 0.006048\n",
      "Epoch [7670/10000] Avg train loss: 0.006047\n",
      "Epoch [7671/10000] Avg train loss: 0.006046\n",
      "Epoch [7672/10000] Avg train loss: 0.006046\n",
      "Epoch [7673/10000] Avg train loss: 0.006045\n",
      "Epoch [7674/10000] Avg train loss: 0.006044\n",
      "Epoch [7675/10000] Avg train loss: 0.006043\n",
      "Epoch [7676/10000] Avg train loss: 0.006042\n",
      "Epoch [7677/10000] Avg train loss: 0.006042\n",
      "Epoch [7678/10000] Avg train loss: 0.006041\n",
      "Epoch [7679/10000] Avg train loss: 0.006040\n",
      "Epoch [7680/10000] Avg train loss: 0.006039\n",
      "Epoch [7681/10000] Avg train loss: 0.006038\n",
      "Epoch [7682/10000] Avg train loss: 0.006038\n",
      "Epoch [7683/10000] Avg train loss: 0.006037\n",
      "Epoch [7684/10000] Avg train loss: 0.006036\n",
      "Epoch [7685/10000] Avg train loss: 0.006035\n",
      "Epoch [7686/10000] Avg train loss: 0.006035\n",
      "Epoch [7687/10000] Avg train loss: 0.006034\n",
      "Epoch [7688/10000] Avg train loss: 0.006033\n",
      "Epoch [7689/10000] Avg train loss: 0.006032\n",
      "Epoch [7690/10000] Avg train loss: 0.006031\n",
      "Epoch [7691/10000] Avg train loss: 0.006031\n",
      "Epoch [7692/10000] Avg train loss: 0.006030\n",
      "Epoch [7693/10000] Avg train loss: 0.006029\n",
      "Epoch [7694/10000] Avg train loss: 0.006028\n",
      "Epoch [7695/10000] Avg train loss: 0.006027\n",
      "Epoch [7696/10000] Avg train loss: 0.006027\n",
      "Epoch [7697/10000] Avg train loss: 0.006026\n",
      "Epoch [7698/10000] Avg train loss: 0.006025\n",
      "Epoch [7699/10000] Avg train loss: 0.006024\n",
      "Epoch [7700/10000] Avg train loss: 0.006024\n",
      "Epoch [7701/10000] Avg train loss: 0.006023\n",
      "Epoch [7702/10000] Avg train loss: 0.006022\n",
      "Epoch [7703/10000] Avg train loss: 0.006021\n",
      "Epoch [7704/10000] Avg train loss: 0.006020\n",
      "Epoch [7705/10000] Avg train loss: 0.006020\n",
      "Epoch [7706/10000] Avg train loss: 0.006019\n",
      "Epoch [7707/10000] Avg train loss: 0.006018\n",
      "Epoch [7708/10000] Avg train loss: 0.006017\n",
      "Epoch [7709/10000] Avg train loss: 0.006017\n",
      "Epoch [7710/10000] Avg train loss: 0.006016\n",
      "Epoch [7711/10000] Avg train loss: 0.006015\n",
      "Epoch [7712/10000] Avg train loss: 0.006014\n",
      "Epoch [7713/10000] Avg train loss: 0.006013\n",
      "Epoch [7714/10000] Avg train loss: 0.006013\n",
      "Epoch [7715/10000] Avg train loss: 0.006012\n",
      "Epoch [7716/10000] Avg train loss: 0.006011\n",
      "Epoch [7717/10000] Avg train loss: 0.006010\n",
      "Epoch [7718/10000] Avg train loss: 0.006009\n",
      "Epoch [7719/10000] Avg train loss: 0.006009\n",
      "Epoch [7720/10000] Avg train loss: 0.006008\n",
      "Epoch [7721/10000] Avg train loss: 0.006007\n",
      "Epoch [7722/10000] Avg train loss: 0.006006\n",
      "Epoch [7723/10000] Avg train loss: 0.006006\n",
      "Epoch [7724/10000] Avg train loss: 0.006005\n",
      "Epoch [7725/10000] Avg train loss: 0.006004\n",
      "Epoch [7726/10000] Avg train loss: 0.006003\n",
      "Epoch [7727/10000] Avg train loss: 0.006002\n",
      "Epoch [7728/10000] Avg train loss: 0.006002\n",
      "Epoch [7729/10000] Avg train loss: 0.006001\n",
      "Epoch [7730/10000] Avg train loss: 0.006000\n",
      "Epoch [7731/10000] Avg train loss: 0.005999\n",
      "Epoch [7732/10000] Avg train loss: 0.005999\n",
      "Epoch [7733/10000] Avg train loss: 0.005998\n",
      "Epoch [7734/10000] Avg train loss: 0.005997\n",
      "Epoch [7735/10000] Avg train loss: 0.005996\n",
      "Epoch [7736/10000] Avg train loss: 0.005996\n",
      "Epoch [7737/10000] Avg train loss: 0.005995\n",
      "Epoch [7738/10000] Avg train loss: 0.005994\n",
      "Epoch [7739/10000] Avg train loss: 0.005993\n",
      "Epoch [7740/10000] Avg train loss: 0.005992\n",
      "Epoch [7741/10000] Avg train loss: 0.005992\n",
      "Epoch [7742/10000] Avg train loss: 0.005991\n",
      "Epoch [7743/10000] Avg train loss: 0.005990\n",
      "Epoch [7744/10000] Avg train loss: 0.005989\n",
      "Epoch [7745/10000] Avg train loss: 0.005989\n",
      "Epoch [7746/10000] Avg train loss: 0.005988\n",
      "Epoch [7747/10000] Avg train loss: 0.005987\n",
      "Epoch [7748/10000] Avg train loss: 0.005986\n",
      "Epoch [7749/10000] Avg train loss: 0.005985\n",
      "Epoch [7750/10000] Avg train loss: 0.005985\n",
      "Epoch [7751/10000] Avg train loss: 0.005984\n",
      "Epoch [7752/10000] Avg train loss: 0.005983\n",
      "Epoch [7753/10000] Avg train loss: 0.005982\n",
      "Epoch [7754/10000] Avg train loss: 0.005982\n",
      "Epoch [7755/10000] Avg train loss: 0.005981\n",
      "Epoch [7756/10000] Avg train loss: 0.005980\n",
      "Epoch [7757/10000] Avg train loss: 0.005979\n",
      "Epoch [7758/10000] Avg train loss: 0.005979\n",
      "Epoch [7759/10000] Avg train loss: 0.005978\n",
      "Epoch [7760/10000] Avg train loss: 0.005977\n",
      "Epoch [7761/10000] Avg train loss: 0.005976\n",
      "Epoch [7762/10000] Avg train loss: 0.005975\n",
      "Epoch [7763/10000] Avg train loss: 0.005975\n",
      "Epoch [7764/10000] Avg train loss: 0.005974\n",
      "Epoch [7765/10000] Avg train loss: 0.005973\n",
      "Epoch [7766/10000] Avg train loss: 0.005972\n",
      "Epoch [7767/10000] Avg train loss: 0.005972\n",
      "Epoch [7768/10000] Avg train loss: 0.005971\n",
      "Epoch [7769/10000] Avg train loss: 0.005970\n",
      "Epoch [7770/10000] Avg train loss: 0.005969\n",
      "Epoch [7771/10000] Avg train loss: 0.005969\n",
      "Epoch [7772/10000] Avg train loss: 0.005968\n",
      "Epoch [7773/10000] Avg train loss: 0.005967\n",
      "Epoch [7774/10000] Avg train loss: 0.005966\n",
      "Epoch [7775/10000] Avg train loss: 0.005965\n",
      "Epoch [7776/10000] Avg train loss: 0.005965\n",
      "Epoch [7777/10000] Avg train loss: 0.005964\n",
      "Epoch [7778/10000] Avg train loss: 0.005963\n",
      "Epoch [7779/10000] Avg train loss: 0.005962\n",
      "Epoch [7780/10000] Avg train loss: 0.005962\n",
      "Epoch [7781/10000] Avg train loss: 0.005961\n",
      "Epoch [7782/10000] Avg train loss: 0.005960\n",
      "Epoch [7783/10000] Avg train loss: 0.005959\n",
      "Epoch [7784/10000] Avg train loss: 0.005959\n",
      "Epoch [7785/10000] Avg train loss: 0.005958\n",
      "Epoch [7786/10000] Avg train loss: 0.005957\n",
      "Epoch [7787/10000] Avg train loss: 0.005956\n",
      "Epoch [7788/10000] Avg train loss: 0.005955\n",
      "Epoch [7789/10000] Avg train loss: 0.005955\n",
      "Epoch [7790/10000] Avg train loss: 0.005954\n",
      "Epoch [7791/10000] Avg train loss: 0.005953\n",
      "Epoch [7792/10000] Avg train loss: 0.005952\n",
      "Epoch [7793/10000] Avg train loss: 0.005952\n",
      "Epoch [7794/10000] Avg train loss: 0.005951\n",
      "Epoch [7795/10000] Avg train loss: 0.005950\n",
      "Epoch [7796/10000] Avg train loss: 0.005949\n",
      "Epoch [7797/10000] Avg train loss: 0.005949\n",
      "Epoch [7798/10000] Avg train loss: 0.005948\n",
      "Epoch [7799/10000] Avg train loss: 0.005947\n",
      "Epoch [7800/10000] Avg train loss: 0.005946\n",
      "Epoch [7801/10000] Avg train loss: 0.005946\n",
      "Epoch [7802/10000] Avg train loss: 0.005945\n",
      "Epoch [7803/10000] Avg train loss: 0.005944\n",
      "Epoch [7804/10000] Avg train loss: 0.005943\n",
      "Epoch [7805/10000] Avg train loss: 0.005943\n",
      "Epoch [7806/10000] Avg train loss: 0.005942\n",
      "Epoch [7807/10000] Avg train loss: 0.005941\n",
      "Epoch [7808/10000] Avg train loss: 0.005940\n",
      "Epoch [7809/10000] Avg train loss: 0.005939\n",
      "Epoch [7810/10000] Avg train loss: 0.005939\n",
      "Epoch [7811/10000] Avg train loss: 0.005938\n",
      "Epoch [7812/10000] Avg train loss: 0.005937\n",
      "Epoch [7813/10000] Avg train loss: 0.005936\n",
      "Epoch [7814/10000] Avg train loss: 0.005936\n",
      "Epoch [7815/10000] Avg train loss: 0.005935\n",
      "Epoch [7816/10000] Avg train loss: 0.005934\n",
      "Epoch [7817/10000] Avg train loss: 0.005933\n",
      "Epoch [7818/10000] Avg train loss: 0.005933\n",
      "Epoch [7819/10000] Avg train loss: 0.005932\n",
      "Epoch [7820/10000] Avg train loss: 0.005931\n",
      "Epoch [7821/10000] Avg train loss: 0.005930\n",
      "Epoch [7822/10000] Avg train loss: 0.005930\n",
      "Epoch [7823/10000] Avg train loss: 0.005929\n",
      "Epoch [7824/10000] Avg train loss: 0.005928\n",
      "Epoch [7825/10000] Avg train loss: 0.005927\n",
      "Epoch [7826/10000] Avg train loss: 0.005927\n",
      "Epoch [7827/10000] Avg train loss: 0.005926\n",
      "Epoch [7828/10000] Avg train loss: 0.005925\n",
      "Epoch [7829/10000] Avg train loss: 0.005924\n",
      "Epoch [7830/10000] Avg train loss: 0.005924\n",
      "Epoch [7831/10000] Avg train loss: 0.005923\n",
      "Epoch [7832/10000] Avg train loss: 0.005922\n",
      "Epoch [7833/10000] Avg train loss: 0.005921\n",
      "Epoch [7834/10000] Avg train loss: 0.005921\n",
      "Epoch [7835/10000] Avg train loss: 0.005920\n",
      "Epoch [7836/10000] Avg train loss: 0.005919\n",
      "Epoch [7837/10000] Avg train loss: 0.005918\n",
      "Epoch [7838/10000] Avg train loss: 0.005917\n",
      "Epoch [7839/10000] Avg train loss: 0.005917\n",
      "Epoch [7840/10000] Avg train loss: 0.005916\n",
      "Epoch [7841/10000] Avg train loss: 0.005915\n",
      "Epoch [7842/10000] Avg train loss: 0.005914\n",
      "Epoch [7843/10000] Avg train loss: 0.005914\n",
      "Epoch [7844/10000] Avg train loss: 0.005913\n",
      "Epoch [7845/10000] Avg train loss: 0.005912\n",
      "Epoch [7846/10000] Avg train loss: 0.005911\n",
      "Epoch [7847/10000] Avg train loss: 0.005911\n",
      "Epoch [7848/10000] Avg train loss: 0.005910\n",
      "Epoch [7849/10000] Avg train loss: 0.005909\n",
      "Epoch [7850/10000] Avg train loss: 0.005908\n",
      "Epoch [7851/10000] Avg train loss: 0.005908\n",
      "Epoch [7852/10000] Avg train loss: 0.005907\n",
      "Epoch [7853/10000] Avg train loss: 0.005906\n",
      "Epoch [7854/10000] Avg train loss: 0.005905\n",
      "Epoch [7855/10000] Avg train loss: 0.005905\n",
      "Epoch [7856/10000] Avg train loss: 0.005904\n",
      "Epoch [7857/10000] Avg train loss: 0.005903\n",
      "Epoch [7858/10000] Avg train loss: 0.005902\n",
      "Epoch [7859/10000] Avg train loss: 0.005902\n",
      "Epoch [7860/10000] Avg train loss: 0.005901\n",
      "Epoch [7861/10000] Avg train loss: 0.005900\n",
      "Epoch [7862/10000] Avg train loss: 0.005899\n",
      "Epoch [7863/10000] Avg train loss: 0.005899\n",
      "Epoch [7864/10000] Avg train loss: 0.005898\n",
      "Epoch [7865/10000] Avg train loss: 0.005897\n",
      "Epoch [7866/10000] Avg train loss: 0.005896\n",
      "Epoch [7867/10000] Avg train loss: 0.005896\n",
      "Epoch [7868/10000] Avg train loss: 0.005895\n",
      "Epoch [7869/10000] Avg train loss: 0.005894\n",
      "Epoch [7870/10000] Avg train loss: 0.005893\n",
      "Epoch [7871/10000] Avg train loss: 0.005893\n",
      "Epoch [7872/10000] Avg train loss: 0.005892\n",
      "Epoch [7873/10000] Avg train loss: 0.005891\n",
      "Epoch [7874/10000] Avg train loss: 0.005890\n",
      "Epoch [7875/10000] Avg train loss: 0.005890\n",
      "Epoch [7876/10000] Avg train loss: 0.005889\n",
      "Epoch [7877/10000] Avg train loss: 0.005888\n",
      "Epoch [7878/10000] Avg train loss: 0.005887\n",
      "Epoch [7879/10000] Avg train loss: 0.005887\n",
      "Epoch [7880/10000] Avg train loss: 0.005886\n",
      "Epoch [7881/10000] Avg train loss: 0.005885\n",
      "Epoch [7882/10000] Avg train loss: 0.005884\n",
      "Epoch [7883/10000] Avg train loss: 0.005884\n",
      "Epoch [7884/10000] Avg train loss: 0.005883\n",
      "Epoch [7885/10000] Avg train loss: 0.005882\n",
      "Epoch [7886/10000] Avg train loss: 0.005881\n",
      "Epoch [7887/10000] Avg train loss: 0.005881\n",
      "Epoch [7888/10000] Avg train loss: 0.005880\n",
      "Epoch [7889/10000] Avg train loss: 0.005879\n",
      "Epoch [7890/10000] Avg train loss: 0.005878\n",
      "Epoch [7891/10000] Avg train loss: 0.005878\n",
      "Epoch [7892/10000] Avg train loss: 0.005877\n",
      "Epoch [7893/10000] Avg train loss: 0.005876\n",
      "Epoch [7894/10000] Avg train loss: 0.005876\n",
      "Epoch [7895/10000] Avg train loss: 0.005875\n",
      "Epoch [7896/10000] Avg train loss: 0.005874\n",
      "Epoch [7897/10000] Avg train loss: 0.005873\n",
      "Epoch [7898/10000] Avg train loss: 0.005873\n",
      "Epoch [7899/10000] Avg train loss: 0.005872\n",
      "Epoch [7900/10000] Avg train loss: 0.005871\n",
      "Epoch [7901/10000] Avg train loss: 0.005870\n",
      "Epoch [7902/10000] Avg train loss: 0.005870\n",
      "Epoch [7903/10000] Avg train loss: 0.005869\n",
      "Epoch [7904/10000] Avg train loss: 0.005868\n",
      "Epoch [7905/10000] Avg train loss: 0.005867\n",
      "Epoch [7906/10000] Avg train loss: 0.005867\n",
      "Epoch [7907/10000] Avg train loss: 0.005866\n",
      "Epoch [7908/10000] Avg train loss: 0.005865\n",
      "Epoch [7909/10000] Avg train loss: 0.005864\n",
      "Epoch [7910/10000] Avg train loss: 0.005864\n",
      "Epoch [7911/10000] Avg train loss: 0.005863\n",
      "Epoch [7912/10000] Avg train loss: 0.005862\n",
      "Epoch [7913/10000] Avg train loss: 0.005861\n",
      "Epoch [7914/10000] Avg train loss: 0.005861\n",
      "Epoch [7915/10000] Avg train loss: 0.005860\n",
      "Epoch [7916/10000] Avg train loss: 0.005859\n",
      "Epoch [7917/10000] Avg train loss: 0.005858\n",
      "Epoch [7918/10000] Avg train loss: 0.005858\n",
      "Epoch [7919/10000] Avg train loss: 0.005857\n",
      "Epoch [7920/10000] Avg train loss: 0.005856\n",
      "Epoch [7921/10000] Avg train loss: 0.005855\n",
      "Epoch [7922/10000] Avg train loss: 0.005855\n",
      "Epoch [7923/10000] Avg train loss: 0.005854\n",
      "Epoch [7924/10000] Avg train loss: 0.005853\n",
      "Epoch [7925/10000] Avg train loss: 0.005853\n",
      "Epoch [7926/10000] Avg train loss: 0.005852\n",
      "Epoch [7927/10000] Avg train loss: 0.005851\n",
      "Epoch [7928/10000] Avg train loss: 0.005850\n",
      "Epoch [7929/10000] Avg train loss: 0.005850\n",
      "Epoch [7930/10000] Avg train loss: 0.005849\n",
      "Epoch [7931/10000] Avg train loss: 0.005848\n",
      "Epoch [7932/10000] Avg train loss: 0.005847\n",
      "Epoch [7933/10000] Avg train loss: 0.005847\n",
      "Epoch [7934/10000] Avg train loss: 0.005846\n",
      "Epoch [7935/10000] Avg train loss: 0.005845\n",
      "Epoch [7936/10000] Avg train loss: 0.005844\n",
      "Epoch [7937/10000] Avg train loss: 0.005844\n",
      "Epoch [7938/10000] Avg train loss: 0.005843\n",
      "Epoch [7939/10000] Avg train loss: 0.005842\n",
      "Epoch [7940/10000] Avg train loss: 0.005841\n",
      "Epoch [7941/10000] Avg train loss: 0.005841\n",
      "Epoch [7942/10000] Avg train loss: 0.005840\n",
      "Epoch [7943/10000] Avg train loss: 0.005839\n",
      "Epoch [7944/10000] Avg train loss: 0.005839\n",
      "Epoch [7945/10000] Avg train loss: 0.005838\n",
      "Epoch [7946/10000] Avg train loss: 0.005837\n",
      "Epoch [7947/10000] Avg train loss: 0.005836\n",
      "Epoch [7948/10000] Avg train loss: 0.005836\n",
      "Epoch [7949/10000] Avg train loss: 0.005835\n",
      "Epoch [7950/10000] Avg train loss: 0.005834\n",
      "Epoch [7951/10000] Avg train loss: 0.005833\n",
      "Epoch [7952/10000] Avg train loss: 0.005833\n",
      "Epoch [7953/10000] Avg train loss: 0.005832\n",
      "Epoch [7954/10000] Avg train loss: 0.005831\n",
      "Epoch [7955/10000] Avg train loss: 0.005830\n",
      "Epoch [7956/10000] Avg train loss: 0.005830\n",
      "Epoch [7957/10000] Avg train loss: 0.005829\n",
      "Epoch [7958/10000] Avg train loss: 0.005828\n",
      "Epoch [7959/10000] Avg train loss: 0.005828\n",
      "Epoch [7960/10000] Avg train loss: 0.005827\n",
      "Epoch [7961/10000] Avg train loss: 0.005826\n",
      "Epoch [7962/10000] Avg train loss: 0.005825\n",
      "Epoch [7963/10000] Avg train loss: 0.005825\n",
      "Epoch [7964/10000] Avg train loss: 0.005824\n",
      "Epoch [7965/10000] Avg train loss: 0.005823\n",
      "Epoch [7966/10000] Avg train loss: 0.005822\n",
      "Epoch [7967/10000] Avg train loss: 0.005822\n",
      "Epoch [7968/10000] Avg train loss: 0.005821\n",
      "Epoch [7969/10000] Avg train loss: 0.005820\n",
      "Epoch [7970/10000] Avg train loss: 0.005819\n",
      "Epoch [7971/10000] Avg train loss: 0.005819\n",
      "Epoch [7972/10000] Avg train loss: 0.005818\n",
      "Epoch [7973/10000] Avg train loss: 0.005817\n",
      "Epoch [7974/10000] Avg train loss: 0.005817\n",
      "Epoch [7975/10000] Avg train loss: 0.005816\n",
      "Epoch [7976/10000] Avg train loss: 0.005815\n",
      "Epoch [7977/10000] Avg train loss: 0.005814\n",
      "Epoch [7978/10000] Avg train loss: 0.005814\n",
      "Epoch [7979/10000] Avg train loss: 0.005813\n",
      "Epoch [7980/10000] Avg train loss: 0.005812\n",
      "Epoch [7981/10000] Avg train loss: 0.005811\n",
      "Epoch [7982/10000] Avg train loss: 0.005811\n",
      "Epoch [7983/10000] Avg train loss: 0.005810\n",
      "Epoch [7984/10000] Avg train loss: 0.005809\n",
      "Epoch [7985/10000] Avg train loss: 0.005809\n",
      "Epoch [7986/10000] Avg train loss: 0.005808\n",
      "Epoch [7987/10000] Avg train loss: 0.005807\n",
      "Epoch [7988/10000] Avg train loss: 0.005806\n",
      "Epoch [7989/10000] Avg train loss: 0.005806\n",
      "Epoch [7990/10000] Avg train loss: 0.005805\n",
      "Epoch [7991/10000] Avg train loss: 0.005804\n",
      "Epoch [7992/10000] Avg train loss: 0.005803\n",
      "Epoch [7993/10000] Avg train loss: 0.005803\n",
      "Epoch [7994/10000] Avg train loss: 0.005802\n",
      "Epoch [7995/10000] Avg train loss: 0.005801\n",
      "Epoch [7996/10000] Avg train loss: 0.005801\n",
      "Epoch [7997/10000] Avg train loss: 0.005800\n",
      "Epoch [7998/10000] Avg train loss: 0.005799\n",
      "Epoch [7999/10000] Avg train loss: 0.005798\n",
      "Epoch [8000/10000] Avg train loss: 0.005798\n",
      "Epoch [8001/10000] Avg train loss: 0.005797\n",
      "Epoch [8002/10000] Avg train loss: 0.005796\n",
      "Epoch [8003/10000] Avg train loss: 0.005795\n",
      "Epoch [8004/10000] Avg train loss: 0.005795\n",
      "Epoch [8005/10000] Avg train loss: 0.005794\n",
      "Epoch [8006/10000] Avg train loss: 0.005793\n",
      "Epoch [8007/10000] Avg train loss: 0.005793\n",
      "Epoch [8008/10000] Avg train loss: 0.005792\n",
      "Epoch [8009/10000] Avg train loss: 0.005791\n",
      "Epoch [8010/10000] Avg train loss: 0.005790\n",
      "Epoch [8011/10000] Avg train loss: 0.005790\n",
      "Epoch [8012/10000] Avg train loss: 0.005789\n",
      "Epoch [8013/10000] Avg train loss: 0.005788\n",
      "Epoch [8014/10000] Avg train loss: 0.005788\n",
      "Epoch [8015/10000] Avg train loss: 0.005787\n",
      "Epoch [8016/10000] Avg train loss: 0.005786\n",
      "Epoch [8017/10000] Avg train loss: 0.005785\n",
      "Epoch [8018/10000] Avg train loss: 0.005785\n",
      "Epoch [8019/10000] Avg train loss: 0.005784\n",
      "Epoch [8020/10000] Avg train loss: 0.005783\n",
      "Epoch [8021/10000] Avg train loss: 0.005782\n",
      "Epoch [8022/10000] Avg train loss: 0.005782\n",
      "Epoch [8023/10000] Avg train loss: 0.005781\n",
      "Epoch [8024/10000] Avg train loss: 0.005780\n",
      "Epoch [8025/10000] Avg train loss: 0.005780\n",
      "Epoch [8026/10000] Avg train loss: 0.005779\n",
      "Epoch [8027/10000] Avg train loss: 0.005778\n",
      "Epoch [8028/10000] Avg train loss: 0.005777\n",
      "Epoch [8029/10000] Avg train loss: 0.005777\n",
      "Epoch [8030/10000] Avg train loss: 0.005776\n",
      "Epoch [8031/10000] Avg train loss: 0.005775\n",
      "Epoch [8032/10000] Avg train loss: 0.005775\n",
      "Epoch [8033/10000] Avg train loss: 0.005774\n",
      "Epoch [8034/10000] Avg train loss: 0.005773\n",
      "Epoch [8035/10000] Avg train loss: 0.005772\n",
      "Epoch [8036/10000] Avg train loss: 0.005772\n",
      "Epoch [8037/10000] Avg train loss: 0.005771\n",
      "Epoch [8038/10000] Avg train loss: 0.005770\n",
      "Epoch [8039/10000] Avg train loss: 0.005770\n",
      "Epoch [8040/10000] Avg train loss: 0.005769\n",
      "Epoch [8041/10000] Avg train loss: 0.005768\n",
      "Epoch [8042/10000] Avg train loss: 0.005767\n",
      "Epoch [8043/10000] Avg train loss: 0.005767\n",
      "Epoch [8044/10000] Avg train loss: 0.005766\n",
      "Epoch [8045/10000] Avg train loss: 0.005765\n",
      "Epoch [8046/10000] Avg train loss: 0.005765\n",
      "Epoch [8047/10000] Avg train loss: 0.005764\n",
      "Epoch [8048/10000] Avg train loss: 0.005763\n",
      "Epoch [8049/10000] Avg train loss: 0.005762\n",
      "Epoch [8050/10000] Avg train loss: 0.005762\n",
      "Epoch [8051/10000] Avg train loss: 0.005761\n",
      "Epoch [8052/10000] Avg train loss: 0.005760\n",
      "Epoch [8053/10000] Avg train loss: 0.005760\n",
      "Epoch [8054/10000] Avg train loss: 0.005759\n",
      "Epoch [8055/10000] Avg train loss: 0.005758\n",
      "Epoch [8056/10000] Avg train loss: 0.005757\n",
      "Epoch [8057/10000] Avg train loss: 0.005757\n",
      "Epoch [8058/10000] Avg train loss: 0.005756\n",
      "Epoch [8059/10000] Avg train loss: 0.005755\n",
      "Epoch [8060/10000] Avg train loss: 0.005754\n",
      "Epoch [8061/10000] Avg train loss: 0.005754\n",
      "Epoch [8062/10000] Avg train loss: 0.005753\n",
      "Epoch [8063/10000] Avg train loss: 0.005752\n",
      "Epoch [8064/10000] Avg train loss: 0.005752\n",
      "Epoch [8065/10000] Avg train loss: 0.005751\n",
      "Epoch [8066/10000] Avg train loss: 0.005750\n",
      "Epoch [8067/10000] Avg train loss: 0.005750\n",
      "Epoch [8068/10000] Avg train loss: 0.005749\n",
      "Epoch [8069/10000] Avg train loss: 0.005748\n",
      "Epoch [8070/10000] Avg train loss: 0.005747\n",
      "Epoch [8071/10000] Avg train loss: 0.005747\n",
      "Epoch [8072/10000] Avg train loss: 0.005746\n",
      "Epoch [8073/10000] Avg train loss: 0.005745\n",
      "Epoch [8074/10000] Avg train loss: 0.005745\n",
      "Epoch [8075/10000] Avg train loss: 0.005744\n",
      "Epoch [8076/10000] Avg train loss: 0.005743\n",
      "Epoch [8077/10000] Avg train loss: 0.005742\n",
      "Epoch [8078/10000] Avg train loss: 0.005742\n",
      "Epoch [8079/10000] Avg train loss: 0.005741\n",
      "Epoch [8080/10000] Avg train loss: 0.005740\n",
      "Epoch [8081/10000] Avg train loss: 0.005740\n",
      "Epoch [8082/10000] Avg train loss: 0.005739\n",
      "Epoch [8083/10000] Avg train loss: 0.005738\n",
      "Epoch [8084/10000] Avg train loss: 0.005737\n",
      "Epoch [8085/10000] Avg train loss: 0.005737\n",
      "Epoch [8086/10000] Avg train loss: 0.005736\n",
      "Epoch [8087/10000] Avg train loss: 0.005735\n",
      "Epoch [8088/10000] Avg train loss: 0.005735\n",
      "Epoch [8089/10000] Avg train loss: 0.005734\n",
      "Epoch [8090/10000] Avg train loss: 0.005733\n",
      "Epoch [8091/10000] Avg train loss: 0.005732\n",
      "Epoch [8092/10000] Avg train loss: 0.005732\n",
      "Epoch [8093/10000] Avg train loss: 0.005731\n",
      "Epoch [8094/10000] Avg train loss: 0.005730\n",
      "Epoch [8095/10000] Avg train loss: 0.005730\n",
      "Epoch [8096/10000] Avg train loss: 0.005729\n",
      "Epoch [8097/10000] Avg train loss: 0.005728\n",
      "Epoch [8098/10000] Avg train loss: 0.005727\n",
      "Epoch [8099/10000] Avg train loss: 0.005727\n",
      "Epoch [8100/10000] Avg train loss: 0.005726\n",
      "Epoch [8101/10000] Avg train loss: 0.005725\n",
      "Epoch [8102/10000] Avg train loss: 0.005725\n",
      "Epoch [8103/10000] Avg train loss: 0.005724\n",
      "Epoch [8104/10000] Avg train loss: 0.005723\n",
      "Epoch [8105/10000] Avg train loss: 0.005723\n",
      "Epoch [8106/10000] Avg train loss: 0.005722\n",
      "Epoch [8107/10000] Avg train loss: 0.005721\n",
      "Epoch [8108/10000] Avg train loss: 0.005720\n",
      "Epoch [8109/10000] Avg train loss: 0.005720\n",
      "Epoch [8110/10000] Avg train loss: 0.005719\n",
      "Epoch [8111/10000] Avg train loss: 0.005718\n",
      "Epoch [8112/10000] Avg train loss: 0.005718\n",
      "Epoch [8113/10000] Avg train loss: 0.005717\n",
      "Epoch [8114/10000] Avg train loss: 0.005716\n",
      "Epoch [8115/10000] Avg train loss: 0.005715\n",
      "Epoch [8116/10000] Avg train loss: 0.005715\n",
      "Epoch [8117/10000] Avg train loss: 0.005714\n",
      "Epoch [8118/10000] Avg train loss: 0.005713\n",
      "Epoch [8119/10000] Avg train loss: 0.005713\n",
      "Epoch [8120/10000] Avg train loss: 0.005712\n",
      "Epoch [8121/10000] Avg train loss: 0.005711\n",
      "Epoch [8122/10000] Avg train loss: 0.005711\n",
      "Epoch [8123/10000] Avg train loss: 0.005710\n",
      "Epoch [8124/10000] Avg train loss: 0.005709\n",
      "Epoch [8125/10000] Avg train loss: 0.005708\n",
      "Epoch [8126/10000] Avg train loss: 0.005708\n",
      "Epoch [8127/10000] Avg train loss: 0.005707\n",
      "Epoch [8128/10000] Avg train loss: 0.005706\n",
      "Epoch [8129/10000] Avg train loss: 0.005706\n",
      "Epoch [8130/10000] Avg train loss: 0.005705\n",
      "Epoch [8131/10000] Avg train loss: 0.005704\n",
      "Epoch [8132/10000] Avg train loss: 0.005704\n",
      "Epoch [8133/10000] Avg train loss: 0.005703\n",
      "Epoch [8134/10000] Avg train loss: 0.005702\n",
      "Epoch [8135/10000] Avg train loss: 0.005701\n",
      "Epoch [8136/10000] Avg train loss: 0.005701\n",
      "Epoch [8137/10000] Avg train loss: 0.005700\n",
      "Epoch [8138/10000] Avg train loss: 0.005699\n",
      "Epoch [8139/10000] Avg train loss: 0.005699\n",
      "Epoch [8140/10000] Avg train loss: 0.005698\n",
      "Epoch [8141/10000] Avg train loss: 0.005697\n",
      "Epoch [8142/10000] Avg train loss: 0.005697\n",
      "Epoch [8143/10000] Avg train loss: 0.005696\n",
      "Epoch [8144/10000] Avg train loss: 0.005695\n",
      "Epoch [8145/10000] Avg train loss: 0.005694\n",
      "Epoch [8146/10000] Avg train loss: 0.005694\n",
      "Epoch [8147/10000] Avg train loss: 0.005693\n",
      "Epoch [8148/10000] Avg train loss: 0.005692\n",
      "Epoch [8149/10000] Avg train loss: 0.005692\n",
      "Epoch [8150/10000] Avg train loss: 0.005691\n",
      "Epoch [8151/10000] Avg train loss: 0.005690\n",
      "Epoch [8152/10000] Avg train loss: 0.005690\n",
      "Epoch [8153/10000] Avg train loss: 0.005689\n",
      "Epoch [8154/10000] Avg train loss: 0.005688\n",
      "Epoch [8155/10000] Avg train loss: 0.005687\n",
      "Epoch [8156/10000] Avg train loss: 0.005687\n",
      "Epoch [8157/10000] Avg train loss: 0.005686\n",
      "Epoch [8158/10000] Avg train loss: 0.005685\n",
      "Epoch [8159/10000] Avg train loss: 0.005685\n",
      "Epoch [8160/10000] Avg train loss: 0.005684\n",
      "Epoch [8161/10000] Avg train loss: 0.005683\n",
      "Epoch [8162/10000] Avg train loss: 0.005683\n",
      "Epoch [8163/10000] Avg train loss: 0.005682\n",
      "Epoch [8164/10000] Avg train loss: 0.005681\n",
      "Epoch [8165/10000] Avg train loss: 0.005680\n",
      "Epoch [8166/10000] Avg train loss: 0.005680\n",
      "Epoch [8167/10000] Avg train loss: 0.005679\n",
      "Epoch [8168/10000] Avg train loss: 0.005678\n",
      "Epoch [8169/10000] Avg train loss: 0.005678\n",
      "Epoch [8170/10000] Avg train loss: 0.005677\n",
      "Epoch [8171/10000] Avg train loss: 0.005676\n",
      "Epoch [8172/10000] Avg train loss: 0.005676\n",
      "Epoch [8173/10000] Avg train loss: 0.005675\n",
      "Epoch [8174/10000] Avg train loss: 0.005674\n",
      "Epoch [8175/10000] Avg train loss: 0.005674\n",
      "Epoch [8176/10000] Avg train loss: 0.005673\n",
      "Epoch [8177/10000] Avg train loss: 0.005672\n",
      "Epoch [8178/10000] Avg train loss: 0.005671\n",
      "Epoch [8179/10000] Avg train loss: 0.005671\n",
      "Epoch [8180/10000] Avg train loss: 0.005670\n",
      "Epoch [8181/10000] Avg train loss: 0.005669\n",
      "Epoch [8182/10000] Avg train loss: 0.005669\n",
      "Epoch [8183/10000] Avg train loss: 0.005668\n",
      "Epoch [8184/10000] Avg train loss: 0.005667\n",
      "Epoch [8185/10000] Avg train loss: 0.005667\n",
      "Epoch [8186/10000] Avg train loss: 0.005666\n",
      "Epoch [8187/10000] Avg train loss: 0.005665\n",
      "Epoch [8188/10000] Avg train loss: 0.005665\n",
      "Epoch [8189/10000] Avg train loss: 0.005664\n",
      "Epoch [8190/10000] Avg train loss: 0.005663\n",
      "Epoch [8191/10000] Avg train loss: 0.005662\n",
      "Epoch [8192/10000] Avg train loss: 0.005662\n",
      "Epoch [8193/10000] Avg train loss: 0.005661\n",
      "Epoch [8194/10000] Avg train loss: 0.005660\n",
      "Epoch [8195/10000] Avg train loss: 0.005660\n",
      "Epoch [8196/10000] Avg train loss: 0.005659\n",
      "Epoch [8197/10000] Avg train loss: 0.005658\n",
      "Epoch [8198/10000] Avg train loss: 0.005658\n",
      "Epoch [8199/10000] Avg train loss: 0.005657\n",
      "Epoch [8200/10000] Avg train loss: 0.005656\n",
      "Epoch [8201/10000] Avg train loss: 0.005656\n",
      "Epoch [8202/10000] Avg train loss: 0.005655\n",
      "Epoch [8203/10000] Avg train loss: 0.005654\n",
      "Epoch [8204/10000] Avg train loss: 0.005653\n",
      "Epoch [8205/10000] Avg train loss: 0.005653\n",
      "Epoch [8206/10000] Avg train loss: 0.005652\n",
      "Epoch [8207/10000] Avg train loss: 0.005651\n",
      "Epoch [8208/10000] Avg train loss: 0.005651\n",
      "Epoch [8209/10000] Avg train loss: 0.005650\n",
      "Epoch [8210/10000] Avg train loss: 0.005649\n",
      "Epoch [8211/10000] Avg train loss: 0.005649\n",
      "Epoch [8212/10000] Avg train loss: 0.005648\n",
      "Epoch [8213/10000] Avg train loss: 0.005647\n",
      "Epoch [8214/10000] Avg train loss: 0.005647\n",
      "Epoch [8215/10000] Avg train loss: 0.005646\n",
      "Epoch [8216/10000] Avg train loss: 0.005645\n",
      "Epoch [8217/10000] Avg train loss: 0.005645\n",
      "Epoch [8218/10000] Avg train loss: 0.005644\n",
      "Epoch [8219/10000] Avg train loss: 0.005643\n",
      "Epoch [8220/10000] Avg train loss: 0.005642\n",
      "Epoch [8221/10000] Avg train loss: 0.005642\n",
      "Epoch [8222/10000] Avg train loss: 0.005641\n",
      "Epoch [8223/10000] Avg train loss: 0.005640\n",
      "Epoch [8224/10000] Avg train loss: 0.005640\n",
      "Epoch [8225/10000] Avg train loss: 0.005639\n",
      "Epoch [8226/10000] Avg train loss: 0.005638\n",
      "Epoch [8227/10000] Avg train loss: 0.005638\n",
      "Epoch [8228/10000] Avg train loss: 0.005637\n",
      "Epoch [8229/10000] Avg train loss: 0.005636\n",
      "Epoch [8230/10000] Avg train loss: 0.005636\n",
      "Epoch [8231/10000] Avg train loss: 0.005635\n",
      "Epoch [8232/10000] Avg train loss: 0.005634\n",
      "Epoch [8233/10000] Avg train loss: 0.005634\n",
      "Epoch [8234/10000] Avg train loss: 0.005633\n",
      "Epoch [8235/10000] Avg train loss: 0.005632\n",
      "Epoch [8236/10000] Avg train loss: 0.005632\n",
      "Epoch [8237/10000] Avg train loss: 0.005631\n",
      "Epoch [8238/10000] Avg train loss: 0.005630\n",
      "Epoch [8239/10000] Avg train loss: 0.005629\n",
      "Epoch [8240/10000] Avg train loss: 0.005629\n",
      "Epoch [8241/10000] Avg train loss: 0.005628\n",
      "Epoch [8242/10000] Avg train loss: 0.005627\n",
      "Epoch [8243/10000] Avg train loss: 0.005627\n",
      "Epoch [8244/10000] Avg train loss: 0.005626\n",
      "Epoch [8245/10000] Avg train loss: 0.005625\n",
      "Epoch [8246/10000] Avg train loss: 0.005625\n",
      "Epoch [8247/10000] Avg train loss: 0.005624\n",
      "Epoch [8248/10000] Avg train loss: 0.005623\n",
      "Epoch [8249/10000] Avg train loss: 0.005623\n",
      "Epoch [8250/10000] Avg train loss: 0.005622\n",
      "Epoch [8251/10000] Avg train loss: 0.005621\n",
      "Epoch [8252/10000] Avg train loss: 0.005621\n",
      "Epoch [8253/10000] Avg train loss: 0.005620\n",
      "Epoch [8254/10000] Avg train loss: 0.005619\n",
      "Epoch [8255/10000] Avg train loss: 0.005619\n",
      "Epoch [8256/10000] Avg train loss: 0.005618\n",
      "Epoch [8257/10000] Avg train loss: 0.005617\n",
      "Epoch [8258/10000] Avg train loss: 0.005617\n",
      "Epoch [8259/10000] Avg train loss: 0.005616\n",
      "Epoch [8260/10000] Avg train loss: 0.005615\n",
      "Epoch [8261/10000] Avg train loss: 0.005614\n",
      "Epoch [8262/10000] Avg train loss: 0.005614\n",
      "Epoch [8263/10000] Avg train loss: 0.005613\n",
      "Epoch [8264/10000] Avg train loss: 0.005612\n",
      "Epoch [8265/10000] Avg train loss: 0.005612\n",
      "Epoch [8266/10000] Avg train loss: 0.005611\n",
      "Epoch [8267/10000] Avg train loss: 0.005610\n",
      "Epoch [8268/10000] Avg train loss: 0.005610\n",
      "Epoch [8269/10000] Avg train loss: 0.005609\n",
      "Epoch [8270/10000] Avg train loss: 0.005608\n",
      "Epoch [8271/10000] Avg train loss: 0.005608\n",
      "Epoch [8272/10000] Avg train loss: 0.005607\n",
      "Epoch [8273/10000] Avg train loss: 0.005606\n",
      "Epoch [8274/10000] Avg train loss: 0.005606\n",
      "Epoch [8275/10000] Avg train loss: 0.005605\n",
      "Epoch [8276/10000] Avg train loss: 0.005604\n",
      "Epoch [8277/10000] Avg train loss: 0.005604\n",
      "Epoch [8278/10000] Avg train loss: 0.005603\n",
      "Epoch [8279/10000] Avg train loss: 0.005602\n",
      "Epoch [8280/10000] Avg train loss: 0.005602\n",
      "Epoch [8281/10000] Avg train loss: 0.005601\n",
      "Epoch [8282/10000] Avg train loss: 0.005600\n",
      "Epoch [8283/10000] Avg train loss: 0.005600\n",
      "Epoch [8284/10000] Avg train loss: 0.005599\n",
      "Epoch [8285/10000] Avg train loss: 0.005598\n",
      "Epoch [8286/10000] Avg train loss: 0.005598\n",
      "Epoch [8287/10000] Avg train loss: 0.005597\n",
      "Epoch [8288/10000] Avg train loss: 0.005596\n",
      "Epoch [8289/10000] Avg train loss: 0.005596\n",
      "Epoch [8290/10000] Avg train loss: 0.005595\n",
      "Epoch [8291/10000] Avg train loss: 0.005594\n",
      "Epoch [8292/10000] Avg train loss: 0.005593\n",
      "Epoch [8293/10000] Avg train loss: 0.005593\n",
      "Epoch [8294/10000] Avg train loss: 0.005592\n",
      "Epoch [8295/10000] Avg train loss: 0.005591\n",
      "Epoch [8296/10000] Avg train loss: 0.005591\n",
      "Epoch [8297/10000] Avg train loss: 0.005590\n",
      "Epoch [8298/10000] Avg train loss: 0.005589\n",
      "Epoch [8299/10000] Avg train loss: 0.005589\n",
      "Epoch [8300/10000] Avg train loss: 0.005588\n",
      "Epoch [8301/10000] Avg train loss: 0.005587\n",
      "Epoch [8302/10000] Avg train loss: 0.005587\n",
      "Epoch [8303/10000] Avg train loss: 0.005586\n",
      "Epoch [8304/10000] Avg train loss: 0.005585\n",
      "Epoch [8305/10000] Avg train loss: 0.005585\n",
      "Epoch [8306/10000] Avg train loss: 0.005584\n",
      "Epoch [8307/10000] Avg train loss: 0.005583\n",
      "Epoch [8308/10000] Avg train loss: 0.005583\n",
      "Epoch [8309/10000] Avg train loss: 0.005582\n",
      "Epoch [8310/10000] Avg train loss: 0.005581\n",
      "Epoch [8311/10000] Avg train loss: 0.005581\n",
      "Epoch [8312/10000] Avg train loss: 0.005580\n",
      "Epoch [8313/10000] Avg train loss: 0.005579\n",
      "Epoch [8314/10000] Avg train loss: 0.005579\n",
      "Epoch [8315/10000] Avg train loss: 0.005578\n",
      "Epoch [8316/10000] Avg train loss: 0.005577\n",
      "Epoch [8317/10000] Avg train loss: 0.005577\n",
      "Epoch [8318/10000] Avg train loss: 0.005576\n",
      "Epoch [8319/10000] Avg train loss: 0.005575\n",
      "Epoch [8320/10000] Avg train loss: 0.005575\n",
      "Epoch [8321/10000] Avg train loss: 0.005574\n",
      "Epoch [8322/10000] Avg train loss: 0.005573\n",
      "Epoch [8323/10000] Avg train loss: 0.005573\n",
      "Epoch [8324/10000] Avg train loss: 0.005572\n",
      "Epoch [8325/10000] Avg train loss: 0.005571\n",
      "Epoch [8326/10000] Avg train loss: 0.005571\n",
      "Epoch [8327/10000] Avg train loss: 0.005570\n",
      "Epoch [8328/10000] Avg train loss: 0.005569\n",
      "Epoch [8329/10000] Avg train loss: 0.005569\n",
      "Epoch [8330/10000] Avg train loss: 0.005568\n",
      "Epoch [8331/10000] Avg train loss: 0.005567\n",
      "Epoch [8332/10000] Avg train loss: 0.005567\n",
      "Epoch [8333/10000] Avg train loss: 0.005566\n",
      "Epoch [8334/10000] Avg train loss: 0.005565\n",
      "Epoch [8335/10000] Avg train loss: 0.005565\n",
      "Epoch [8336/10000] Avg train loss: 0.005564\n",
      "Epoch [8337/10000] Avg train loss: 0.005563\n",
      "Epoch [8338/10000] Avg train loss: 0.005563\n",
      "Epoch [8339/10000] Avg train loss: 0.005562\n",
      "Epoch [8340/10000] Avg train loss: 0.005561\n",
      "Epoch [8341/10000] Avg train loss: 0.005561\n",
      "Epoch [8342/10000] Avg train loss: 0.005560\n",
      "Epoch [8343/10000] Avg train loss: 0.005559\n",
      "Epoch [8344/10000] Avg train loss: 0.005559\n",
      "Epoch [8345/10000] Avg train loss: 0.005558\n",
      "Epoch [8346/10000] Avg train loss: 0.005557\n",
      "Epoch [8347/10000] Avg train loss: 0.005557\n",
      "Epoch [8348/10000] Avg train loss: 0.005556\n",
      "Epoch [8349/10000] Avg train loss: 0.005555\n",
      "Epoch [8350/10000] Avg train loss: 0.005555\n",
      "Epoch [8351/10000] Avg train loss: 0.005554\n",
      "Epoch [8352/10000] Avg train loss: 0.005553\n",
      "Epoch [8353/10000] Avg train loss: 0.005553\n",
      "Epoch [8354/10000] Avg train loss: 0.005552\n",
      "Epoch [8355/10000] Avg train loss: 0.005551\n",
      "Epoch [8356/10000] Avg train loss: 0.005551\n",
      "Epoch [8357/10000] Avg train loss: 0.005550\n",
      "Epoch [8358/10000] Avg train loss: 0.005549\n",
      "Epoch [8359/10000] Avg train loss: 0.005549\n",
      "Epoch [8360/10000] Avg train loss: 0.005548\n",
      "Epoch [8361/10000] Avg train loss: 0.005547\n",
      "Epoch [8362/10000] Avg train loss: 0.005547\n",
      "Epoch [8363/10000] Avg train loss: 0.005546\n",
      "Epoch [8364/10000] Avg train loss: 0.005545\n",
      "Epoch [8365/10000] Avg train loss: 0.005545\n",
      "Epoch [8366/10000] Avg train loss: 0.005544\n",
      "Epoch [8367/10000] Avg train loss: 0.005543\n",
      "Epoch [8368/10000] Avg train loss: 0.005543\n",
      "Epoch [8369/10000] Avg train loss: 0.005542\n",
      "Epoch [8370/10000] Avg train loss: 0.005541\n",
      "Epoch [8371/10000] Avg train loss: 0.005541\n",
      "Epoch [8372/10000] Avg train loss: 0.005540\n",
      "Epoch [8373/10000] Avg train loss: 0.005539\n",
      "Epoch [8374/10000] Avg train loss: 0.005539\n",
      "Epoch [8375/10000] Avg train loss: 0.005538\n",
      "Epoch [8376/10000] Avg train loss: 0.005537\n",
      "Epoch [8377/10000] Avg train loss: 0.005537\n",
      "Epoch [8378/10000] Avg train loss: 0.005536\n",
      "Epoch [8379/10000] Avg train loss: 0.005535\n",
      "Epoch [8380/10000] Avg train loss: 0.005535\n",
      "Epoch [8381/10000] Avg train loss: 0.005534\n",
      "Epoch [8382/10000] Avg train loss: 0.005533\n",
      "Epoch [8383/10000] Avg train loss: 0.005533\n",
      "Epoch [8384/10000] Avg train loss: 0.005532\n",
      "Epoch [8385/10000] Avg train loss: 0.005531\n",
      "Epoch [8386/10000] Avg train loss: 0.005531\n",
      "Epoch [8387/10000] Avg train loss: 0.005530\n",
      "Epoch [8388/10000] Avg train loss: 0.005529\n",
      "Epoch [8389/10000] Avg train loss: 0.005529\n",
      "Epoch [8390/10000] Avg train loss: 0.005528\n",
      "Epoch [8391/10000] Avg train loss: 0.005528\n",
      "Epoch [8392/10000] Avg train loss: 0.005527\n",
      "Epoch [8393/10000] Avg train loss: 0.005526\n",
      "Epoch [8394/10000] Avg train loss: 0.005526\n",
      "Epoch [8395/10000] Avg train loss: 0.005525\n",
      "Epoch [8396/10000] Avg train loss: 0.005524\n",
      "Epoch [8397/10000] Avg train loss: 0.005524\n",
      "Epoch [8398/10000] Avg train loss: 0.005523\n",
      "Epoch [8399/10000] Avg train loss: 0.005522\n",
      "Epoch [8400/10000] Avg train loss: 0.005522\n",
      "Epoch [8401/10000] Avg train loss: 0.005521\n",
      "Epoch [8402/10000] Avg train loss: 0.005520\n",
      "Epoch [8403/10000] Avg train loss: 0.005520\n",
      "Epoch [8404/10000] Avg train loss: 0.005519\n",
      "Epoch [8405/10000] Avg train loss: 0.005518\n",
      "Epoch [8406/10000] Avg train loss: 0.005518\n",
      "Epoch [8407/10000] Avg train loss: 0.005517\n",
      "Epoch [8408/10000] Avg train loss: 0.005516\n",
      "Epoch [8409/10000] Avg train loss: 0.005516\n",
      "Epoch [8410/10000] Avg train loss: 0.005515\n",
      "Epoch [8411/10000] Avg train loss: 0.005514\n",
      "Epoch [8412/10000] Avg train loss: 0.005514\n",
      "Epoch [8413/10000] Avg train loss: 0.005513\n",
      "Epoch [8414/10000] Avg train loss: 0.005512\n",
      "Epoch [8415/10000] Avg train loss: 0.005512\n",
      "Epoch [8416/10000] Avg train loss: 0.005511\n",
      "Epoch [8417/10000] Avg train loss: 0.005510\n",
      "Epoch [8418/10000] Avg train loss: 0.005510\n",
      "Epoch [8419/10000] Avg train loss: 0.005509\n",
      "Epoch [8420/10000] Avg train loss: 0.005508\n",
      "Epoch [8421/10000] Avg train loss: 0.005508\n",
      "Epoch [8422/10000] Avg train loss: 0.005507\n",
      "Epoch [8423/10000] Avg train loss: 0.005507\n",
      "Epoch [8424/10000] Avg train loss: 0.005506\n",
      "Epoch [8425/10000] Avg train loss: 0.005505\n",
      "Epoch [8426/10000] Avg train loss: 0.005505\n",
      "Epoch [8427/10000] Avg train loss: 0.005504\n",
      "Epoch [8428/10000] Avg train loss: 0.005503\n",
      "Epoch [8429/10000] Avg train loss: 0.005503\n",
      "Epoch [8430/10000] Avg train loss: 0.005502\n",
      "Epoch [8431/10000] Avg train loss: 0.005501\n",
      "Epoch [8432/10000] Avg train loss: 0.005501\n",
      "Epoch [8433/10000] Avg train loss: 0.005500\n",
      "Epoch [8434/10000] Avg train loss: 0.005499\n",
      "Epoch [8435/10000] Avg train loss: 0.005499\n",
      "Epoch [8436/10000] Avg train loss: 0.005498\n",
      "Epoch [8437/10000] Avg train loss: 0.005497\n",
      "Epoch [8438/10000] Avg train loss: 0.005497\n",
      "Epoch [8439/10000] Avg train loss: 0.005496\n",
      "Epoch [8440/10000] Avg train loss: 0.005495\n",
      "Epoch [8441/10000] Avg train loss: 0.005495\n",
      "Epoch [8442/10000] Avg train loss: 0.005494\n",
      "Epoch [8443/10000] Avg train loss: 0.005493\n",
      "Epoch [8444/10000] Avg train loss: 0.005493\n",
      "Epoch [8445/10000] Avg train loss: 0.005492\n",
      "Epoch [8446/10000] Avg train loss: 0.005492\n",
      "Epoch [8447/10000] Avg train loss: 0.005491\n",
      "Epoch [8448/10000] Avg train loss: 0.005490\n",
      "Epoch [8449/10000] Avg train loss: 0.005490\n",
      "Epoch [8450/10000] Avg train loss: 0.005489\n",
      "Epoch [8451/10000] Avg train loss: 0.005488\n",
      "Epoch [8452/10000] Avg train loss: 0.005488\n",
      "Epoch [8453/10000] Avg train loss: 0.005487\n",
      "Epoch [8454/10000] Avg train loss: 0.005486\n",
      "Epoch [8455/10000] Avg train loss: 0.005486\n",
      "Epoch [8456/10000] Avg train loss: 0.005485\n",
      "Epoch [8457/10000] Avg train loss: 0.005484\n",
      "Epoch [8458/10000] Avg train loss: 0.005484\n",
      "Epoch [8459/10000] Avg train loss: 0.005483\n",
      "Epoch [8460/10000] Avg train loss: 0.005482\n",
      "Epoch [8461/10000] Avg train loss: 0.005482\n",
      "Epoch [8462/10000] Avg train loss: 0.005481\n",
      "Epoch [8463/10000] Avg train loss: 0.005480\n",
      "Epoch [8464/10000] Avg train loss: 0.005480\n",
      "Epoch [8465/10000] Avg train loss: 0.005479\n",
      "Epoch [8466/10000] Avg train loss: 0.005479\n",
      "Epoch [8467/10000] Avg train loss: 0.005478\n",
      "Epoch [8468/10000] Avg train loss: 0.005477\n",
      "Epoch [8469/10000] Avg train loss: 0.005477\n",
      "Epoch [8470/10000] Avg train loss: 0.005476\n",
      "Epoch [8471/10000] Avg train loss: 0.005475\n",
      "Epoch [8472/10000] Avg train loss: 0.005475\n",
      "Epoch [8473/10000] Avg train loss: 0.005474\n",
      "Epoch [8474/10000] Avg train loss: 0.005473\n",
      "Epoch [8475/10000] Avg train loss: 0.005473\n",
      "Epoch [8476/10000] Avg train loss: 0.005472\n",
      "Epoch [8477/10000] Avg train loss: 0.005471\n",
      "Epoch [8478/10000] Avg train loss: 0.005471\n",
      "Epoch [8479/10000] Avg train loss: 0.005470\n",
      "Epoch [8480/10000] Avg train loss: 0.005469\n",
      "Epoch [8481/10000] Avg train loss: 0.005469\n",
      "Epoch [8482/10000] Avg train loss: 0.005468\n",
      "Epoch [8483/10000] Avg train loss: 0.005468\n",
      "Epoch [8484/10000] Avg train loss: 0.005467\n",
      "Epoch [8485/10000] Avg train loss: 0.005466\n",
      "Epoch [8486/10000] Avg train loss: 0.005466\n",
      "Epoch [8487/10000] Avg train loss: 0.005465\n",
      "Epoch [8488/10000] Avg train loss: 0.005464\n",
      "Epoch [8489/10000] Avg train loss: 0.005464\n",
      "Epoch [8490/10000] Avg train loss: 0.005463\n",
      "Epoch [8491/10000] Avg train loss: 0.005462\n",
      "Epoch [8492/10000] Avg train loss: 0.005462\n",
      "Epoch [8493/10000] Avg train loss: 0.005461\n",
      "Epoch [8494/10000] Avg train loss: 0.005460\n",
      "Epoch [8495/10000] Avg train loss: 0.005460\n",
      "Epoch [8496/10000] Avg train loss: 0.005459\n",
      "Epoch [8497/10000] Avg train loss: 0.005459\n",
      "Epoch [8498/10000] Avg train loss: 0.005458\n",
      "Epoch [8499/10000] Avg train loss: 0.005457\n",
      "Epoch [8500/10000] Avg train loss: 0.005457\n",
      "Epoch [8501/10000] Avg train loss: 0.005456\n",
      "Epoch [8502/10000] Avg train loss: 0.005455\n",
      "Epoch [8503/10000] Avg train loss: 0.005455\n",
      "Epoch [8504/10000] Avg train loss: 0.005454\n",
      "Epoch [8505/10000] Avg train loss: 0.005453\n",
      "Epoch [8506/10000] Avg train loss: 0.005453\n",
      "Epoch [8507/10000] Avg train loss: 0.005452\n",
      "Epoch [8508/10000] Avg train loss: 0.005451\n",
      "Epoch [8509/10000] Avg train loss: 0.005451\n",
      "Epoch [8510/10000] Avg train loss: 0.005450\n",
      "Epoch [8511/10000] Avg train loss: 0.005450\n",
      "Epoch [8512/10000] Avg train loss: 0.005449\n",
      "Epoch [8513/10000] Avg train loss: 0.005448\n",
      "Epoch [8514/10000] Avg train loss: 0.005448\n",
      "Epoch [8515/10000] Avg train loss: 0.005447\n",
      "Epoch [8516/10000] Avg train loss: 0.005446\n",
      "Epoch [8517/10000] Avg train loss: 0.005446\n",
      "Epoch [8518/10000] Avg train loss: 0.005445\n",
      "Epoch [8519/10000] Avg train loss: 0.005444\n",
      "Epoch [8520/10000] Avg train loss: 0.005444\n",
      "Epoch [8521/10000] Avg train loss: 0.005443\n",
      "Epoch [8522/10000] Avg train loss: 0.005443\n",
      "Epoch [8523/10000] Avg train loss: 0.005442\n",
      "Epoch [8524/10000] Avg train loss: 0.005441\n",
      "Epoch [8525/10000] Avg train loss: 0.005441\n",
      "Epoch [8526/10000] Avg train loss: 0.005440\n",
      "Epoch [8527/10000] Avg train loss: 0.005439\n",
      "Epoch [8528/10000] Avg train loss: 0.005439\n",
      "Epoch [8529/10000] Avg train loss: 0.005438\n",
      "Epoch [8530/10000] Avg train loss: 0.005437\n",
      "Epoch [8531/10000] Avg train loss: 0.005437\n",
      "Epoch [8532/10000] Avg train loss: 0.005436\n",
      "Epoch [8533/10000] Avg train loss: 0.005436\n",
      "Epoch [8534/10000] Avg train loss: 0.005435\n",
      "Epoch [8535/10000] Avg train loss: 0.005434\n",
      "Epoch [8536/10000] Avg train loss: 0.005434\n",
      "Epoch [8537/10000] Avg train loss: 0.005433\n",
      "Epoch [8538/10000] Avg train loss: 0.005432\n",
      "Epoch [8539/10000] Avg train loss: 0.005432\n",
      "Epoch [8540/10000] Avg train loss: 0.005431\n",
      "Epoch [8541/10000] Avg train loss: 0.005430\n",
      "Epoch [8542/10000] Avg train loss: 0.005430\n",
      "Epoch [8543/10000] Avg train loss: 0.005429\n",
      "Epoch [8544/10000] Avg train loss: 0.005429\n",
      "Epoch [8545/10000] Avg train loss: 0.005428\n",
      "Epoch [8546/10000] Avg train loss: 0.005427\n",
      "Epoch [8547/10000] Avg train loss: 0.005427\n",
      "Epoch [8548/10000] Avg train loss: 0.005426\n",
      "Epoch [8549/10000] Avg train loss: 0.005425\n",
      "Epoch [8550/10000] Avg train loss: 0.005425\n",
      "Epoch [8551/10000] Avg train loss: 0.005424\n",
      "Epoch [8552/10000] Avg train loss: 0.005423\n",
      "Epoch [8553/10000] Avg train loss: 0.005423\n",
      "Epoch [8554/10000] Avg train loss: 0.005422\n",
      "Epoch [8555/10000] Avg train loss: 0.005422\n",
      "Epoch [8556/10000] Avg train loss: 0.005421\n",
      "Epoch [8557/10000] Avg train loss: 0.005420\n",
      "Epoch [8558/10000] Avg train loss: 0.005420\n",
      "Epoch [8559/10000] Avg train loss: 0.005419\n",
      "Epoch [8560/10000] Avg train loss: 0.005418\n",
      "Epoch [8561/10000] Avg train loss: 0.005418\n",
      "Epoch [8562/10000] Avg train loss: 0.005417\n",
      "Epoch [8563/10000] Avg train loss: 0.005416\n",
      "Epoch [8564/10000] Avg train loss: 0.005416\n",
      "Epoch [8565/10000] Avg train loss: 0.005415\n",
      "Epoch [8566/10000] Avg train loss: 0.005415\n",
      "Epoch [8567/10000] Avg train loss: 0.005414\n",
      "Epoch [8568/10000] Avg train loss: 0.005413\n",
      "Epoch [8569/10000] Avg train loss: 0.005413\n",
      "Epoch [8570/10000] Avg train loss: 0.005412\n",
      "Epoch [8571/10000] Avg train loss: 0.005411\n",
      "Epoch [8572/10000] Avg train loss: 0.005411\n",
      "Epoch [8573/10000] Avg train loss: 0.005410\n",
      "Epoch [8574/10000] Avg train loss: 0.005410\n",
      "Epoch [8575/10000] Avg train loss: 0.005409\n",
      "Epoch [8576/10000] Avg train loss: 0.005408\n",
      "Epoch [8577/10000] Avg train loss: 0.005408\n",
      "Epoch [8578/10000] Avg train loss: 0.005407\n",
      "Epoch [8579/10000] Avg train loss: 0.005406\n",
      "Epoch [8580/10000] Avg train loss: 0.005406\n",
      "Epoch [8581/10000] Avg train loss: 0.005405\n",
      "Epoch [8582/10000] Avg train loss: 0.005404\n",
      "Epoch [8583/10000] Avg train loss: 0.005404\n",
      "Epoch [8584/10000] Avg train loss: 0.005403\n",
      "Epoch [8585/10000] Avg train loss: 0.005403\n",
      "Epoch [8586/10000] Avg train loss: 0.005402\n",
      "Epoch [8587/10000] Avg train loss: 0.005401\n",
      "Epoch [8588/10000] Avg train loss: 0.005401\n",
      "Epoch [8589/10000] Avg train loss: 0.005400\n",
      "Epoch [8590/10000] Avg train loss: 0.005399\n",
      "Epoch [8591/10000] Avg train loss: 0.005399\n",
      "Epoch [8592/10000] Avg train loss: 0.005398\n",
      "Epoch [8593/10000] Avg train loss: 0.005398\n",
      "Epoch [8594/10000] Avg train loss: 0.005397\n",
      "Epoch [8595/10000] Avg train loss: 0.005396\n",
      "Epoch [8596/10000] Avg train loss: 0.005396\n",
      "Epoch [8597/10000] Avg train loss: 0.005395\n",
      "Epoch [8598/10000] Avg train loss: 0.005394\n",
      "Epoch [8599/10000] Avg train loss: 0.005394\n",
      "Epoch [8600/10000] Avg train loss: 0.005393\n",
      "Epoch [8601/10000] Avg train loss: 0.005393\n",
      "Epoch [8602/10000] Avg train loss: 0.005392\n",
      "Epoch [8603/10000] Avg train loss: 0.005391\n",
      "Epoch [8604/10000] Avg train loss: 0.005391\n",
      "Epoch [8605/10000] Avg train loss: 0.005390\n",
      "Epoch [8606/10000] Avg train loss: 0.005389\n",
      "Epoch [8607/10000] Avg train loss: 0.005389\n",
      "Epoch [8608/10000] Avg train loss: 0.005388\n",
      "Epoch [8609/10000] Avg train loss: 0.005388\n",
      "Epoch [8610/10000] Avg train loss: 0.005387\n",
      "Epoch [8611/10000] Avg train loss: 0.005386\n",
      "Epoch [8612/10000] Avg train loss: 0.005386\n",
      "Epoch [8613/10000] Avg train loss: 0.005385\n",
      "Epoch [8614/10000] Avg train loss: 0.005384\n",
      "Epoch [8615/10000] Avg train loss: 0.005384\n",
      "Epoch [8616/10000] Avg train loss: 0.005383\n",
      "Epoch [8617/10000] Avg train loss: 0.005383\n",
      "Epoch [8618/10000] Avg train loss: 0.005382\n",
      "Epoch [8619/10000] Avg train loss: 0.005381\n",
      "Epoch [8620/10000] Avg train loss: 0.005381\n",
      "Epoch [8621/10000] Avg train loss: 0.005380\n",
      "Epoch [8622/10000] Avg train loss: 0.005379\n",
      "Epoch [8623/10000] Avg train loss: 0.005379\n",
      "Epoch [8624/10000] Avg train loss: 0.005378\n",
      "Epoch [8625/10000] Avg train loss: 0.005378\n",
      "Epoch [8626/10000] Avg train loss: 0.005377\n",
      "Epoch [8627/10000] Avg train loss: 0.005376\n",
      "Epoch [8628/10000] Avg train loss: 0.005376\n",
      "Epoch [8629/10000] Avg train loss: 0.005375\n",
      "Epoch [8630/10000] Avg train loss: 0.005374\n",
      "Epoch [8631/10000] Avg train loss: 0.005374\n",
      "Epoch [8632/10000] Avg train loss: 0.005373\n",
      "Epoch [8633/10000] Avg train loss: 0.005373\n",
      "Epoch [8634/10000] Avg train loss: 0.005372\n",
      "Epoch [8635/10000] Avg train loss: 0.005371\n",
      "Epoch [8636/10000] Avg train loss: 0.005371\n",
      "Epoch [8637/10000] Avg train loss: 0.005370\n",
      "Epoch [8638/10000] Avg train loss: 0.005369\n",
      "Epoch [8639/10000] Avg train loss: 0.005369\n",
      "Epoch [8640/10000] Avg train loss: 0.005368\n",
      "Epoch [8641/10000] Avg train loss: 0.005368\n",
      "Epoch [8642/10000] Avg train loss: 0.005367\n",
      "Epoch [8643/10000] Avg train loss: 0.005366\n",
      "Epoch [8644/10000] Avg train loss: 0.005366\n",
      "Epoch [8645/10000] Avg train loss: 0.005365\n",
      "Epoch [8646/10000] Avg train loss: 0.005364\n",
      "Epoch [8647/10000] Avg train loss: 0.005364\n",
      "Epoch [8648/10000] Avg train loss: 0.005363\n",
      "Epoch [8649/10000] Avg train loss: 0.005363\n",
      "Epoch [8650/10000] Avg train loss: 0.005362\n",
      "Epoch [8651/10000] Avg train loss: 0.005361\n",
      "Epoch [8652/10000] Avg train loss: 0.005361\n",
      "Epoch [8653/10000] Avg train loss: 0.005360\n",
      "Epoch [8654/10000] Avg train loss: 0.005360\n",
      "Epoch [8655/10000] Avg train loss: 0.005359\n",
      "Epoch [8656/10000] Avg train loss: 0.005358\n",
      "Epoch [8657/10000] Avg train loss: 0.005358\n",
      "Epoch [8658/10000] Avg train loss: 0.005357\n",
      "Epoch [8659/10000] Avg train loss: 0.005356\n",
      "Epoch [8660/10000] Avg train loss: 0.005356\n",
      "Epoch [8661/10000] Avg train loss: 0.005355\n",
      "Epoch [8662/10000] Avg train loss: 0.005355\n",
      "Epoch [8663/10000] Avg train loss: 0.005354\n",
      "Epoch [8664/10000] Avg train loss: 0.005353\n",
      "Epoch [8665/10000] Avg train loss: 0.005353\n",
      "Epoch [8666/10000] Avg train loss: 0.005352\n",
      "Epoch [8667/10000] Avg train loss: 0.005351\n",
      "Epoch [8668/10000] Avg train loss: 0.005351\n",
      "Epoch [8669/10000] Avg train loss: 0.005350\n",
      "Epoch [8670/10000] Avg train loss: 0.005350\n",
      "Epoch [8671/10000] Avg train loss: 0.005349\n",
      "Epoch [8672/10000] Avg train loss: 0.005348\n",
      "Epoch [8673/10000] Avg train loss: 0.005348\n",
      "Epoch [8674/10000] Avg train loss: 0.005347\n",
      "Epoch [8675/10000] Avg train loss: 0.005347\n",
      "Epoch [8676/10000] Avg train loss: 0.005346\n",
      "Epoch [8677/10000] Avg train loss: 0.005345\n",
      "Epoch [8678/10000] Avg train loss: 0.005345\n",
      "Epoch [8679/10000] Avg train loss: 0.005344\n",
      "Epoch [8680/10000] Avg train loss: 0.005343\n",
      "Epoch [8681/10000] Avg train loss: 0.005343\n",
      "Epoch [8682/10000] Avg train loss: 0.005342\n",
      "Epoch [8683/10000] Avg train loss: 0.005342\n",
      "Epoch [8684/10000] Avg train loss: 0.005341\n",
      "Epoch [8685/10000] Avg train loss: 0.005340\n",
      "Epoch [8686/10000] Avg train loss: 0.005340\n",
      "Epoch [8687/10000] Avg train loss: 0.005339\n",
      "Epoch [8688/10000] Avg train loss: 0.005339\n",
      "Epoch [8689/10000] Avg train loss: 0.005338\n",
      "Epoch [8690/10000] Avg train loss: 0.005337\n",
      "Epoch [8691/10000] Avg train loss: 0.005337\n",
      "Epoch [8692/10000] Avg train loss: 0.005336\n",
      "Epoch [8693/10000] Avg train loss: 0.005335\n",
      "Epoch [8694/10000] Avg train loss: 0.005335\n",
      "Epoch [8695/10000] Avg train loss: 0.005334\n",
      "Epoch [8696/10000] Avg train loss: 0.005334\n",
      "Epoch [8697/10000] Avg train loss: 0.005333\n",
      "Epoch [8698/10000] Avg train loss: 0.005332\n",
      "Epoch [8699/10000] Avg train loss: 0.005332\n",
      "Epoch [8700/10000] Avg train loss: 0.005331\n",
      "Epoch [8701/10000] Avg train loss: 0.005331\n",
      "Epoch [8702/10000] Avg train loss: 0.005330\n",
      "Epoch [8703/10000] Avg train loss: 0.005329\n",
      "Epoch [8704/10000] Avg train loss: 0.005329\n",
      "Epoch [8705/10000] Avg train loss: 0.005328\n",
      "Epoch [8706/10000] Avg train loss: 0.005328\n",
      "Epoch [8707/10000] Avg train loss: 0.005327\n",
      "Epoch [8708/10000] Avg train loss: 0.005326\n",
      "Epoch [8709/10000] Avg train loss: 0.005326\n",
      "Epoch [8710/10000] Avg train loss: 0.005325\n",
      "Epoch [8711/10000] Avg train loss: 0.005324\n",
      "Epoch [8712/10000] Avg train loss: 0.005324\n",
      "Epoch [8713/10000] Avg train loss: 0.005323\n",
      "Epoch [8714/10000] Avg train loss: 0.005323\n",
      "Epoch [8715/10000] Avg train loss: 0.005322\n",
      "Epoch [8716/10000] Avg train loss: 0.005321\n",
      "Epoch [8717/10000] Avg train loss: 0.005321\n",
      "Epoch [8718/10000] Avg train loss: 0.005320\n",
      "Epoch [8719/10000] Avg train loss: 0.005320\n",
      "Epoch [8720/10000] Avg train loss: 0.005319\n",
      "Epoch [8721/10000] Avg train loss: 0.005318\n",
      "Epoch [8722/10000] Avg train loss: 0.005318\n",
      "Epoch [8723/10000] Avg train loss: 0.005317\n",
      "Epoch [8724/10000] Avg train loss: 0.005317\n",
      "Epoch [8725/10000] Avg train loss: 0.005316\n",
      "Epoch [8726/10000] Avg train loss: 0.005315\n",
      "Epoch [8727/10000] Avg train loss: 0.005315\n",
      "Epoch [8728/10000] Avg train loss: 0.005314\n",
      "Epoch [8729/10000] Avg train loss: 0.005313\n",
      "Epoch [8730/10000] Avg train loss: 0.005313\n",
      "Epoch [8731/10000] Avg train loss: 0.005312\n",
      "Epoch [8732/10000] Avg train loss: 0.005312\n",
      "Epoch [8733/10000] Avg train loss: 0.005311\n",
      "Epoch [8734/10000] Avg train loss: 0.005310\n",
      "Epoch [8735/10000] Avg train loss: 0.005310\n",
      "Epoch [8736/10000] Avg train loss: 0.005309\n",
      "Epoch [8737/10000] Avg train loss: 0.005309\n",
      "Epoch [8738/10000] Avg train loss: 0.005308\n",
      "Epoch [8739/10000] Avg train loss: 0.005307\n",
      "Epoch [8740/10000] Avg train loss: 0.005307\n",
      "Epoch [8741/10000] Avg train loss: 0.005306\n",
      "Epoch [8742/10000] Avg train loss: 0.005306\n",
      "Epoch [8743/10000] Avg train loss: 0.005305\n",
      "Epoch [8744/10000] Avg train loss: 0.005304\n",
      "Epoch [8745/10000] Avg train loss: 0.005304\n",
      "Epoch [8746/10000] Avg train loss: 0.005303\n",
      "Epoch [8747/10000] Avg train loss: 0.005303\n",
      "Epoch [8748/10000] Avg train loss: 0.005302\n",
      "Epoch [8749/10000] Avg train loss: 0.005301\n",
      "Epoch [8750/10000] Avg train loss: 0.005301\n",
      "Epoch [8751/10000] Avg train loss: 0.005300\n",
      "Epoch [8752/10000] Avg train loss: 0.005300\n",
      "Epoch [8753/10000] Avg train loss: 0.005299\n",
      "Epoch [8754/10000] Avg train loss: 0.005298\n",
      "Epoch [8755/10000] Avg train loss: 0.005298\n",
      "Epoch [8756/10000] Avg train loss: 0.005297\n",
      "Epoch [8757/10000] Avg train loss: 0.005296\n",
      "Epoch [8758/10000] Avg train loss: 0.005296\n",
      "Epoch [8759/10000] Avg train loss: 0.005295\n",
      "Epoch [8760/10000] Avg train loss: 0.005295\n",
      "Epoch [8761/10000] Avg train loss: 0.005294\n",
      "Epoch [8762/10000] Avg train loss: 0.005293\n",
      "Epoch [8763/10000] Avg train loss: 0.005293\n",
      "Epoch [8764/10000] Avg train loss: 0.005292\n",
      "Epoch [8765/10000] Avg train loss: 0.005292\n",
      "Epoch [8766/10000] Avg train loss: 0.005291\n",
      "Epoch [8767/10000] Avg train loss: 0.005290\n",
      "Epoch [8768/10000] Avg train loss: 0.005290\n",
      "Epoch [8769/10000] Avg train loss: 0.005289\n",
      "Epoch [8770/10000] Avg train loss: 0.005289\n",
      "Epoch [8771/10000] Avg train loss: 0.005288\n",
      "Epoch [8772/10000] Avg train loss: 0.005287\n",
      "Epoch [8773/10000] Avg train loss: 0.005287\n",
      "Epoch [8774/10000] Avg train loss: 0.005286\n",
      "Epoch [8775/10000] Avg train loss: 0.005286\n",
      "Epoch [8776/10000] Avg train loss: 0.005285\n",
      "Epoch [8777/10000] Avg train loss: 0.005284\n",
      "Epoch [8778/10000] Avg train loss: 0.005284\n",
      "Epoch [8779/10000] Avg train loss: 0.005283\n",
      "Epoch [8780/10000] Avg train loss: 0.005283\n",
      "Epoch [8781/10000] Avg train loss: 0.005282\n",
      "Epoch [8782/10000] Avg train loss: 0.005281\n",
      "Epoch [8783/10000] Avg train loss: 0.005281\n",
      "Epoch [8784/10000] Avg train loss: 0.005280\n",
      "Epoch [8785/10000] Avg train loss: 0.005280\n",
      "Epoch [8786/10000] Avg train loss: 0.005279\n",
      "Epoch [8787/10000] Avg train loss: 0.005278\n",
      "Epoch [8788/10000] Avg train loss: 0.005278\n",
      "Epoch [8789/10000] Avg train loss: 0.005277\n",
      "Epoch [8790/10000] Avg train loss: 0.005277\n",
      "Epoch [8791/10000] Avg train loss: 0.005276\n",
      "Epoch [8792/10000] Avg train loss: 0.005275\n",
      "Epoch [8793/10000] Avg train loss: 0.005275\n",
      "Epoch [8794/10000] Avg train loss: 0.005274\n",
      "Epoch [8795/10000] Avg train loss: 0.005274\n",
      "Epoch [8796/10000] Avg train loss: 0.005273\n",
      "Epoch [8797/10000] Avg train loss: 0.005272\n",
      "Epoch [8798/10000] Avg train loss: 0.005272\n",
      "Epoch [8799/10000] Avg train loss: 0.005271\n",
      "Epoch [8800/10000] Avg train loss: 0.005271\n",
      "Epoch [8801/10000] Avg train loss: 0.005270\n",
      "Epoch [8802/10000] Avg train loss: 0.005269\n",
      "Epoch [8803/10000] Avg train loss: 0.005269\n",
      "Epoch [8804/10000] Avg train loss: 0.005268\n",
      "Epoch [8805/10000] Avg train loss: 0.005268\n",
      "Epoch [8806/10000] Avg train loss: 0.005267\n",
      "Epoch [8807/10000] Avg train loss: 0.005266\n",
      "Epoch [8808/10000] Avg train loss: 0.005266\n",
      "Epoch [8809/10000] Avg train loss: 0.005265\n",
      "Epoch [8810/10000] Avg train loss: 0.005265\n",
      "Epoch [8811/10000] Avg train loss: 0.005264\n",
      "Epoch [8812/10000] Avg train loss: 0.005263\n",
      "Epoch [8813/10000] Avg train loss: 0.005263\n",
      "Epoch [8814/10000] Avg train loss: 0.005262\n",
      "Epoch [8815/10000] Avg train loss: 0.005262\n",
      "Epoch [8816/10000] Avg train loss: 0.005261\n",
      "Epoch [8817/10000] Avg train loss: 0.005260\n",
      "Epoch [8818/10000] Avg train loss: 0.005260\n",
      "Epoch [8819/10000] Avg train loss: 0.005259\n",
      "Epoch [8820/10000] Avg train loss: 0.005259\n",
      "Epoch [8821/10000] Avg train loss: 0.005258\n",
      "Epoch [8822/10000] Avg train loss: 0.005257\n",
      "Epoch [8823/10000] Avg train loss: 0.005257\n",
      "Epoch [8824/10000] Avg train loss: 0.005256\n",
      "Epoch [8825/10000] Avg train loss: 0.005256\n",
      "Epoch [8826/10000] Avg train loss: 0.005255\n",
      "Epoch [8827/10000] Avg train loss: 0.005254\n",
      "Epoch [8828/10000] Avg train loss: 0.005254\n",
      "Epoch [8829/10000] Avg train loss: 0.005253\n",
      "Epoch [8830/10000] Avg train loss: 0.005253\n",
      "Epoch [8831/10000] Avg train loss: 0.005252\n",
      "Epoch [8832/10000] Avg train loss: 0.005252\n",
      "Epoch [8833/10000] Avg train loss: 0.005251\n",
      "Epoch [8834/10000] Avg train loss: 0.005250\n",
      "Epoch [8835/10000] Avg train loss: 0.005250\n",
      "Epoch [8836/10000] Avg train loss: 0.005249\n",
      "Epoch [8837/10000] Avg train loss: 0.005249\n",
      "Epoch [8838/10000] Avg train loss: 0.005248\n",
      "Epoch [8839/10000] Avg train loss: 0.005247\n",
      "Epoch [8840/10000] Avg train loss: 0.005247\n",
      "Epoch [8841/10000] Avg train loss: 0.005246\n",
      "Epoch [8842/10000] Avg train loss: 0.005246\n",
      "Epoch [8843/10000] Avg train loss: 0.005245\n",
      "Epoch [8844/10000] Avg train loss: 0.005244\n",
      "Epoch [8845/10000] Avg train loss: 0.005244\n",
      "Epoch [8846/10000] Avg train loss: 0.005243\n",
      "Epoch [8847/10000] Avg train loss: 0.005243\n",
      "Epoch [8848/10000] Avg train loss: 0.005242\n",
      "Epoch [8849/10000] Avg train loss: 0.005241\n",
      "Epoch [8850/10000] Avg train loss: 0.005241\n",
      "Epoch [8851/10000] Avg train loss: 0.005240\n",
      "Epoch [8852/10000] Avg train loss: 0.005240\n",
      "Epoch [8853/10000] Avg train loss: 0.005239\n",
      "Epoch [8854/10000] Avg train loss: 0.005238\n",
      "Epoch [8855/10000] Avg train loss: 0.005238\n",
      "Epoch [8856/10000] Avg train loss: 0.005237\n",
      "Epoch [8857/10000] Avg train loss: 0.005237\n",
      "Epoch [8858/10000] Avg train loss: 0.005236\n",
      "Epoch [8859/10000] Avg train loss: 0.005235\n",
      "Epoch [8860/10000] Avg train loss: 0.005235\n",
      "Epoch [8861/10000] Avg train loss: 0.005234\n",
      "Epoch [8862/10000] Avg train loss: 0.005234\n",
      "Epoch [8863/10000] Avg train loss: 0.005233\n",
      "Epoch [8864/10000] Avg train loss: 0.005233\n",
      "Epoch [8865/10000] Avg train loss: 0.005232\n",
      "Epoch [8866/10000] Avg train loss: 0.005231\n",
      "Epoch [8867/10000] Avg train loss: 0.005231\n",
      "Epoch [8868/10000] Avg train loss: 0.005230\n",
      "Epoch [8869/10000] Avg train loss: 0.005230\n",
      "Epoch [8870/10000] Avg train loss: 0.005229\n",
      "Epoch [8871/10000] Avg train loss: 0.005228\n",
      "Epoch [8872/10000] Avg train loss: 0.005228\n",
      "Epoch [8873/10000] Avg train loss: 0.005227\n",
      "Epoch [8874/10000] Avg train loss: 0.005227\n",
      "Epoch [8875/10000] Avg train loss: 0.005226\n",
      "Epoch [8876/10000] Avg train loss: 0.005225\n",
      "Epoch [8877/10000] Avg train loss: 0.005225\n",
      "Epoch [8878/10000] Avg train loss: 0.005224\n",
      "Epoch [8879/10000] Avg train loss: 0.005224\n",
      "Epoch [8880/10000] Avg train loss: 0.005223\n",
      "Epoch [8881/10000] Avg train loss: 0.005223\n",
      "Epoch [8882/10000] Avg train loss: 0.005222\n",
      "Epoch [8883/10000] Avg train loss: 0.005221\n",
      "Epoch [8884/10000] Avg train loss: 0.005221\n",
      "Epoch [8885/10000] Avg train loss: 0.005220\n",
      "Epoch [8886/10000] Avg train loss: 0.005220\n",
      "Epoch [8887/10000] Avg train loss: 0.005219\n",
      "Epoch [8888/10000] Avg train loss: 0.005218\n",
      "Epoch [8889/10000] Avg train loss: 0.005218\n",
      "Epoch [8890/10000] Avg train loss: 0.005217\n",
      "Epoch [8891/10000] Avg train loss: 0.005217\n",
      "Epoch [8892/10000] Avg train loss: 0.005216\n",
      "Epoch [8893/10000] Avg train loss: 0.005215\n",
      "Epoch [8894/10000] Avg train loss: 0.005215\n",
      "Epoch [8895/10000] Avg train loss: 0.005214\n",
      "Epoch [8896/10000] Avg train loss: 0.005214\n",
      "Epoch [8897/10000] Avg train loss: 0.005213\n",
      "Epoch [8898/10000] Avg train loss: 0.005213\n",
      "Epoch [8899/10000] Avg train loss: 0.005212\n",
      "Epoch [8900/10000] Avg train loss: 0.005211\n",
      "Epoch [8901/10000] Avg train loss: 0.005211\n",
      "Epoch [8902/10000] Avg train loss: 0.005210\n",
      "Epoch [8903/10000] Avg train loss: 0.005210\n",
      "Epoch [8904/10000] Avg train loss: 0.005209\n",
      "Epoch [8905/10000] Avg train loss: 0.005208\n",
      "Epoch [8906/10000] Avg train loss: 0.005208\n",
      "Epoch [8907/10000] Avg train loss: 0.005207\n",
      "Epoch [8908/10000] Avg train loss: 0.005207\n",
      "Epoch [8909/10000] Avg train loss: 0.005206\n",
      "Epoch [8910/10000] Avg train loss: 0.005206\n",
      "Epoch [8911/10000] Avg train loss: 0.005205\n",
      "Epoch [8912/10000] Avg train loss: 0.005204\n",
      "Epoch [8913/10000] Avg train loss: 0.005204\n",
      "Epoch [8914/10000] Avg train loss: 0.005203\n",
      "Epoch [8915/10000] Avg train loss: 0.005203\n",
      "Epoch [8916/10000] Avg train loss: 0.005202\n",
      "Epoch [8917/10000] Avg train loss: 0.005201\n",
      "Epoch [8918/10000] Avg train loss: 0.005201\n",
      "Epoch [8919/10000] Avg train loss: 0.005200\n",
      "Epoch [8920/10000] Avg train loss: 0.005200\n",
      "Epoch [8921/10000] Avg train loss: 0.005199\n",
      "Epoch [8922/10000] Avg train loss: 0.005199\n",
      "Epoch [8923/10000] Avg train loss: 0.005198\n",
      "Epoch [8924/10000] Avg train loss: 0.005197\n",
      "Epoch [8925/10000] Avg train loss: 0.005197\n",
      "Epoch [8926/10000] Avg train loss: 0.005196\n",
      "Epoch [8927/10000] Avg train loss: 0.005196\n",
      "Epoch [8928/10000] Avg train loss: 0.005195\n",
      "Epoch [8929/10000] Avg train loss: 0.005194\n",
      "Epoch [8930/10000] Avg train loss: 0.005194\n",
      "Epoch [8931/10000] Avg train loss: 0.005193\n",
      "Epoch [8932/10000] Avg train loss: 0.005193\n",
      "Epoch [8933/10000] Avg train loss: 0.005192\n",
      "Epoch [8934/10000] Avg train loss: 0.005192\n",
      "Epoch [8935/10000] Avg train loss: 0.005191\n",
      "Epoch [8936/10000] Avg train loss: 0.005190\n",
      "Epoch [8937/10000] Avg train loss: 0.005190\n",
      "Epoch [8938/10000] Avg train loss: 0.005189\n",
      "Epoch [8939/10000] Avg train loss: 0.005189\n",
      "Epoch [8940/10000] Avg train loss: 0.005188\n",
      "Epoch [8941/10000] Avg train loss: 0.005187\n",
      "Epoch [8942/10000] Avg train loss: 0.005187\n",
      "Epoch [8943/10000] Avg train loss: 0.005186\n",
      "Epoch [8944/10000] Avg train loss: 0.005186\n",
      "Epoch [8945/10000] Avg train loss: 0.005185\n",
      "Epoch [8946/10000] Avg train loss: 0.005185\n",
      "Epoch [8947/10000] Avg train loss: 0.005184\n",
      "Epoch [8948/10000] Avg train loss: 0.005183\n",
      "Epoch [8949/10000] Avg train loss: 0.005183\n",
      "Epoch [8950/10000] Avg train loss: 0.005182\n",
      "Epoch [8951/10000] Avg train loss: 0.005182\n",
      "Epoch [8952/10000] Avg train loss: 0.005181\n",
      "Epoch [8953/10000] Avg train loss: 0.005181\n",
      "Epoch [8954/10000] Avg train loss: 0.005180\n",
      "Epoch [8955/10000] Avg train loss: 0.005179\n",
      "Epoch [8956/10000] Avg train loss: 0.005179\n",
      "Epoch [8957/10000] Avg train loss: 0.005178\n",
      "Epoch [8958/10000] Avg train loss: 0.005178\n",
      "Epoch [8959/10000] Avg train loss: 0.005177\n",
      "Epoch [8960/10000] Avg train loss: 0.005176\n",
      "Epoch [8961/10000] Avg train loss: 0.005176\n",
      "Epoch [8962/10000] Avg train loss: 0.005175\n",
      "Epoch [8963/10000] Avg train loss: 0.005175\n",
      "Epoch [8964/10000] Avg train loss: 0.005174\n",
      "Epoch [8965/10000] Avg train loss: 0.005174\n",
      "Epoch [8966/10000] Avg train loss: 0.005173\n",
      "Epoch [8967/10000] Avg train loss: 0.005172\n",
      "Epoch [8968/10000] Avg train loss: 0.005172\n",
      "Epoch [8969/10000] Avg train loss: 0.005171\n",
      "Epoch [8970/10000] Avg train loss: 0.005171\n",
      "Epoch [8971/10000] Avg train loss: 0.005170\n",
      "Epoch [8972/10000] Avg train loss: 0.005170\n",
      "Epoch [8973/10000] Avg train loss: 0.005169\n",
      "Epoch [8974/10000] Avg train loss: 0.005168\n",
      "Epoch [8975/10000] Avg train loss: 0.005168\n",
      "Epoch [8976/10000] Avg train loss: 0.005167\n",
      "Epoch [8977/10000] Avg train loss: 0.005167\n",
      "Epoch [8978/10000] Avg train loss: 0.005166\n",
      "Epoch [8979/10000] Avg train loss: 0.005166\n",
      "Epoch [8980/10000] Avg train loss: 0.005165\n",
      "Epoch [8981/10000] Avg train loss: 0.005164\n",
      "Epoch [8982/10000] Avg train loss: 0.005164\n",
      "Epoch [8983/10000] Avg train loss: 0.005163\n",
      "Epoch [8984/10000] Avg train loss: 0.005163\n",
      "Epoch [8985/10000] Avg train loss: 0.005162\n",
      "Epoch [8986/10000] Avg train loss: 0.005162\n",
      "Epoch [8987/10000] Avg train loss: 0.005161\n",
      "Epoch [8988/10000] Avg train loss: 0.005160\n",
      "Epoch [8989/10000] Avg train loss: 0.005160\n",
      "Epoch [8990/10000] Avg train loss: 0.005159\n",
      "Epoch [8991/10000] Avg train loss: 0.005159\n",
      "Epoch [8992/10000] Avg train loss: 0.005158\n",
      "Epoch [8993/10000] Avg train loss: 0.005157\n",
      "Epoch [8994/10000] Avg train loss: 0.005157\n",
      "Epoch [8995/10000] Avg train loss: 0.005156\n",
      "Epoch [8996/10000] Avg train loss: 0.005156\n",
      "Epoch [8997/10000] Avg train loss: 0.005155\n",
      "Epoch [8998/10000] Avg train loss: 0.005155\n",
      "Epoch [8999/10000] Avg train loss: 0.005154\n",
      "Epoch [9000/10000] Avg train loss: 0.005153\n",
      "Epoch [9001/10000] Avg train loss: 0.005153\n",
      "Epoch [9002/10000] Avg train loss: 0.005152\n",
      "Epoch [9003/10000] Avg train loss: 0.005152\n",
      "Epoch [9004/10000] Avg train loss: 0.005151\n",
      "Epoch [9005/10000] Avg train loss: 0.005151\n",
      "Epoch [9006/10000] Avg train loss: 0.005150\n",
      "Epoch [9007/10000] Avg train loss: 0.005149\n",
      "Epoch [9008/10000] Avg train loss: 0.005149\n",
      "Epoch [9009/10000] Avg train loss: 0.005148\n",
      "Epoch [9010/10000] Avg train loss: 0.005148\n",
      "Epoch [9011/10000] Avg train loss: 0.005147\n",
      "Epoch [9012/10000] Avg train loss: 0.005147\n",
      "Epoch [9013/10000] Avg train loss: 0.005146\n",
      "Epoch [9014/10000] Avg train loss: 0.005145\n",
      "Epoch [9015/10000] Avg train loss: 0.005145\n",
      "Epoch [9016/10000] Avg train loss: 0.005144\n",
      "Epoch [9017/10000] Avg train loss: 0.005144\n",
      "Epoch [9018/10000] Avg train loss: 0.005143\n",
      "Epoch [9019/10000] Avg train loss: 0.005143\n",
      "Epoch [9020/10000] Avg train loss: 0.005142\n",
      "Epoch [9021/10000] Avg train loss: 0.005141\n",
      "Epoch [9022/10000] Avg train loss: 0.005141\n",
      "Epoch [9023/10000] Avg train loss: 0.005140\n",
      "Epoch [9024/10000] Avg train loss: 0.005140\n",
      "Epoch [9025/10000] Avg train loss: 0.005139\n",
      "Epoch [9026/10000] Avg train loss: 0.005139\n",
      "Epoch [9027/10000] Avg train loss: 0.005138\n",
      "Epoch [9028/10000] Avg train loss: 0.005137\n",
      "Epoch [9029/10000] Avg train loss: 0.005137\n",
      "Epoch [9030/10000] Avg train loss: 0.005136\n",
      "Epoch [9031/10000] Avg train loss: 0.005136\n",
      "Epoch [9032/10000] Avg train loss: 0.005135\n",
      "Epoch [9033/10000] Avg train loss: 0.005135\n",
      "Epoch [9034/10000] Avg train loss: 0.005134\n",
      "Epoch [9035/10000] Avg train loss: 0.005134\n",
      "Epoch [9036/10000] Avg train loss: 0.005133\n",
      "Epoch [9037/10000] Avg train loss: 0.005132\n",
      "Epoch [9038/10000] Avg train loss: 0.005132\n",
      "Epoch [9039/10000] Avg train loss: 0.005131\n",
      "Epoch [9040/10000] Avg train loss: 0.005131\n",
      "Epoch [9041/10000] Avg train loss: 0.005130\n",
      "Epoch [9042/10000] Avg train loss: 0.005130\n",
      "Epoch [9043/10000] Avg train loss: 0.005129\n",
      "Epoch [9044/10000] Avg train loss: 0.005128\n",
      "Epoch [9045/10000] Avg train loss: 0.005128\n",
      "Epoch [9046/10000] Avg train loss: 0.005127\n",
      "Epoch [9047/10000] Avg train loss: 0.005127\n",
      "Epoch [9048/10000] Avg train loss: 0.005126\n",
      "Epoch [9049/10000] Avg train loss: 0.005126\n",
      "Epoch [9050/10000] Avg train loss: 0.005125\n",
      "Epoch [9051/10000] Avg train loss: 0.005124\n",
      "Epoch [9052/10000] Avg train loss: 0.005124\n",
      "Epoch [9053/10000] Avg train loss: 0.005123\n",
      "Epoch [9054/10000] Avg train loss: 0.005123\n",
      "Epoch [9055/10000] Avg train loss: 0.005122\n",
      "Epoch [9056/10000] Avg train loss: 0.005122\n",
      "Epoch [9057/10000] Avg train loss: 0.005121\n",
      "Epoch [9058/10000] Avg train loss: 0.005120\n",
      "Epoch [9059/10000] Avg train loss: 0.005120\n",
      "Epoch [9060/10000] Avg train loss: 0.005119\n",
      "Epoch [9061/10000] Avg train loss: 0.005119\n",
      "Epoch [9062/10000] Avg train loss: 0.005118\n",
      "Epoch [9063/10000] Avg train loss: 0.005118\n",
      "Epoch [9064/10000] Avg train loss: 0.005117\n",
      "Epoch [9065/10000] Avg train loss: 0.005117\n",
      "Epoch [9066/10000] Avg train loss: 0.005116\n",
      "Epoch [9067/10000] Avg train loss: 0.005115\n",
      "Epoch [9068/10000] Avg train loss: 0.005115\n",
      "Epoch [9069/10000] Avg train loss: 0.005114\n",
      "Epoch [9070/10000] Avg train loss: 0.005114\n",
      "Epoch [9071/10000] Avg train loss: 0.005113\n",
      "Epoch [9072/10000] Avg train loss: 0.005113\n",
      "Epoch [9073/10000] Avg train loss: 0.005112\n",
      "Epoch [9074/10000] Avg train loss: 0.005111\n",
      "Epoch [9075/10000] Avg train loss: 0.005111\n",
      "Epoch [9076/10000] Avg train loss: 0.005110\n",
      "Epoch [9077/10000] Avg train loss: 0.005110\n",
      "Epoch [9078/10000] Avg train loss: 0.005109\n",
      "Epoch [9079/10000] Avg train loss: 0.005109\n",
      "Epoch [9080/10000] Avg train loss: 0.005108\n",
      "Epoch [9081/10000] Avg train loss: 0.005108\n",
      "Epoch [9082/10000] Avg train loss: 0.005107\n",
      "Epoch [9083/10000] Avg train loss: 0.005106\n",
      "Epoch [9084/10000] Avg train loss: 0.005106\n",
      "Epoch [9085/10000] Avg train loss: 0.005105\n",
      "Epoch [9086/10000] Avg train loss: 0.005105\n",
      "Epoch [9087/10000] Avg train loss: 0.005104\n",
      "Epoch [9088/10000] Avg train loss: 0.005104\n",
      "Epoch [9089/10000] Avg train loss: 0.005103\n",
      "Epoch [9090/10000] Avg train loss: 0.005102\n",
      "Epoch [9091/10000] Avg train loss: 0.005102\n",
      "Epoch [9092/10000] Avg train loss: 0.005101\n",
      "Epoch [9093/10000] Avg train loss: 0.005101\n",
      "Epoch [9094/10000] Avg train loss: 0.005100\n",
      "Epoch [9095/10000] Avg train loss: 0.005100\n",
      "Epoch [9096/10000] Avg train loss: 0.005099\n",
      "Epoch [9097/10000] Avg train loss: 0.005099\n",
      "Epoch [9098/10000] Avg train loss: 0.005098\n",
      "Epoch [9099/10000] Avg train loss: 0.005097\n",
      "Epoch [9100/10000] Avg train loss: 0.005097\n",
      "Epoch [9101/10000] Avg train loss: 0.005096\n",
      "Epoch [9102/10000] Avg train loss: 0.005096\n",
      "Epoch [9103/10000] Avg train loss: 0.005095\n",
      "Epoch [9104/10000] Avg train loss: 0.005095\n",
      "Epoch [9105/10000] Avg train loss: 0.005094\n",
      "Epoch [9106/10000] Avg train loss: 0.005093\n",
      "Epoch [9107/10000] Avg train loss: 0.005093\n",
      "Epoch [9108/10000] Avg train loss: 0.005092\n",
      "Epoch [9109/10000] Avg train loss: 0.005092\n",
      "Epoch [9110/10000] Avg train loss: 0.005091\n",
      "Epoch [9111/10000] Avg train loss: 0.005091\n",
      "Epoch [9112/10000] Avg train loss: 0.005090\n",
      "Epoch [9113/10000] Avg train loss: 0.005090\n",
      "Epoch [9114/10000] Avg train loss: 0.005089\n",
      "Epoch [9115/10000] Avg train loss: 0.005088\n",
      "Epoch [9116/10000] Avg train loss: 0.005088\n",
      "Epoch [9117/10000] Avg train loss: 0.005087\n",
      "Epoch [9118/10000] Avg train loss: 0.005087\n",
      "Epoch [9119/10000] Avg train loss: 0.005086\n",
      "Epoch [9120/10000] Avg train loss: 0.005086\n",
      "Epoch [9121/10000] Avg train loss: 0.005085\n",
      "Epoch [9122/10000] Avg train loss: 0.005085\n",
      "Epoch [9123/10000] Avg train loss: 0.005084\n",
      "Epoch [9124/10000] Avg train loss: 0.005083\n",
      "Epoch [9125/10000] Avg train loss: 0.005083\n",
      "Epoch [9126/10000] Avg train loss: 0.005082\n",
      "Epoch [9127/10000] Avg train loss: 0.005082\n",
      "Epoch [9128/10000] Avg train loss: 0.005081\n",
      "Epoch [9129/10000] Avg train loss: 0.005081\n",
      "Epoch [9130/10000] Avg train loss: 0.005080\n",
      "Epoch [9131/10000] Avg train loss: 0.005080\n",
      "Epoch [9132/10000] Avg train loss: 0.005079\n",
      "Epoch [9133/10000] Avg train loss: 0.005078\n",
      "Epoch [9134/10000] Avg train loss: 0.005078\n",
      "Epoch [9135/10000] Avg train loss: 0.005077\n",
      "Epoch [9136/10000] Avg train loss: 0.005077\n",
      "Epoch [9137/10000] Avg train loss: 0.005076\n",
      "Epoch [9138/10000] Avg train loss: 0.005076\n",
      "Epoch [9139/10000] Avg train loss: 0.005075\n",
      "Epoch [9140/10000] Avg train loss: 0.005075\n",
      "Epoch [9141/10000] Avg train loss: 0.005074\n",
      "Epoch [9142/10000] Avg train loss: 0.005073\n",
      "Epoch [9143/10000] Avg train loss: 0.005073\n",
      "Epoch [9144/10000] Avg train loss: 0.005072\n",
      "Epoch [9145/10000] Avg train loss: 0.005072\n",
      "Epoch [9146/10000] Avg train loss: 0.005071\n",
      "Epoch [9147/10000] Avg train loss: 0.005071\n",
      "Epoch [9148/10000] Avg train loss: 0.005070\n",
      "Epoch [9149/10000] Avg train loss: 0.005070\n",
      "Epoch [9150/10000] Avg train loss: 0.005069\n",
      "Epoch [9151/10000] Avg train loss: 0.005068\n",
      "Epoch [9152/10000] Avg train loss: 0.005068\n",
      "Epoch [9153/10000] Avg train loss: 0.005067\n",
      "Epoch [9154/10000] Avg train loss: 0.005067\n",
      "Epoch [9155/10000] Avg train loss: 0.005066\n",
      "Epoch [9156/10000] Avg train loss: 0.005066\n",
      "Epoch [9157/10000] Avg train loss: 0.005065\n",
      "Epoch [9158/10000] Avg train loss: 0.005065\n",
      "Epoch [9159/10000] Avg train loss: 0.005064\n",
      "Epoch [9160/10000] Avg train loss: 0.005063\n",
      "Epoch [9161/10000] Avg train loss: 0.005063\n",
      "Epoch [9162/10000] Avg train loss: 0.005062\n",
      "Epoch [9163/10000] Avg train loss: 0.005062\n",
      "Epoch [9164/10000] Avg train loss: 0.005061\n",
      "Epoch [9165/10000] Avg train loss: 0.005061\n",
      "Epoch [9166/10000] Avg train loss: 0.005060\n",
      "Epoch [9167/10000] Avg train loss: 0.005060\n",
      "Epoch [9168/10000] Avg train loss: 0.005059\n",
      "Epoch [9169/10000] Avg train loss: 0.005058\n",
      "Epoch [9170/10000] Avg train loss: 0.005058\n",
      "Epoch [9171/10000] Avg train loss: 0.005057\n",
      "Epoch [9172/10000] Avg train loss: 0.005057\n",
      "Epoch [9173/10000] Avg train loss: 0.005056\n",
      "Epoch [9174/10000] Avg train loss: 0.005056\n",
      "Epoch [9175/10000] Avg train loss: 0.005055\n",
      "Epoch [9176/10000] Avg train loss: 0.005055\n",
      "Epoch [9177/10000] Avg train loss: 0.005054\n",
      "Epoch [9178/10000] Avg train loss: 0.005054\n",
      "Epoch [9179/10000] Avg train loss: 0.005053\n",
      "Epoch [9180/10000] Avg train loss: 0.005052\n",
      "Epoch [9181/10000] Avg train loss: 0.005052\n",
      "Epoch [9182/10000] Avg train loss: 0.005051\n",
      "Epoch [9183/10000] Avg train loss: 0.005051\n",
      "Epoch [9184/10000] Avg train loss: 0.005050\n",
      "Epoch [9185/10000] Avg train loss: 0.005050\n",
      "Epoch [9186/10000] Avg train loss: 0.005049\n",
      "Epoch [9187/10000] Avg train loss: 0.005049\n",
      "Epoch [9188/10000] Avg train loss: 0.005048\n",
      "Epoch [9189/10000] Avg train loss: 0.005047\n",
      "Epoch [9190/10000] Avg train loss: 0.005047\n",
      "Epoch [9191/10000] Avg train loss: 0.005046\n",
      "Epoch [9192/10000] Avg train loss: 0.005046\n",
      "Epoch [9193/10000] Avg train loss: 0.005045\n",
      "Epoch [9194/10000] Avg train loss: 0.005045\n",
      "Epoch [9195/10000] Avg train loss: 0.005044\n",
      "Epoch [9196/10000] Avg train loss: 0.005044\n",
      "Epoch [9197/10000] Avg train loss: 0.005043\n",
      "Epoch [9198/10000] Avg train loss: 0.005043\n",
      "Epoch [9199/10000] Avg train loss: 0.005042\n",
      "Epoch [9200/10000] Avg train loss: 0.005041\n",
      "Epoch [9201/10000] Avg train loss: 0.005041\n",
      "Epoch [9202/10000] Avg train loss: 0.005040\n",
      "Epoch [9203/10000] Avg train loss: 0.005040\n",
      "Epoch [9204/10000] Avg train loss: 0.005039\n",
      "Epoch [9205/10000] Avg train loss: 0.005039\n",
      "Epoch [9206/10000] Avg train loss: 0.005038\n",
      "Epoch [9207/10000] Avg train loss: 0.005038\n",
      "Epoch [9208/10000] Avg train loss: 0.005037\n",
      "Epoch [9209/10000] Avg train loss: 0.005037\n",
      "Epoch [9210/10000] Avg train loss: 0.005036\n",
      "Epoch [9211/10000] Avg train loss: 0.005035\n",
      "Epoch [9212/10000] Avg train loss: 0.005035\n",
      "Epoch [9213/10000] Avg train loss: 0.005034\n",
      "Epoch [9214/10000] Avg train loss: 0.005034\n",
      "Epoch [9215/10000] Avg train loss: 0.005033\n",
      "Epoch [9216/10000] Avg train loss: 0.005033\n",
      "Epoch [9217/10000] Avg train loss: 0.005032\n",
      "Epoch [9218/10000] Avg train loss: 0.005032\n",
      "Epoch [9219/10000] Avg train loss: 0.005031\n",
      "Epoch [9220/10000] Avg train loss: 0.005031\n",
      "Epoch [9221/10000] Avg train loss: 0.005030\n",
      "Epoch [9222/10000] Avg train loss: 0.005029\n",
      "Epoch [9223/10000] Avg train loss: 0.005029\n",
      "Epoch [9224/10000] Avg train loss: 0.005028\n",
      "Epoch [9225/10000] Avg train loss: 0.005028\n",
      "Epoch [9226/10000] Avg train loss: 0.005027\n",
      "Epoch [9227/10000] Avg train loss: 0.005027\n",
      "Epoch [9228/10000] Avg train loss: 0.005026\n",
      "Epoch [9229/10000] Avg train loss: 0.005026\n",
      "Epoch [9230/10000] Avg train loss: 0.005025\n",
      "Epoch [9231/10000] Avg train loss: 0.005025\n",
      "Epoch [9232/10000] Avg train loss: 0.005024\n",
      "Epoch [9233/10000] Avg train loss: 0.005023\n",
      "Epoch [9234/10000] Avg train loss: 0.005023\n",
      "Epoch [9235/10000] Avg train loss: 0.005022\n",
      "Epoch [9236/10000] Avg train loss: 0.005022\n",
      "Epoch [9237/10000] Avg train loss: 0.005021\n",
      "Epoch [9238/10000] Avg train loss: 0.005021\n",
      "Epoch [9239/10000] Avg train loss: 0.005020\n",
      "Epoch [9240/10000] Avg train loss: 0.005020\n",
      "Epoch [9241/10000] Avg train loss: 0.005019\n",
      "Epoch [9242/10000] Avg train loss: 0.005019\n",
      "Epoch [9243/10000] Avg train loss: 0.005018\n",
      "Epoch [9244/10000] Avg train loss: 0.005017\n",
      "Epoch [9245/10000] Avg train loss: 0.005017\n",
      "Epoch [9246/10000] Avg train loss: 0.005016\n",
      "Epoch [9247/10000] Avg train loss: 0.005016\n",
      "Epoch [9248/10000] Avg train loss: 0.005015\n",
      "Epoch [9249/10000] Avg train loss: 0.005015\n",
      "Epoch [9250/10000] Avg train loss: 0.005014\n",
      "Epoch [9251/10000] Avg train loss: 0.005014\n",
      "Epoch [9252/10000] Avg train loss: 0.005013\n",
      "Epoch [9253/10000] Avg train loss: 0.005013\n",
      "Epoch [9254/10000] Avg train loss: 0.005012\n",
      "Epoch [9255/10000] Avg train loss: 0.005011\n",
      "Epoch [9256/10000] Avg train loss: 0.005011\n",
      "Epoch [9257/10000] Avg train loss: 0.005010\n",
      "Epoch [9258/10000] Avg train loss: 0.005010\n",
      "Epoch [9259/10000] Avg train loss: 0.005009\n",
      "Epoch [9260/10000] Avg train loss: 0.005009\n",
      "Epoch [9261/10000] Avg train loss: 0.005008\n",
      "Epoch [9262/10000] Avg train loss: 0.005008\n",
      "Epoch [9263/10000] Avg train loss: 0.005007\n",
      "Epoch [9264/10000] Avg train loss: 0.005007\n",
      "Epoch [9265/10000] Avg train loss: 0.005006\n",
      "Epoch [9266/10000] Avg train loss: 0.005006\n",
      "Epoch [9267/10000] Avg train loss: 0.005005\n",
      "Epoch [9268/10000] Avg train loss: 0.005004\n",
      "Epoch [9269/10000] Avg train loss: 0.005004\n",
      "Epoch [9270/10000] Avg train loss: 0.005003\n",
      "Epoch [9271/10000] Avg train loss: 0.005003\n",
      "Epoch [9272/10000] Avg train loss: 0.005002\n",
      "Epoch [9273/10000] Avg train loss: 0.005002\n",
      "Epoch [9274/10000] Avg train loss: 0.005001\n",
      "Epoch [9275/10000] Avg train loss: 0.005001\n",
      "Epoch [9276/10000] Avg train loss: 0.005000\n",
      "Epoch [9277/10000] Avg train loss: 0.005000\n",
      "Epoch [9278/10000] Avg train loss: 0.004999\n",
      "Epoch [9279/10000] Avg train loss: 0.004999\n",
      "Epoch [9280/10000] Avg train loss: 0.004998\n",
      "Epoch [9281/10000] Avg train loss: 0.004997\n",
      "Epoch [9282/10000] Avg train loss: 0.004997\n",
      "Epoch [9283/10000] Avg train loss: 0.004996\n",
      "Epoch [9284/10000] Avg train loss: 0.004996\n",
      "Epoch [9285/10000] Avg train loss: 0.004995\n",
      "Epoch [9286/10000] Avg train loss: 0.004995\n",
      "Epoch [9287/10000] Avg train loss: 0.004994\n",
      "Epoch [9288/10000] Avg train loss: 0.004994\n",
      "Epoch [9289/10000] Avg train loss: 0.004993\n",
      "Epoch [9290/10000] Avg train loss: 0.004993\n",
      "Epoch [9291/10000] Avg train loss: 0.004992\n",
      "Epoch [9292/10000] Avg train loss: 0.004992\n",
      "Epoch [9293/10000] Avg train loss: 0.004991\n",
      "Epoch [9294/10000] Avg train loss: 0.004990\n",
      "Epoch [9295/10000] Avg train loss: 0.004990\n",
      "Epoch [9296/10000] Avg train loss: 0.004989\n",
      "Epoch [9297/10000] Avg train loss: 0.004989\n",
      "Epoch [9298/10000] Avg train loss: 0.004988\n",
      "Epoch [9299/10000] Avg train loss: 0.004988\n",
      "Epoch [9300/10000] Avg train loss: 0.004987\n",
      "Epoch [9301/10000] Avg train loss: 0.004987\n",
      "Epoch [9302/10000] Avg train loss: 0.004986\n",
      "Epoch [9303/10000] Avg train loss: 0.004986\n",
      "Epoch [9304/10000] Avg train loss: 0.004985\n",
      "Epoch [9305/10000] Avg train loss: 0.004985\n",
      "Epoch [9306/10000] Avg train loss: 0.004984\n",
      "Epoch [9307/10000] Avg train loss: 0.004983\n",
      "Epoch [9308/10000] Avg train loss: 0.004983\n",
      "Epoch [9309/10000] Avg train loss: 0.004982\n",
      "Epoch [9310/10000] Avg train loss: 0.004982\n",
      "Epoch [9311/10000] Avg train loss: 0.004981\n",
      "Epoch [9312/10000] Avg train loss: 0.004981\n",
      "Epoch [9313/10000] Avg train loss: 0.004980\n",
      "Epoch [9314/10000] Avg train loss: 0.004980\n",
      "Epoch [9315/10000] Avg train loss: 0.004979\n",
      "Epoch [9316/10000] Avg train loss: 0.004979\n",
      "Epoch [9317/10000] Avg train loss: 0.004978\n",
      "Epoch [9318/10000] Avg train loss: 0.004978\n",
      "Epoch [9319/10000] Avg train loss: 0.004977\n",
      "Epoch [9320/10000] Avg train loss: 0.004977\n",
      "Epoch [9321/10000] Avg train loss: 0.004976\n",
      "Epoch [9322/10000] Avg train loss: 0.004975\n",
      "Epoch [9323/10000] Avg train loss: 0.004975\n",
      "Epoch [9324/10000] Avg train loss: 0.004974\n",
      "Epoch [9325/10000] Avg train loss: 0.004974\n",
      "Epoch [9326/10000] Avg train loss: 0.004973\n",
      "Epoch [9327/10000] Avg train loss: 0.004973\n",
      "Epoch [9328/10000] Avg train loss: 0.004972\n",
      "Epoch [9329/10000] Avg train loss: 0.004972\n",
      "Epoch [9330/10000] Avg train loss: 0.004971\n",
      "Epoch [9331/10000] Avg train loss: 0.004971\n",
      "Epoch [9332/10000] Avg train loss: 0.004970\n",
      "Epoch [9333/10000] Avg train loss: 0.004970\n",
      "Epoch [9334/10000] Avg train loss: 0.004969\n",
      "Epoch [9335/10000] Avg train loss: 0.004969\n",
      "Epoch [9336/10000] Avg train loss: 0.004968\n",
      "Epoch [9337/10000] Avg train loss: 0.004967\n",
      "Epoch [9338/10000] Avg train loss: 0.004967\n",
      "Epoch [9339/10000] Avg train loss: 0.004966\n",
      "Epoch [9340/10000] Avg train loss: 0.004966\n",
      "Epoch [9341/10000] Avg train loss: 0.004965\n",
      "Epoch [9342/10000] Avg train loss: 0.004965\n",
      "Epoch [9343/10000] Avg train loss: 0.004964\n",
      "Epoch [9344/10000] Avg train loss: 0.004964\n",
      "Epoch [9345/10000] Avg train loss: 0.004963\n",
      "Epoch [9346/10000] Avg train loss: 0.004963\n",
      "Epoch [9347/10000] Avg train loss: 0.004962\n",
      "Epoch [9348/10000] Avg train loss: 0.004962\n",
      "Epoch [9349/10000] Avg train loss: 0.004961\n",
      "Epoch [9350/10000] Avg train loss: 0.004961\n",
      "Epoch [9351/10000] Avg train loss: 0.004960\n",
      "Epoch [9352/10000] Avg train loss: 0.004960\n",
      "Epoch [9353/10000] Avg train loss: 0.004959\n",
      "Epoch [9354/10000] Avg train loss: 0.004958\n",
      "Epoch [9355/10000] Avg train loss: 0.004958\n",
      "Epoch [9356/10000] Avg train loss: 0.004957\n",
      "Epoch [9357/10000] Avg train loss: 0.004957\n",
      "Epoch [9358/10000] Avg train loss: 0.004956\n",
      "Epoch [9359/10000] Avg train loss: 0.004956\n",
      "Epoch [9360/10000] Avg train loss: 0.004955\n",
      "Epoch [9361/10000] Avg train loss: 0.004955\n",
      "Epoch [9362/10000] Avg train loss: 0.004954\n",
      "Epoch [9363/10000] Avg train loss: 0.004954\n",
      "Epoch [9364/10000] Avg train loss: 0.004953\n",
      "Epoch [9365/10000] Avg train loss: 0.004953\n",
      "Epoch [9366/10000] Avg train loss: 0.004952\n",
      "Epoch [9367/10000] Avg train loss: 0.004952\n",
      "Epoch [9368/10000] Avg train loss: 0.004951\n",
      "Epoch [9369/10000] Avg train loss: 0.004951\n",
      "Epoch [9370/10000] Avg train loss: 0.004950\n",
      "Epoch [9371/10000] Avg train loss: 0.004949\n",
      "Epoch [9372/10000] Avg train loss: 0.004949\n",
      "Epoch [9373/10000] Avg train loss: 0.004948\n",
      "Epoch [9374/10000] Avg train loss: 0.004948\n",
      "Epoch [9375/10000] Avg train loss: 0.004947\n",
      "Epoch [9376/10000] Avg train loss: 0.004947\n",
      "Epoch [9377/10000] Avg train loss: 0.004946\n",
      "Epoch [9378/10000] Avg train loss: 0.004946\n",
      "Epoch [9379/10000] Avg train loss: 0.004945\n",
      "Epoch [9380/10000] Avg train loss: 0.004945\n",
      "Epoch [9381/10000] Avg train loss: 0.004944\n",
      "Epoch [9382/10000] Avg train loss: 0.004944\n",
      "Epoch [9383/10000] Avg train loss: 0.004943\n",
      "Epoch [9384/10000] Avg train loss: 0.004943\n",
      "Epoch [9385/10000] Avg train loss: 0.004942\n",
      "Epoch [9386/10000] Avg train loss: 0.004942\n",
      "Epoch [9387/10000] Avg train loss: 0.004941\n",
      "Epoch [9388/10000] Avg train loss: 0.004940\n",
      "Epoch [9389/10000] Avg train loss: 0.004940\n",
      "Epoch [9390/10000] Avg train loss: 0.004939\n",
      "Epoch [9391/10000] Avg train loss: 0.004939\n",
      "Epoch [9392/10000] Avg train loss: 0.004938\n",
      "Epoch [9393/10000] Avg train loss: 0.004938\n",
      "Epoch [9394/10000] Avg train loss: 0.004937\n",
      "Epoch [9395/10000] Avg train loss: 0.004937\n",
      "Epoch [9396/10000] Avg train loss: 0.004936\n",
      "Epoch [9397/10000] Avg train loss: 0.004936\n",
      "Epoch [9398/10000] Avg train loss: 0.004935\n",
      "Epoch [9399/10000] Avg train loss: 0.004935\n",
      "Epoch [9400/10000] Avg train loss: 0.004934\n",
      "Epoch [9401/10000] Avg train loss: 0.004934\n",
      "Epoch [9402/10000] Avg train loss: 0.004933\n",
      "Epoch [9403/10000] Avg train loss: 0.004933\n",
      "Epoch [9404/10000] Avg train loss: 0.004932\n",
      "Epoch [9405/10000] Avg train loss: 0.004932\n",
      "Epoch [9406/10000] Avg train loss: 0.004931\n",
      "Epoch [9407/10000] Avg train loss: 0.004931\n",
      "Epoch [9408/10000] Avg train loss: 0.004930\n",
      "Epoch [9409/10000] Avg train loss: 0.004929\n",
      "Epoch [9410/10000] Avg train loss: 0.004929\n",
      "Epoch [9411/10000] Avg train loss: 0.004928\n",
      "Epoch [9412/10000] Avg train loss: 0.004928\n",
      "Epoch [9413/10000] Avg train loss: 0.004927\n",
      "Epoch [9414/10000] Avg train loss: 0.004927\n",
      "Epoch [9415/10000] Avg train loss: 0.004926\n",
      "Epoch [9416/10000] Avg train loss: 0.004926\n",
      "Epoch [9417/10000] Avg train loss: 0.004925\n",
      "Epoch [9418/10000] Avg train loss: 0.004925\n",
      "Epoch [9419/10000] Avg train loss: 0.004924\n",
      "Epoch [9420/10000] Avg train loss: 0.004924\n",
      "Epoch [9421/10000] Avg train loss: 0.004923\n",
      "Epoch [9422/10000] Avg train loss: 0.004923\n",
      "Epoch [9423/10000] Avg train loss: 0.004922\n",
      "Epoch [9424/10000] Avg train loss: 0.004922\n",
      "Epoch [9425/10000] Avg train loss: 0.004921\n",
      "Epoch [9426/10000] Avg train loss: 0.004921\n",
      "Epoch [9427/10000] Avg train loss: 0.004920\n",
      "Epoch [9428/10000] Avg train loss: 0.004920\n",
      "Epoch [9429/10000] Avg train loss: 0.004919\n",
      "Epoch [9430/10000] Avg train loss: 0.004918\n",
      "Epoch [9431/10000] Avg train loss: 0.004918\n",
      "Epoch [9432/10000] Avg train loss: 0.004917\n",
      "Epoch [9433/10000] Avg train loss: 0.004917\n",
      "Epoch [9434/10000] Avg train loss: 0.004916\n",
      "Epoch [9435/10000] Avg train loss: 0.004916\n",
      "Epoch [9436/10000] Avg train loss: 0.004915\n",
      "Epoch [9437/10000] Avg train loss: 0.004915\n",
      "Epoch [9438/10000] Avg train loss: 0.004914\n",
      "Epoch [9439/10000] Avg train loss: 0.004914\n",
      "Epoch [9440/10000] Avg train loss: 0.004913\n",
      "Epoch [9441/10000] Avg train loss: 0.004913\n",
      "Epoch [9442/10000] Avg train loss: 0.004912\n",
      "Epoch [9443/10000] Avg train loss: 0.004912\n",
      "Epoch [9444/10000] Avg train loss: 0.004911\n",
      "Epoch [9445/10000] Avg train loss: 0.004911\n",
      "Epoch [9446/10000] Avg train loss: 0.004910\n",
      "Epoch [9447/10000] Avg train loss: 0.004910\n",
      "Epoch [9448/10000] Avg train loss: 0.004909\n",
      "Epoch [9449/10000] Avg train loss: 0.004909\n",
      "Epoch [9450/10000] Avg train loss: 0.004908\n",
      "Epoch [9451/10000] Avg train loss: 0.004908\n",
      "Epoch [9452/10000] Avg train loss: 0.004907\n",
      "Epoch [9453/10000] Avg train loss: 0.004907\n",
      "Epoch [9454/10000] Avg train loss: 0.004906\n",
      "Epoch [9455/10000] Avg train loss: 0.004905\n",
      "Epoch [9456/10000] Avg train loss: 0.004905\n",
      "Epoch [9457/10000] Avg train loss: 0.004904\n",
      "Epoch [9458/10000] Avg train loss: 0.004904\n",
      "Epoch [9459/10000] Avg train loss: 0.004903\n",
      "Epoch [9460/10000] Avg train loss: 0.004903\n",
      "Epoch [9461/10000] Avg train loss: 0.004902\n",
      "Epoch [9462/10000] Avg train loss: 0.004902\n",
      "Epoch [9463/10000] Avg train loss: 0.004901\n",
      "Epoch [9464/10000] Avg train loss: 0.004901\n",
      "Epoch [9465/10000] Avg train loss: 0.004900\n",
      "Epoch [9466/10000] Avg train loss: 0.004900\n",
      "Epoch [9467/10000] Avg train loss: 0.004899\n",
      "Epoch [9468/10000] Avg train loss: 0.004899\n",
      "Epoch [9469/10000] Avg train loss: 0.004898\n",
      "Epoch [9470/10000] Avg train loss: 0.004898\n",
      "Epoch [9471/10000] Avg train loss: 0.004897\n",
      "Epoch [9472/10000] Avg train loss: 0.004897\n",
      "Epoch [9473/10000] Avg train loss: 0.004896\n",
      "Epoch [9474/10000] Avg train loss: 0.004896\n",
      "Epoch [9475/10000] Avg train loss: 0.004895\n",
      "Epoch [9476/10000] Avg train loss: 0.004895\n",
      "Epoch [9477/10000] Avg train loss: 0.004894\n",
      "Epoch [9478/10000] Avg train loss: 0.004894\n",
      "Epoch [9479/10000] Avg train loss: 0.004893\n",
      "Epoch [9480/10000] Avg train loss: 0.004893\n",
      "Epoch [9481/10000] Avg train loss: 0.004892\n",
      "Epoch [9482/10000] Avg train loss: 0.004892\n",
      "Epoch [9483/10000] Avg train loss: 0.004891\n",
      "Epoch [9484/10000] Avg train loss: 0.004890\n",
      "Epoch [9485/10000] Avg train loss: 0.004890\n",
      "Epoch [9486/10000] Avg train loss: 0.004889\n",
      "Epoch [9487/10000] Avg train loss: 0.004889\n",
      "Epoch [9488/10000] Avg train loss: 0.004888\n",
      "Epoch [9489/10000] Avg train loss: 0.004888\n",
      "Epoch [9490/10000] Avg train loss: 0.004887\n",
      "Epoch [9491/10000] Avg train loss: 0.004887\n",
      "Epoch [9492/10000] Avg train loss: 0.004886\n",
      "Epoch [9493/10000] Avg train loss: 0.004886\n",
      "Epoch [9494/10000] Avg train loss: 0.004885\n",
      "Epoch [9495/10000] Avg train loss: 0.004885\n",
      "Epoch [9496/10000] Avg train loss: 0.004884\n",
      "Epoch [9497/10000] Avg train loss: 0.004884\n",
      "Epoch [9498/10000] Avg train loss: 0.004883\n",
      "Epoch [9499/10000] Avg train loss: 0.004883\n",
      "Epoch [9500/10000] Avg train loss: 0.004882\n",
      "Epoch [9501/10000] Avg train loss: 0.004882\n",
      "Epoch [9502/10000] Avg train loss: 0.004881\n",
      "Epoch [9503/10000] Avg train loss: 0.004881\n",
      "Epoch [9504/10000] Avg train loss: 0.004880\n",
      "Epoch [9505/10000] Avg train loss: 0.004880\n",
      "Epoch [9506/10000] Avg train loss: 0.004879\n",
      "Epoch [9507/10000] Avg train loss: 0.004879\n",
      "Epoch [9508/10000] Avg train loss: 0.004878\n",
      "Epoch [9509/10000] Avg train loss: 0.004878\n",
      "Epoch [9510/10000] Avg train loss: 0.004877\n",
      "Epoch [9511/10000] Avg train loss: 0.004877\n",
      "Epoch [9512/10000] Avg train loss: 0.004876\n",
      "Epoch [9513/10000] Avg train loss: 0.004876\n",
      "Epoch [9514/10000] Avg train loss: 0.004875\n",
      "Epoch [9515/10000] Avg train loss: 0.004875\n",
      "Epoch [9516/10000] Avg train loss: 0.004874\n",
      "Epoch [9517/10000] Avg train loss: 0.004874\n",
      "Epoch [9518/10000] Avg train loss: 0.004873\n",
      "Epoch [9519/10000] Avg train loss: 0.004872\n",
      "Epoch [9520/10000] Avg train loss: 0.004872\n",
      "Epoch [9521/10000] Avg train loss: 0.004871\n",
      "Epoch [9522/10000] Avg train loss: 0.004871\n",
      "Epoch [9523/10000] Avg train loss: 0.004870\n",
      "Epoch [9524/10000] Avg train loss: 0.004870\n",
      "Epoch [9525/10000] Avg train loss: 0.004869\n",
      "Epoch [9526/10000] Avg train loss: 0.004869\n",
      "Epoch [9527/10000] Avg train loss: 0.004868\n",
      "Epoch [9528/10000] Avg train loss: 0.004868\n",
      "Epoch [9529/10000] Avg train loss: 0.004867\n",
      "Epoch [9530/10000] Avg train loss: 0.004867\n",
      "Epoch [9531/10000] Avg train loss: 0.004866\n",
      "Epoch [9532/10000] Avg train loss: 0.004866\n",
      "Epoch [9533/10000] Avg train loss: 0.004865\n",
      "Epoch [9534/10000] Avg train loss: 0.004865\n",
      "Epoch [9535/10000] Avg train loss: 0.004864\n",
      "Epoch [9536/10000] Avg train loss: 0.004864\n",
      "Epoch [9537/10000] Avg train loss: 0.004863\n",
      "Epoch [9538/10000] Avg train loss: 0.004863\n",
      "Epoch [9539/10000] Avg train loss: 0.004862\n",
      "Epoch [9540/10000] Avg train loss: 0.004862\n",
      "Epoch [9541/10000] Avg train loss: 0.004861\n",
      "Epoch [9542/10000] Avg train loss: 0.004861\n",
      "Epoch [9543/10000] Avg train loss: 0.004860\n",
      "Epoch [9544/10000] Avg train loss: 0.004860\n",
      "Epoch [9545/10000] Avg train loss: 0.004859\n",
      "Epoch [9546/10000] Avg train loss: 0.004859\n",
      "Epoch [9547/10000] Avg train loss: 0.004858\n",
      "Epoch [9548/10000] Avg train loss: 0.004858\n",
      "Epoch [9549/10000] Avg train loss: 0.004857\n",
      "Epoch [9550/10000] Avg train loss: 0.004857\n",
      "Epoch [9551/10000] Avg train loss: 0.004856\n",
      "Epoch [9552/10000] Avg train loss: 0.004856\n",
      "Epoch [9553/10000] Avg train loss: 0.004855\n",
      "Epoch [9554/10000] Avg train loss: 0.004855\n",
      "Epoch [9555/10000] Avg train loss: 0.004854\n",
      "Epoch [9556/10000] Avg train loss: 0.004854\n",
      "Epoch [9557/10000] Avg train loss: 0.004853\n",
      "Epoch [9558/10000] Avg train loss: 0.004853\n",
      "Epoch [9559/10000] Avg train loss: 0.004852\n",
      "Epoch [9560/10000] Avg train loss: 0.004852\n",
      "Epoch [9561/10000] Avg train loss: 0.004851\n",
      "Epoch [9562/10000] Avg train loss: 0.004851\n",
      "Epoch [9563/10000] Avg train loss: 0.004850\n",
      "Epoch [9564/10000] Avg train loss: 0.004850\n",
      "Epoch [9565/10000] Avg train loss: 0.004849\n",
      "Epoch [9566/10000] Avg train loss: 0.004849\n",
      "Epoch [9567/10000] Avg train loss: 0.004848\n",
      "Epoch [9568/10000] Avg train loss: 0.004848\n",
      "Epoch [9569/10000] Avg train loss: 0.004847\n",
      "Epoch [9570/10000] Avg train loss: 0.004847\n",
      "Epoch [9571/10000] Avg train loss: 0.004846\n",
      "Epoch [9572/10000] Avg train loss: 0.004846\n",
      "Epoch [9573/10000] Avg train loss: 0.004845\n",
      "Epoch [9574/10000] Avg train loss: 0.004845\n",
      "Epoch [9575/10000] Avg train loss: 0.004844\n",
      "Epoch [9576/10000] Avg train loss: 0.004843\n",
      "Epoch [9577/10000] Avg train loss: 0.004843\n",
      "Epoch [9578/10000] Avg train loss: 0.004842\n",
      "Epoch [9579/10000] Avg train loss: 0.004842\n",
      "Epoch [9580/10000] Avg train loss: 0.004841\n",
      "Epoch [9581/10000] Avg train loss: 0.004841\n",
      "Epoch [9582/10000] Avg train loss: 0.004840\n",
      "Epoch [9583/10000] Avg train loss: 0.004840\n",
      "Epoch [9584/10000] Avg train loss: 0.004839\n",
      "Epoch [9585/10000] Avg train loss: 0.004839\n",
      "Epoch [9586/10000] Avg train loss: 0.004838\n",
      "Epoch [9587/10000] Avg train loss: 0.004838\n",
      "Epoch [9588/10000] Avg train loss: 0.004837\n",
      "Epoch [9589/10000] Avg train loss: 0.004837\n",
      "Epoch [9590/10000] Avg train loss: 0.004836\n",
      "Epoch [9591/10000] Avg train loss: 0.004836\n",
      "Epoch [9592/10000] Avg train loss: 0.004835\n",
      "Epoch [9593/10000] Avg train loss: 0.004835\n",
      "Epoch [9594/10000] Avg train loss: 0.004834\n",
      "Epoch [9595/10000] Avg train loss: 0.004834\n",
      "Epoch [9596/10000] Avg train loss: 0.004833\n",
      "Epoch [9597/10000] Avg train loss: 0.004833\n",
      "Epoch [9598/10000] Avg train loss: 0.004832\n",
      "Epoch [9599/10000] Avg train loss: 0.004832\n",
      "Epoch [9600/10000] Avg train loss: 0.004831\n",
      "Epoch [9601/10000] Avg train loss: 0.004831\n",
      "Epoch [9602/10000] Avg train loss: 0.004830\n",
      "Epoch [9603/10000] Avg train loss: 0.004830\n",
      "Epoch [9604/10000] Avg train loss: 0.004829\n",
      "Epoch [9605/10000] Avg train loss: 0.004829\n",
      "Epoch [9606/10000] Avg train loss: 0.004828\n",
      "Epoch [9607/10000] Avg train loss: 0.004828\n",
      "Epoch [9608/10000] Avg train loss: 0.004827\n",
      "Epoch [9609/10000] Avg train loss: 0.004827\n",
      "Epoch [9610/10000] Avg train loss: 0.004826\n",
      "Epoch [9611/10000] Avg train loss: 0.004826\n",
      "Epoch [9612/10000] Avg train loss: 0.004825\n",
      "Epoch [9613/10000] Avg train loss: 0.004825\n",
      "Epoch [9614/10000] Avg train loss: 0.004824\n",
      "Epoch [9615/10000] Avg train loss: 0.004824\n",
      "Epoch [9616/10000] Avg train loss: 0.004823\n",
      "Epoch [9617/10000] Avg train loss: 0.004823\n",
      "Epoch [9618/10000] Avg train loss: 0.004822\n",
      "Epoch [9619/10000] Avg train loss: 0.004822\n",
      "Epoch [9620/10000] Avg train loss: 0.004821\n",
      "Epoch [9621/10000] Avg train loss: 0.004821\n",
      "Epoch [9622/10000] Avg train loss: 0.004820\n",
      "Epoch [9623/10000] Avg train loss: 0.004820\n",
      "Epoch [9624/10000] Avg train loss: 0.004819\n",
      "Epoch [9625/10000] Avg train loss: 0.004819\n",
      "Epoch [9626/10000] Avg train loss: 0.004818\n",
      "Epoch [9627/10000] Avg train loss: 0.004818\n",
      "Epoch [9628/10000] Avg train loss: 0.004817\n",
      "Epoch [9629/10000] Avg train loss: 0.004817\n",
      "Epoch [9630/10000] Avg train loss: 0.004816\n",
      "Epoch [9631/10000] Avg train loss: 0.004816\n",
      "Epoch [9632/10000] Avg train loss: 0.004815\n",
      "Epoch [9633/10000] Avg train loss: 0.004815\n",
      "Epoch [9634/10000] Avg train loss: 0.004814\n",
      "Epoch [9635/10000] Avg train loss: 0.004814\n",
      "Epoch [9636/10000] Avg train loss: 0.004813\n",
      "Epoch [9637/10000] Avg train loss: 0.004813\n",
      "Epoch [9638/10000] Avg train loss: 0.004812\n",
      "Epoch [9639/10000] Avg train loss: 0.004812\n",
      "Epoch [9640/10000] Avg train loss: 0.004811\n",
      "Epoch [9641/10000] Avg train loss: 0.004811\n",
      "Epoch [9642/10000] Avg train loss: 0.004810\n",
      "Epoch [9643/10000] Avg train loss: 0.004810\n",
      "Epoch [9644/10000] Avg train loss: 0.004809\n",
      "Epoch [9645/10000] Avg train loss: 0.004809\n",
      "Epoch [9646/10000] Avg train loss: 0.004808\n",
      "Epoch [9647/10000] Avg train loss: 0.004808\n",
      "Epoch [9648/10000] Avg train loss: 0.004807\n",
      "Epoch [9649/10000] Avg train loss: 0.004807\n",
      "Epoch [9650/10000] Avg train loss: 0.004806\n",
      "Epoch [9651/10000] Avg train loss: 0.004806\n",
      "Epoch [9652/10000] Avg train loss: 0.004805\n",
      "Epoch [9653/10000] Avg train loss: 0.004805\n",
      "Epoch [9654/10000] Avg train loss: 0.004804\n",
      "Epoch [9655/10000] Avg train loss: 0.004804\n",
      "Epoch [9656/10000] Avg train loss: 0.004803\n",
      "Epoch [9657/10000] Avg train loss: 0.004803\n",
      "Epoch [9658/10000] Avg train loss: 0.004802\n",
      "Epoch [9659/10000] Avg train loss: 0.004802\n",
      "Epoch [9660/10000] Avg train loss: 0.004801\n",
      "Epoch [9661/10000] Avg train loss: 0.004801\n",
      "Epoch [9662/10000] Avg train loss: 0.004800\n",
      "Epoch [9663/10000] Avg train loss: 0.004800\n",
      "Epoch [9664/10000] Avg train loss: 0.004799\n",
      "Epoch [9665/10000] Avg train loss: 0.004799\n",
      "Epoch [9666/10000] Avg train loss: 0.004798\n",
      "Epoch [9667/10000] Avg train loss: 0.004798\n",
      "Epoch [9668/10000] Avg train loss: 0.004797\n",
      "Epoch [9669/10000] Avg train loss: 0.004797\n",
      "Epoch [9670/10000] Avg train loss: 0.004796\n",
      "Epoch [9671/10000] Avg train loss: 0.004796\n",
      "Epoch [9672/10000] Avg train loss: 0.004795\n",
      "Epoch [9673/10000] Avg train loss: 0.004795\n",
      "Epoch [9674/10000] Avg train loss: 0.004794\n",
      "Epoch [9675/10000] Avg train loss: 0.004794\n",
      "Epoch [9676/10000] Avg train loss: 0.004793\n",
      "Epoch [9677/10000] Avg train loss: 0.004793\n",
      "Epoch [9678/10000] Avg train loss: 0.004792\n",
      "Epoch [9679/10000] Avg train loss: 0.004792\n",
      "Epoch [9680/10000] Avg train loss: 0.004791\n",
      "Epoch [9681/10000] Avg train loss: 0.004791\n",
      "Epoch [9682/10000] Avg train loss: 0.004790\n",
      "Epoch [9683/10000] Avg train loss: 0.004790\n",
      "Epoch [9684/10000] Avg train loss: 0.004789\n",
      "Epoch [9685/10000] Avg train loss: 0.004789\n",
      "Epoch [9686/10000] Avg train loss: 0.004788\n",
      "Epoch [9687/10000] Avg train loss: 0.004788\n",
      "Epoch [9688/10000] Avg train loss: 0.004787\n",
      "Epoch [9689/10000] Avg train loss: 0.004787\n",
      "Epoch [9690/10000] Avg train loss: 0.004787\n",
      "Epoch [9691/10000] Avg train loss: 0.004786\n",
      "Epoch [9692/10000] Avg train loss: 0.004786\n",
      "Epoch [9693/10000] Avg train loss: 0.004785\n",
      "Epoch [9694/10000] Avg train loss: 0.004785\n",
      "Epoch [9695/10000] Avg train loss: 0.004784\n",
      "Epoch [9696/10000] Avg train loss: 0.004784\n",
      "Epoch [9697/10000] Avg train loss: 0.004783\n",
      "Epoch [9698/10000] Avg train loss: 0.004783\n",
      "Epoch [9699/10000] Avg train loss: 0.004782\n",
      "Epoch [9700/10000] Avg train loss: 0.004782\n",
      "Epoch [9701/10000] Avg train loss: 0.004781\n",
      "Epoch [9702/10000] Avg train loss: 0.004781\n",
      "Epoch [9703/10000] Avg train loss: 0.004780\n",
      "Epoch [9704/10000] Avg train loss: 0.004780\n",
      "Epoch [9705/10000] Avg train loss: 0.004779\n",
      "Epoch [9706/10000] Avg train loss: 0.004779\n",
      "Epoch [9707/10000] Avg train loss: 0.004778\n",
      "Epoch [9708/10000] Avg train loss: 0.004778\n",
      "Epoch [9709/10000] Avg train loss: 0.004777\n",
      "Epoch [9710/10000] Avg train loss: 0.004777\n",
      "Epoch [9711/10000] Avg train loss: 0.004776\n",
      "Epoch [9712/10000] Avg train loss: 0.004776\n",
      "Epoch [9713/10000] Avg train loss: 0.004775\n",
      "Epoch [9714/10000] Avg train loss: 0.004775\n",
      "Epoch [9715/10000] Avg train loss: 0.004774\n",
      "Epoch [9716/10000] Avg train loss: 0.004774\n",
      "Epoch [9717/10000] Avg train loss: 0.004773\n",
      "Epoch [9718/10000] Avg train loss: 0.004773\n",
      "Epoch [9719/10000] Avg train loss: 0.004772\n",
      "Epoch [9720/10000] Avg train loss: 0.004772\n",
      "Epoch [9721/10000] Avg train loss: 0.004771\n",
      "Epoch [9722/10000] Avg train loss: 0.004771\n",
      "Epoch [9723/10000] Avg train loss: 0.004770\n",
      "Epoch [9724/10000] Avg train loss: 0.004770\n",
      "Epoch [9725/10000] Avg train loss: 0.004769\n",
      "Epoch [9726/10000] Avg train loss: 0.004769\n",
      "Epoch [9727/10000] Avg train loss: 0.004768\n",
      "Epoch [9728/10000] Avg train loss: 0.004768\n",
      "Epoch [9729/10000] Avg train loss: 0.004767\n",
      "Epoch [9730/10000] Avg train loss: 0.004767\n",
      "Epoch [9731/10000] Avg train loss: 0.004766\n",
      "Epoch [9732/10000] Avg train loss: 0.004766\n",
      "Epoch [9733/10000] Avg train loss: 0.004765\n",
      "Epoch [9734/10000] Avg train loss: 0.004765\n",
      "Epoch [9735/10000] Avg train loss: 0.004764\n",
      "Epoch [9736/10000] Avg train loss: 0.004764\n",
      "Epoch [9737/10000] Avg train loss: 0.004763\n",
      "Epoch [9738/10000] Avg train loss: 0.004763\n",
      "Epoch [9739/10000] Avg train loss: 0.004762\n",
      "Epoch [9740/10000] Avg train loss: 0.004762\n",
      "Epoch [9741/10000] Avg train loss: 0.004761\n",
      "Epoch [9742/10000] Avg train loss: 0.004761\n",
      "Epoch [9743/10000] Avg train loss: 0.004760\n",
      "Epoch [9744/10000] Avg train loss: 0.004760\n",
      "Epoch [9745/10000] Avg train loss: 0.004759\n",
      "Epoch [9746/10000] Avg train loss: 0.004759\n",
      "Epoch [9747/10000] Avg train loss: 0.004759\n",
      "Epoch [9748/10000] Avg train loss: 0.004758\n",
      "Epoch [9749/10000] Avg train loss: 0.004758\n",
      "Epoch [9750/10000] Avg train loss: 0.004757\n",
      "Epoch [9751/10000] Avg train loss: 0.004757\n",
      "Epoch [9752/10000] Avg train loss: 0.004756\n",
      "Epoch [9753/10000] Avg train loss: 0.004756\n",
      "Epoch [9754/10000] Avg train loss: 0.004755\n",
      "Epoch [9755/10000] Avg train loss: 0.004755\n",
      "Epoch [9756/10000] Avg train loss: 0.004754\n",
      "Epoch [9757/10000] Avg train loss: 0.004754\n",
      "Epoch [9758/10000] Avg train loss: 0.004753\n",
      "Epoch [9759/10000] Avg train loss: 0.004753\n",
      "Epoch [9760/10000] Avg train loss: 0.004752\n",
      "Epoch [9761/10000] Avg train loss: 0.004752\n",
      "Epoch [9762/10000] Avg train loss: 0.004751\n",
      "Epoch [9763/10000] Avg train loss: 0.004751\n",
      "Epoch [9764/10000] Avg train loss: 0.004750\n",
      "Epoch [9765/10000] Avg train loss: 0.004750\n",
      "Epoch [9766/10000] Avg train loss: 0.004749\n",
      "Epoch [9767/10000] Avg train loss: 0.004749\n",
      "Epoch [9768/10000] Avg train loss: 0.004748\n",
      "Epoch [9769/10000] Avg train loss: 0.004748\n",
      "Epoch [9770/10000] Avg train loss: 0.004747\n",
      "Epoch [9771/10000] Avg train loss: 0.004747\n",
      "Epoch [9772/10000] Avg train loss: 0.004746\n",
      "Epoch [9773/10000] Avg train loss: 0.004746\n",
      "Epoch [9774/10000] Avg train loss: 0.004745\n",
      "Epoch [9775/10000] Avg train loss: 0.004745\n",
      "Epoch [9776/10000] Avg train loss: 0.004744\n",
      "Epoch [9777/10000] Avg train loss: 0.004744\n",
      "Epoch [9778/10000] Avg train loss: 0.004743\n",
      "Epoch [9779/10000] Avg train loss: 0.004743\n",
      "Epoch [9780/10000] Avg train loss: 0.004742\n",
      "Epoch [9781/10000] Avg train loss: 0.004742\n",
      "Epoch [9782/10000] Avg train loss: 0.004741\n",
      "Epoch [9783/10000] Avg train loss: 0.004741\n",
      "Epoch [9784/10000] Avg train loss: 0.004741\n",
      "Epoch [9785/10000] Avg train loss: 0.004740\n",
      "Epoch [9786/10000] Avg train loss: 0.004740\n",
      "Epoch [9787/10000] Avg train loss: 0.004739\n",
      "Epoch [9788/10000] Avg train loss: 0.004739\n",
      "Epoch [9789/10000] Avg train loss: 0.004738\n",
      "Epoch [9790/10000] Avg train loss: 0.004738\n",
      "Epoch [9791/10000] Avg train loss: 0.004737\n",
      "Epoch [9792/10000] Avg train loss: 0.004737\n",
      "Epoch [9793/10000] Avg train loss: 0.004736\n",
      "Epoch [9794/10000] Avg train loss: 0.004736\n",
      "Epoch [9795/10000] Avg train loss: 0.004735\n",
      "Epoch [9796/10000] Avg train loss: 0.004735\n",
      "Epoch [9797/10000] Avg train loss: 0.004734\n",
      "Epoch [9798/10000] Avg train loss: 0.004734\n",
      "Epoch [9799/10000] Avg train loss: 0.004733\n",
      "Epoch [9800/10000] Avg train loss: 0.004733\n",
      "Epoch [9801/10000] Avg train loss: 0.004732\n",
      "Epoch [9802/10000] Avg train loss: 0.004732\n",
      "Epoch [9803/10000] Avg train loss: 0.004731\n",
      "Epoch [9804/10000] Avg train loss: 0.004731\n",
      "Epoch [9805/10000] Avg train loss: 0.004730\n",
      "Epoch [9806/10000] Avg train loss: 0.004730\n",
      "Epoch [9807/10000] Avg train loss: 0.004729\n",
      "Epoch [9808/10000] Avg train loss: 0.004729\n",
      "Epoch [9809/10000] Avg train loss: 0.004728\n",
      "Epoch [9810/10000] Avg train loss: 0.004728\n",
      "Epoch [9811/10000] Avg train loss: 0.004727\n",
      "Epoch [9812/10000] Avg train loss: 0.004727\n",
      "Epoch [9813/10000] Avg train loss: 0.004727\n",
      "Epoch [9814/10000] Avg train loss: 0.004726\n",
      "Epoch [9815/10000] Avg train loss: 0.004726\n",
      "Epoch [9816/10000] Avg train loss: 0.004725\n",
      "Epoch [9817/10000] Avg train loss: 0.004725\n",
      "Epoch [9818/10000] Avg train loss: 0.004724\n",
      "Epoch [9819/10000] Avg train loss: 0.004724\n",
      "Epoch [9820/10000] Avg train loss: 0.004723\n",
      "Epoch [9821/10000] Avg train loss: 0.004723\n",
      "Epoch [9822/10000] Avg train loss: 0.004722\n",
      "Epoch [9823/10000] Avg train loss: 0.004722\n",
      "Epoch [9824/10000] Avg train loss: 0.004721\n",
      "Epoch [9825/10000] Avg train loss: 0.004721\n",
      "Epoch [9826/10000] Avg train loss: 0.004720\n",
      "Epoch [9827/10000] Avg train loss: 0.004720\n",
      "Epoch [9828/10000] Avg train loss: 0.004719\n",
      "Epoch [9829/10000] Avg train loss: 0.004719\n",
      "Epoch [9830/10000] Avg train loss: 0.004718\n",
      "Epoch [9831/10000] Avg train loss: 0.004718\n",
      "Epoch [9832/10000] Avg train loss: 0.004717\n",
      "Epoch [9833/10000] Avg train loss: 0.004717\n",
      "Epoch [9834/10000] Avg train loss: 0.004716\n",
      "Epoch [9835/10000] Avg train loss: 0.004716\n",
      "Epoch [9836/10000] Avg train loss: 0.004715\n",
      "Epoch [9837/10000] Avg train loss: 0.004715\n",
      "Epoch [9838/10000] Avg train loss: 0.004715\n",
      "Epoch [9839/10000] Avg train loss: 0.004714\n",
      "Epoch [9840/10000] Avg train loss: 0.004714\n",
      "Epoch [9841/10000] Avg train loss: 0.004713\n",
      "Epoch [9842/10000] Avg train loss: 0.004713\n",
      "Epoch [9843/10000] Avg train loss: 0.004712\n",
      "Epoch [9844/10000] Avg train loss: 0.004712\n",
      "Epoch [9845/10000] Avg train loss: 0.004711\n",
      "Epoch [9846/10000] Avg train loss: 0.004711\n",
      "Epoch [9847/10000] Avg train loss: 0.004710\n",
      "Epoch [9848/10000] Avg train loss: 0.004710\n",
      "Epoch [9849/10000] Avg train loss: 0.004709\n",
      "Epoch [9850/10000] Avg train loss: 0.004709\n",
      "Epoch [9851/10000] Avg train loss: 0.004708\n",
      "Epoch [9852/10000] Avg train loss: 0.004708\n",
      "Epoch [9853/10000] Avg train loss: 0.004707\n",
      "Epoch [9854/10000] Avg train loss: 0.004707\n",
      "Epoch [9855/10000] Avg train loss: 0.004706\n",
      "Epoch [9856/10000] Avg train loss: 0.004706\n",
      "Epoch [9857/10000] Avg train loss: 0.004705\n",
      "Epoch [9858/10000] Avg train loss: 0.004705\n",
      "Epoch [9859/10000] Avg train loss: 0.004704\n",
      "Epoch [9860/10000] Avg train loss: 0.004704\n",
      "Epoch [9861/10000] Avg train loss: 0.004704\n",
      "Epoch [9862/10000] Avg train loss: 0.004703\n",
      "Epoch [9863/10000] Avg train loss: 0.004703\n",
      "Epoch [9864/10000] Avg train loss: 0.004702\n",
      "Epoch [9865/10000] Avg train loss: 0.004702\n",
      "Epoch [9866/10000] Avg train loss: 0.004701\n",
      "Epoch [9867/10000] Avg train loss: 0.004701\n",
      "Epoch [9868/10000] Avg train loss: 0.004700\n",
      "Epoch [9869/10000] Avg train loss: 0.004700\n",
      "Epoch [9870/10000] Avg train loss: 0.004699\n",
      "Epoch [9871/10000] Avg train loss: 0.004699\n",
      "Epoch [9872/10000] Avg train loss: 0.004698\n",
      "Epoch [9873/10000] Avg train loss: 0.004698\n",
      "Epoch [9874/10000] Avg train loss: 0.004697\n",
      "Epoch [9875/10000] Avg train loss: 0.004697\n",
      "Epoch [9876/10000] Avg train loss: 0.004696\n",
      "Epoch [9877/10000] Avg train loss: 0.004696\n",
      "Epoch [9878/10000] Avg train loss: 0.004695\n",
      "Epoch [9879/10000] Avg train loss: 0.004695\n",
      "Epoch [9880/10000] Avg train loss: 0.004694\n",
      "Epoch [9881/10000] Avg train loss: 0.004694\n",
      "Epoch [9882/10000] Avg train loss: 0.004694\n",
      "Epoch [9883/10000] Avg train loss: 0.004693\n",
      "Epoch [9884/10000] Avg train loss: 0.004693\n",
      "Epoch [9885/10000] Avg train loss: 0.004692\n",
      "Epoch [9886/10000] Avg train loss: 0.004692\n",
      "Epoch [9887/10000] Avg train loss: 0.004691\n",
      "Epoch [9888/10000] Avg train loss: 0.004691\n",
      "Epoch [9889/10000] Avg train loss: 0.004690\n",
      "Epoch [9890/10000] Avg train loss: 0.004690\n",
      "Epoch [9891/10000] Avg train loss: 0.004689\n",
      "Epoch [9892/10000] Avg train loss: 0.004689\n",
      "Epoch [9893/10000] Avg train loss: 0.004688\n",
      "Epoch [9894/10000] Avg train loss: 0.004688\n",
      "Epoch [9895/10000] Avg train loss: 0.004687\n",
      "Epoch [9896/10000] Avg train loss: 0.004687\n",
      "Epoch [9897/10000] Avg train loss: 0.004686\n",
      "Epoch [9898/10000] Avg train loss: 0.004686\n",
      "Epoch [9899/10000] Avg train loss: 0.004685\n",
      "Epoch [9900/10000] Avg train loss: 0.004685\n",
      "Epoch [9901/10000] Avg train loss: 0.004685\n",
      "Epoch [9902/10000] Avg train loss: 0.004684\n",
      "Epoch [9903/10000] Avg train loss: 0.004684\n",
      "Epoch [9904/10000] Avg train loss: 0.004683\n",
      "Epoch [9905/10000] Avg train loss: 0.004683\n",
      "Epoch [9906/10000] Avg train loss: 0.004682\n",
      "Epoch [9907/10000] Avg train loss: 0.004682\n",
      "Epoch [9908/10000] Avg train loss: 0.004681\n",
      "Epoch [9909/10000] Avg train loss: 0.004681\n",
      "Epoch [9910/10000] Avg train loss: 0.004680\n",
      "Epoch [9911/10000] Avg train loss: 0.004680\n",
      "Epoch [9912/10000] Avg train loss: 0.004679\n",
      "Epoch [9913/10000] Avg train loss: 0.004679\n",
      "Epoch [9914/10000] Avg train loss: 0.004678\n",
      "Epoch [9915/10000] Avg train loss: 0.004678\n",
      "Epoch [9916/10000] Avg train loss: 0.004677\n",
      "Epoch [9917/10000] Avg train loss: 0.004677\n",
      "Epoch [9918/10000] Avg train loss: 0.004676\n",
      "Epoch [9919/10000] Avg train loss: 0.004676\n",
      "Epoch [9920/10000] Avg train loss: 0.004676\n",
      "Epoch [9921/10000] Avg train loss: 0.004675\n",
      "Epoch [9922/10000] Avg train loss: 0.004675\n",
      "Epoch [9923/10000] Avg train loss: 0.004674\n",
      "Epoch [9924/10000] Avg train loss: 0.004674\n",
      "Epoch [9925/10000] Avg train loss: 0.004673\n",
      "Epoch [9926/10000] Avg train loss: 0.004673\n",
      "Epoch [9927/10000] Avg train loss: 0.004672\n",
      "Epoch [9928/10000] Avg train loss: 0.004672\n",
      "Epoch [9929/10000] Avg train loss: 0.004671\n",
      "Epoch [9930/10000] Avg train loss: 0.004671\n",
      "Epoch [9931/10000] Avg train loss: 0.004670\n",
      "Epoch [9932/10000] Avg train loss: 0.004670\n",
      "Epoch [9933/10000] Avg train loss: 0.004669\n",
      "Epoch [9934/10000] Avg train loss: 0.004669\n",
      "Epoch [9935/10000] Avg train loss: 0.004668\n",
      "Epoch [9936/10000] Avg train loss: 0.004668\n",
      "Epoch [9937/10000] Avg train loss: 0.004668\n",
      "Epoch [9938/10000] Avg train loss: 0.004667\n",
      "Epoch [9939/10000] Avg train loss: 0.004667\n",
      "Epoch [9940/10000] Avg train loss: 0.004666\n",
      "Epoch [9941/10000] Avg train loss: 0.004666\n",
      "Epoch [9942/10000] Avg train loss: 0.004665\n",
      "Epoch [9943/10000] Avg train loss: 0.004665\n",
      "Epoch [9944/10000] Avg train loss: 0.004664\n",
      "Epoch [9945/10000] Avg train loss: 0.004664\n",
      "Epoch [9946/10000] Avg train loss: 0.004663\n",
      "Epoch [9947/10000] Avg train loss: 0.004663\n",
      "Epoch [9948/10000] Avg train loss: 0.004662\n",
      "Epoch [9949/10000] Avg train loss: 0.004662\n",
      "Epoch [9950/10000] Avg train loss: 0.004661\n",
      "Epoch [9951/10000] Avg train loss: 0.004661\n",
      "Epoch [9952/10000] Avg train loss: 0.004660\n",
      "Epoch [9953/10000] Avg train loss: 0.004660\n",
      "Epoch [9954/10000] Avg train loss: 0.004660\n",
      "Epoch [9955/10000] Avg train loss: 0.004659\n",
      "Epoch [9956/10000] Avg train loss: 0.004659\n",
      "Epoch [9957/10000] Avg train loss: 0.004658\n",
      "Epoch [9958/10000] Avg train loss: 0.004658\n",
      "Epoch [9959/10000] Avg train loss: 0.004657\n",
      "Epoch [9960/10000] Avg train loss: 0.004657\n",
      "Epoch [9961/10000] Avg train loss: 0.004656\n",
      "Epoch [9962/10000] Avg train loss: 0.004656\n",
      "Epoch [9963/10000] Avg train loss: 0.004655\n",
      "Epoch [9964/10000] Avg train loss: 0.004655\n",
      "Epoch [9965/10000] Avg train loss: 0.004654\n",
      "Epoch [9966/10000] Avg train loss: 0.004654\n",
      "Epoch [9967/10000] Avg train loss: 0.004653\n",
      "Epoch [9968/10000] Avg train loss: 0.004653\n",
      "Epoch [9969/10000] Avg train loss: 0.004653\n",
      "Epoch [9970/10000] Avg train loss: 0.004652\n",
      "Epoch [9971/10000] Avg train loss: 0.004652\n",
      "Epoch [9972/10000] Avg train loss: 0.004651\n",
      "Epoch [9973/10000] Avg train loss: 0.004651\n",
      "Epoch [9974/10000] Avg train loss: 0.004650\n",
      "Epoch [9975/10000] Avg train loss: 0.004650\n",
      "Epoch [9976/10000] Avg train loss: 0.004649\n",
      "Epoch [9977/10000] Avg train loss: 0.004649\n",
      "Epoch [9978/10000] Avg train loss: 0.004648\n",
      "Epoch [9979/10000] Avg train loss: 0.004648\n",
      "Epoch [9980/10000] Avg train loss: 0.004647\n",
      "Epoch [9981/10000] Avg train loss: 0.004647\n",
      "Epoch [9982/10000] Avg train loss: 0.004646\n",
      "Epoch [9983/10000] Avg train loss: 0.004646\n",
      "Epoch [9984/10000] Avg train loss: 0.004646\n",
      "Epoch [9985/10000] Avg train loss: 0.004645\n",
      "Epoch [9986/10000] Avg train loss: 0.004645\n",
      "Epoch [9987/10000] Avg train loss: 0.004644\n",
      "Epoch [9988/10000] Avg train loss: 0.004644\n",
      "Epoch [9989/10000] Avg train loss: 0.004643\n",
      "Epoch [9990/10000] Avg train loss: 0.004643\n",
      "Epoch [9991/10000] Avg train loss: 0.004642\n",
      "Epoch [9992/10000] Avg train loss: 0.004642\n",
      "Epoch [9993/10000] Avg train loss: 0.004641\n",
      "Epoch [9994/10000] Avg train loss: 0.004641\n",
      "Epoch [9995/10000] Avg train loss: 0.004640\n",
      "Epoch [9996/10000] Avg train loss: 0.004640\n",
      "Epoch [9997/10000] Avg train loss: 0.004640\n",
      "Epoch [9998/10000] Avg train loss: 0.004639\n",
      "Epoch [9999/10000] Avg train loss: 0.004639\n",
      "Epoch [10000/10000] Avg train loss: 0.004638\n"
     ]
    }
   ],
   "source": [
    "set_seed(42)  # Fixing the seed\n",
    "import torch.optim as optim\n",
    "# Proceed with the rest of the setup (loss, optimizer) and training loop as before\n",
    "# Training loop\n",
    "\n",
    "# careful, for minibatch 32, 400 is enough!!!\n",
    "num_epochs = 10000  # You might need more epochs for a deep network\n",
    "losses = []\n",
    "cool_loss = []\n",
    "for epoch in range(num_epochs):\n",
    "    # now we loop through the mini-batches\n",
    "    for inputs, targets in train_loader:\n",
    "        # Forward pass\n",
    "        outputs = model(inputs) # this is f(x;\\theta)\n",
    "        loss = criterion(outputs, targets) # this is \\sum_{t\\in mini batch} \\ell(y_t, f(x_t;\\theta))\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad() # forget the gradients from old steps of Gradient Descent (GD)\n",
    "        loss.backward() # compute the new gradient, \\nabla_\\theta L(\\theta)\n",
    "        optimizer.step() # \\theta_{k+1}=\\theta_k - \\eta \\nabla_\\theta L(\\theta_k).\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    cool_loss.append(np.mean(losses))\n",
    "    losses = []\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] Avg train loss: {np.mean(cool_loss):.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "6c30d56e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "14040164",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x35b3f19a0>]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plot_e = np.linspace(1,  num_epochs, num= num_epochs)\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.plot(plot_e, np.array(cool_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "e6e5b96f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure the model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Function to get predictions\n",
    "def get_predictions(loader):\n",
    "    targets = []\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            outputs = model(inputs)\n",
    "            targets.extend(labels.numpy())\n",
    "            predictions.extend(outputs.numpy())\n",
    "\n",
    "    return np.array(targets).flatten(), np.array(predictions).flatten()\n",
    "\n",
    "# Get predictions for both training and test sets\n",
    "train_targets, train_predictions = get_predictions(train_loader)\n",
    "test_targets, test_predictions = get_predictions(test_loader)\n",
    "#test_predictions=0.5*test_predictions\n",
    "\n",
    "# Calculate MSE for both training and test sets\n",
    "train_mse = np.mean((train_targets - train_predictions) ** 2) / np.mean((train_targets) ** 2)\n",
    "test_mse = np.mean((test_targets - test_predictions) ** 2) / np.mean((test_targets) ** 2)\n",
    "\n",
    "# Plotting\n",
    "fig, axs = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Training set subplot\n",
    "axs[0].scatter(train_targets, train_predictions)\n",
    "axs[0].plot([train_targets.min(), train_targets.max()], [train_targets.min(), train_targets.max()], 'k--', lw=2)  # Diagonal line\n",
    "axs[0].set_xlabel('Actual values')\n",
    "axs[0].set_ylabel('Predicted values')\n",
    "axs[0].set_title(f'Training Set Predicted vs Actual\\nMSE: {train_mse:.4f}')\n",
    "\n",
    "# Test set subplot\n",
    "axs[1].scatter(test_targets, test_predictions)\n",
    "axs[1].plot([test_targets.min(), test_targets.max()], [test_targets.min(), test_targets.max()], 'k--', lw=2)  # Diagonal line\n",
    "axs[1].set_xlabel('Actual values')\n",
    "axs[1].set_ylabel('Predicted values')\n",
    "axs[1].set_title(f'Test Set Predicted vs Actual\\nMSE: {test_mse:.4f}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "d25cfb0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test sign prediction accuracy: 44.93%\n",
      "99% CI under null: [39.92%, 49.94%] — Significant? YES\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "# === 1. SIGN PREDICTION ACCURACY (TEST ONLY) ===\n",
    "def sign_accuracy(y_true, y_pred):\n",
    "    return np.mean(np.sign(y_true) == np.sign(y_pred))\n",
    "\n",
    "test_sign_acc = sign_accuracy(test_targets, test_predictions)\n",
    "\n",
    "# === 1.1. Confidence interval around 0.5 (coin toss null hypothesis) ===\n",
    "n_test = len(test_targets)\n",
    "p_hat = test_sign_acc\n",
    "p_0 = 0.5\n",
    "z = norm.ppf(1 - 0.01 / 2)  # two-tailed 99% → z ≈ 2.576\n",
    "se = np.sqrt(p_0 * (1 - p_0) / n_test)\n",
    "ci_low = p_hat - z * se\n",
    "ci_high = p_hat + z * se\n",
    "significant = \"YES\" if ci_low > 0.5 or ci_high < 0.5 else \"NO\"\n",
    "\n",
    "print(f\"Test sign prediction accuracy: {test_sign_acc:.2%}\")\n",
    "print(f\"99% CI under null: [{ci_low:.2%}, {ci_high:.2%}] — Significant? {significant}\")\n",
    "\n",
    "# === 2. STRATEGY RETURNS (TEST ONLY) ===\n",
    "test_strategy_returns = np.sign(test_predictions) * y_test\n",
    "\n",
    "# === 3. PLOT CUMULATIVE RETURNS (TEST ONLY) ===\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "\n",
    "ax.plot(np.cumsum(test_strategy_returns), label='Strategy (Test)', linestyle='--')\n",
    "ax.plot(np.cumsum(y_test), label='Asset (Test)', alpha=0.7)\n",
    "\n",
    "ax.set_title('Cumulative Returns on Test Set')\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_ylabel('Cumulative Return')\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLfinanceHW2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
