{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3f603006",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch                     # for all things PyTorch\n",
    "import pandas as pd\n",
    "import torch.nn as nn            # for torch.nn.Module, the parent object for PyTorch models\n",
    "import torch.nn.functional as F  # for the activation function\n",
    "import numpy as np\n",
    "import random\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a259413a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlexibleMLP(nn.Module):\n",
    "    def __init__(self, layers: list, scale: float = 1.0, bias_scale: float = 0.0, activation=nn.GELU()):\n",
    "        \"\"\"\n",
    "        Initialize a customizable Multi-Layer Perceptron (MLP) with flexible architecture and initialization.\n",
    "\n",
    "        Args:\n",
    "            layers (list of int): A list of integers where each value defines the size of each layer.\n",
    "                                  For example, layers=[10, 64, 32, 1] defines a network with input dimension 10,\n",
    "                                  two hidden layers with sizes 64 and 32, and output dimension 1.\n",
    "            scale (float): Scaling factor for weight initialization. Controls the standard deviation of\n",
    "                           the normal distribution used in initializing weights. Recommended to be 1.0 for LeCun initialization.\n",
    "            bias_scale (float): Scaling factor for bias initialization. Often set to 0.0 to start with no initial bias.\n",
    "            activation (nn.Module): Activation function applied after each linear transformation except the last layer.\n",
    "                                    Defaults to nn.GELU(), but can be any activation like nn.ReLU(), nn.Tanh(), etc.\n",
    "        \"\"\"\n",
    "        # Call the constructor of the parent class (nn.Module) to initialize all internal PyTorch machinery.\n",
    "        # This is crucial because nn.Module handles a lot of behind-the-scenes logic like:\n",
    "        # - registering parameters (weights and biases) for automatic optimization\n",
    "        # - setting up .to(device), .eval(), .train(), etc.\n",
    "        # - tracking submodules (layers, activations, etc.)\n",
    "        #\n",
    "        # If you omit this line, the module will NOT work correctly in PyTorch:\n",
    "        # things like model.cuda(), model.parameters(), model.state_dict(), etc. will all break.\n",
    "        #\n",
    "        # The super() call here:\n",
    "        # - FlexibleMLP is our class\n",
    "        # - nn.Module is the parent class\n",
    "        # - self.__init__() is the method we want to call from the parent\n",
    "        super(FlexibleMLP, self).__init__()\n",
    "\n",
    "        # Save arguments as attributes for reuse in reset_parameters\n",
    "        self.layer_sizes = layers\n",
    "        self.scale = scale\n",
    "        self.bias_scale = bias_scale\n",
    "        self.activation_fn = activation\n",
    "\n",
    "        # Create containers to hold layers and activations\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.activations = nn.ModuleList()\n",
    "\n",
    "        # Build network structure (but not weights yet)\n",
    "        self._build_layers()\n",
    "\n",
    "        # Initialize weights and biases\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def _build_layers(self):\n",
    "        \"\"\"\n",
    "        Build the linear layers and corresponding activations (except for initialization).\n",
    "        \"\"\"\n",
    "        for i in range(len(self.layer_sizes) - 1):\n",
    "            # Create a linear layer from layer i to layer i+1\n",
    "            layer = nn.Linear(self.layer_sizes[i], self.layer_sizes[i + 1])\n",
    "            self.layers.append(layer)\n",
    "\n",
    "            # Add an activation function unless it's the final layer\n",
    "            if i < len(self.layer_sizes) - 2:\n",
    "                self.activations.append(self.activation_fn)\n",
    "            else:\n",
    "                # Final layer doesn't use activation (use Identity to keep list structure consistent)\n",
    "                self.activations.append(nn.Identity())\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"\n",
    "        Apply custom initialization to all layers using the given scale and bias_scale.\n",
    "        \"\"\"\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            # Apply LeCun-style initialization for better gradient behavior\n",
    "            nn.init.normal_(layer.weight, mean=0.0, std=self.scale * np.sqrt(1 / self.layer_sizes[i]))\n",
    "            nn.init.normal_(layer.bias, mean=0.0, std=self.bias_scale * np.sqrt(1 / self.layer_sizes[i]))\n",
    "\n",
    "    def forward(self, x, return_last_hidden=False):\n",
    "        \"\"\"\n",
    "        Perform a forward pass through the network.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, input_dim).\n",
    "            return_last_hidden (bool): If True, returns both the final output and the last hidden layer's output.\n",
    "                                       Useful for feature extraction, analysis, or interpretability.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output of the final layer.\n",
    "            torch.Tensor (optional): Output of the last hidden layer (before final linear layer),\n",
    "                                     if return_last_hidden is set to True.\n",
    "        \"\"\"\n",
    "        last_hidden = None  # Will store the output of the last hidden layer\n",
    "\n",
    "        # Apply all but the last layer with activation\n",
    "        for layer, activation in zip(self.layers[:-1], self.activations[:-1]):\n",
    "            x = activation(layer(x))  # Apply linear transformation and activation\n",
    "            last_hidden = x  # Save the last hidden output\n",
    "\n",
    "        # Final layer (linear transformation only, no activation)\n",
    "        x = self.layers[-1](x)\n",
    "\n",
    "        if return_last_hidden:\n",
    "            return x, last_hidden\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "81390c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed_value=42):\n",
    "    \"\"\"\n",
    "    Set random seed across NumPy, Python, and PyTorch to ensure reproducibility.\n",
    "\n",
    "    Args:\n",
    "        seed_value (int): The seed value to use. Default is 42, a commonly used arbitrary number.\n",
    "\n",
    "    This function ensures that experiments produce the same results across different runs,\n",
    "    which is critical for debugging, comparing models, and scientific reproducibility.\n",
    "\n",
    "    It sets the seed for:\n",
    "        - NumPy (used for numerical ops like matrix generation)\n",
    "        - Python's built-in random module (used in random sampling, shuffling, etc.)\n",
    "        - PyTorch (both CPU and GPU)\n",
    "\n",
    "    For GPU reproducibility:\n",
    "        - It manually sets the CUDA seeds (for single and multi-GPU setups)\n",
    "        - It disables the CUDA backend benchmarking feature to ensure deterministic behavior\n",
    "          (at the potential cost of performance).\n",
    "    \"\"\"\n",
    "\n",
    "    # Set seed for NumPy (used in data shuffling, batch generation, etc.)\n",
    "    np.random.seed(seed_value)\n",
    "\n",
    "    # Set seed for PyTorch operations on CPU\n",
    "    torch.manual_seed(seed_value)\n",
    "\n",
    "    # Set seed for Python's built-in random module (e.g., random.shuffle, random.randint)\n",
    "    random.seed(seed_value)\n",
    "\n",
    "    # Set seeds for PyTorch operations on GPU\n",
    "    if torch.cuda.is_available():\n",
    "        # Set seed for single-GPU\n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "\n",
    "        # Set seed for all available GPUs (multi-GPU training)\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "        # Ensures that CUDA uses deterministic algorithms\n",
    "        # This disables non-deterministic optimizations and ensures reproducible behavior\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "        # Disables cuDNN auto-tuner which selects the best algorithm for each configuration\n",
    "        # When disabled, it uses deterministic algorithms, but this might make training slower\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Set the seed globally so every run starts from the same state\n",
    "set_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "09bbd150",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "def prepare_lagged_mlp_data(df: pd.DataFrame, symbol: str, max_lag: int = 10, test_size: float = 0.8, seed: int = 42):\n",
    "    \"\"\"\n",
    "    Prepare lagged feature matrix and binary (0/1) classification target from stock returns,\n",
    "    and split into PyTorch-ready training and test sets. Also returns test set raw returns\n",
    "    for strategy evaluation.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): must contain ['SYMBOL', 'DATE', 'TIME', 'RETURN']\n",
    "        symbol (str): asset to process\n",
    "        max_lag (int): number of lagged returns as features\n",
    "        test_size (float): fraction of data to allocate to test set\n",
    "        seed (int): random seed for reproducibility\n",
    "\n",
    "    Returns:\n",
    "        train_loader, test_loader: PyTorch DataLoader objects\n",
    "        input_dim: int, number of input features (should be max_lag)\n",
    "        test_returns: np.ndarray of raw returns (not labels) aligned with test set\n",
    "    \"\"\"\n",
    "    group = df[df['SYMBOL'] == symbol].sort_values(by=['DATE', 'TIME']).reset_index(drop=True)\n",
    "    returns = group['RETURN'].values.astype(np.float32)\n",
    "\n",
    "    if len(returns) < max_lag + 1:\n",
    "        raise ValueError(\"Not enough data to build lagged features.\")\n",
    "\n",
    "    # Build lagged features (X), binary targets (y), and raw next-step returns (y_raw)\n",
    "    X = np.column_stack([returns[i:len(returns)-max_lag+i] for i in range(max_lag)])\n",
    "    y_raw = returns[max_lag:]  # actual returns (not labels)\n",
    "    y = np.where(y_raw > 0, 1.0, 0.0).astype(np.float32)\n",
    "\n",
    "    # Split data (same split applies to y_raw)\n",
    "    X_train, X_test, y_train, y_test, y_raw_train, y_raw_test = train_test_split(\n",
    "        X, y, y_raw, test_size=test_size, random_state=seed, shuffle=False\n",
    "    )\n",
    "\n",
    "    # Standardize features\n",
    "    X_scaler = StandardScaler()\n",
    "    X_train_scaled = X_scaler.fit_transform(X_train)\n",
    "    X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "    X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "    # Wrap in datasets\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "    # DataLoaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader, X.shape[1], y_raw_test  # <--- Added test_returns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ad0c39dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_parquet('/Users/emanueledurante/Desktop/LGMB/lausanne/epfl/MLfinance/High-Frequency-Trading-with-Deep-Learning/data/high_10m.parquet')\n",
    "symbol = 'GS'\n",
    "input_dim = 10\n",
    "train_loader, test_loader, input_dim,test_returns = prepare_lagged_mlp_data(df, symbol, max_lag=input_dim)\n",
    "\n",
    "model = FlexibleMLP([input_dim, 64, 32, 32, 32, 1], scale=1.0, bias_scale=0.0)\n",
    "#model = FlexibleMLP([input_dim, 64, 32, 1], scale=1.0, bias_scale=0.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "beb6d78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)  # Fixing the seed\n",
    "criterion = nn.BCEWithLogitsLoss()  #For prediction, MSE is the standard objective; but other, custom objective might be better;\n",
    "# choose loss appropriate for your task\n",
    "# experiment with learning rates, lr = 0.02, 0.01, 0.001\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=0.001) # this is one of the most popular gradient descent algorithms\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01) # experiment with 0.1, 0.2, 0.5. 0.5 is super interesting, achives well OOS!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1ee520c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10000] Avg train loss: 0.705851\n",
      "Epoch [2/10000] Avg train loss: 0.706066\n",
      "Epoch [3/10000] Avg train loss: 0.705019\n",
      "Epoch [4/10000] Avg train loss: 0.705278\n",
      "Epoch [5/10000] Avg train loss: 0.704731\n",
      "Epoch [6/10000] Avg train loss: 0.704229\n",
      "Epoch [7/10000] Avg train loss: 0.704280\n",
      "Epoch [8/10000] Avg train loss: 0.703355\n",
      "Epoch [9/10000] Avg train loss: 0.703361\n",
      "Epoch [10/10000] Avg train loss: 0.703508\n",
      "Epoch [11/10000] Avg train loss: 0.702430\n",
      "Epoch [12/10000] Avg train loss: 0.702199\n",
      "Epoch [13/10000] Avg train loss: 0.702052\n",
      "Epoch [14/10000] Avg train loss: 0.701357\n",
      "Epoch [15/10000] Avg train loss: 0.701097\n",
      "Epoch [16/10000] Avg train loss: 0.700827\n",
      "Epoch [17/10000] Avg train loss: 0.700355\n",
      "Epoch [18/10000] Avg train loss: 0.699728\n",
      "Epoch [19/10000] Avg train loss: 0.699481\n",
      "Epoch [20/10000] Avg train loss: 0.699353\n",
      "Epoch [21/10000] Avg train loss: 0.698996\n",
      "Epoch [22/10000] Avg train loss: 0.698513\n",
      "Epoch [23/10000] Avg train loss: 0.698212\n",
      "Epoch [24/10000] Avg train loss: 0.698019\n",
      "Epoch [25/10000] Avg train loss: 0.697894\n",
      "Epoch [26/10000] Avg train loss: 0.697816\n",
      "Epoch [27/10000] Avg train loss: 0.697661\n",
      "Epoch [28/10000] Avg train loss: 0.697349\n",
      "Epoch [29/10000] Avg train loss: 0.697005\n",
      "Epoch [30/10000] Avg train loss: 0.696717\n",
      "Epoch [31/10000] Avg train loss: 0.696710\n",
      "Epoch [32/10000] Avg train loss: 0.696434\n",
      "Epoch [33/10000] Avg train loss: 0.696363\n",
      "Epoch [34/10000] Avg train loss: 0.696094\n",
      "Epoch [35/10000] Avg train loss: 0.695909\n",
      "Epoch [36/10000] Avg train loss: 0.695619\n",
      "Epoch [37/10000] Avg train loss: 0.695201\n",
      "Epoch [38/10000] Avg train loss: 0.695105\n",
      "Epoch [39/10000] Avg train loss: 0.695069\n",
      "Epoch [40/10000] Avg train loss: 0.694956\n",
      "Epoch [41/10000] Avg train loss: 0.694790\n",
      "Epoch [42/10000] Avg train loss: 0.694563\n",
      "Epoch [43/10000] Avg train loss: 0.694451\n",
      "Epoch [44/10000] Avg train loss: 0.694225\n",
      "Epoch [45/10000] Avg train loss: 0.694168\n",
      "Epoch [46/10000] Avg train loss: 0.693953\n",
      "Epoch [47/10000] Avg train loss: 0.693750\n",
      "Epoch [48/10000] Avg train loss: 0.693441\n",
      "Epoch [49/10000] Avg train loss: 0.693150\n",
      "Epoch [50/10000] Avg train loss: 0.692862\n",
      "Epoch [51/10000] Avg train loss: 0.692503\n",
      "Epoch [52/10000] Avg train loss: 0.692318\n",
      "Epoch [53/10000] Avg train loss: 0.692207\n",
      "Epoch [54/10000] Avg train loss: 0.692026\n",
      "Epoch [55/10000] Avg train loss: 0.691902\n",
      "Epoch [56/10000] Avg train loss: 0.691667\n",
      "Epoch [57/10000] Avg train loss: 0.691584\n",
      "Epoch [58/10000] Avg train loss: 0.691355\n",
      "Epoch [59/10000] Avg train loss: 0.691115\n",
      "Epoch [60/10000] Avg train loss: 0.691040\n",
      "Epoch [61/10000] Avg train loss: 0.690906\n",
      "Epoch [62/10000] Avg train loss: 0.690527\n",
      "Epoch [63/10000] Avg train loss: 0.690337\n",
      "Epoch [64/10000] Avg train loss: 0.690207\n",
      "Epoch [65/10000] Avg train loss: 0.690020\n",
      "Epoch [66/10000] Avg train loss: 0.689633\n",
      "Epoch [67/10000] Avg train loss: 0.689430\n",
      "Epoch [68/10000] Avg train loss: 0.689253\n",
      "Epoch [69/10000] Avg train loss: 0.689113\n",
      "Epoch [70/10000] Avg train loss: 0.688830\n",
      "Epoch [71/10000] Avg train loss: 0.688669\n",
      "Epoch [72/10000] Avg train loss: 0.688438\n",
      "Epoch [73/10000] Avg train loss: 0.688314\n",
      "Epoch [74/10000] Avg train loss: 0.688128\n",
      "Epoch [75/10000] Avg train loss: 0.687966\n",
      "Epoch [76/10000] Avg train loss: 0.687755\n",
      "Epoch [77/10000] Avg train loss: 0.687476\n",
      "Epoch [78/10000] Avg train loss: 0.687332\n",
      "Epoch [79/10000] Avg train loss: 0.687129\n",
      "Epoch [80/10000] Avg train loss: 0.686971\n",
      "Epoch [81/10000] Avg train loss: 0.686728\n",
      "Epoch [82/10000] Avg train loss: 0.686563\n",
      "Epoch [83/10000] Avg train loss: 0.686295\n",
      "Epoch [84/10000] Avg train loss: 0.686235\n",
      "Epoch [85/10000] Avg train loss: 0.686120\n",
      "Epoch [86/10000] Avg train loss: 0.685858\n",
      "Epoch [87/10000] Avg train loss: 0.685583\n",
      "Epoch [88/10000] Avg train loss: 0.685421\n",
      "Epoch [89/10000] Avg train loss: 0.685270\n",
      "Epoch [90/10000] Avg train loss: 0.685103\n",
      "Epoch [91/10000] Avg train loss: 0.684953\n",
      "Epoch [92/10000] Avg train loss: 0.684697\n",
      "Epoch [93/10000] Avg train loss: 0.684507\n",
      "Epoch [94/10000] Avg train loss: 0.684196\n",
      "Epoch [95/10000] Avg train loss: 0.683907\n",
      "Epoch [96/10000] Avg train loss: 0.683656\n",
      "Epoch [97/10000] Avg train loss: 0.683393\n",
      "Epoch [98/10000] Avg train loss: 0.683267\n",
      "Epoch [99/10000] Avg train loss: 0.683054\n",
      "Epoch [100/10000] Avg train loss: 0.682794\n",
      "Epoch [101/10000] Avg train loss: 0.682313\n",
      "Epoch [102/10000] Avg train loss: 0.682020\n",
      "Epoch [103/10000] Avg train loss: 0.681613\n",
      "Epoch [104/10000] Avg train loss: 0.681336\n",
      "Epoch [105/10000] Avg train loss: 0.681168\n",
      "Epoch [106/10000] Avg train loss: 0.680952\n",
      "Epoch [107/10000] Avg train loss: 0.680730\n",
      "Epoch [108/10000] Avg train loss: 0.680373\n",
      "Epoch [109/10000] Avg train loss: 0.680245\n",
      "Epoch [110/10000] Avg train loss: 0.679977\n",
      "Epoch [111/10000] Avg train loss: 0.679704\n",
      "Epoch [112/10000] Avg train loss: 0.679534\n",
      "Epoch [113/10000] Avg train loss: 0.679168\n",
      "Epoch [114/10000] Avg train loss: 0.678867\n",
      "Epoch [115/10000] Avg train loss: 0.678824\n",
      "Epoch [116/10000] Avg train loss: 0.678534\n",
      "Epoch [117/10000] Avg train loss: 0.678375\n",
      "Epoch [118/10000] Avg train loss: 0.678203\n",
      "Epoch [119/10000] Avg train loss: 0.677952\n",
      "Epoch [120/10000] Avg train loss: 0.677675\n",
      "Epoch [121/10000] Avg train loss: 0.677454\n",
      "Epoch [122/10000] Avg train loss: 0.677300\n",
      "Epoch [123/10000] Avg train loss: 0.677099\n",
      "Epoch [124/10000] Avg train loss: 0.676805\n",
      "Epoch [125/10000] Avg train loss: 0.676429\n",
      "Epoch [126/10000] Avg train loss: 0.676176\n",
      "Epoch [127/10000] Avg train loss: 0.675931\n",
      "Epoch [128/10000] Avg train loss: 0.675634\n",
      "Epoch [129/10000] Avg train loss: 0.675389\n",
      "Epoch [130/10000] Avg train loss: 0.675221\n",
      "Epoch [131/10000] Avg train loss: 0.674929\n",
      "Epoch [132/10000] Avg train loss: 0.674743\n",
      "Epoch [133/10000] Avg train loss: 0.674527\n",
      "Epoch [134/10000] Avg train loss: 0.674193\n",
      "Epoch [135/10000] Avg train loss: 0.673783\n",
      "Epoch [136/10000] Avg train loss: 0.673502\n",
      "Epoch [137/10000] Avg train loss: 0.673166\n",
      "Epoch [138/10000] Avg train loss: 0.672895\n",
      "Epoch [139/10000] Avg train loss: 0.672682\n",
      "Epoch [140/10000] Avg train loss: 0.672306\n",
      "Epoch [141/10000] Avg train loss: 0.672062\n",
      "Epoch [142/10000] Avg train loss: 0.671803\n",
      "Epoch [143/10000] Avg train loss: 0.671551\n",
      "Epoch [144/10000] Avg train loss: 0.671253\n",
      "Epoch [145/10000] Avg train loss: 0.670978\n",
      "Epoch [146/10000] Avg train loss: 0.670641\n",
      "Epoch [147/10000] Avg train loss: 0.670275\n",
      "Epoch [148/10000] Avg train loss: 0.669981\n",
      "Epoch [149/10000] Avg train loss: 0.669723\n",
      "Epoch [150/10000] Avg train loss: 0.669407\n",
      "Epoch [151/10000] Avg train loss: 0.669082\n",
      "Epoch [152/10000] Avg train loss: 0.669007\n",
      "Epoch [153/10000] Avg train loss: 0.668751\n",
      "Epoch [154/10000] Avg train loss: 0.668542\n",
      "Epoch [155/10000] Avg train loss: 0.668185\n",
      "Epoch [156/10000] Avg train loss: 0.667890\n",
      "Epoch [157/10000] Avg train loss: 0.667616\n",
      "Epoch [158/10000] Avg train loss: 0.667365\n",
      "Epoch [159/10000] Avg train loss: 0.667083\n",
      "Epoch [160/10000] Avg train loss: 0.666839\n",
      "Epoch [161/10000] Avg train loss: 0.666522\n",
      "Epoch [162/10000] Avg train loss: 0.666179\n",
      "Epoch [163/10000] Avg train loss: 0.665851\n",
      "Epoch [164/10000] Avg train loss: 0.665516\n",
      "Epoch [165/10000] Avg train loss: 0.665299\n",
      "Epoch [166/10000] Avg train loss: 0.664930\n",
      "Epoch [167/10000] Avg train loss: 0.664630\n",
      "Epoch [168/10000] Avg train loss: 0.664404\n",
      "Epoch [169/10000] Avg train loss: 0.664001\n",
      "Epoch [170/10000] Avg train loss: 0.663621\n",
      "Epoch [171/10000] Avg train loss: 0.663360\n",
      "Epoch [172/10000] Avg train loss: 0.663067\n",
      "Epoch [173/10000] Avg train loss: 0.662723\n",
      "Epoch [174/10000] Avg train loss: 0.662329\n",
      "Epoch [175/10000] Avg train loss: 0.662078\n",
      "Epoch [176/10000] Avg train loss: 0.661724\n",
      "Epoch [177/10000] Avg train loss: 0.661269\n",
      "Epoch [178/10000] Avg train loss: 0.660752\n",
      "Epoch [179/10000] Avg train loss: 0.660435\n",
      "Epoch [180/10000] Avg train loss: 0.660031\n",
      "Epoch [181/10000] Avg train loss: 0.659754\n",
      "Epoch [182/10000] Avg train loss: 0.659359\n",
      "Epoch [183/10000] Avg train loss: 0.659074\n",
      "Epoch [184/10000] Avg train loss: 0.658819\n",
      "Epoch [185/10000] Avg train loss: 0.658589\n",
      "Epoch [186/10000] Avg train loss: 0.658196\n",
      "Epoch [187/10000] Avg train loss: 0.657980\n",
      "Epoch [188/10000] Avg train loss: 0.657631\n",
      "Epoch [189/10000] Avg train loss: 0.657289\n",
      "Epoch [190/10000] Avg train loss: 0.656941\n",
      "Epoch [191/10000] Avg train loss: 0.656653\n",
      "Epoch [192/10000] Avg train loss: 0.656305\n",
      "Epoch [193/10000] Avg train loss: 0.656062\n",
      "Epoch [194/10000] Avg train loss: 0.655676\n",
      "Epoch [195/10000] Avg train loss: 0.655368\n",
      "Epoch [196/10000] Avg train loss: 0.655035\n",
      "Epoch [197/10000] Avg train loss: 0.654543\n",
      "Epoch [198/10000] Avg train loss: 0.654355\n",
      "Epoch [199/10000] Avg train loss: 0.653985\n",
      "Epoch [200/10000] Avg train loss: 0.653625\n",
      "Epoch [201/10000] Avg train loss: 0.653289\n",
      "Epoch [202/10000] Avg train loss: 0.652837\n",
      "Epoch [203/10000] Avg train loss: 0.652601\n",
      "Epoch [204/10000] Avg train loss: 0.652232\n",
      "Epoch [205/10000] Avg train loss: 0.651922\n",
      "Epoch [206/10000] Avg train loss: 0.651568\n",
      "Epoch [207/10000] Avg train loss: 0.651287\n",
      "Epoch [208/10000] Avg train loss: 0.651085\n",
      "Epoch [209/10000] Avg train loss: 0.650705\n",
      "Epoch [210/10000] Avg train loss: 0.650284\n",
      "Epoch [211/10000] Avg train loss: 0.649994\n",
      "Epoch [212/10000] Avg train loss: 0.649619\n",
      "Epoch [213/10000] Avg train loss: 0.649386\n",
      "Epoch [214/10000] Avg train loss: 0.649032\n",
      "Epoch [215/10000] Avg train loss: 0.648668\n",
      "Epoch [216/10000] Avg train loss: 0.648355\n",
      "Epoch [217/10000] Avg train loss: 0.647862\n",
      "Epoch [218/10000] Avg train loss: 0.647583\n",
      "Epoch [219/10000] Avg train loss: 0.647373\n",
      "Epoch [220/10000] Avg train loss: 0.647010\n",
      "Epoch [221/10000] Avg train loss: 0.646655\n",
      "Epoch [222/10000] Avg train loss: 0.646284\n",
      "Epoch [223/10000] Avg train loss: 0.646037\n",
      "Epoch [224/10000] Avg train loss: 0.645581\n",
      "Epoch [225/10000] Avg train loss: 0.645285\n",
      "Epoch [226/10000] Avg train loss: 0.644837\n",
      "Epoch [227/10000] Avg train loss: 0.644428\n",
      "Epoch [228/10000] Avg train loss: 0.644132\n",
      "Epoch [229/10000] Avg train loss: 0.643834\n",
      "Epoch [230/10000] Avg train loss: 0.643472\n",
      "Epoch [231/10000] Avg train loss: 0.642983\n",
      "Epoch [232/10000] Avg train loss: 0.642489\n",
      "Epoch [233/10000] Avg train loss: 0.641998\n",
      "Epoch [234/10000] Avg train loss: 0.641641\n",
      "Epoch [235/10000] Avg train loss: 0.641250\n",
      "Epoch [236/10000] Avg train loss: 0.640897\n",
      "Epoch [237/10000] Avg train loss: 0.640491\n",
      "Epoch [238/10000] Avg train loss: 0.640112\n",
      "Epoch [239/10000] Avg train loss: 0.639763\n",
      "Epoch [240/10000] Avg train loss: 0.639375\n",
      "Epoch [241/10000] Avg train loss: 0.639067\n",
      "Epoch [242/10000] Avg train loss: 0.638606\n",
      "Epoch [243/10000] Avg train loss: 0.638171\n",
      "Epoch [244/10000] Avg train loss: 0.637804\n",
      "Epoch [245/10000] Avg train loss: 0.637383\n",
      "Epoch [246/10000] Avg train loss: 0.637013\n",
      "Epoch [247/10000] Avg train loss: 0.636706\n",
      "Epoch [248/10000] Avg train loss: 0.636293\n",
      "Epoch [249/10000] Avg train loss: 0.635945\n",
      "Epoch [250/10000] Avg train loss: 0.635575\n",
      "Epoch [251/10000] Avg train loss: 0.635206\n",
      "Epoch [252/10000] Avg train loss: 0.634694\n",
      "Epoch [253/10000] Avg train loss: 0.634293\n",
      "Epoch [254/10000] Avg train loss: 0.633953\n",
      "Epoch [255/10000] Avg train loss: 0.633618\n",
      "Epoch [256/10000] Avg train loss: 0.633229\n",
      "Epoch [257/10000] Avg train loss: 0.632972\n",
      "Epoch [258/10000] Avg train loss: 0.632518\n",
      "Epoch [259/10000] Avg train loss: 0.632055\n",
      "Epoch [260/10000] Avg train loss: 0.631706\n",
      "Epoch [261/10000] Avg train loss: 0.631249\n",
      "Epoch [262/10000] Avg train loss: 0.630850\n",
      "Epoch [263/10000] Avg train loss: 0.630366\n",
      "Epoch [264/10000] Avg train loss: 0.629977\n",
      "Epoch [265/10000] Avg train loss: 0.629631\n",
      "Epoch [266/10000] Avg train loss: 0.629181\n",
      "Epoch [267/10000] Avg train loss: 0.628844\n",
      "Epoch [268/10000] Avg train loss: 0.628409\n",
      "Epoch [269/10000] Avg train loss: 0.627900\n",
      "Epoch [270/10000] Avg train loss: 0.627497\n",
      "Epoch [271/10000] Avg train loss: 0.627150\n",
      "Epoch [272/10000] Avg train loss: 0.626692\n",
      "Epoch [273/10000] Avg train loss: 0.626421\n",
      "Epoch [274/10000] Avg train loss: 0.626056\n",
      "Epoch [275/10000] Avg train loss: 0.625643\n",
      "Epoch [276/10000] Avg train loss: 0.625103\n",
      "Epoch [277/10000] Avg train loss: 0.624703\n",
      "Epoch [278/10000] Avg train loss: 0.624381\n",
      "Epoch [279/10000] Avg train loss: 0.623898\n",
      "Epoch [280/10000] Avg train loss: 0.623367\n",
      "Epoch [281/10000] Avg train loss: 0.623016\n",
      "Epoch [282/10000] Avg train loss: 0.622529\n",
      "Epoch [283/10000] Avg train loss: 0.622031\n",
      "Epoch [284/10000] Avg train loss: 0.621523\n",
      "Epoch [285/10000] Avg train loss: 0.621253\n",
      "Epoch [286/10000] Avg train loss: 0.620809\n",
      "Epoch [287/10000] Avg train loss: 0.620324\n",
      "Epoch [288/10000] Avg train loss: 0.619882\n",
      "Epoch [289/10000] Avg train loss: 0.619417\n",
      "Epoch [290/10000] Avg train loss: 0.619006\n",
      "Epoch [291/10000] Avg train loss: 0.618642\n",
      "Epoch [292/10000] Avg train loss: 0.618153\n",
      "Epoch [293/10000] Avg train loss: 0.617814\n",
      "Epoch [294/10000] Avg train loss: 0.617304\n",
      "Epoch [295/10000] Avg train loss: 0.616807\n",
      "Epoch [296/10000] Avg train loss: 0.616353\n",
      "Epoch [297/10000] Avg train loss: 0.615925\n",
      "Epoch [298/10000] Avg train loss: 0.615447\n",
      "Epoch [299/10000] Avg train loss: 0.615031\n",
      "Epoch [300/10000] Avg train loss: 0.614525\n",
      "Epoch [301/10000] Avg train loss: 0.614014\n",
      "Epoch [302/10000] Avg train loss: 0.613552\n",
      "Epoch [303/10000] Avg train loss: 0.613033\n",
      "Epoch [304/10000] Avg train loss: 0.612557\n",
      "Epoch [305/10000] Avg train loss: 0.612111\n",
      "Epoch [306/10000] Avg train loss: 0.611685\n",
      "Epoch [307/10000] Avg train loss: 0.611269\n",
      "Epoch [308/10000] Avg train loss: 0.610760\n",
      "Epoch [309/10000] Avg train loss: 0.610318\n",
      "Epoch [310/10000] Avg train loss: 0.609783\n",
      "Epoch [311/10000] Avg train loss: 0.609418\n",
      "Epoch [312/10000] Avg train loss: 0.608875\n",
      "Epoch [313/10000] Avg train loss: 0.608432\n",
      "Epoch [314/10000] Avg train loss: 0.607907\n",
      "Epoch [315/10000] Avg train loss: 0.607474\n",
      "Epoch [316/10000] Avg train loss: 0.607046\n",
      "Epoch [317/10000] Avg train loss: 0.606496\n",
      "Epoch [318/10000] Avg train loss: 0.606194\n",
      "Epoch [319/10000] Avg train loss: 0.605709\n",
      "Epoch [320/10000] Avg train loss: 0.605184\n",
      "Epoch [321/10000] Avg train loss: 0.604705\n",
      "Epoch [322/10000] Avg train loss: 0.604251\n",
      "Epoch [323/10000] Avg train loss: 0.603764\n",
      "Epoch [324/10000] Avg train loss: 0.603228\n",
      "Epoch [325/10000] Avg train loss: 0.602763\n",
      "Epoch [326/10000] Avg train loss: 0.602347\n",
      "Epoch [327/10000] Avg train loss: 0.601875\n",
      "Epoch [328/10000] Avg train loss: 0.601373\n",
      "Epoch [329/10000] Avg train loss: 0.600910\n",
      "Epoch [330/10000] Avg train loss: 0.600363\n",
      "Epoch [331/10000] Avg train loss: 0.599909\n",
      "Epoch [332/10000] Avg train loss: 0.599380\n",
      "Epoch [333/10000] Avg train loss: 0.598947\n",
      "Epoch [334/10000] Avg train loss: 0.598469\n",
      "Epoch [335/10000] Avg train loss: 0.598084\n",
      "Epoch [336/10000] Avg train loss: 0.597590\n",
      "Epoch [337/10000] Avg train loss: 0.597117\n",
      "Epoch [338/10000] Avg train loss: 0.596561\n",
      "Epoch [339/10000] Avg train loss: 0.596124\n",
      "Epoch [340/10000] Avg train loss: 0.595731\n",
      "Epoch [341/10000] Avg train loss: 0.595359\n",
      "Epoch [342/10000] Avg train loss: 0.594884\n",
      "Epoch [343/10000] Avg train loss: 0.594317\n",
      "Epoch [344/10000] Avg train loss: 0.593843\n",
      "Epoch [345/10000] Avg train loss: 0.593425\n",
      "Epoch [346/10000] Avg train loss: 0.592914\n",
      "Epoch [347/10000] Avg train loss: 0.592500\n",
      "Epoch [348/10000] Avg train loss: 0.591926\n",
      "Epoch [349/10000] Avg train loss: 0.591527\n",
      "Epoch [350/10000] Avg train loss: 0.591059\n",
      "Epoch [351/10000] Avg train loss: 0.590516\n",
      "Epoch [352/10000] Avg train loss: 0.589941\n",
      "Epoch [353/10000] Avg train loss: 0.589459\n",
      "Epoch [354/10000] Avg train loss: 0.588928\n",
      "Epoch [355/10000] Avg train loss: 0.588466\n",
      "Epoch [356/10000] Avg train loss: 0.587995\n",
      "Epoch [357/10000] Avg train loss: 0.587478\n",
      "Epoch [358/10000] Avg train loss: 0.586913\n",
      "Epoch [359/10000] Avg train loss: 0.586485\n",
      "Epoch [360/10000] Avg train loss: 0.585964\n",
      "Epoch [361/10000] Avg train loss: 0.585435\n",
      "Epoch [362/10000] Avg train loss: 0.584985\n",
      "Epoch [363/10000] Avg train loss: 0.584633\n",
      "Epoch [364/10000] Avg train loss: 0.584145\n",
      "Epoch [365/10000] Avg train loss: 0.583618\n",
      "Epoch [366/10000] Avg train loss: 0.583130\n",
      "Epoch [367/10000] Avg train loss: 0.582619\n",
      "Epoch [368/10000] Avg train loss: 0.582108\n",
      "Epoch [369/10000] Avg train loss: 0.581608\n",
      "Epoch [370/10000] Avg train loss: 0.581056\n",
      "Epoch [371/10000] Avg train loss: 0.580454\n",
      "Epoch [372/10000] Avg train loss: 0.579966\n",
      "Epoch [373/10000] Avg train loss: 0.579497\n",
      "Epoch [374/10000] Avg train loss: 0.579033\n",
      "Epoch [375/10000] Avg train loss: 0.578476\n",
      "Epoch [376/10000] Avg train loss: 0.577961\n",
      "Epoch [377/10000] Avg train loss: 0.577464\n",
      "Epoch [378/10000] Avg train loss: 0.576980\n",
      "Epoch [379/10000] Avg train loss: 0.576386\n",
      "Epoch [380/10000] Avg train loss: 0.575809\n",
      "Epoch [381/10000] Avg train loss: 0.575316\n",
      "Epoch [382/10000] Avg train loss: 0.574809\n",
      "Epoch [383/10000] Avg train loss: 0.574327\n",
      "Epoch [384/10000] Avg train loss: 0.573865\n",
      "Epoch [385/10000] Avg train loss: 0.573309\n",
      "Epoch [386/10000] Avg train loss: 0.572817\n",
      "Epoch [387/10000] Avg train loss: 0.572365\n",
      "Epoch [388/10000] Avg train loss: 0.571772\n",
      "Epoch [389/10000] Avg train loss: 0.571217\n",
      "Epoch [390/10000] Avg train loss: 0.570755\n",
      "Epoch [391/10000] Avg train loss: 0.570278\n",
      "Epoch [392/10000] Avg train loss: 0.569734\n",
      "Epoch [393/10000] Avg train loss: 0.569124\n",
      "Epoch [394/10000] Avg train loss: 0.568669\n",
      "Epoch [395/10000] Avg train loss: 0.568032\n",
      "Epoch [396/10000] Avg train loss: 0.567504\n",
      "Epoch [397/10000] Avg train loss: 0.566995\n",
      "Epoch [398/10000] Avg train loss: 0.566383\n",
      "Epoch [399/10000] Avg train loss: 0.565868\n",
      "Epoch [400/10000] Avg train loss: 0.565306\n",
      "Epoch [401/10000] Avg train loss: 0.564717\n",
      "Epoch [402/10000] Avg train loss: 0.564120\n",
      "Epoch [403/10000] Avg train loss: 0.563731\n",
      "Epoch [404/10000] Avg train loss: 0.563280\n",
      "Epoch [405/10000] Avg train loss: 0.562689\n",
      "Epoch [406/10000] Avg train loss: 0.562108\n",
      "Epoch [407/10000] Avg train loss: 0.561534\n",
      "Epoch [408/10000] Avg train loss: 0.560933\n",
      "Epoch [409/10000] Avg train loss: 0.560470\n",
      "Epoch [410/10000] Avg train loss: 0.559849\n",
      "Epoch [411/10000] Avg train loss: 0.559261\n",
      "Epoch [412/10000] Avg train loss: 0.558686\n",
      "Epoch [413/10000] Avg train loss: 0.558170\n",
      "Epoch [414/10000] Avg train loss: 0.557584\n",
      "Epoch [415/10000] Avg train loss: 0.557016\n",
      "Epoch [416/10000] Avg train loss: 0.556404\n",
      "Epoch [417/10000] Avg train loss: 0.555897\n",
      "Epoch [418/10000] Avg train loss: 0.555268\n",
      "Epoch [419/10000] Avg train loss: 0.554687\n",
      "Epoch [420/10000] Avg train loss: 0.554114\n",
      "Epoch [421/10000] Avg train loss: 0.553573\n",
      "Epoch [422/10000] Avg train loss: 0.553025\n",
      "Epoch [423/10000] Avg train loss: 0.552432\n",
      "Epoch [424/10000] Avg train loss: 0.551875\n",
      "Epoch [425/10000] Avg train loss: 0.551245\n",
      "Epoch [426/10000] Avg train loss: 0.550737\n",
      "Epoch [427/10000] Avg train loss: 0.550138\n",
      "Epoch [428/10000] Avg train loss: 0.549623\n",
      "Epoch [429/10000] Avg train loss: 0.549008\n",
      "Epoch [430/10000] Avg train loss: 0.548491\n",
      "Epoch [431/10000] Avg train loss: 0.547923\n",
      "Epoch [432/10000] Avg train loss: 0.547289\n",
      "Epoch [433/10000] Avg train loss: 0.546826\n",
      "Epoch [434/10000] Avg train loss: 0.546277\n",
      "Epoch [435/10000] Avg train loss: 0.545749\n",
      "Epoch [436/10000] Avg train loss: 0.545271\n",
      "Epoch [437/10000] Avg train loss: 0.544716\n",
      "Epoch [438/10000] Avg train loss: 0.544119\n",
      "Epoch [439/10000] Avg train loss: 0.543616\n",
      "Epoch [440/10000] Avg train loss: 0.543044\n",
      "Epoch [441/10000] Avg train loss: 0.542470\n",
      "Epoch [442/10000] Avg train loss: 0.541862\n",
      "Epoch [443/10000] Avg train loss: 0.541288\n",
      "Epoch [444/10000] Avg train loss: 0.540670\n",
      "Epoch [445/10000] Avg train loss: 0.540198\n",
      "Epoch [446/10000] Avg train loss: 0.539619\n",
      "Epoch [447/10000] Avg train loss: 0.539076\n",
      "Epoch [448/10000] Avg train loss: 0.538473\n",
      "Epoch [449/10000] Avg train loss: 0.537985\n",
      "Epoch [450/10000] Avg train loss: 0.537388\n",
      "Epoch [451/10000] Avg train loss: 0.536771\n",
      "Epoch [452/10000] Avg train loss: 0.536153\n",
      "Epoch [453/10000] Avg train loss: 0.535536\n",
      "Epoch [454/10000] Avg train loss: 0.534915\n",
      "Epoch [455/10000] Avg train loss: 0.534326\n",
      "Epoch [456/10000] Avg train loss: 0.533678\n",
      "Epoch [457/10000] Avg train loss: 0.533039\n",
      "Epoch [458/10000] Avg train loss: 0.532485\n",
      "Epoch [459/10000] Avg train loss: 0.531906\n",
      "Epoch [460/10000] Avg train loss: 0.531348\n",
      "Epoch [461/10000] Avg train loss: 0.530776\n",
      "Epoch [462/10000] Avg train loss: 0.530190\n",
      "Epoch [463/10000] Avg train loss: 0.529727\n",
      "Epoch [464/10000] Avg train loss: 0.529098\n",
      "Epoch [465/10000] Avg train loss: 0.528568\n",
      "Epoch [466/10000] Avg train loss: 0.527982\n",
      "Epoch [467/10000] Avg train loss: 0.527394\n",
      "Epoch [468/10000] Avg train loss: 0.526827\n",
      "Epoch [469/10000] Avg train loss: 0.526318\n",
      "Epoch [470/10000] Avg train loss: 0.525773\n",
      "Epoch [471/10000] Avg train loss: 0.525177\n",
      "Epoch [472/10000] Avg train loss: 0.524595\n",
      "Epoch [473/10000] Avg train loss: 0.524064\n",
      "Epoch [474/10000] Avg train loss: 0.523493\n",
      "Epoch [475/10000] Avg train loss: 0.522870\n",
      "Epoch [476/10000] Avg train loss: 0.522287\n",
      "Epoch [477/10000] Avg train loss: 0.521682\n",
      "Epoch [478/10000] Avg train loss: 0.521147\n",
      "Epoch [479/10000] Avg train loss: 0.520579\n",
      "Epoch [480/10000] Avg train loss: 0.520093\n",
      "Epoch [481/10000] Avg train loss: 0.519528\n",
      "Epoch [482/10000] Avg train loss: 0.518960\n",
      "Epoch [483/10000] Avg train loss: 0.518390\n",
      "Epoch [484/10000] Avg train loss: 0.517795\n",
      "Epoch [485/10000] Avg train loss: 0.517239\n",
      "Epoch [486/10000] Avg train loss: 0.516609\n",
      "Epoch [487/10000] Avg train loss: 0.516080\n",
      "Epoch [488/10000] Avg train loss: 0.515485\n",
      "Epoch [489/10000] Avg train loss: 0.514942\n",
      "Epoch [490/10000] Avg train loss: 0.514337\n",
      "Epoch [491/10000] Avg train loss: 0.513753\n",
      "Epoch [492/10000] Avg train loss: 0.513170\n",
      "Epoch [493/10000] Avg train loss: 0.512544\n",
      "Epoch [494/10000] Avg train loss: 0.511976\n",
      "Epoch [495/10000] Avg train loss: 0.511381\n",
      "Epoch [496/10000] Avg train loss: 0.510838\n",
      "Epoch [497/10000] Avg train loss: 0.510231\n",
      "Epoch [498/10000] Avg train loss: 0.509661\n",
      "Epoch [499/10000] Avg train loss: 0.509116\n",
      "Epoch [500/10000] Avg train loss: 0.508525\n",
      "Epoch [501/10000] Avg train loss: 0.507901\n",
      "Epoch [502/10000] Avg train loss: 0.507289\n",
      "Epoch [503/10000] Avg train loss: 0.506714\n",
      "Epoch [504/10000] Avg train loss: 0.506107\n",
      "Epoch [505/10000] Avg train loss: 0.505537\n",
      "Epoch [506/10000] Avg train loss: 0.504960\n",
      "Epoch [507/10000] Avg train loss: 0.504381\n",
      "Epoch [508/10000] Avg train loss: 0.503848\n",
      "Epoch [509/10000] Avg train loss: 0.503251\n",
      "Epoch [510/10000] Avg train loss: 0.502680\n",
      "Epoch [511/10000] Avg train loss: 0.502133\n",
      "Epoch [512/10000] Avg train loss: 0.501549\n",
      "Epoch [513/10000] Avg train loss: 0.500976\n",
      "Epoch [514/10000] Avg train loss: 0.500358\n",
      "Epoch [515/10000] Avg train loss: 0.499801\n",
      "Epoch [516/10000] Avg train loss: 0.499259\n",
      "Epoch [517/10000] Avg train loss: 0.498714\n",
      "Epoch [518/10000] Avg train loss: 0.498113\n",
      "Epoch [519/10000] Avg train loss: 0.497549\n",
      "Epoch [520/10000] Avg train loss: 0.496960\n",
      "Epoch [521/10000] Avg train loss: 0.496428\n",
      "Epoch [522/10000] Avg train loss: 0.495840\n",
      "Epoch [523/10000] Avg train loss: 0.495302\n",
      "Epoch [524/10000] Avg train loss: 0.494714\n",
      "Epoch [525/10000] Avg train loss: 0.494135\n",
      "Epoch [526/10000] Avg train loss: 0.493584\n",
      "Epoch [527/10000] Avg train loss: 0.492979\n",
      "Epoch [528/10000] Avg train loss: 0.492377\n",
      "Epoch [529/10000] Avg train loss: 0.491840\n",
      "Epoch [530/10000] Avg train loss: 0.491273\n",
      "Epoch [531/10000] Avg train loss: 0.490689\n",
      "Epoch [532/10000] Avg train loss: 0.490168\n",
      "Epoch [533/10000] Avg train loss: 0.489604\n",
      "Epoch [534/10000] Avg train loss: 0.489019\n",
      "Epoch [535/10000] Avg train loss: 0.488459\n",
      "Epoch [536/10000] Avg train loss: 0.487907\n",
      "Epoch [537/10000] Avg train loss: 0.487362\n",
      "Epoch [538/10000] Avg train loss: 0.486793\n",
      "Epoch [539/10000] Avg train loss: 0.486200\n",
      "Epoch [540/10000] Avg train loss: 0.485607\n",
      "Epoch [541/10000] Avg train loss: 0.485048\n",
      "Epoch [542/10000] Avg train loss: 0.484494\n",
      "Epoch [543/10000] Avg train loss: 0.483914\n",
      "Epoch [544/10000] Avg train loss: 0.483418\n",
      "Epoch [545/10000] Avg train loss: 0.482896\n",
      "Epoch [546/10000] Avg train loss: 0.482383\n",
      "Epoch [547/10000] Avg train loss: 0.481798\n",
      "Epoch [548/10000] Avg train loss: 0.481226\n",
      "Epoch [549/10000] Avg train loss: 0.480676\n",
      "Epoch [550/10000] Avg train loss: 0.480165\n",
      "Epoch [551/10000] Avg train loss: 0.479634\n",
      "Epoch [552/10000] Avg train loss: 0.479116\n",
      "Epoch [553/10000] Avg train loss: 0.478552\n",
      "Epoch [554/10000] Avg train loss: 0.477967\n",
      "Epoch [555/10000] Avg train loss: 0.477394\n",
      "Epoch [556/10000] Avg train loss: 0.476846\n",
      "Epoch [557/10000] Avg train loss: 0.476290\n",
      "Epoch [558/10000] Avg train loss: 0.475712\n",
      "Epoch [559/10000] Avg train loss: 0.475130\n",
      "Epoch [560/10000] Avg train loss: 0.474536\n",
      "Epoch [561/10000] Avg train loss: 0.473959\n",
      "Epoch [562/10000] Avg train loss: 0.473396\n",
      "Epoch [563/10000] Avg train loss: 0.472858\n",
      "Epoch [564/10000] Avg train loss: 0.472261\n",
      "Epoch [565/10000] Avg train loss: 0.471726\n",
      "Epoch [566/10000] Avg train loss: 0.471199\n",
      "Epoch [567/10000] Avg train loss: 0.470696\n",
      "Epoch [568/10000] Avg train loss: 0.470169\n",
      "Epoch [569/10000] Avg train loss: 0.469597\n",
      "Epoch [570/10000] Avg train loss: 0.469012\n",
      "Epoch [571/10000] Avg train loss: 0.468439\n",
      "Epoch [572/10000] Avg train loss: 0.467868\n",
      "Epoch [573/10000] Avg train loss: 0.467303\n",
      "Epoch [574/10000] Avg train loss: 0.466785\n",
      "Epoch [575/10000] Avg train loss: 0.466196\n",
      "Epoch [576/10000] Avg train loss: 0.465609\n",
      "Epoch [577/10000] Avg train loss: 0.465017\n",
      "Epoch [578/10000] Avg train loss: 0.464431\n",
      "Epoch [579/10000] Avg train loss: 0.463839\n",
      "Epoch [580/10000] Avg train loss: 0.463315\n",
      "Epoch [581/10000] Avg train loss: 0.462734\n",
      "Epoch [582/10000] Avg train loss: 0.462180\n",
      "Epoch [583/10000] Avg train loss: 0.461608\n",
      "Epoch [584/10000] Avg train loss: 0.461037\n",
      "Epoch [585/10000] Avg train loss: 0.460495\n",
      "Epoch [586/10000] Avg train loss: 0.459937\n",
      "Epoch [587/10000] Avg train loss: 0.459380\n",
      "Epoch [588/10000] Avg train loss: 0.458854\n",
      "Epoch [589/10000] Avg train loss: 0.458312\n",
      "Epoch [590/10000] Avg train loss: 0.457750\n",
      "Epoch [591/10000] Avg train loss: 0.457175\n",
      "Epoch [592/10000] Avg train loss: 0.456616\n",
      "Epoch [593/10000] Avg train loss: 0.456041\n",
      "Epoch [594/10000] Avg train loss: 0.455486\n",
      "Epoch [595/10000] Avg train loss: 0.454960\n",
      "Epoch [596/10000] Avg train loss: 0.454399\n",
      "Epoch [597/10000] Avg train loss: 0.453827\n",
      "Epoch [598/10000] Avg train loss: 0.453264\n",
      "Epoch [599/10000] Avg train loss: 0.452766\n",
      "Epoch [600/10000] Avg train loss: 0.452209\n",
      "Epoch [601/10000] Avg train loss: 0.451645\n",
      "Epoch [602/10000] Avg train loss: 0.451084\n",
      "Epoch [603/10000] Avg train loss: 0.450547\n",
      "Epoch [604/10000] Avg train loss: 0.449989\n",
      "Epoch [605/10000] Avg train loss: 0.449444\n",
      "Epoch [606/10000] Avg train loss: 0.448940\n",
      "Epoch [607/10000] Avg train loss: 0.448397\n",
      "Epoch [608/10000] Avg train loss: 0.447829\n",
      "Epoch [609/10000] Avg train loss: 0.447270\n",
      "Epoch [610/10000] Avg train loss: 0.446778\n",
      "Epoch [611/10000] Avg train loss: 0.446228\n",
      "Epoch [612/10000] Avg train loss: 0.445688\n",
      "Epoch [613/10000] Avg train loss: 0.445134\n",
      "Epoch [614/10000] Avg train loss: 0.444587\n",
      "Epoch [615/10000] Avg train loss: 0.444074\n",
      "Epoch [616/10000] Avg train loss: 0.443518\n",
      "Epoch [617/10000] Avg train loss: 0.442981\n",
      "Epoch [618/10000] Avg train loss: 0.442433\n",
      "Epoch [619/10000] Avg train loss: 0.441951\n",
      "Epoch [620/10000] Avg train loss: 0.441401\n",
      "Epoch [621/10000] Avg train loss: 0.440863\n",
      "Epoch [622/10000] Avg train loss: 0.440310\n",
      "Epoch [623/10000] Avg train loss: 0.439767\n",
      "Epoch [624/10000] Avg train loss: 0.439225\n",
      "Epoch [625/10000] Avg train loss: 0.438684\n",
      "Epoch [626/10000] Avg train loss: 0.438198\n",
      "Epoch [627/10000] Avg train loss: 0.437644\n",
      "Epoch [628/10000] Avg train loss: 0.437121\n",
      "Epoch [629/10000] Avg train loss: 0.436581\n",
      "Epoch [630/10000] Avg train loss: 0.436100\n",
      "Epoch [631/10000] Avg train loss: 0.435564\n",
      "Epoch [632/10000] Avg train loss: 0.435023\n",
      "Epoch [633/10000] Avg train loss: 0.434520\n",
      "Epoch [634/10000] Avg train loss: 0.434031\n",
      "Epoch [635/10000] Avg train loss: 0.433514\n",
      "Epoch [636/10000] Avg train loss: 0.432992\n",
      "Epoch [637/10000] Avg train loss: 0.432451\n",
      "Epoch [638/10000] Avg train loss: 0.431919\n",
      "Epoch [639/10000] Avg train loss: 0.431400\n",
      "Epoch [640/10000] Avg train loss: 0.430896\n",
      "Epoch [641/10000] Avg train loss: 0.430362\n",
      "Epoch [642/10000] Avg train loss: 0.429830\n",
      "Epoch [643/10000] Avg train loss: 0.429304\n",
      "Epoch [644/10000] Avg train loss: 0.428785\n",
      "Epoch [645/10000] Avg train loss: 0.428250\n",
      "Epoch [646/10000] Avg train loss: 0.427734\n",
      "Epoch [647/10000] Avg train loss: 0.427208\n",
      "Epoch [648/10000] Avg train loss: 0.426686\n",
      "Epoch [649/10000] Avg train loss: 0.426174\n",
      "Epoch [650/10000] Avg train loss: 0.425649\n",
      "Epoch [651/10000] Avg train loss: 0.425129\n",
      "Epoch [652/10000] Avg train loss: 0.424661\n",
      "Epoch [653/10000] Avg train loss: 0.424141\n",
      "Epoch [654/10000] Avg train loss: 0.423621\n",
      "Epoch [655/10000] Avg train loss: 0.423123\n",
      "Epoch [656/10000] Avg train loss: 0.422645\n",
      "Epoch [657/10000] Avg train loss: 0.422135\n",
      "Epoch [658/10000] Avg train loss: 0.421615\n",
      "Epoch [659/10000] Avg train loss: 0.421091\n",
      "Epoch [660/10000] Avg train loss: 0.420573\n",
      "Epoch [661/10000] Avg train loss: 0.420049\n",
      "Epoch [662/10000] Avg train loss: 0.419529\n",
      "Epoch [663/10000] Avg train loss: 0.419034\n",
      "Epoch [664/10000] Avg train loss: 0.418533\n",
      "Epoch [665/10000] Avg train loss: 0.418035\n",
      "Epoch [666/10000] Avg train loss: 0.417519\n",
      "Epoch [667/10000] Avg train loss: 0.417003\n",
      "Epoch [668/10000] Avg train loss: 0.416484\n",
      "Epoch [669/10000] Avg train loss: 0.415968\n",
      "Epoch [670/10000] Avg train loss: 0.415481\n",
      "Epoch [671/10000] Avg train loss: 0.414972\n",
      "Epoch [672/10000] Avg train loss: 0.414471\n",
      "Epoch [673/10000] Avg train loss: 0.413981\n",
      "Epoch [674/10000] Avg train loss: 0.413470\n",
      "Epoch [675/10000] Avg train loss: 0.412962\n",
      "Epoch [676/10000] Avg train loss: 0.412455\n",
      "Epoch [677/10000] Avg train loss: 0.411948\n",
      "Epoch [678/10000] Avg train loss: 0.411441\n",
      "Epoch [679/10000] Avg train loss: 0.410964\n",
      "Epoch [680/10000] Avg train loss: 0.410478\n",
      "Epoch [681/10000] Avg train loss: 0.409994\n",
      "Epoch [682/10000] Avg train loss: 0.409492\n",
      "Epoch [683/10000] Avg train loss: 0.408990\n",
      "Epoch [684/10000] Avg train loss: 0.408556\n",
      "Epoch [685/10000] Avg train loss: 0.408074\n",
      "Epoch [686/10000] Avg train loss: 0.407574\n",
      "Epoch [687/10000] Avg train loss: 0.407099\n",
      "Epoch [688/10000] Avg train loss: 0.406626\n",
      "Epoch [689/10000] Avg train loss: 0.406140\n",
      "Epoch [690/10000] Avg train loss: 0.405668\n",
      "Epoch [691/10000] Avg train loss: 0.405199\n",
      "Epoch [692/10000] Avg train loss: 0.404719\n",
      "Epoch [693/10000] Avg train loss: 0.404219\n",
      "Epoch [694/10000] Avg train loss: 0.403825\n",
      "Epoch [695/10000] Avg train loss: 0.403354\n",
      "Epoch [696/10000] Avg train loss: 0.402895\n",
      "Epoch [697/10000] Avg train loss: 0.402407\n",
      "Epoch [698/10000] Avg train loss: 0.401934\n",
      "Epoch [699/10000] Avg train loss: 0.401468\n",
      "Epoch [700/10000] Avg train loss: 0.400997\n",
      "Epoch [701/10000] Avg train loss: 0.400515\n",
      "Epoch [702/10000] Avg train loss: 0.400024\n",
      "Epoch [703/10000] Avg train loss: 0.399577\n",
      "Epoch [704/10000] Avg train loss: 0.399093\n",
      "Epoch [705/10000] Avg train loss: 0.398606\n",
      "Epoch [706/10000] Avg train loss: 0.398118\n",
      "Epoch [707/10000] Avg train loss: 0.397661\n",
      "Epoch [708/10000] Avg train loss: 0.397194\n",
      "Epoch [709/10000] Avg train loss: 0.396718\n",
      "Epoch [710/10000] Avg train loss: 0.396237\n",
      "Epoch [711/10000] Avg train loss: 0.395752\n",
      "Epoch [712/10000] Avg train loss: 0.395268\n",
      "Epoch [713/10000] Avg train loss: 0.394785\n",
      "Epoch [714/10000] Avg train loss: 0.394317\n",
      "Epoch [715/10000] Avg train loss: 0.393840\n",
      "Epoch [716/10000] Avg train loss: 0.393370\n",
      "Epoch [717/10000] Avg train loss: 0.392894\n",
      "Epoch [718/10000] Avg train loss: 0.392446\n",
      "Epoch [719/10000] Avg train loss: 0.391971\n",
      "Epoch [720/10000] Avg train loss: 0.391497\n",
      "Epoch [721/10000] Avg train loss: 0.391024\n",
      "Epoch [722/10000] Avg train loss: 0.390551\n",
      "Epoch [723/10000] Avg train loss: 0.390101\n",
      "Epoch [724/10000] Avg train loss: 0.389644\n",
      "Epoch [725/10000] Avg train loss: 0.389178\n",
      "Epoch [726/10000] Avg train loss: 0.388714\n",
      "Epoch [727/10000] Avg train loss: 0.388257\n",
      "Epoch [728/10000] Avg train loss: 0.387789\n",
      "Epoch [729/10000] Avg train loss: 0.387349\n",
      "Epoch [730/10000] Avg train loss: 0.386897\n",
      "Epoch [731/10000] Avg train loss: 0.386434\n",
      "Epoch [732/10000] Avg train loss: 0.385996\n",
      "Epoch [733/10000] Avg train loss: 0.385542\n",
      "Epoch [734/10000] Avg train loss: 0.385078\n",
      "Epoch [735/10000] Avg train loss: 0.384618\n",
      "Epoch [736/10000] Avg train loss: 0.384166\n",
      "Epoch [737/10000] Avg train loss: 0.383742\n",
      "Epoch [738/10000] Avg train loss: 0.383288\n",
      "Epoch [739/10000] Avg train loss: 0.382829\n",
      "Epoch [740/10000] Avg train loss: 0.382370\n",
      "Epoch [741/10000] Avg train loss: 0.381941\n",
      "Epoch [742/10000] Avg train loss: 0.381491\n",
      "Epoch [743/10000] Avg train loss: 0.381036\n",
      "Epoch [744/10000] Avg train loss: 0.380581\n",
      "Epoch [745/10000] Avg train loss: 0.380127\n",
      "Epoch [746/10000] Avg train loss: 0.379677\n",
      "Epoch [747/10000] Avg train loss: 0.379228\n",
      "Epoch [748/10000] Avg train loss: 0.378798\n",
      "Epoch [749/10000] Avg train loss: 0.378352\n",
      "Epoch [750/10000] Avg train loss: 0.377900\n",
      "Epoch [751/10000] Avg train loss: 0.377452\n",
      "Epoch [752/10000] Avg train loss: 0.377002\n",
      "Epoch [753/10000] Avg train loss: 0.376609\n",
      "Epoch [754/10000] Avg train loss: 0.376161\n",
      "Epoch [755/10000] Avg train loss: 0.375733\n",
      "Epoch [756/10000] Avg train loss: 0.375288\n",
      "Epoch [757/10000] Avg train loss: 0.374854\n",
      "Epoch [758/10000] Avg train loss: 0.374416\n",
      "Epoch [759/10000] Avg train loss: 0.373981\n",
      "Epoch [760/10000] Avg train loss: 0.373542\n",
      "Epoch [761/10000] Avg train loss: 0.373101\n",
      "Epoch [762/10000] Avg train loss: 0.372672\n",
      "Epoch [763/10000] Avg train loss: 0.372237\n",
      "Epoch [764/10000] Avg train loss: 0.371809\n",
      "Epoch [765/10000] Avg train loss: 0.371375\n",
      "Epoch [766/10000] Avg train loss: 0.370948\n",
      "Epoch [767/10000] Avg train loss: 0.370530\n",
      "Epoch [768/10000] Avg train loss: 0.370095\n",
      "Epoch [769/10000] Avg train loss: 0.369664\n",
      "Epoch [770/10000] Avg train loss: 0.369238\n",
      "Epoch [771/10000] Avg train loss: 0.368811\n",
      "Epoch [772/10000] Avg train loss: 0.368377\n",
      "Epoch [773/10000] Avg train loss: 0.367944\n",
      "Epoch [774/10000] Avg train loss: 0.367511\n",
      "Epoch [775/10000] Avg train loss: 0.367079\n",
      "Epoch [776/10000] Avg train loss: 0.366656\n",
      "Epoch [777/10000] Avg train loss: 0.366225\n",
      "Epoch [778/10000] Avg train loss: 0.365831\n",
      "Epoch [779/10000] Avg train loss: 0.365406\n",
      "Epoch [780/10000] Avg train loss: 0.364988\n",
      "Epoch [781/10000] Avg train loss: 0.364563\n",
      "Epoch [782/10000] Avg train loss: 0.364146\n",
      "Epoch [783/10000] Avg train loss: 0.363722\n",
      "Epoch [784/10000] Avg train loss: 0.363299\n",
      "Epoch [785/10000] Avg train loss: 0.362882\n",
      "Epoch [786/10000] Avg train loss: 0.362459\n",
      "Epoch [787/10000] Avg train loss: 0.362038\n",
      "Epoch [788/10000] Avg train loss: 0.361621\n",
      "Epoch [789/10000] Avg train loss: 0.361202\n",
      "Epoch [790/10000] Avg train loss: 0.360792\n",
      "Epoch [791/10000] Avg train loss: 0.360381\n",
      "Epoch [792/10000] Avg train loss: 0.359979\n",
      "Epoch [793/10000] Avg train loss: 0.359563\n",
      "Epoch [794/10000] Avg train loss: 0.359152\n",
      "Epoch [795/10000] Avg train loss: 0.358738\n",
      "Epoch [796/10000] Avg train loss: 0.358325\n",
      "Epoch [797/10000] Avg train loss: 0.357924\n",
      "Epoch [798/10000] Avg train loss: 0.357513\n",
      "Epoch [799/10000] Avg train loss: 0.357102\n",
      "Epoch [800/10000] Avg train loss: 0.356690\n",
      "Epoch [801/10000] Avg train loss: 0.356285\n",
      "Epoch [802/10000] Avg train loss: 0.355874\n",
      "Epoch [803/10000] Avg train loss: 0.355466\n",
      "Epoch [804/10000] Avg train loss: 0.355068\n",
      "Epoch [805/10000] Avg train loss: 0.354664\n",
      "Epoch [806/10000] Avg train loss: 0.354275\n",
      "Epoch [807/10000] Avg train loss: 0.353876\n",
      "Epoch [808/10000] Avg train loss: 0.353471\n",
      "Epoch [809/10000] Avg train loss: 0.353066\n",
      "Epoch [810/10000] Avg train loss: 0.352662\n",
      "Epoch [811/10000] Avg train loss: 0.352270\n",
      "Epoch [812/10000] Avg train loss: 0.351873\n",
      "Epoch [813/10000] Avg train loss: 0.351492\n",
      "Epoch [814/10000] Avg train loss: 0.351091\n",
      "Epoch [815/10000] Avg train loss: 0.350703\n",
      "Epoch [816/10000] Avg train loss: 0.350313\n",
      "Epoch [817/10000] Avg train loss: 0.349920\n",
      "Epoch [818/10000] Avg train loss: 0.349522\n",
      "Epoch [819/10000] Avg train loss: 0.349125\n",
      "Epoch [820/10000] Avg train loss: 0.348731\n",
      "Epoch [821/10000] Avg train loss: 0.348338\n",
      "Epoch [822/10000] Avg train loss: 0.347942\n",
      "Epoch [823/10000] Avg train loss: 0.347555\n",
      "Epoch [824/10000] Avg train loss: 0.347170\n",
      "Epoch [825/10000] Avg train loss: 0.346777\n",
      "Epoch [826/10000] Avg train loss: 0.346388\n",
      "Epoch [827/10000] Avg train loss: 0.345996\n",
      "Epoch [828/10000] Avg train loss: 0.345606\n",
      "Epoch [829/10000] Avg train loss: 0.345218\n",
      "Epoch [830/10000] Avg train loss: 0.344829\n",
      "Epoch [831/10000] Avg train loss: 0.344445\n",
      "Epoch [832/10000] Avg train loss: 0.344064\n",
      "Epoch [833/10000] Avg train loss: 0.343677\n",
      "Epoch [834/10000] Avg train loss: 0.343292\n",
      "Epoch [835/10000] Avg train loss: 0.342908\n",
      "Epoch [836/10000] Avg train loss: 0.342525\n",
      "Epoch [837/10000] Avg train loss: 0.342152\n",
      "Epoch [838/10000] Avg train loss: 0.341769\n",
      "Epoch [839/10000] Avg train loss: 0.341387\n",
      "Epoch [840/10000] Avg train loss: 0.341014\n",
      "Epoch [841/10000] Avg train loss: 0.340649\n",
      "Epoch [842/10000] Avg train loss: 0.340269\n",
      "Epoch [843/10000] Avg train loss: 0.339895\n",
      "Epoch [844/10000] Avg train loss: 0.339532\n",
      "Epoch [845/10000] Avg train loss: 0.339153\n",
      "Epoch [846/10000] Avg train loss: 0.338782\n",
      "Epoch [847/10000] Avg train loss: 0.338414\n",
      "Epoch [848/10000] Avg train loss: 0.338043\n",
      "Epoch [849/10000] Avg train loss: 0.337668\n",
      "Epoch [850/10000] Avg train loss: 0.337304\n",
      "Epoch [851/10000] Avg train loss: 0.336930\n",
      "Epoch [852/10000] Avg train loss: 0.336557\n",
      "Epoch [853/10000] Avg train loss: 0.336185\n",
      "Epoch [854/10000] Avg train loss: 0.335813\n",
      "Epoch [855/10000] Avg train loss: 0.335444\n",
      "Epoch [856/10000] Avg train loss: 0.335075\n",
      "Epoch [857/10000] Avg train loss: 0.334705\n",
      "Epoch [858/10000] Avg train loss: 0.334336\n",
      "Epoch [859/10000] Avg train loss: 0.333967\n",
      "Epoch [860/10000] Avg train loss: 0.333603\n",
      "Epoch [861/10000] Avg train loss: 0.333237\n",
      "Epoch [862/10000] Avg train loss: 0.332872\n",
      "Epoch [863/10000] Avg train loss: 0.332509\n",
      "Epoch [864/10000] Avg train loss: 0.332146\n",
      "Epoch [865/10000] Avg train loss: 0.331784\n",
      "Epoch [866/10000] Avg train loss: 0.331423\n",
      "Epoch [867/10000] Avg train loss: 0.331066\n",
      "Epoch [868/10000] Avg train loss: 0.330710\n",
      "Epoch [869/10000] Avg train loss: 0.330350\n",
      "Epoch [870/10000] Avg train loss: 0.329992\n",
      "Epoch [871/10000] Avg train loss: 0.329633\n",
      "Epoch [872/10000] Avg train loss: 0.329274\n",
      "Epoch [873/10000] Avg train loss: 0.328916\n",
      "Epoch [874/10000] Avg train loss: 0.328560\n",
      "Epoch [875/10000] Avg train loss: 0.328205\n",
      "Epoch [876/10000] Avg train loss: 0.327850\n",
      "Epoch [877/10000] Avg train loss: 0.327496\n",
      "Epoch [878/10000] Avg train loss: 0.327144\n",
      "Epoch [879/10000] Avg train loss: 0.326790\n",
      "Epoch [880/10000] Avg train loss: 0.326449\n",
      "Epoch [881/10000] Avg train loss: 0.326097\n",
      "Epoch [882/10000] Avg train loss: 0.325746\n",
      "Epoch [883/10000] Avg train loss: 0.325403\n",
      "Epoch [884/10000] Avg train loss: 0.325056\n",
      "Epoch [885/10000] Avg train loss: 0.324711\n",
      "Epoch [886/10000] Avg train loss: 0.324363\n",
      "Epoch [887/10000] Avg train loss: 0.324016\n",
      "Epoch [888/10000] Avg train loss: 0.323672\n",
      "Epoch [889/10000] Avg train loss: 0.323328\n",
      "Epoch [890/10000] Avg train loss: 0.322982\n",
      "Epoch [891/10000] Avg train loss: 0.322638\n",
      "Epoch [892/10000] Avg train loss: 0.322293\n",
      "Epoch [893/10000] Avg train loss: 0.321948\n",
      "Epoch [894/10000] Avg train loss: 0.321609\n",
      "Epoch [895/10000] Avg train loss: 0.321272\n",
      "Epoch [896/10000] Avg train loss: 0.320930\n",
      "Epoch [897/10000] Avg train loss: 0.320591\n",
      "Epoch [898/10000] Avg train loss: 0.320253\n",
      "Epoch [899/10000] Avg train loss: 0.319920\n",
      "Epoch [900/10000] Avg train loss: 0.319580\n",
      "Epoch [901/10000] Avg train loss: 0.319243\n",
      "Epoch [902/10000] Avg train loss: 0.318905\n",
      "Epoch [903/10000] Avg train loss: 0.318573\n",
      "Epoch [904/10000] Avg train loss: 0.318236\n",
      "Epoch [905/10000] Avg train loss: 0.317901\n",
      "Epoch [906/10000] Avg train loss: 0.317570\n",
      "Epoch [907/10000] Avg train loss: 0.317236\n",
      "Epoch [908/10000] Avg train loss: 0.316901\n",
      "Epoch [909/10000] Avg train loss: 0.316570\n",
      "Epoch [910/10000] Avg train loss: 0.316237\n",
      "Epoch [911/10000] Avg train loss: 0.315908\n",
      "Epoch [912/10000] Avg train loss: 0.315577\n",
      "Epoch [913/10000] Avg train loss: 0.315247\n",
      "Epoch [914/10000] Avg train loss: 0.314917\n",
      "Epoch [915/10000] Avg train loss: 0.314591\n",
      "Epoch [916/10000] Avg train loss: 0.314261\n",
      "Epoch [917/10000] Avg train loss: 0.313933\n",
      "Epoch [918/10000] Avg train loss: 0.313605\n",
      "Epoch [919/10000] Avg train loss: 0.313278\n",
      "Epoch [920/10000] Avg train loss: 0.312951\n",
      "Epoch [921/10000] Avg train loss: 0.312633\n",
      "Epoch [922/10000] Avg train loss: 0.312311\n",
      "Epoch [923/10000] Avg train loss: 0.311987\n",
      "Epoch [924/10000] Avg train loss: 0.311663\n",
      "Epoch [925/10000] Avg train loss: 0.311341\n",
      "Epoch [926/10000] Avg train loss: 0.311018\n",
      "Epoch [927/10000] Avg train loss: 0.310700\n",
      "Epoch [928/10000] Avg train loss: 0.310380\n",
      "Epoch [929/10000] Avg train loss: 0.310060\n",
      "Epoch [930/10000] Avg train loss: 0.309742\n",
      "Epoch [931/10000] Avg train loss: 0.309428\n",
      "Epoch [932/10000] Avg train loss: 0.309110\n",
      "Epoch [933/10000] Avg train loss: 0.308791\n",
      "Epoch [934/10000] Avg train loss: 0.308473\n",
      "Epoch [935/10000] Avg train loss: 0.308156\n",
      "Epoch [936/10000] Avg train loss: 0.307839\n",
      "Epoch [937/10000] Avg train loss: 0.307523\n",
      "Epoch [938/10000] Avg train loss: 0.307208\n",
      "Epoch [939/10000] Avg train loss: 0.306893\n",
      "Epoch [940/10000] Avg train loss: 0.306588\n",
      "Epoch [941/10000] Avg train loss: 0.306275\n",
      "Epoch [942/10000] Avg train loss: 0.305963\n",
      "Epoch [943/10000] Avg train loss: 0.305653\n",
      "Epoch [944/10000] Avg train loss: 0.305342\n",
      "Epoch [945/10000] Avg train loss: 0.305032\n",
      "Epoch [946/10000] Avg train loss: 0.304721\n",
      "Epoch [947/10000] Avg train loss: 0.304410\n",
      "Epoch [948/10000] Avg train loss: 0.304102\n",
      "Epoch [949/10000] Avg train loss: 0.303795\n",
      "Epoch [950/10000] Avg train loss: 0.303488\n",
      "Epoch [951/10000] Avg train loss: 0.303181\n",
      "Epoch [952/10000] Avg train loss: 0.302878\n",
      "Epoch [953/10000] Avg train loss: 0.302576\n",
      "Epoch [954/10000] Avg train loss: 0.302273\n",
      "Epoch [955/10000] Avg train loss: 0.301971\n",
      "Epoch [956/10000] Avg train loss: 0.301667\n",
      "Epoch [957/10000] Avg train loss: 0.301365\n",
      "Epoch [958/10000] Avg train loss: 0.301063\n",
      "Epoch [959/10000] Avg train loss: 0.300761\n",
      "Epoch [960/10000] Avg train loss: 0.300458\n",
      "Epoch [961/10000] Avg train loss: 0.300157\n",
      "Epoch [962/10000] Avg train loss: 0.299855\n",
      "Epoch [963/10000] Avg train loss: 0.299554\n",
      "Epoch [964/10000] Avg train loss: 0.299257\n",
      "Epoch [965/10000] Avg train loss: 0.298957\n",
      "Epoch [966/10000] Avg train loss: 0.298662\n",
      "Epoch [967/10000] Avg train loss: 0.298363\n",
      "Epoch [968/10000] Avg train loss: 0.298068\n",
      "Epoch [969/10000] Avg train loss: 0.297771\n",
      "Epoch [970/10000] Avg train loss: 0.297473\n",
      "Epoch [971/10000] Avg train loss: 0.297177\n",
      "Epoch [972/10000] Avg train loss: 0.296881\n",
      "Epoch [973/10000] Avg train loss: 0.296586\n",
      "Epoch [974/10000] Avg train loss: 0.296291\n",
      "Epoch [975/10000] Avg train loss: 0.296000\n",
      "Epoch [976/10000] Avg train loss: 0.295707\n",
      "Epoch [977/10000] Avg train loss: 0.295417\n",
      "Epoch [978/10000] Avg train loss: 0.295126\n",
      "Epoch [979/10000] Avg train loss: 0.294836\n",
      "Epoch [980/10000] Avg train loss: 0.294545\n",
      "Epoch [981/10000] Avg train loss: 0.294254\n",
      "Epoch [982/10000] Avg train loss: 0.293965\n",
      "Epoch [983/10000] Avg train loss: 0.293675\n",
      "Epoch [984/10000] Avg train loss: 0.293387\n",
      "Epoch [985/10000] Avg train loss: 0.293102\n",
      "Epoch [986/10000] Avg train loss: 0.292814\n",
      "Epoch [987/10000] Avg train loss: 0.292527\n",
      "Epoch [988/10000] Avg train loss: 0.292241\n",
      "Epoch [989/10000] Avg train loss: 0.291954\n",
      "Epoch [990/10000] Avg train loss: 0.291670\n",
      "Epoch [991/10000] Avg train loss: 0.291385\n",
      "Epoch [992/10000] Avg train loss: 0.291100\n",
      "Epoch [993/10000] Avg train loss: 0.290820\n",
      "Epoch [994/10000] Avg train loss: 0.290537\n",
      "Epoch [995/10000] Avg train loss: 0.290254\n",
      "Epoch [996/10000] Avg train loss: 0.289972\n",
      "Epoch [997/10000] Avg train loss: 0.289690\n",
      "Epoch [998/10000] Avg train loss: 0.289412\n",
      "Epoch [999/10000] Avg train loss: 0.289133\n",
      "Epoch [1000/10000] Avg train loss: 0.288852\n",
      "Epoch [1001/10000] Avg train loss: 0.288573\n",
      "Epoch [1002/10000] Avg train loss: 0.288293\n",
      "Epoch [1003/10000] Avg train loss: 0.288014\n",
      "Epoch [1004/10000] Avg train loss: 0.287736\n",
      "Epoch [1005/10000] Avg train loss: 0.287461\n",
      "Epoch [1006/10000] Avg train loss: 0.287185\n",
      "Epoch [1007/10000] Avg train loss: 0.286911\n",
      "Epoch [1008/10000] Avg train loss: 0.286636\n",
      "Epoch [1009/10000] Avg train loss: 0.286360\n",
      "Epoch [1010/10000] Avg train loss: 0.286084\n",
      "Epoch [1011/10000] Avg train loss: 0.285810\n",
      "Epoch [1012/10000] Avg train loss: 0.285536\n",
      "Epoch [1013/10000] Avg train loss: 0.285262\n",
      "Epoch [1014/10000] Avg train loss: 0.284989\n",
      "Epoch [1015/10000] Avg train loss: 0.284715\n",
      "Epoch [1016/10000] Avg train loss: 0.284443\n",
      "Epoch [1017/10000] Avg train loss: 0.284171\n",
      "Epoch [1018/10000] Avg train loss: 0.283899\n",
      "Epoch [1019/10000] Avg train loss: 0.283628\n",
      "Epoch [1020/10000] Avg train loss: 0.283361\n",
      "Epoch [1021/10000] Avg train loss: 0.283091\n",
      "Epoch [1022/10000] Avg train loss: 0.282823\n",
      "Epoch [1023/10000] Avg train loss: 0.282554\n",
      "Epoch [1024/10000] Avg train loss: 0.282286\n",
      "Epoch [1025/10000] Avg train loss: 0.282020\n",
      "Epoch [1026/10000] Avg train loss: 0.281753\n",
      "Epoch [1027/10000] Avg train loss: 0.281486\n",
      "Epoch [1028/10000] Avg train loss: 0.281220\n",
      "Epoch [1029/10000] Avg train loss: 0.280954\n",
      "Epoch [1030/10000] Avg train loss: 0.280688\n",
      "Epoch [1031/10000] Avg train loss: 0.280423\n",
      "Epoch [1032/10000] Avg train loss: 0.280159\n",
      "Epoch [1033/10000] Avg train loss: 0.279895\n",
      "Epoch [1034/10000] Avg train loss: 0.279632\n",
      "Epoch [1035/10000] Avg train loss: 0.279372\n",
      "Epoch [1036/10000] Avg train loss: 0.279111\n",
      "Epoch [1037/10000] Avg train loss: 0.278851\n",
      "Epoch [1038/10000] Avg train loss: 0.278589\n",
      "Epoch [1039/10000] Avg train loss: 0.278328\n",
      "Epoch [1040/10000] Avg train loss: 0.278067\n",
      "Epoch [1041/10000] Avg train loss: 0.277806\n",
      "Epoch [1042/10000] Avg train loss: 0.277547\n",
      "Epoch [1043/10000] Avg train loss: 0.277289\n",
      "Epoch [1044/10000] Avg train loss: 0.277030\n",
      "Epoch [1045/10000] Avg train loss: 0.276771\n",
      "Epoch [1046/10000] Avg train loss: 0.276513\n",
      "Epoch [1047/10000] Avg train loss: 0.276258\n",
      "Epoch [1048/10000] Avg train loss: 0.276001\n",
      "Epoch [1049/10000] Avg train loss: 0.275745\n",
      "Epoch [1050/10000] Avg train loss: 0.275490\n",
      "Epoch [1051/10000] Avg train loss: 0.275237\n",
      "Epoch [1052/10000] Avg train loss: 0.274981\n",
      "Epoch [1053/10000] Avg train loss: 0.274726\n",
      "Epoch [1054/10000] Avg train loss: 0.274472\n",
      "Epoch [1055/10000] Avg train loss: 0.274218\n",
      "Epoch [1056/10000] Avg train loss: 0.273968\n",
      "Epoch [1057/10000] Avg train loss: 0.273715\n",
      "Epoch [1058/10000] Avg train loss: 0.273464\n",
      "Epoch [1059/10000] Avg train loss: 0.273213\n",
      "Epoch [1060/10000] Avg train loss: 0.272963\n",
      "Epoch [1061/10000] Avg train loss: 0.272714\n",
      "Epoch [1062/10000] Avg train loss: 0.272464\n",
      "Epoch [1063/10000] Avg train loss: 0.272216\n",
      "Epoch [1064/10000] Avg train loss: 0.271967\n",
      "Epoch [1065/10000] Avg train loss: 0.271717\n",
      "Epoch [1066/10000] Avg train loss: 0.271468\n",
      "Epoch [1067/10000] Avg train loss: 0.271219\n",
      "Epoch [1068/10000] Avg train loss: 0.270972\n",
      "Epoch [1069/10000] Avg train loss: 0.270724\n",
      "Epoch [1070/10000] Avg train loss: 0.270477\n",
      "Epoch [1071/10000] Avg train loss: 0.270230\n",
      "Epoch [1072/10000] Avg train loss: 0.269984\n",
      "Epoch [1073/10000] Avg train loss: 0.269738\n",
      "Epoch [1074/10000] Avg train loss: 0.269493\n",
      "Epoch [1075/10000] Avg train loss: 0.269247\n",
      "Epoch [1076/10000] Avg train loss: 0.269003\n",
      "Epoch [1077/10000] Avg train loss: 0.268758\n",
      "Epoch [1078/10000] Avg train loss: 0.268514\n",
      "Epoch [1079/10000] Avg train loss: 0.268272\n",
      "Epoch [1080/10000] Avg train loss: 0.268029\n",
      "Epoch [1081/10000] Avg train loss: 0.267787\n",
      "Epoch [1082/10000] Avg train loss: 0.267545\n",
      "Epoch [1083/10000] Avg train loss: 0.267305\n",
      "Epoch [1084/10000] Avg train loss: 0.267064\n",
      "Epoch [1085/10000] Avg train loss: 0.266823\n",
      "Epoch [1086/10000] Avg train loss: 0.266583\n",
      "Epoch [1087/10000] Avg train loss: 0.266345\n",
      "Epoch [1088/10000] Avg train loss: 0.266106\n",
      "Epoch [1089/10000] Avg train loss: 0.265866\n",
      "Epoch [1090/10000] Avg train loss: 0.265628\n",
      "Epoch [1091/10000] Avg train loss: 0.265390\n",
      "Epoch [1092/10000] Avg train loss: 0.265152\n",
      "Epoch [1093/10000] Avg train loss: 0.264916\n",
      "Epoch [1094/10000] Avg train loss: 0.264679\n",
      "Epoch [1095/10000] Avg train loss: 0.264442\n",
      "Epoch [1096/10000] Avg train loss: 0.264206\n",
      "Epoch [1097/10000] Avg train loss: 0.263972\n",
      "Epoch [1098/10000] Avg train loss: 0.263737\n",
      "Epoch [1099/10000] Avg train loss: 0.263501\n",
      "Epoch [1100/10000] Avg train loss: 0.263267\n",
      "Epoch [1101/10000] Avg train loss: 0.263033\n",
      "Epoch [1102/10000] Avg train loss: 0.262800\n",
      "Epoch [1103/10000] Avg train loss: 0.262567\n",
      "Epoch [1104/10000] Avg train loss: 0.262340\n",
      "Epoch [1105/10000] Avg train loss: 0.262107\n",
      "Epoch [1106/10000] Avg train loss: 0.261878\n",
      "Epoch [1107/10000] Avg train loss: 0.261647\n",
      "Epoch [1108/10000] Avg train loss: 0.261416\n",
      "Epoch [1109/10000] Avg train loss: 0.261185\n",
      "Epoch [1110/10000] Avg train loss: 0.260955\n",
      "Epoch [1111/10000] Avg train loss: 0.260725\n",
      "Epoch [1112/10000] Avg train loss: 0.260496\n",
      "Epoch [1113/10000] Avg train loss: 0.260267\n",
      "Epoch [1114/10000] Avg train loss: 0.260040\n",
      "Epoch [1115/10000] Avg train loss: 0.259813\n",
      "Epoch [1116/10000] Avg train loss: 0.259585\n",
      "Epoch [1117/10000] Avg train loss: 0.259358\n",
      "Epoch [1118/10000] Avg train loss: 0.259131\n",
      "Epoch [1119/10000] Avg train loss: 0.258904\n",
      "Epoch [1120/10000] Avg train loss: 0.258678\n",
      "Epoch [1121/10000] Avg train loss: 0.258455\n",
      "Epoch [1122/10000] Avg train loss: 0.258230\n",
      "Epoch [1123/10000] Avg train loss: 0.258005\n",
      "Epoch [1124/10000] Avg train loss: 0.257780\n",
      "Epoch [1125/10000] Avg train loss: 0.257557\n",
      "Epoch [1126/10000] Avg train loss: 0.257333\n",
      "Epoch [1127/10000] Avg train loss: 0.257109\n",
      "Epoch [1128/10000] Avg train loss: 0.256886\n",
      "Epoch [1129/10000] Avg train loss: 0.256663\n",
      "Epoch [1130/10000] Avg train loss: 0.256441\n",
      "Epoch [1131/10000] Avg train loss: 0.256219\n",
      "Epoch [1132/10000] Avg train loss: 0.255997\n",
      "Epoch [1133/10000] Avg train loss: 0.255775\n",
      "Epoch [1134/10000] Avg train loss: 0.255555\n",
      "Epoch [1135/10000] Avg train loss: 0.255334\n",
      "Epoch [1136/10000] Avg train loss: 0.255114\n",
      "Epoch [1137/10000] Avg train loss: 0.254894\n",
      "Epoch [1138/10000] Avg train loss: 0.254675\n",
      "Epoch [1139/10000] Avg train loss: 0.254455\n",
      "Epoch [1140/10000] Avg train loss: 0.254237\n",
      "Epoch [1141/10000] Avg train loss: 0.254018\n",
      "Epoch [1142/10000] Avg train loss: 0.253801\n",
      "Epoch [1143/10000] Avg train loss: 0.253584\n",
      "Epoch [1144/10000] Avg train loss: 0.253366\n",
      "Epoch [1145/10000] Avg train loss: 0.253149\n",
      "Epoch [1146/10000] Avg train loss: 0.252934\n",
      "Epoch [1147/10000] Avg train loss: 0.252720\n",
      "Epoch [1148/10000] Avg train loss: 0.252504\n",
      "Epoch [1149/10000] Avg train loss: 0.252289\n",
      "Epoch [1150/10000] Avg train loss: 0.252074\n",
      "Epoch [1151/10000] Avg train loss: 0.251860\n",
      "Epoch [1152/10000] Avg train loss: 0.251645\n",
      "Epoch [1153/10000] Avg train loss: 0.251431\n",
      "Epoch [1154/10000] Avg train loss: 0.251217\n",
      "Epoch [1155/10000] Avg train loss: 0.251005\n",
      "Epoch [1156/10000] Avg train loss: 0.250791\n",
      "Epoch [1157/10000] Avg train loss: 0.250579\n",
      "Epoch [1158/10000] Avg train loss: 0.250367\n",
      "Epoch [1159/10000] Avg train loss: 0.250156\n",
      "Epoch [1160/10000] Avg train loss: 0.249944\n",
      "Epoch [1161/10000] Avg train loss: 0.249733\n",
      "Epoch [1162/10000] Avg train loss: 0.249524\n",
      "Epoch [1163/10000] Avg train loss: 0.249313\n",
      "Epoch [1164/10000] Avg train loss: 0.249103\n",
      "Epoch [1165/10000] Avg train loss: 0.248893\n",
      "Epoch [1166/10000] Avg train loss: 0.248683\n",
      "Epoch [1167/10000] Avg train loss: 0.248473\n",
      "Epoch [1168/10000] Avg train loss: 0.248266\n",
      "Epoch [1169/10000] Avg train loss: 0.248057\n",
      "Epoch [1170/10000] Avg train loss: 0.247849\n",
      "Epoch [1171/10000] Avg train loss: 0.247641\n",
      "Epoch [1172/10000] Avg train loss: 0.247434\n",
      "Epoch [1173/10000] Avg train loss: 0.247227\n",
      "Epoch [1174/10000] Avg train loss: 0.247021\n",
      "Epoch [1175/10000] Avg train loss: 0.246814\n",
      "Epoch [1176/10000] Avg train loss: 0.246608\n",
      "Epoch [1177/10000] Avg train loss: 0.246402\n",
      "Epoch [1178/10000] Avg train loss: 0.246196\n",
      "Epoch [1179/10000] Avg train loss: 0.245991\n",
      "Epoch [1180/10000] Avg train loss: 0.245786\n",
      "Epoch [1181/10000] Avg train loss: 0.245581\n",
      "Epoch [1182/10000] Avg train loss: 0.245377\n",
      "Epoch [1183/10000] Avg train loss: 0.245174\n",
      "Epoch [1184/10000] Avg train loss: 0.244970\n",
      "Epoch [1185/10000] Avg train loss: 0.244767\n",
      "Epoch [1186/10000] Avg train loss: 0.244564\n",
      "Epoch [1187/10000] Avg train loss: 0.244362\n",
      "Epoch [1188/10000] Avg train loss: 0.244159\n",
      "Epoch [1189/10000] Avg train loss: 0.243959\n",
      "Epoch [1190/10000] Avg train loss: 0.243757\n",
      "Epoch [1191/10000] Avg train loss: 0.243556\n",
      "Epoch [1192/10000] Avg train loss: 0.243356\n",
      "Epoch [1193/10000] Avg train loss: 0.243155\n",
      "Epoch [1194/10000] Avg train loss: 0.242955\n",
      "Epoch [1195/10000] Avg train loss: 0.242755\n",
      "Epoch [1196/10000] Avg train loss: 0.242556\n",
      "Epoch [1197/10000] Avg train loss: 0.242356\n",
      "Epoch [1198/10000] Avg train loss: 0.242158\n",
      "Epoch [1199/10000] Avg train loss: 0.241960\n",
      "Epoch [1200/10000] Avg train loss: 0.241763\n",
      "Epoch [1201/10000] Avg train loss: 0.241565\n",
      "Epoch [1202/10000] Avg train loss: 0.241367\n",
      "Epoch [1203/10000] Avg train loss: 0.241170\n",
      "Epoch [1204/10000] Avg train loss: 0.240973\n",
      "Epoch [1205/10000] Avg train loss: 0.240778\n",
      "Epoch [1206/10000] Avg train loss: 0.240582\n",
      "Epoch [1207/10000] Avg train loss: 0.240388\n",
      "Epoch [1208/10000] Avg train loss: 0.240193\n",
      "Epoch [1209/10000] Avg train loss: 0.239997\n",
      "Epoch [1210/10000] Avg train loss: 0.239802\n",
      "Epoch [1211/10000] Avg train loss: 0.239607\n",
      "Epoch [1212/10000] Avg train loss: 0.239413\n",
      "Epoch [1213/10000] Avg train loss: 0.239218\n",
      "Epoch [1214/10000] Avg train loss: 0.239024\n",
      "Epoch [1215/10000] Avg train loss: 0.238831\n",
      "Epoch [1216/10000] Avg train loss: 0.238637\n",
      "Epoch [1217/10000] Avg train loss: 0.238446\n",
      "Epoch [1218/10000] Avg train loss: 0.238253\n",
      "Epoch [1219/10000] Avg train loss: 0.238061\n",
      "Epoch [1220/10000] Avg train loss: 0.237869\n",
      "Epoch [1221/10000] Avg train loss: 0.237677\n",
      "Epoch [1222/10000] Avg train loss: 0.237486\n",
      "Epoch [1223/10000] Avg train loss: 0.237295\n",
      "Epoch [1224/10000] Avg train loss: 0.237104\n",
      "Epoch [1225/10000] Avg train loss: 0.236913\n",
      "Epoch [1226/10000] Avg train loss: 0.236724\n",
      "Epoch [1227/10000] Avg train loss: 0.236534\n",
      "Epoch [1228/10000] Avg train loss: 0.236345\n",
      "Epoch [1229/10000] Avg train loss: 0.236156\n",
      "Epoch [1230/10000] Avg train loss: 0.235967\n",
      "Epoch [1231/10000] Avg train loss: 0.235779\n",
      "Epoch [1232/10000] Avg train loss: 0.235590\n",
      "Epoch [1233/10000] Avg train loss: 0.235402\n",
      "Epoch [1234/10000] Avg train loss: 0.235214\n",
      "Epoch [1235/10000] Avg train loss: 0.235027\n",
      "Epoch [1236/10000] Avg train loss: 0.234840\n",
      "Epoch [1237/10000] Avg train loss: 0.234653\n",
      "Epoch [1238/10000] Avg train loss: 0.234466\n",
      "Epoch [1239/10000] Avg train loss: 0.234280\n",
      "Epoch [1240/10000] Avg train loss: 0.234094\n",
      "Epoch [1241/10000] Avg train loss: 0.233909\n",
      "Epoch [1242/10000] Avg train loss: 0.233723\n",
      "Epoch [1243/10000] Avg train loss: 0.233538\n",
      "Epoch [1244/10000] Avg train loss: 0.233353\n",
      "Epoch [1245/10000] Avg train loss: 0.233168\n",
      "Epoch [1246/10000] Avg train loss: 0.232985\n",
      "Epoch [1247/10000] Avg train loss: 0.232801\n",
      "Epoch [1248/10000] Avg train loss: 0.232617\n",
      "Epoch [1249/10000] Avg train loss: 0.232434\n",
      "Epoch [1250/10000] Avg train loss: 0.232251\n",
      "Epoch [1251/10000] Avg train loss: 0.232068\n",
      "Epoch [1252/10000] Avg train loss: 0.231885\n",
      "Epoch [1253/10000] Avg train loss: 0.231703\n",
      "Epoch [1254/10000] Avg train loss: 0.231521\n",
      "Epoch [1255/10000] Avg train loss: 0.231339\n",
      "Epoch [1256/10000] Avg train loss: 0.231157\n",
      "Epoch [1257/10000] Avg train loss: 0.230978\n",
      "Epoch [1258/10000] Avg train loss: 0.230796\n",
      "Epoch [1259/10000] Avg train loss: 0.230616\n",
      "Epoch [1260/10000] Avg train loss: 0.230435\n",
      "Epoch [1261/10000] Avg train loss: 0.230257\n",
      "Epoch [1262/10000] Avg train loss: 0.230077\n",
      "Epoch [1263/10000] Avg train loss: 0.229897\n",
      "Epoch [1264/10000] Avg train loss: 0.229718\n",
      "Epoch [1265/10000] Avg train loss: 0.229539\n",
      "Epoch [1266/10000] Avg train loss: 0.229361\n",
      "Epoch [1267/10000] Avg train loss: 0.229183\n",
      "Epoch [1268/10000] Avg train loss: 0.229005\n",
      "Epoch [1269/10000] Avg train loss: 0.228827\n",
      "Epoch [1270/10000] Avg train loss: 0.228650\n",
      "Epoch [1271/10000] Avg train loss: 0.228472\n",
      "Epoch [1272/10000] Avg train loss: 0.228296\n",
      "Epoch [1273/10000] Avg train loss: 0.228120\n",
      "Epoch [1274/10000] Avg train loss: 0.227944\n",
      "Epoch [1275/10000] Avg train loss: 0.227767\n",
      "Epoch [1276/10000] Avg train loss: 0.227592\n",
      "Epoch [1277/10000] Avg train loss: 0.227416\n",
      "Epoch [1278/10000] Avg train loss: 0.227241\n",
      "Epoch [1279/10000] Avg train loss: 0.227066\n",
      "Epoch [1280/10000] Avg train loss: 0.226890\n",
      "Epoch [1281/10000] Avg train loss: 0.226716\n",
      "Epoch [1282/10000] Avg train loss: 0.226541\n",
      "Epoch [1283/10000] Avg train loss: 0.226367\n",
      "Epoch [1284/10000] Avg train loss: 0.226195\n",
      "Epoch [1285/10000] Avg train loss: 0.226021\n",
      "Epoch [1286/10000] Avg train loss: 0.225848\n",
      "Epoch [1287/10000] Avg train loss: 0.225675\n",
      "Epoch [1288/10000] Avg train loss: 0.225502\n",
      "Epoch [1289/10000] Avg train loss: 0.225329\n",
      "Epoch [1290/10000] Avg train loss: 0.225157\n",
      "Epoch [1291/10000] Avg train loss: 0.224985\n",
      "Epoch [1292/10000] Avg train loss: 0.224813\n",
      "Epoch [1293/10000] Avg train loss: 0.224641\n",
      "Epoch [1294/10000] Avg train loss: 0.224470\n",
      "Epoch [1295/10000] Avg train loss: 0.224299\n",
      "Epoch [1296/10000] Avg train loss: 0.224128\n",
      "Epoch [1297/10000] Avg train loss: 0.223958\n",
      "Epoch [1298/10000] Avg train loss: 0.223788\n",
      "Epoch [1299/10000] Avg train loss: 0.223618\n",
      "Epoch [1300/10000] Avg train loss: 0.223448\n",
      "Epoch [1301/10000] Avg train loss: 0.223279\n",
      "Epoch [1302/10000] Avg train loss: 0.223110\n",
      "Epoch [1303/10000] Avg train loss: 0.222940\n",
      "Epoch [1304/10000] Avg train loss: 0.222772\n",
      "Epoch [1305/10000] Avg train loss: 0.222604\n",
      "Epoch [1306/10000] Avg train loss: 0.222436\n",
      "Epoch [1307/10000] Avg train loss: 0.222268\n",
      "Epoch [1308/10000] Avg train loss: 0.222100\n",
      "Epoch [1309/10000] Avg train loss: 0.221934\n",
      "Epoch [1310/10000] Avg train loss: 0.221767\n",
      "Epoch [1311/10000] Avg train loss: 0.221600\n",
      "Epoch [1312/10000] Avg train loss: 0.221433\n",
      "Epoch [1313/10000] Avg train loss: 0.221267\n",
      "Epoch [1314/10000] Avg train loss: 0.221100\n",
      "Epoch [1315/10000] Avg train loss: 0.220934\n",
      "Epoch [1316/10000] Avg train loss: 0.220769\n",
      "Epoch [1317/10000] Avg train loss: 0.220603\n",
      "Epoch [1318/10000] Avg train loss: 0.220439\n",
      "Epoch [1319/10000] Avg train loss: 0.220274\n",
      "Epoch [1320/10000] Avg train loss: 0.220109\n",
      "Epoch [1321/10000] Avg train loss: 0.219945\n",
      "Epoch [1322/10000] Avg train loss: 0.219781\n",
      "Epoch [1323/10000] Avg train loss: 0.219618\n",
      "Epoch [1324/10000] Avg train loss: 0.219455\n",
      "Epoch [1325/10000] Avg train loss: 0.219291\n",
      "Epoch [1326/10000] Avg train loss: 0.219128\n",
      "Epoch [1327/10000] Avg train loss: 0.218965\n",
      "Epoch [1328/10000] Avg train loss: 0.218802\n",
      "Epoch [1329/10000] Avg train loss: 0.218640\n",
      "Epoch [1330/10000] Avg train loss: 0.218478\n",
      "Epoch [1331/10000] Avg train loss: 0.218317\n",
      "Epoch [1332/10000] Avg train loss: 0.218155\n",
      "Epoch [1333/10000] Avg train loss: 0.217994\n",
      "Epoch [1334/10000] Avg train loss: 0.217832\n",
      "Epoch [1335/10000] Avg train loss: 0.217671\n",
      "Epoch [1336/10000] Avg train loss: 0.217510\n",
      "Epoch [1337/10000] Avg train loss: 0.217350\n",
      "Epoch [1338/10000] Avg train loss: 0.217190\n",
      "Epoch [1339/10000] Avg train loss: 0.217030\n",
      "Epoch [1340/10000] Avg train loss: 0.216871\n",
      "Epoch [1341/10000] Avg train loss: 0.216711\n",
      "Epoch [1342/10000] Avg train loss: 0.216552\n",
      "Epoch [1343/10000] Avg train loss: 0.216393\n",
      "Epoch [1344/10000] Avg train loss: 0.216234\n",
      "Epoch [1345/10000] Avg train loss: 0.216075\n",
      "Epoch [1346/10000] Avg train loss: 0.215917\n",
      "Epoch [1347/10000] Avg train loss: 0.215759\n",
      "Epoch [1348/10000] Avg train loss: 0.215602\n",
      "Epoch [1349/10000] Avg train loss: 0.215444\n",
      "Epoch [1350/10000] Avg train loss: 0.215287\n",
      "Epoch [1351/10000] Avg train loss: 0.215130\n",
      "Epoch [1352/10000] Avg train loss: 0.214972\n",
      "Epoch [1353/10000] Avg train loss: 0.214816\n",
      "Epoch [1354/10000] Avg train loss: 0.214659\n",
      "Epoch [1355/10000] Avg train loss: 0.214503\n",
      "Epoch [1356/10000] Avg train loss: 0.214346\n",
      "Epoch [1357/10000] Avg train loss: 0.214190\n",
      "Epoch [1358/10000] Avg train loss: 0.214034\n",
      "Epoch [1359/10000] Avg train loss: 0.213879\n",
      "Epoch [1360/10000] Avg train loss: 0.213724\n",
      "Epoch [1361/10000] Avg train loss: 0.213569\n",
      "Epoch [1362/10000] Avg train loss: 0.213414\n",
      "Epoch [1363/10000] Avg train loss: 0.213259\n",
      "Epoch [1364/10000] Avg train loss: 0.213105\n",
      "Epoch [1365/10000] Avg train loss: 0.212951\n",
      "Epoch [1366/10000] Avg train loss: 0.212797\n",
      "Epoch [1367/10000] Avg train loss: 0.212643\n",
      "Epoch [1368/10000] Avg train loss: 0.212490\n",
      "Epoch [1369/10000] Avg train loss: 0.212336\n",
      "Epoch [1370/10000] Avg train loss: 0.212183\n",
      "Epoch [1371/10000] Avg train loss: 0.212030\n",
      "Epoch [1372/10000] Avg train loss: 0.211878\n",
      "Epoch [1373/10000] Avg train loss: 0.211726\n",
      "Epoch [1374/10000] Avg train loss: 0.211574\n",
      "Epoch [1375/10000] Avg train loss: 0.211421\n",
      "Epoch [1376/10000] Avg train loss: 0.211270\n",
      "Epoch [1377/10000] Avg train loss: 0.211118\n",
      "Epoch [1378/10000] Avg train loss: 0.210967\n",
      "Epoch [1379/10000] Avg train loss: 0.210815\n",
      "Epoch [1380/10000] Avg train loss: 0.210665\n",
      "Epoch [1381/10000] Avg train loss: 0.210514\n",
      "Epoch [1382/10000] Avg train loss: 0.210363\n",
      "Epoch [1383/10000] Avg train loss: 0.210213\n",
      "Epoch [1384/10000] Avg train loss: 0.210063\n",
      "Epoch [1385/10000] Avg train loss: 0.209913\n",
      "Epoch [1386/10000] Avg train loss: 0.209764\n",
      "Epoch [1387/10000] Avg train loss: 0.209614\n",
      "Epoch [1388/10000] Avg train loss: 0.209465\n",
      "Epoch [1389/10000] Avg train loss: 0.209317\n",
      "Epoch [1390/10000] Avg train loss: 0.209168\n",
      "Epoch [1391/10000] Avg train loss: 0.209019\n",
      "Epoch [1392/10000] Avg train loss: 0.208871\n",
      "Epoch [1393/10000] Avg train loss: 0.208722\n",
      "Epoch [1394/10000] Avg train loss: 0.208574\n",
      "Epoch [1395/10000] Avg train loss: 0.208427\n",
      "Epoch [1396/10000] Avg train loss: 0.208279\n",
      "Epoch [1397/10000] Avg train loss: 0.208132\n",
      "Epoch [1398/10000] Avg train loss: 0.207985\n",
      "Epoch [1399/10000] Avg train loss: 0.207838\n",
      "Epoch [1400/10000] Avg train loss: 0.207691\n",
      "Epoch [1401/10000] Avg train loss: 0.207545\n",
      "Epoch [1402/10000] Avg train loss: 0.207399\n",
      "Epoch [1403/10000] Avg train loss: 0.207253\n",
      "Epoch [1404/10000] Avg train loss: 0.207107\n",
      "Epoch [1405/10000] Avg train loss: 0.206961\n",
      "Epoch [1406/10000] Avg train loss: 0.206816\n",
      "Epoch [1407/10000] Avg train loss: 0.206670\n",
      "Epoch [1408/10000] Avg train loss: 0.206525\n",
      "Epoch [1409/10000] Avg train loss: 0.206381\n",
      "Epoch [1410/10000] Avg train loss: 0.206236\n",
      "Epoch [1411/10000] Avg train loss: 0.206092\n",
      "Epoch [1412/10000] Avg train loss: 0.205948\n",
      "Epoch [1413/10000] Avg train loss: 0.205804\n",
      "Epoch [1414/10000] Avg train loss: 0.205660\n",
      "Epoch [1415/10000] Avg train loss: 0.205517\n",
      "Epoch [1416/10000] Avg train loss: 0.205373\n",
      "Epoch [1417/10000] Avg train loss: 0.205230\n",
      "Epoch [1418/10000] Avg train loss: 0.205087\n",
      "Epoch [1419/10000] Avg train loss: 0.204944\n",
      "Epoch [1420/10000] Avg train loss: 0.204802\n",
      "Epoch [1421/10000] Avg train loss: 0.204659\n",
      "Epoch [1422/10000] Avg train loss: 0.204517\n",
      "Epoch [1423/10000] Avg train loss: 0.204376\n",
      "Epoch [1424/10000] Avg train loss: 0.204234\n",
      "Epoch [1425/10000] Avg train loss: 0.204092\n",
      "Epoch [1426/10000] Avg train loss: 0.203951\n",
      "Epoch [1427/10000] Avg train loss: 0.203809\n",
      "Epoch [1428/10000] Avg train loss: 0.203668\n",
      "Epoch [1429/10000] Avg train loss: 0.203528\n",
      "Epoch [1430/10000] Avg train loss: 0.203387\n",
      "Epoch [1431/10000] Avg train loss: 0.203246\n",
      "Epoch [1432/10000] Avg train loss: 0.203106\n",
      "Epoch [1433/10000] Avg train loss: 0.202966\n",
      "Epoch [1434/10000] Avg train loss: 0.202826\n",
      "Epoch [1435/10000] Avg train loss: 0.202686\n",
      "Epoch [1436/10000] Avg train loss: 0.202547\n",
      "Epoch [1437/10000] Avg train loss: 0.202408\n",
      "Epoch [1438/10000] Avg train loss: 0.202269\n",
      "Epoch [1439/10000] Avg train loss: 0.202129\n",
      "Epoch [1440/10000] Avg train loss: 0.201990\n",
      "Epoch [1441/10000] Avg train loss: 0.201852\n",
      "Epoch [1442/10000] Avg train loss: 0.201714\n",
      "Epoch [1443/10000] Avg train loss: 0.201575\n",
      "Epoch [1444/10000] Avg train loss: 0.201437\n",
      "Epoch [1445/10000] Avg train loss: 0.201300\n",
      "Epoch [1446/10000] Avg train loss: 0.201162\n",
      "Epoch [1447/10000] Avg train loss: 0.201024\n",
      "Epoch [1448/10000] Avg train loss: 0.200887\n",
      "Epoch [1449/10000] Avg train loss: 0.200750\n",
      "Epoch [1450/10000] Avg train loss: 0.200613\n",
      "Epoch [1451/10000] Avg train loss: 0.200477\n",
      "Epoch [1452/10000] Avg train loss: 0.200340\n",
      "Epoch [1453/10000] Avg train loss: 0.200204\n",
      "Epoch [1454/10000] Avg train loss: 0.200067\n",
      "Epoch [1455/10000] Avg train loss: 0.199931\n",
      "Epoch [1456/10000] Avg train loss: 0.199795\n",
      "Epoch [1457/10000] Avg train loss: 0.199660\n",
      "Epoch [1458/10000] Avg train loss: 0.199525\n",
      "Epoch [1459/10000] Avg train loss: 0.199389\n",
      "Epoch [1460/10000] Avg train loss: 0.199254\n",
      "Epoch [1461/10000] Avg train loss: 0.199120\n",
      "Epoch [1462/10000] Avg train loss: 0.198985\n",
      "Epoch [1463/10000] Avg train loss: 0.198850\n",
      "Epoch [1464/10000] Avg train loss: 0.198717\n",
      "Epoch [1465/10000] Avg train loss: 0.198582\n",
      "Epoch [1466/10000] Avg train loss: 0.198448\n",
      "Epoch [1467/10000] Avg train loss: 0.198314\n",
      "Epoch [1468/10000] Avg train loss: 0.198181\n",
      "Epoch [1469/10000] Avg train loss: 0.198048\n",
      "Epoch [1470/10000] Avg train loss: 0.197914\n",
      "Epoch [1471/10000] Avg train loss: 0.197781\n",
      "Epoch [1472/10000] Avg train loss: 0.197648\n",
      "Epoch [1473/10000] Avg train loss: 0.197515\n",
      "Epoch [1474/10000] Avg train loss: 0.197383\n",
      "Epoch [1475/10000] Avg train loss: 0.197250\n",
      "Epoch [1476/10000] Avg train loss: 0.197118\n",
      "Epoch [1477/10000] Avg train loss: 0.196986\n",
      "Epoch [1478/10000] Avg train loss: 0.196854\n",
      "Epoch [1479/10000] Avg train loss: 0.196722\n",
      "Epoch [1480/10000] Avg train loss: 0.196591\n",
      "Epoch [1481/10000] Avg train loss: 0.196459\n",
      "Epoch [1482/10000] Avg train loss: 0.196328\n",
      "Epoch [1483/10000] Avg train loss: 0.196197\n",
      "Epoch [1484/10000] Avg train loss: 0.196066\n",
      "Epoch [1485/10000] Avg train loss: 0.195935\n",
      "Epoch [1486/10000] Avg train loss: 0.195805\n",
      "Epoch [1487/10000] Avg train loss: 0.195675\n",
      "Epoch [1488/10000] Avg train loss: 0.195545\n",
      "Epoch [1489/10000] Avg train loss: 0.195415\n",
      "Epoch [1490/10000] Avg train loss: 0.195285\n",
      "Epoch [1491/10000] Avg train loss: 0.195155\n",
      "Epoch [1492/10000] Avg train loss: 0.195026\n",
      "Epoch [1493/10000] Avg train loss: 0.194897\n",
      "Epoch [1494/10000] Avg train loss: 0.194768\n",
      "Epoch [1495/10000] Avg train loss: 0.194639\n",
      "Epoch [1496/10000] Avg train loss: 0.194510\n",
      "Epoch [1497/10000] Avg train loss: 0.194382\n",
      "Epoch [1498/10000] Avg train loss: 0.194254\n",
      "Epoch [1499/10000] Avg train loss: 0.194126\n",
      "Epoch [1500/10000] Avg train loss: 0.193998\n",
      "Epoch [1501/10000] Avg train loss: 0.193870\n",
      "Epoch [1502/10000] Avg train loss: 0.193742\n",
      "Epoch [1503/10000] Avg train loss: 0.193614\n",
      "Epoch [1504/10000] Avg train loss: 0.193487\n",
      "Epoch [1505/10000] Avg train loss: 0.193360\n",
      "Epoch [1506/10000] Avg train loss: 0.193232\n",
      "Epoch [1507/10000] Avg train loss: 0.193106\n",
      "Epoch [1508/10000] Avg train loss: 0.192979\n",
      "Epoch [1509/10000] Avg train loss: 0.192853\n",
      "Epoch [1510/10000] Avg train loss: 0.192726\n",
      "Epoch [1511/10000] Avg train loss: 0.192600\n",
      "Epoch [1512/10000] Avg train loss: 0.192474\n",
      "Epoch [1513/10000] Avg train loss: 0.192348\n",
      "Epoch [1514/10000] Avg train loss: 0.192222\n",
      "Epoch [1515/10000] Avg train loss: 0.192096\n",
      "Epoch [1516/10000] Avg train loss: 0.191971\n",
      "Epoch [1517/10000] Avg train loss: 0.191846\n",
      "Epoch [1518/10000] Avg train loss: 0.191721\n",
      "Epoch [1519/10000] Avg train loss: 0.191597\n",
      "Epoch [1520/10000] Avg train loss: 0.191472\n",
      "Epoch [1521/10000] Avg train loss: 0.191347\n",
      "Epoch [1522/10000] Avg train loss: 0.191222\n",
      "Epoch [1523/10000] Avg train loss: 0.191098\n",
      "Epoch [1524/10000] Avg train loss: 0.190974\n",
      "Epoch [1525/10000] Avg train loss: 0.190850\n",
      "Epoch [1526/10000] Avg train loss: 0.190726\n",
      "Epoch [1527/10000] Avg train loss: 0.190603\n",
      "Epoch [1528/10000] Avg train loss: 0.190479\n",
      "Epoch [1529/10000] Avg train loss: 0.190356\n",
      "Epoch [1530/10000] Avg train loss: 0.190233\n",
      "Epoch [1531/10000] Avg train loss: 0.190110\n",
      "Epoch [1532/10000] Avg train loss: 0.189987\n",
      "Epoch [1533/10000] Avg train loss: 0.189864\n",
      "Epoch [1534/10000] Avg train loss: 0.189742\n",
      "Epoch [1535/10000] Avg train loss: 0.189619\n",
      "Epoch [1536/10000] Avg train loss: 0.189497\n",
      "Epoch [1537/10000] Avg train loss: 0.189375\n",
      "Epoch [1538/10000] Avg train loss: 0.189253\n",
      "Epoch [1539/10000] Avg train loss: 0.189132\n",
      "Epoch [1540/10000] Avg train loss: 0.189010\n",
      "Epoch [1541/10000] Avg train loss: 0.188889\n",
      "Epoch [1542/10000] Avg train loss: 0.188768\n",
      "Epoch [1543/10000] Avg train loss: 0.188647\n",
      "Epoch [1544/10000] Avg train loss: 0.188526\n",
      "Epoch [1545/10000] Avg train loss: 0.188405\n",
      "Epoch [1546/10000] Avg train loss: 0.188284\n",
      "Epoch [1547/10000] Avg train loss: 0.188164\n",
      "Epoch [1548/10000] Avg train loss: 0.188043\n",
      "Epoch [1549/10000] Avg train loss: 0.187923\n",
      "Epoch [1550/10000] Avg train loss: 0.187803\n",
      "Epoch [1551/10000] Avg train loss: 0.187683\n",
      "Epoch [1552/10000] Avg train loss: 0.187563\n",
      "Epoch [1553/10000] Avg train loss: 0.187444\n",
      "Epoch [1554/10000] Avg train loss: 0.187324\n",
      "Epoch [1555/10000] Avg train loss: 0.187205\n",
      "Epoch [1556/10000] Avg train loss: 0.187086\n",
      "Epoch [1557/10000] Avg train loss: 0.186967\n",
      "Epoch [1558/10000] Avg train loss: 0.186848\n",
      "Epoch [1559/10000] Avg train loss: 0.186730\n",
      "Epoch [1560/10000] Avg train loss: 0.186611\n",
      "Epoch [1561/10000] Avg train loss: 0.186493\n",
      "Epoch [1562/10000] Avg train loss: 0.186374\n",
      "Epoch [1563/10000] Avg train loss: 0.186257\n",
      "Epoch [1564/10000] Avg train loss: 0.186139\n",
      "Epoch [1565/10000] Avg train loss: 0.186021\n",
      "Epoch [1566/10000] Avg train loss: 0.185903\n",
      "Epoch [1567/10000] Avg train loss: 0.185786\n",
      "Epoch [1568/10000] Avg train loss: 0.185669\n",
      "Epoch [1569/10000] Avg train loss: 0.185551\n",
      "Epoch [1570/10000] Avg train loss: 0.185435\n",
      "Epoch [1571/10000] Avg train loss: 0.185318\n",
      "Epoch [1572/10000] Avg train loss: 0.185201\n",
      "Epoch [1573/10000] Avg train loss: 0.185084\n",
      "Epoch [1574/10000] Avg train loss: 0.184968\n",
      "Epoch [1575/10000] Avg train loss: 0.184851\n",
      "Epoch [1576/10000] Avg train loss: 0.184735\n",
      "Epoch [1577/10000] Avg train loss: 0.184619\n",
      "Epoch [1578/10000] Avg train loss: 0.184504\n",
      "Epoch [1579/10000] Avg train loss: 0.184388\n",
      "Epoch [1580/10000] Avg train loss: 0.184272\n",
      "Epoch [1581/10000] Avg train loss: 0.184157\n",
      "Epoch [1582/10000] Avg train loss: 0.184041\n",
      "Epoch [1583/10000] Avg train loss: 0.183926\n",
      "Epoch [1584/10000] Avg train loss: 0.183811\n",
      "Epoch [1585/10000] Avg train loss: 0.183696\n",
      "Epoch [1586/10000] Avg train loss: 0.183581\n",
      "Epoch [1587/10000] Avg train loss: 0.183467\n",
      "Epoch [1588/10000] Avg train loss: 0.183352\n",
      "Epoch [1589/10000] Avg train loss: 0.183238\n",
      "Epoch [1590/10000] Avg train loss: 0.183124\n",
      "Epoch [1591/10000] Avg train loss: 0.183010\n",
      "Epoch [1592/10000] Avg train loss: 0.182896\n",
      "Epoch [1593/10000] Avg train loss: 0.182782\n",
      "Epoch [1594/10000] Avg train loss: 0.182669\n",
      "Epoch [1595/10000] Avg train loss: 0.182555\n",
      "Epoch [1596/10000] Avg train loss: 0.182442\n",
      "Epoch [1597/10000] Avg train loss: 0.182329\n",
      "Epoch [1598/10000] Avg train loss: 0.182217\n",
      "Epoch [1599/10000] Avg train loss: 0.182104\n",
      "Epoch [1600/10000] Avg train loss: 0.181991\n",
      "Epoch [1601/10000] Avg train loss: 0.181878\n",
      "Epoch [1602/10000] Avg train loss: 0.181766\n",
      "Epoch [1603/10000] Avg train loss: 0.181653\n",
      "Epoch [1604/10000] Avg train loss: 0.181541\n",
      "Epoch [1605/10000] Avg train loss: 0.181429\n",
      "Epoch [1606/10000] Avg train loss: 0.181317\n",
      "Epoch [1607/10000] Avg train loss: 0.181205\n",
      "Epoch [1608/10000] Avg train loss: 0.181094\n",
      "Epoch [1609/10000] Avg train loss: 0.180983\n",
      "Epoch [1610/10000] Avg train loss: 0.180871\n",
      "Epoch [1611/10000] Avg train loss: 0.180760\n",
      "Epoch [1612/10000] Avg train loss: 0.180649\n",
      "Epoch [1613/10000] Avg train loss: 0.180538\n",
      "Epoch [1614/10000] Avg train loss: 0.180427\n",
      "Epoch [1615/10000] Avg train loss: 0.180317\n",
      "Epoch [1616/10000] Avg train loss: 0.180206\n",
      "Epoch [1617/10000] Avg train loss: 0.180096\n",
      "Epoch [1618/10000] Avg train loss: 0.179986\n",
      "Epoch [1619/10000] Avg train loss: 0.179875\n",
      "Epoch [1620/10000] Avg train loss: 0.179765\n",
      "Epoch [1621/10000] Avg train loss: 0.179655\n",
      "Epoch [1622/10000] Avg train loss: 0.179546\n",
      "Epoch [1623/10000] Avg train loss: 0.179437\n",
      "Epoch [1624/10000] Avg train loss: 0.179327\n",
      "Epoch [1625/10000] Avg train loss: 0.179218\n",
      "Epoch [1626/10000] Avg train loss: 0.179109\n",
      "Epoch [1627/10000] Avg train loss: 0.179000\n",
      "Epoch [1628/10000] Avg train loss: 0.178891\n",
      "Epoch [1629/10000] Avg train loss: 0.178782\n",
      "Epoch [1630/10000] Avg train loss: 0.178673\n",
      "Epoch [1631/10000] Avg train loss: 0.178564\n",
      "Epoch [1632/10000] Avg train loss: 0.178456\n",
      "Epoch [1633/10000] Avg train loss: 0.178348\n",
      "Epoch [1634/10000] Avg train loss: 0.178239\n",
      "Epoch [1635/10000] Avg train loss: 0.178131\n",
      "Epoch [1636/10000] Avg train loss: 0.178023\n",
      "Epoch [1637/10000] Avg train loss: 0.177916\n",
      "Epoch [1638/10000] Avg train loss: 0.177808\n",
      "Epoch [1639/10000] Avg train loss: 0.177701\n",
      "Epoch [1640/10000] Avg train loss: 0.177593\n",
      "Epoch [1641/10000] Avg train loss: 0.177486\n",
      "Epoch [1642/10000] Avg train loss: 0.177379\n",
      "Epoch [1643/10000] Avg train loss: 0.177273\n",
      "Epoch [1644/10000] Avg train loss: 0.177166\n",
      "Epoch [1645/10000] Avg train loss: 0.177059\n",
      "Epoch [1646/10000] Avg train loss: 0.176953\n",
      "Epoch [1647/10000] Avg train loss: 0.176847\n",
      "Epoch [1648/10000] Avg train loss: 0.176741\n",
      "Epoch [1649/10000] Avg train loss: 0.176634\n",
      "Epoch [1650/10000] Avg train loss: 0.176528\n",
      "Epoch [1651/10000] Avg train loss: 0.176422\n",
      "Epoch [1652/10000] Avg train loss: 0.176316\n",
      "Epoch [1653/10000] Avg train loss: 0.176211\n",
      "Epoch [1654/10000] Avg train loss: 0.176105\n",
      "Epoch [1655/10000] Avg train loss: 0.176000\n",
      "Epoch [1656/10000] Avg train loss: 0.175895\n",
      "Epoch [1657/10000] Avg train loss: 0.175789\n",
      "Epoch [1658/10000] Avg train loss: 0.175684\n",
      "Epoch [1659/10000] Avg train loss: 0.175580\n",
      "Epoch [1660/10000] Avg train loss: 0.175475\n",
      "Epoch [1661/10000] Avg train loss: 0.175370\n",
      "Epoch [1662/10000] Avg train loss: 0.175266\n",
      "Epoch [1663/10000] Avg train loss: 0.175161\n",
      "Epoch [1664/10000] Avg train loss: 0.175057\n",
      "Epoch [1665/10000] Avg train loss: 0.174953\n",
      "Epoch [1666/10000] Avg train loss: 0.174848\n",
      "Epoch [1667/10000] Avg train loss: 0.174744\n",
      "Epoch [1668/10000] Avg train loss: 0.174640\n",
      "Epoch [1669/10000] Avg train loss: 0.174537\n",
      "Epoch [1670/10000] Avg train loss: 0.174433\n",
      "Epoch [1671/10000] Avg train loss: 0.174330\n",
      "Epoch [1672/10000] Avg train loss: 0.174226\n",
      "Epoch [1673/10000] Avg train loss: 0.174123\n",
      "Epoch [1674/10000] Avg train loss: 0.174020\n",
      "Epoch [1675/10000] Avg train loss: 0.173917\n",
      "Epoch [1676/10000] Avg train loss: 0.173814\n",
      "Epoch [1677/10000] Avg train loss: 0.173711\n",
      "Epoch [1678/10000] Avg train loss: 0.173609\n",
      "Epoch [1679/10000] Avg train loss: 0.173506\n",
      "Epoch [1680/10000] Avg train loss: 0.173404\n",
      "Epoch [1681/10000] Avg train loss: 0.173302\n",
      "Epoch [1682/10000] Avg train loss: 0.173200\n",
      "Epoch [1683/10000] Avg train loss: 0.173098\n",
      "Epoch [1684/10000] Avg train loss: 0.172996\n",
      "Epoch [1685/10000] Avg train loss: 0.172894\n",
      "Epoch [1686/10000] Avg train loss: 0.172792\n",
      "Epoch [1687/10000] Avg train loss: 0.172691\n",
      "Epoch [1688/10000] Avg train loss: 0.172589\n",
      "Epoch [1689/10000] Avg train loss: 0.172488\n",
      "Epoch [1690/10000] Avg train loss: 0.172387\n",
      "Epoch [1691/10000] Avg train loss: 0.172286\n",
      "Epoch [1692/10000] Avg train loss: 0.172185\n",
      "Epoch [1693/10000] Avg train loss: 0.172084\n",
      "Epoch [1694/10000] Avg train loss: 0.171983\n",
      "Epoch [1695/10000] Avg train loss: 0.171883\n",
      "Epoch [1696/10000] Avg train loss: 0.171782\n",
      "Epoch [1697/10000] Avg train loss: 0.171682\n",
      "Epoch [1698/10000] Avg train loss: 0.171582\n",
      "Epoch [1699/10000] Avg train loss: 0.171482\n",
      "Epoch [1700/10000] Avg train loss: 0.171382\n",
      "Epoch [1701/10000] Avg train loss: 0.171282\n",
      "Epoch [1702/10000] Avg train loss: 0.171182\n",
      "Epoch [1703/10000] Avg train loss: 0.171082\n",
      "Epoch [1704/10000] Avg train loss: 0.170982\n",
      "Epoch [1705/10000] Avg train loss: 0.170883\n",
      "Epoch [1706/10000] Avg train loss: 0.170784\n",
      "Epoch [1707/10000] Avg train loss: 0.170685\n",
      "Epoch [1708/10000] Avg train loss: 0.170586\n",
      "Epoch [1709/10000] Avg train loss: 0.170487\n",
      "Epoch [1710/10000] Avg train loss: 0.170388\n",
      "Epoch [1711/10000] Avg train loss: 0.170290\n",
      "Epoch [1712/10000] Avg train loss: 0.170191\n",
      "Epoch [1713/10000] Avg train loss: 0.170092\n",
      "Epoch [1714/10000] Avg train loss: 0.169994\n",
      "Epoch [1715/10000] Avg train loss: 0.169896\n",
      "Epoch [1716/10000] Avg train loss: 0.169798\n",
      "Epoch [1717/10000] Avg train loss: 0.169700\n",
      "Epoch [1718/10000] Avg train loss: 0.169602\n",
      "Epoch [1719/10000] Avg train loss: 0.169504\n",
      "Epoch [1720/10000] Avg train loss: 0.169406\n",
      "Epoch [1721/10000] Avg train loss: 0.169309\n",
      "Epoch [1722/10000] Avg train loss: 0.169211\n",
      "Epoch [1723/10000] Avg train loss: 0.169114\n",
      "Epoch [1724/10000] Avg train loss: 0.169016\n",
      "Epoch [1725/10000] Avg train loss: 0.168919\n",
      "Epoch [1726/10000] Avg train loss: 0.168822\n",
      "Epoch [1727/10000] Avg train loss: 0.168725\n",
      "Epoch [1728/10000] Avg train loss: 0.168628\n",
      "Epoch [1729/10000] Avg train loss: 0.168532\n",
      "Epoch [1730/10000] Avg train loss: 0.168435\n",
      "Epoch [1731/10000] Avg train loss: 0.168338\n",
      "Epoch [1732/10000] Avg train loss: 0.168242\n",
      "Epoch [1733/10000] Avg train loss: 0.168146\n",
      "Epoch [1734/10000] Avg train loss: 0.168049\n",
      "Epoch [1735/10000] Avg train loss: 0.167953\n",
      "Epoch [1736/10000] Avg train loss: 0.167858\n",
      "Epoch [1737/10000] Avg train loss: 0.167762\n",
      "Epoch [1738/10000] Avg train loss: 0.167666\n",
      "Epoch [1739/10000] Avg train loss: 0.167570\n",
      "Epoch [1740/10000] Avg train loss: 0.167475\n",
      "Epoch [1741/10000] Avg train loss: 0.167379\n",
      "Epoch [1742/10000] Avg train loss: 0.167284\n",
      "Epoch [1743/10000] Avg train loss: 0.167189\n",
      "Epoch [1744/10000] Avg train loss: 0.167094\n",
      "Epoch [1745/10000] Avg train loss: 0.166999\n",
      "Epoch [1746/10000] Avg train loss: 0.166904\n",
      "Epoch [1747/10000] Avg train loss: 0.166809\n",
      "Epoch [1748/10000] Avg train loss: 0.166714\n",
      "Epoch [1749/10000] Avg train loss: 0.166620\n",
      "Epoch [1750/10000] Avg train loss: 0.166525\n",
      "Epoch [1751/10000] Avg train loss: 0.166431\n",
      "Epoch [1752/10000] Avg train loss: 0.166337\n",
      "Epoch [1753/10000] Avg train loss: 0.166242\n",
      "Epoch [1754/10000] Avg train loss: 0.166149\n",
      "Epoch [1755/10000] Avg train loss: 0.166055\n",
      "Epoch [1756/10000] Avg train loss: 0.165961\n",
      "Epoch [1757/10000] Avg train loss: 0.165867\n",
      "Epoch [1758/10000] Avg train loss: 0.165774\n",
      "Epoch [1759/10000] Avg train loss: 0.165680\n",
      "Epoch [1760/10000] Avg train loss: 0.165587\n",
      "Epoch [1761/10000] Avg train loss: 0.165494\n",
      "Epoch [1762/10000] Avg train loss: 0.165401\n",
      "Epoch [1763/10000] Avg train loss: 0.165308\n",
      "Epoch [1764/10000] Avg train loss: 0.165215\n",
      "Epoch [1765/10000] Avg train loss: 0.165122\n",
      "Epoch [1766/10000] Avg train loss: 0.165029\n",
      "Epoch [1767/10000] Avg train loss: 0.164936\n",
      "Epoch [1768/10000] Avg train loss: 0.164844\n",
      "Epoch [1769/10000] Avg train loss: 0.164751\n",
      "Epoch [1770/10000] Avg train loss: 0.164659\n",
      "Epoch [1771/10000] Avg train loss: 0.164567\n",
      "Epoch [1772/10000] Avg train loss: 0.164475\n",
      "Epoch [1773/10000] Avg train loss: 0.164383\n",
      "Epoch [1774/10000] Avg train loss: 0.164291\n",
      "Epoch [1775/10000] Avg train loss: 0.164199\n",
      "Epoch [1776/10000] Avg train loss: 0.164108\n",
      "Epoch [1777/10000] Avg train loss: 0.164016\n",
      "Epoch [1778/10000] Avg train loss: 0.163925\n",
      "Epoch [1779/10000] Avg train loss: 0.163833\n",
      "Epoch [1780/10000] Avg train loss: 0.163742\n",
      "Epoch [1781/10000] Avg train loss: 0.163651\n",
      "Epoch [1782/10000] Avg train loss: 0.163560\n",
      "Epoch [1783/10000] Avg train loss: 0.163469\n",
      "Epoch [1784/10000] Avg train loss: 0.163378\n",
      "Epoch [1785/10000] Avg train loss: 0.163287\n",
      "Epoch [1786/10000] Avg train loss: 0.163196\n",
      "Epoch [1787/10000] Avg train loss: 0.163106\n",
      "Epoch [1788/10000] Avg train loss: 0.163015\n",
      "Epoch [1789/10000] Avg train loss: 0.162925\n",
      "Epoch [1790/10000] Avg train loss: 0.162834\n",
      "Epoch [1791/10000] Avg train loss: 0.162744\n",
      "Epoch [1792/10000] Avg train loss: 0.162654\n",
      "Epoch [1793/10000] Avg train loss: 0.162564\n",
      "Epoch [1794/10000] Avg train loss: 0.162474\n",
      "Epoch [1795/10000] Avg train loss: 0.162384\n",
      "Epoch [1796/10000] Avg train loss: 0.162295\n",
      "Epoch [1797/10000] Avg train loss: 0.162205\n",
      "Epoch [1798/10000] Avg train loss: 0.162116\n",
      "Epoch [1799/10000] Avg train loss: 0.162027\n",
      "Epoch [1800/10000] Avg train loss: 0.161937\n",
      "Epoch [1801/10000] Avg train loss: 0.161848\n",
      "Epoch [1802/10000] Avg train loss: 0.161759\n",
      "Epoch [1803/10000] Avg train loss: 0.161670\n",
      "Epoch [1804/10000] Avg train loss: 0.161581\n",
      "Epoch [1805/10000] Avg train loss: 0.161492\n",
      "Epoch [1806/10000] Avg train loss: 0.161403\n",
      "Epoch [1807/10000] Avg train loss: 0.161314\n",
      "Epoch [1808/10000] Avg train loss: 0.161226\n",
      "Epoch [1809/10000] Avg train loss: 0.161137\n",
      "Epoch [1810/10000] Avg train loss: 0.161049\n",
      "Epoch [1811/10000] Avg train loss: 0.160961\n",
      "Epoch [1812/10000] Avg train loss: 0.160873\n",
      "Epoch [1813/10000] Avg train loss: 0.160785\n",
      "Epoch [1814/10000] Avg train loss: 0.160697\n",
      "Epoch [1815/10000] Avg train loss: 0.160609\n",
      "Epoch [1816/10000] Avg train loss: 0.160521\n",
      "Epoch [1817/10000] Avg train loss: 0.160433\n",
      "Epoch [1818/10000] Avg train loss: 0.160346\n",
      "Epoch [1819/10000] Avg train loss: 0.160258\n",
      "Epoch [1820/10000] Avg train loss: 0.160171\n",
      "Epoch [1821/10000] Avg train loss: 0.160083\n",
      "Epoch [1822/10000] Avg train loss: 0.159996\n",
      "Epoch [1823/10000] Avg train loss: 0.159909\n",
      "Epoch [1824/10000] Avg train loss: 0.159822\n",
      "Epoch [1825/10000] Avg train loss: 0.159735\n",
      "Epoch [1826/10000] Avg train loss: 0.159649\n",
      "Epoch [1827/10000] Avg train loss: 0.159562\n",
      "Epoch [1828/10000] Avg train loss: 0.159475\n",
      "Epoch [1829/10000] Avg train loss: 0.159389\n",
      "Epoch [1830/10000] Avg train loss: 0.159302\n",
      "Epoch [1831/10000] Avg train loss: 0.159216\n",
      "Epoch [1832/10000] Avg train loss: 0.159130\n",
      "Epoch [1833/10000] Avg train loss: 0.159044\n",
      "Epoch [1834/10000] Avg train loss: 0.158958\n",
      "Epoch [1835/10000] Avg train loss: 0.158872\n",
      "Epoch [1836/10000] Avg train loss: 0.158786\n",
      "Epoch [1837/10000] Avg train loss: 0.158700\n",
      "Epoch [1838/10000] Avg train loss: 0.158615\n",
      "Epoch [1839/10000] Avg train loss: 0.158529\n",
      "Epoch [1840/10000] Avg train loss: 0.158444\n",
      "Epoch [1841/10000] Avg train loss: 0.158358\n",
      "Epoch [1842/10000] Avg train loss: 0.158273\n",
      "Epoch [1843/10000] Avg train loss: 0.158188\n",
      "Epoch [1844/10000] Avg train loss: 0.158102\n",
      "Epoch [1845/10000] Avg train loss: 0.158017\n",
      "Epoch [1846/10000] Avg train loss: 0.157932\n",
      "Epoch [1847/10000] Avg train loss: 0.157847\n",
      "Epoch [1848/10000] Avg train loss: 0.157763\n",
      "Epoch [1849/10000] Avg train loss: 0.157678\n",
      "Epoch [1850/10000] Avg train loss: 0.157593\n",
      "Epoch [1851/10000] Avg train loss: 0.157509\n",
      "Epoch [1852/10000] Avg train loss: 0.157425\n",
      "Epoch [1853/10000] Avg train loss: 0.157340\n",
      "Epoch [1854/10000] Avg train loss: 0.157256\n",
      "Epoch [1855/10000] Avg train loss: 0.157172\n",
      "Epoch [1856/10000] Avg train loss: 0.157088\n",
      "Epoch [1857/10000] Avg train loss: 0.157004\n",
      "Epoch [1858/10000] Avg train loss: 0.156920\n",
      "Epoch [1859/10000] Avg train loss: 0.156836\n",
      "Epoch [1860/10000] Avg train loss: 0.156752\n",
      "Epoch [1861/10000] Avg train loss: 0.156669\n",
      "Epoch [1862/10000] Avg train loss: 0.156585\n",
      "Epoch [1863/10000] Avg train loss: 0.156502\n",
      "Epoch [1864/10000] Avg train loss: 0.156419\n",
      "Epoch [1865/10000] Avg train loss: 0.156335\n",
      "Epoch [1866/10000] Avg train loss: 0.156252\n",
      "Epoch [1867/10000] Avg train loss: 0.156169\n",
      "Epoch [1868/10000] Avg train loss: 0.156086\n",
      "Epoch [1869/10000] Avg train loss: 0.156003\n",
      "Epoch [1870/10000] Avg train loss: 0.155920\n",
      "Epoch [1871/10000] Avg train loss: 0.155838\n",
      "Epoch [1872/10000] Avg train loss: 0.155755\n",
      "Epoch [1873/10000] Avg train loss: 0.155673\n",
      "Epoch [1874/10000] Avg train loss: 0.155590\n",
      "Epoch [1875/10000] Avg train loss: 0.155508\n",
      "Epoch [1876/10000] Avg train loss: 0.155426\n",
      "Epoch [1877/10000] Avg train loss: 0.155343\n",
      "Epoch [1878/10000] Avg train loss: 0.155261\n",
      "Epoch [1879/10000] Avg train loss: 0.155179\n",
      "Epoch [1880/10000] Avg train loss: 0.155098\n",
      "Epoch [1881/10000] Avg train loss: 0.155016\n",
      "Epoch [1882/10000] Avg train loss: 0.154934\n",
      "Epoch [1883/10000] Avg train loss: 0.154852\n",
      "Epoch [1884/10000] Avg train loss: 0.154771\n",
      "Epoch [1885/10000] Avg train loss: 0.154689\n",
      "Epoch [1886/10000] Avg train loss: 0.154608\n",
      "Epoch [1887/10000] Avg train loss: 0.154527\n",
      "Epoch [1888/10000] Avg train loss: 0.154446\n",
      "Epoch [1889/10000] Avg train loss: 0.154364\n",
      "Epoch [1890/10000] Avg train loss: 0.154283\n",
      "Epoch [1891/10000] Avg train loss: 0.154202\n",
      "Epoch [1892/10000] Avg train loss: 0.154122\n",
      "Epoch [1893/10000] Avg train loss: 0.154041\n",
      "Epoch [1894/10000] Avg train loss: 0.153960\n",
      "Epoch [1895/10000] Avg train loss: 0.153879\n",
      "Epoch [1896/10000] Avg train loss: 0.153799\n",
      "Epoch [1897/10000] Avg train loss: 0.153718\n",
      "Epoch [1898/10000] Avg train loss: 0.153638\n",
      "Epoch [1899/10000] Avg train loss: 0.153558\n",
      "Epoch [1900/10000] Avg train loss: 0.153478\n",
      "Epoch [1901/10000] Avg train loss: 0.153397\n",
      "Epoch [1902/10000] Avg train loss: 0.153317\n",
      "Epoch [1903/10000] Avg train loss: 0.153237\n",
      "Epoch [1904/10000] Avg train loss: 0.153158\n",
      "Epoch [1905/10000] Avg train loss: 0.153078\n",
      "Epoch [1906/10000] Avg train loss: 0.152998\n",
      "Epoch [1907/10000] Avg train loss: 0.152918\n",
      "Epoch [1908/10000] Avg train loss: 0.152839\n",
      "Epoch [1909/10000] Avg train loss: 0.152759\n",
      "Epoch [1910/10000] Avg train loss: 0.152680\n",
      "Epoch [1911/10000] Avg train loss: 0.152600\n",
      "Epoch [1912/10000] Avg train loss: 0.152521\n",
      "Epoch [1913/10000] Avg train loss: 0.152442\n",
      "Epoch [1914/10000] Avg train loss: 0.152363\n",
      "Epoch [1915/10000] Avg train loss: 0.152284\n",
      "Epoch [1916/10000] Avg train loss: 0.152205\n",
      "Epoch [1917/10000] Avg train loss: 0.152126\n",
      "Epoch [1918/10000] Avg train loss: 0.152047\n",
      "Epoch [1919/10000] Avg train loss: 0.151969\n",
      "Epoch [1920/10000] Avg train loss: 0.151890\n",
      "Epoch [1921/10000] Avg train loss: 0.151812\n",
      "Epoch [1922/10000] Avg train loss: 0.151733\n",
      "Epoch [1923/10000] Avg train loss: 0.151655\n",
      "Epoch [1924/10000] Avg train loss: 0.151577\n",
      "Epoch [1925/10000] Avg train loss: 0.151498\n",
      "Epoch [1926/10000] Avg train loss: 0.151420\n",
      "Epoch [1927/10000] Avg train loss: 0.151342\n",
      "Epoch [1928/10000] Avg train loss: 0.151264\n",
      "Epoch [1929/10000] Avg train loss: 0.151186\n",
      "Epoch [1930/10000] Avg train loss: 0.151109\n",
      "Epoch [1931/10000] Avg train loss: 0.151031\n",
      "Epoch [1932/10000] Avg train loss: 0.150953\n",
      "Epoch [1933/10000] Avg train loss: 0.150876\n",
      "Epoch [1934/10000] Avg train loss: 0.150798\n",
      "Epoch [1935/10000] Avg train loss: 0.150721\n",
      "Epoch [1936/10000] Avg train loss: 0.150644\n",
      "Epoch [1937/10000] Avg train loss: 0.150567\n",
      "Epoch [1938/10000] Avg train loss: 0.150489\n",
      "Epoch [1939/10000] Avg train loss: 0.150412\n",
      "Epoch [1940/10000] Avg train loss: 0.150335\n",
      "Epoch [1941/10000] Avg train loss: 0.150258\n",
      "Epoch [1942/10000] Avg train loss: 0.150181\n",
      "Epoch [1943/10000] Avg train loss: 0.150105\n",
      "Epoch [1944/10000] Avg train loss: 0.150028\n",
      "Epoch [1945/10000] Avg train loss: 0.149951\n",
      "Epoch [1946/10000] Avg train loss: 0.149875\n",
      "Epoch [1947/10000] Avg train loss: 0.149798\n",
      "Epoch [1948/10000] Avg train loss: 0.149722\n",
      "Epoch [1949/10000] Avg train loss: 0.149646\n",
      "Epoch [1950/10000] Avg train loss: 0.149569\n",
      "Epoch [1951/10000] Avg train loss: 0.149493\n",
      "Epoch [1952/10000] Avg train loss: 0.149417\n",
      "Epoch [1953/10000] Avg train loss: 0.149341\n",
      "Epoch [1954/10000] Avg train loss: 0.149265\n",
      "Epoch [1955/10000] Avg train loss: 0.149190\n",
      "Epoch [1956/10000] Avg train loss: 0.149114\n",
      "Epoch [1957/10000] Avg train loss: 0.149038\n",
      "Epoch [1958/10000] Avg train loss: 0.148963\n",
      "Epoch [1959/10000] Avg train loss: 0.148887\n",
      "Epoch [1960/10000] Avg train loss: 0.148812\n",
      "Epoch [1961/10000] Avg train loss: 0.148737\n",
      "Epoch [1962/10000] Avg train loss: 0.148661\n",
      "Epoch [1963/10000] Avg train loss: 0.148586\n",
      "Epoch [1964/10000] Avg train loss: 0.148511\n",
      "Epoch [1965/10000] Avg train loss: 0.148436\n",
      "Epoch [1966/10000] Avg train loss: 0.148361\n",
      "Epoch [1967/10000] Avg train loss: 0.148286\n",
      "Epoch [1968/10000] Avg train loss: 0.148211\n",
      "Epoch [1969/10000] Avg train loss: 0.148136\n",
      "Epoch [1970/10000] Avg train loss: 0.148061\n",
      "Epoch [1971/10000] Avg train loss: 0.147987\n",
      "Epoch [1972/10000] Avg train loss: 0.147912\n",
      "Epoch [1973/10000] Avg train loss: 0.147838\n",
      "Epoch [1974/10000] Avg train loss: 0.147763\n",
      "Epoch [1975/10000] Avg train loss: 0.147689\n",
      "Epoch [1976/10000] Avg train loss: 0.147615\n",
      "Epoch [1977/10000] Avg train loss: 0.147541\n",
      "Epoch [1978/10000] Avg train loss: 0.147467\n",
      "Epoch [1979/10000] Avg train loss: 0.147393\n",
      "Epoch [1980/10000] Avg train loss: 0.147319\n",
      "Epoch [1981/10000] Avg train loss: 0.147245\n",
      "Epoch [1982/10000] Avg train loss: 0.147171\n",
      "Epoch [1983/10000] Avg train loss: 0.147097\n",
      "Epoch [1984/10000] Avg train loss: 0.147024\n",
      "Epoch [1985/10000] Avg train loss: 0.146950\n",
      "Epoch [1986/10000] Avg train loss: 0.146877\n",
      "Epoch [1987/10000] Avg train loss: 0.146803\n",
      "Epoch [1988/10000] Avg train loss: 0.146730\n",
      "Epoch [1989/10000] Avg train loss: 0.146657\n",
      "Epoch [1990/10000] Avg train loss: 0.146583\n",
      "Epoch [1991/10000] Avg train loss: 0.146510\n",
      "Epoch [1992/10000] Avg train loss: 0.146437\n",
      "Epoch [1993/10000] Avg train loss: 0.146364\n",
      "Epoch [1994/10000] Avg train loss: 0.146291\n",
      "Epoch [1995/10000] Avg train loss: 0.146218\n",
      "Epoch [1996/10000] Avg train loss: 0.146146\n",
      "Epoch [1997/10000] Avg train loss: 0.146073\n",
      "Epoch [1998/10000] Avg train loss: 0.146000\n",
      "Epoch [1999/10000] Avg train loss: 0.145928\n",
      "Epoch [2000/10000] Avg train loss: 0.145855\n",
      "Epoch [2001/10000] Avg train loss: 0.145783\n",
      "Epoch [2002/10000] Avg train loss: 0.145710\n",
      "Epoch [2003/10000] Avg train loss: 0.145638\n",
      "Epoch [2004/10000] Avg train loss: 0.145566\n",
      "Epoch [2005/10000] Avg train loss: 0.145494\n",
      "Epoch [2006/10000] Avg train loss: 0.145422\n",
      "Epoch [2007/10000] Avg train loss: 0.145350\n",
      "Epoch [2008/10000] Avg train loss: 0.145278\n",
      "Epoch [2009/10000] Avg train loss: 0.145206\n",
      "Epoch [2010/10000] Avg train loss: 0.145134\n",
      "Epoch [2011/10000] Avg train loss: 0.145063\n",
      "Epoch [2012/10000] Avg train loss: 0.144991\n",
      "Epoch [2013/10000] Avg train loss: 0.144919\n",
      "Epoch [2014/10000] Avg train loss: 0.144848\n",
      "Epoch [2015/10000] Avg train loss: 0.144776\n",
      "Epoch [2016/10000] Avg train loss: 0.144705\n",
      "Epoch [2017/10000] Avg train loss: 0.144634\n",
      "Epoch [2018/10000] Avg train loss: 0.144563\n",
      "Epoch [2019/10000] Avg train loss: 0.144492\n",
      "Epoch [2020/10000] Avg train loss: 0.144421\n",
      "Epoch [2021/10000] Avg train loss: 0.144350\n",
      "Epoch [2022/10000] Avg train loss: 0.144279\n",
      "Epoch [2023/10000] Avg train loss: 0.144208\n",
      "Epoch [2024/10000] Avg train loss: 0.144137\n",
      "Epoch [2025/10000] Avg train loss: 0.144066\n",
      "Epoch [2026/10000] Avg train loss: 0.143996\n",
      "Epoch [2027/10000] Avg train loss: 0.143925\n",
      "Epoch [2028/10000] Avg train loss: 0.143855\n",
      "Epoch [2029/10000] Avg train loss: 0.143784\n",
      "Epoch [2030/10000] Avg train loss: 0.143714\n",
      "Epoch [2031/10000] Avg train loss: 0.143643\n",
      "Epoch [2032/10000] Avg train loss: 0.143573\n",
      "Epoch [2033/10000] Avg train loss: 0.143503\n",
      "Epoch [2034/10000] Avg train loss: 0.143433\n",
      "Epoch [2035/10000] Avg train loss: 0.143363\n",
      "Epoch [2036/10000] Avg train loss: 0.143293\n",
      "Epoch [2037/10000] Avg train loss: 0.143223\n",
      "Epoch [2038/10000] Avg train loss: 0.143153\n",
      "Epoch [2039/10000] Avg train loss: 0.143084\n",
      "Epoch [2040/10000] Avg train loss: 0.143014\n",
      "Epoch [2041/10000] Avg train loss: 0.142944\n",
      "Epoch [2042/10000] Avg train loss: 0.142875\n",
      "Epoch [2043/10000] Avg train loss: 0.142805\n",
      "Epoch [2044/10000] Avg train loss: 0.142736\n",
      "Epoch [2045/10000] Avg train loss: 0.142666\n",
      "Epoch [2046/10000] Avg train loss: 0.142597\n",
      "Epoch [2047/10000] Avg train loss: 0.142528\n",
      "Epoch [2048/10000] Avg train loss: 0.142459\n",
      "Epoch [2049/10000] Avg train loss: 0.142390\n",
      "Epoch [2050/10000] Avg train loss: 0.142320\n",
      "Epoch [2051/10000] Avg train loss: 0.142252\n",
      "Epoch [2052/10000] Avg train loss: 0.142183\n",
      "Epoch [2053/10000] Avg train loss: 0.142114\n",
      "Epoch [2054/10000] Avg train loss: 0.142045\n",
      "Epoch [2055/10000] Avg train loss: 0.141976\n",
      "Epoch [2056/10000] Avg train loss: 0.141908\n",
      "Epoch [2057/10000] Avg train loss: 0.141839\n",
      "Epoch [2058/10000] Avg train loss: 0.141771\n",
      "Epoch [2059/10000] Avg train loss: 0.141702\n",
      "Epoch [2060/10000] Avg train loss: 0.141634\n",
      "Epoch [2061/10000] Avg train loss: 0.141566\n",
      "Epoch [2062/10000] Avg train loss: 0.141498\n",
      "Epoch [2063/10000] Avg train loss: 0.141430\n",
      "Epoch [2064/10000] Avg train loss: 0.141362\n",
      "Epoch [2065/10000] Avg train loss: 0.141294\n",
      "Epoch [2066/10000] Avg train loss: 0.141226\n",
      "Epoch [2067/10000] Avg train loss: 0.141158\n",
      "Epoch [2068/10000] Avg train loss: 0.141090\n",
      "Epoch [2069/10000] Avg train loss: 0.141022\n",
      "Epoch [2070/10000] Avg train loss: 0.140955\n",
      "Epoch [2071/10000] Avg train loss: 0.140887\n",
      "Epoch [2072/10000] Avg train loss: 0.140819\n",
      "Epoch [2073/10000] Avg train loss: 0.140752\n",
      "Epoch [2074/10000] Avg train loss: 0.140684\n",
      "Epoch [2075/10000] Avg train loss: 0.140617\n",
      "Epoch [2076/10000] Avg train loss: 0.140550\n",
      "Epoch [2077/10000] Avg train loss: 0.140483\n",
      "Epoch [2078/10000] Avg train loss: 0.140415\n",
      "Epoch [2079/10000] Avg train loss: 0.140348\n",
      "Epoch [2080/10000] Avg train loss: 0.140281\n",
      "Epoch [2081/10000] Avg train loss: 0.140214\n",
      "Epoch [2082/10000] Avg train loss: 0.140147\n",
      "Epoch [2083/10000] Avg train loss: 0.140081\n",
      "Epoch [2084/10000] Avg train loss: 0.140014\n",
      "Epoch [2085/10000] Avg train loss: 0.139947\n",
      "Epoch [2086/10000] Avg train loss: 0.139880\n",
      "Epoch [2087/10000] Avg train loss: 0.139814\n",
      "Epoch [2088/10000] Avg train loss: 0.139747\n",
      "Epoch [2089/10000] Avg train loss: 0.139681\n",
      "Epoch [2090/10000] Avg train loss: 0.139614\n",
      "Epoch [2091/10000] Avg train loss: 0.139548\n",
      "Epoch [2092/10000] Avg train loss: 0.139482\n",
      "Epoch [2093/10000] Avg train loss: 0.139416\n",
      "Epoch [2094/10000] Avg train loss: 0.139350\n",
      "Epoch [2095/10000] Avg train loss: 0.139283\n",
      "Epoch [2096/10000] Avg train loss: 0.139218\n",
      "Epoch [2097/10000] Avg train loss: 0.139152\n",
      "Epoch [2098/10000] Avg train loss: 0.139086\n",
      "Epoch [2099/10000] Avg train loss: 0.139020\n",
      "Epoch [2100/10000] Avg train loss: 0.138954\n",
      "Epoch [2101/10000] Avg train loss: 0.138888\n",
      "Epoch [2102/10000] Avg train loss: 0.138823\n",
      "Epoch [2103/10000] Avg train loss: 0.138757\n",
      "Epoch [2104/10000] Avg train loss: 0.138692\n",
      "Epoch [2105/10000] Avg train loss: 0.138626\n",
      "Epoch [2106/10000] Avg train loss: 0.138561\n",
      "Epoch [2107/10000] Avg train loss: 0.138495\n",
      "Epoch [2108/10000] Avg train loss: 0.138430\n",
      "Epoch [2109/10000] Avg train loss: 0.138365\n",
      "Epoch [2110/10000] Avg train loss: 0.138300\n",
      "Epoch [2111/10000] Avg train loss: 0.138235\n",
      "Epoch [2112/10000] Avg train loss: 0.138170\n",
      "Epoch [2113/10000] Avg train loss: 0.138105\n",
      "Epoch [2114/10000] Avg train loss: 0.138040\n",
      "Epoch [2115/10000] Avg train loss: 0.137975\n",
      "Epoch [2116/10000] Avg train loss: 0.137910\n",
      "Epoch [2117/10000] Avg train loss: 0.137846\n",
      "Epoch [2118/10000] Avg train loss: 0.137781\n",
      "Epoch [2119/10000] Avg train loss: 0.137716\n",
      "Epoch [2120/10000] Avg train loss: 0.137652\n",
      "Epoch [2121/10000] Avg train loss: 0.137587\n",
      "Epoch [2122/10000] Avg train loss: 0.137523\n",
      "Epoch [2123/10000] Avg train loss: 0.137458\n",
      "Epoch [2124/10000] Avg train loss: 0.137394\n",
      "Epoch [2125/10000] Avg train loss: 0.137330\n",
      "Epoch [2126/10000] Avg train loss: 0.137265\n",
      "Epoch [2127/10000] Avg train loss: 0.137201\n",
      "Epoch [2128/10000] Avg train loss: 0.137137\n",
      "Epoch [2129/10000] Avg train loss: 0.137073\n",
      "Epoch [2130/10000] Avg train loss: 0.137009\n",
      "Epoch [2131/10000] Avg train loss: 0.136946\n",
      "Epoch [2132/10000] Avg train loss: 0.136882\n",
      "Epoch [2133/10000] Avg train loss: 0.136818\n",
      "Epoch [2134/10000] Avg train loss: 0.136754\n",
      "Epoch [2135/10000] Avg train loss: 0.136690\n",
      "Epoch [2136/10000] Avg train loss: 0.136627\n",
      "Epoch [2137/10000] Avg train loss: 0.136563\n",
      "Epoch [2138/10000] Avg train loss: 0.136500\n",
      "Epoch [2139/10000] Avg train loss: 0.136436\n",
      "Epoch [2140/10000] Avg train loss: 0.136373\n",
      "Epoch [2141/10000] Avg train loss: 0.136310\n",
      "Epoch [2142/10000] Avg train loss: 0.136246\n",
      "Epoch [2143/10000] Avg train loss: 0.136183\n",
      "Epoch [2144/10000] Avg train loss: 0.136120\n",
      "Epoch [2145/10000] Avg train loss: 0.136057\n",
      "Epoch [2146/10000] Avg train loss: 0.135994\n",
      "Epoch [2147/10000] Avg train loss: 0.135931\n",
      "Epoch [2148/10000] Avg train loss: 0.135868\n",
      "Epoch [2149/10000] Avg train loss: 0.135805\n",
      "Epoch [2150/10000] Avg train loss: 0.135743\n",
      "Epoch [2151/10000] Avg train loss: 0.135680\n",
      "Epoch [2152/10000] Avg train loss: 0.135617\n",
      "Epoch [2153/10000] Avg train loss: 0.135555\n",
      "Epoch [2154/10000] Avg train loss: 0.135492\n",
      "Epoch [2155/10000] Avg train loss: 0.135430\n",
      "Epoch [2156/10000] Avg train loss: 0.135367\n",
      "Epoch [2157/10000] Avg train loss: 0.135305\n",
      "Epoch [2158/10000] Avg train loss: 0.135243\n",
      "Epoch [2159/10000] Avg train loss: 0.135180\n",
      "Epoch [2160/10000] Avg train loss: 0.135118\n",
      "Epoch [2161/10000] Avg train loss: 0.135056\n",
      "Epoch [2162/10000] Avg train loss: 0.134994\n",
      "Epoch [2163/10000] Avg train loss: 0.134932\n",
      "Epoch [2164/10000] Avg train loss: 0.134870\n",
      "Epoch [2165/10000] Avg train loss: 0.134808\n",
      "Epoch [2166/10000] Avg train loss: 0.134746\n",
      "Epoch [2167/10000] Avg train loss: 0.134684\n",
      "Epoch [2168/10000] Avg train loss: 0.134622\n",
      "Epoch [2169/10000] Avg train loss: 0.134561\n",
      "Epoch [2170/10000] Avg train loss: 0.134499\n",
      "Epoch [2171/10000] Avg train loss: 0.134437\n",
      "Epoch [2172/10000] Avg train loss: 0.134376\n",
      "Epoch [2173/10000] Avg train loss: 0.134314\n",
      "Epoch [2174/10000] Avg train loss: 0.134253\n",
      "Epoch [2175/10000] Avg train loss: 0.134192\n",
      "Epoch [2176/10000] Avg train loss: 0.134130\n",
      "Epoch [2177/10000] Avg train loss: 0.134069\n",
      "Epoch [2178/10000] Avg train loss: 0.134008\n",
      "Epoch [2179/10000] Avg train loss: 0.133947\n",
      "Epoch [2180/10000] Avg train loss: 0.133886\n",
      "Epoch [2181/10000] Avg train loss: 0.133825\n",
      "Epoch [2182/10000] Avg train loss: 0.133764\n",
      "Epoch [2183/10000] Avg train loss: 0.133703\n",
      "Epoch [2184/10000] Avg train loss: 0.133642\n",
      "Epoch [2185/10000] Avg train loss: 0.133581\n",
      "Epoch [2186/10000] Avg train loss: 0.133521\n",
      "Epoch [2187/10000] Avg train loss: 0.133460\n",
      "Epoch [2188/10000] Avg train loss: 0.133399\n",
      "Epoch [2189/10000] Avg train loss: 0.133339\n",
      "Epoch [2190/10000] Avg train loss: 0.133278\n",
      "Epoch [2191/10000] Avg train loss: 0.133218\n",
      "Epoch [2192/10000] Avg train loss: 0.133157\n",
      "Epoch [2193/10000] Avg train loss: 0.133097\n",
      "Epoch [2194/10000] Avg train loss: 0.133037\n",
      "Epoch [2195/10000] Avg train loss: 0.132976\n",
      "Epoch [2196/10000] Avg train loss: 0.132916\n",
      "Epoch [2197/10000] Avg train loss: 0.132856\n",
      "Epoch [2198/10000] Avg train loss: 0.132796\n",
      "Epoch [2199/10000] Avg train loss: 0.132736\n",
      "Epoch [2200/10000] Avg train loss: 0.132676\n",
      "Epoch [2201/10000] Avg train loss: 0.132616\n",
      "Epoch [2202/10000] Avg train loss: 0.132556\n",
      "Epoch [2203/10000] Avg train loss: 0.132496\n",
      "Epoch [2204/10000] Avg train loss: 0.132437\n",
      "Epoch [2205/10000] Avg train loss: 0.132377\n",
      "Epoch [2206/10000] Avg train loss: 0.132317\n",
      "Epoch [2207/10000] Avg train loss: 0.132258\n",
      "Epoch [2208/10000] Avg train loss: 0.132198\n",
      "Epoch [2209/10000] Avg train loss: 0.132139\n",
      "Epoch [2210/10000] Avg train loss: 0.132080\n",
      "Epoch [2211/10000] Avg train loss: 0.132020\n",
      "Epoch [2212/10000] Avg train loss: 0.131961\n",
      "Epoch [2213/10000] Avg train loss: 0.131902\n",
      "Epoch [2214/10000] Avg train loss: 0.131842\n",
      "Epoch [2215/10000] Avg train loss: 0.131783\n",
      "Epoch [2216/10000] Avg train loss: 0.131724\n",
      "Epoch [2217/10000] Avg train loss: 0.131665\n",
      "Epoch [2218/10000] Avg train loss: 0.131606\n",
      "Epoch [2219/10000] Avg train loss: 0.131547\n",
      "Epoch [2220/10000] Avg train loss: 0.131488\n",
      "Epoch [2221/10000] Avg train loss: 0.131429\n",
      "Epoch [2222/10000] Avg train loss: 0.131371\n",
      "Epoch [2223/10000] Avg train loss: 0.131312\n",
      "Epoch [2224/10000] Avg train loss: 0.131253\n",
      "Epoch [2225/10000] Avg train loss: 0.131195\n",
      "Epoch [2226/10000] Avg train loss: 0.131136\n",
      "Epoch [2227/10000] Avg train loss: 0.131077\n",
      "Epoch [2228/10000] Avg train loss: 0.131019\n",
      "Epoch [2229/10000] Avg train loss: 0.130960\n",
      "Epoch [2230/10000] Avg train loss: 0.130902\n",
      "Epoch [2231/10000] Avg train loss: 0.130844\n",
      "Epoch [2232/10000] Avg train loss: 0.130785\n",
      "Epoch [2233/10000] Avg train loss: 0.130727\n",
      "Epoch [2234/10000] Avg train loss: 0.130669\n",
      "Epoch [2235/10000] Avg train loss: 0.130611\n",
      "Epoch [2236/10000] Avg train loss: 0.130553\n",
      "Epoch [2237/10000] Avg train loss: 0.130495\n",
      "Epoch [2238/10000] Avg train loss: 0.130437\n",
      "Epoch [2239/10000] Avg train loss: 0.130379\n",
      "Epoch [2240/10000] Avg train loss: 0.130321\n",
      "Epoch [2241/10000] Avg train loss: 0.130263\n",
      "Epoch [2242/10000] Avg train loss: 0.130205\n",
      "Epoch [2243/10000] Avg train loss: 0.130148\n",
      "Epoch [2244/10000] Avg train loss: 0.130090\n",
      "Epoch [2245/10000] Avg train loss: 0.130032\n",
      "Epoch [2246/10000] Avg train loss: 0.129975\n",
      "Epoch [2247/10000] Avg train loss: 0.129917\n",
      "Epoch [2248/10000] Avg train loss: 0.129860\n",
      "Epoch [2249/10000] Avg train loss: 0.129802\n",
      "Epoch [2250/10000] Avg train loss: 0.129745\n",
      "Epoch [2251/10000] Avg train loss: 0.129688\n",
      "Epoch [2252/10000] Avg train loss: 0.129631\n",
      "Epoch [2253/10000] Avg train loss: 0.129573\n",
      "Epoch [2254/10000] Avg train loss: 0.129516\n",
      "Epoch [2255/10000] Avg train loss: 0.129459\n",
      "Epoch [2256/10000] Avg train loss: 0.129402\n",
      "Epoch [2257/10000] Avg train loss: 0.129345\n",
      "Epoch [2258/10000] Avg train loss: 0.129288\n",
      "Epoch [2259/10000] Avg train loss: 0.129231\n",
      "Epoch [2260/10000] Avg train loss: 0.129174\n",
      "Epoch [2261/10000] Avg train loss: 0.129118\n",
      "Epoch [2262/10000] Avg train loss: 0.129061\n",
      "Epoch [2263/10000] Avg train loss: 0.129004\n",
      "Epoch [2264/10000] Avg train loss: 0.128948\n",
      "Epoch [2265/10000] Avg train loss: 0.128891\n",
      "Epoch [2266/10000] Avg train loss: 0.128834\n",
      "Epoch [2267/10000] Avg train loss: 0.128778\n",
      "Epoch [2268/10000] Avg train loss: 0.128722\n",
      "Epoch [2269/10000] Avg train loss: 0.128665\n",
      "Epoch [2270/10000] Avg train loss: 0.128609\n",
      "Epoch [2271/10000] Avg train loss: 0.128552\n",
      "Epoch [2272/10000] Avg train loss: 0.128496\n",
      "Epoch [2273/10000] Avg train loss: 0.128440\n",
      "Epoch [2274/10000] Avg train loss: 0.128384\n",
      "Epoch [2275/10000] Avg train loss: 0.128328\n",
      "Epoch [2276/10000] Avg train loss: 0.128272\n",
      "Epoch [2277/10000] Avg train loss: 0.128216\n",
      "Epoch [2278/10000] Avg train loss: 0.128160\n",
      "Epoch [2279/10000] Avg train loss: 0.128104\n",
      "Epoch [2280/10000] Avg train loss: 0.128048\n",
      "Epoch [2281/10000] Avg train loss: 0.127992\n",
      "Epoch [2282/10000] Avg train loss: 0.127936\n",
      "Epoch [2283/10000] Avg train loss: 0.127881\n",
      "Epoch [2284/10000] Avg train loss: 0.127825\n",
      "Epoch [2285/10000] Avg train loss: 0.127769\n",
      "Epoch [2286/10000] Avg train loss: 0.127714\n",
      "Epoch [2287/10000] Avg train loss: 0.127658\n",
      "Epoch [2288/10000] Avg train loss: 0.127603\n",
      "Epoch [2289/10000] Avg train loss: 0.127548\n",
      "Epoch [2290/10000] Avg train loss: 0.127492\n",
      "Epoch [2291/10000] Avg train loss: 0.127437\n",
      "Epoch [2292/10000] Avg train loss: 0.127382\n",
      "Epoch [2293/10000] Avg train loss: 0.127326\n",
      "Epoch [2294/10000] Avg train loss: 0.127271\n",
      "Epoch [2295/10000] Avg train loss: 0.127216\n",
      "Epoch [2296/10000] Avg train loss: 0.127161\n",
      "Epoch [2297/10000] Avg train loss: 0.127106\n",
      "Epoch [2298/10000] Avg train loss: 0.127051\n",
      "Epoch [2299/10000] Avg train loss: 0.126996\n",
      "Epoch [2300/10000] Avg train loss: 0.126941\n",
      "Epoch [2301/10000] Avg train loss: 0.126886\n",
      "Epoch [2302/10000] Avg train loss: 0.126831\n",
      "Epoch [2303/10000] Avg train loss: 0.126776\n",
      "Epoch [2304/10000] Avg train loss: 0.126722\n",
      "Epoch [2305/10000] Avg train loss: 0.126667\n",
      "Epoch [2306/10000] Avg train loss: 0.126612\n",
      "Epoch [2307/10000] Avg train loss: 0.126558\n",
      "Epoch [2308/10000] Avg train loss: 0.126503\n",
      "Epoch [2309/10000] Avg train loss: 0.126449\n",
      "Epoch [2310/10000] Avg train loss: 0.126394\n",
      "Epoch [2311/10000] Avg train loss: 0.126340\n",
      "Epoch [2312/10000] Avg train loss: 0.126286\n",
      "Epoch [2313/10000] Avg train loss: 0.126231\n",
      "Epoch [2314/10000] Avg train loss: 0.126177\n",
      "Epoch [2315/10000] Avg train loss: 0.126123\n",
      "Epoch [2316/10000] Avg train loss: 0.126069\n",
      "Epoch [2317/10000] Avg train loss: 0.126015\n",
      "Epoch [2318/10000] Avg train loss: 0.125961\n",
      "Epoch [2319/10000] Avg train loss: 0.125907\n",
      "Epoch [2320/10000] Avg train loss: 0.125853\n",
      "Epoch [2321/10000] Avg train loss: 0.125799\n",
      "Epoch [2322/10000] Avg train loss: 0.125745\n",
      "Epoch [2323/10000] Avg train loss: 0.125691\n",
      "Epoch [2324/10000] Avg train loss: 0.125637\n",
      "Epoch [2325/10000] Avg train loss: 0.125584\n",
      "Epoch [2326/10000] Avg train loss: 0.125530\n",
      "Epoch [2327/10000] Avg train loss: 0.125476\n",
      "Epoch [2328/10000] Avg train loss: 0.125423\n",
      "Epoch [2329/10000] Avg train loss: 0.125369\n",
      "Epoch [2330/10000] Avg train loss: 0.125316\n",
      "Epoch [2331/10000] Avg train loss: 0.125262\n",
      "Epoch [2332/10000] Avg train loss: 0.125209\n",
      "Epoch [2333/10000] Avg train loss: 0.125155\n",
      "Epoch [2334/10000] Avg train loss: 0.125102\n",
      "Epoch [2335/10000] Avg train loss: 0.125049\n",
      "Epoch [2336/10000] Avg train loss: 0.124996\n",
      "Epoch [2337/10000] Avg train loss: 0.124942\n",
      "Epoch [2338/10000] Avg train loss: 0.124889\n",
      "Epoch [2339/10000] Avg train loss: 0.124836\n",
      "Epoch [2340/10000] Avg train loss: 0.124783\n",
      "Epoch [2341/10000] Avg train loss: 0.124730\n",
      "Epoch [2342/10000] Avg train loss: 0.124677\n",
      "Epoch [2343/10000] Avg train loss: 0.124624\n",
      "Epoch [2344/10000] Avg train loss: 0.124571\n",
      "Epoch [2345/10000] Avg train loss: 0.124519\n",
      "Epoch [2346/10000] Avg train loss: 0.124466\n",
      "Epoch [2347/10000] Avg train loss: 0.124413\n",
      "Epoch [2348/10000] Avg train loss: 0.124361\n",
      "Epoch [2349/10000] Avg train loss: 0.124308\n",
      "Epoch [2350/10000] Avg train loss: 0.124255\n",
      "Epoch [2351/10000] Avg train loss: 0.124203\n",
      "Epoch [2352/10000] Avg train loss: 0.124150\n",
      "Epoch [2353/10000] Avg train loss: 0.124098\n",
      "Epoch [2354/10000] Avg train loss: 0.124045\n",
      "Epoch [2355/10000] Avg train loss: 0.123993\n",
      "Epoch [2356/10000] Avg train loss: 0.123941\n",
      "Epoch [2357/10000] Avg train loss: 0.123888\n",
      "Epoch [2358/10000] Avg train loss: 0.123836\n",
      "Epoch [2359/10000] Avg train loss: 0.123784\n",
      "Epoch [2360/10000] Avg train loss: 0.123732\n",
      "Epoch [2361/10000] Avg train loss: 0.123680\n",
      "Epoch [2362/10000] Avg train loss: 0.123628\n",
      "Epoch [2363/10000] Avg train loss: 0.123576\n",
      "Epoch [2364/10000] Avg train loss: 0.123524\n",
      "Epoch [2365/10000] Avg train loss: 0.123472\n",
      "Epoch [2366/10000] Avg train loss: 0.123420\n",
      "Epoch [2367/10000] Avg train loss: 0.123368\n",
      "Epoch [2368/10000] Avg train loss: 0.123316\n",
      "Epoch [2369/10000] Avg train loss: 0.123264\n",
      "Epoch [2370/10000] Avg train loss: 0.123212\n",
      "Epoch [2371/10000] Avg train loss: 0.123161\n",
      "Epoch [2372/10000] Avg train loss: 0.123109\n",
      "Epoch [2373/10000] Avg train loss: 0.123058\n",
      "Epoch [2374/10000] Avg train loss: 0.123006\n",
      "Epoch [2375/10000] Avg train loss: 0.122955\n",
      "Epoch [2376/10000] Avg train loss: 0.122903\n",
      "Epoch [2377/10000] Avg train loss: 0.122852\n",
      "Epoch [2378/10000] Avg train loss: 0.122800\n",
      "Epoch [2379/10000] Avg train loss: 0.122749\n",
      "Epoch [2380/10000] Avg train loss: 0.122698\n",
      "Epoch [2381/10000] Avg train loss: 0.122646\n",
      "Epoch [2382/10000] Avg train loss: 0.122595\n",
      "Epoch [2383/10000] Avg train loss: 0.122544\n",
      "Epoch [2384/10000] Avg train loss: 0.122493\n",
      "Epoch [2385/10000] Avg train loss: 0.122442\n",
      "Epoch [2386/10000] Avg train loss: 0.122391\n",
      "Epoch [2387/10000] Avg train loss: 0.122340\n",
      "Epoch [2388/10000] Avg train loss: 0.122289\n",
      "Epoch [2389/10000] Avg train loss: 0.122238\n",
      "Epoch [2390/10000] Avg train loss: 0.122187\n",
      "Epoch [2391/10000] Avg train loss: 0.122136\n",
      "Epoch [2392/10000] Avg train loss: 0.122085\n",
      "Epoch [2393/10000] Avg train loss: 0.122035\n",
      "Epoch [2394/10000] Avg train loss: 0.121984\n",
      "Epoch [2395/10000] Avg train loss: 0.121933\n",
      "Epoch [2396/10000] Avg train loss: 0.121883\n",
      "Epoch [2397/10000] Avg train loss: 0.121832\n",
      "Epoch [2398/10000] Avg train loss: 0.121782\n",
      "Epoch [2399/10000] Avg train loss: 0.121731\n",
      "Epoch [2400/10000] Avg train loss: 0.121681\n",
      "Epoch [2401/10000] Avg train loss: 0.121630\n",
      "Epoch [2402/10000] Avg train loss: 0.121580\n",
      "Epoch [2403/10000] Avg train loss: 0.121530\n",
      "Epoch [2404/10000] Avg train loss: 0.121479\n",
      "Epoch [2405/10000] Avg train loss: 0.121429\n",
      "Epoch [2406/10000] Avg train loss: 0.121379\n",
      "Epoch [2407/10000] Avg train loss: 0.121329\n",
      "Epoch [2408/10000] Avg train loss: 0.121279\n",
      "Epoch [2409/10000] Avg train loss: 0.121229\n",
      "Epoch [2410/10000] Avg train loss: 0.121179\n",
      "Epoch [2411/10000] Avg train loss: 0.121129\n",
      "Epoch [2412/10000] Avg train loss: 0.121079\n",
      "Epoch [2413/10000] Avg train loss: 0.121029\n",
      "Epoch [2414/10000] Avg train loss: 0.120979\n",
      "Epoch [2415/10000] Avg train loss: 0.120929\n",
      "Epoch [2416/10000] Avg train loss: 0.120879\n",
      "Epoch [2417/10000] Avg train loss: 0.120829\n",
      "Epoch [2418/10000] Avg train loss: 0.120780\n",
      "Epoch [2419/10000] Avg train loss: 0.120730\n",
      "Epoch [2420/10000] Avg train loss: 0.120680\n",
      "Epoch [2421/10000] Avg train loss: 0.120631\n",
      "Epoch [2422/10000] Avg train loss: 0.120581\n",
      "Epoch [2423/10000] Avg train loss: 0.120532\n",
      "Epoch [2424/10000] Avg train loss: 0.120482\n",
      "Epoch [2425/10000] Avg train loss: 0.120433\n",
      "Epoch [2426/10000] Avg train loss: 0.120384\n",
      "Epoch [2427/10000] Avg train loss: 0.120334\n",
      "Epoch [2428/10000] Avg train loss: 0.120285\n",
      "Epoch [2429/10000] Avg train loss: 0.120236\n",
      "Epoch [2430/10000] Avg train loss: 0.120187\n",
      "Epoch [2431/10000] Avg train loss: 0.120137\n",
      "Epoch [2432/10000] Avg train loss: 0.120088\n",
      "Epoch [2433/10000] Avg train loss: 0.120039\n",
      "Epoch [2434/10000] Avg train loss: 0.119990\n",
      "Epoch [2435/10000] Avg train loss: 0.119941\n",
      "Epoch [2436/10000] Avg train loss: 0.119892\n",
      "Epoch [2437/10000] Avg train loss: 0.119843\n",
      "Epoch [2438/10000] Avg train loss: 0.119794\n",
      "Epoch [2439/10000] Avg train loss: 0.119746\n",
      "Epoch [2440/10000] Avg train loss: 0.119697\n",
      "Epoch [2441/10000] Avg train loss: 0.119648\n",
      "Epoch [2442/10000] Avg train loss: 0.119599\n",
      "Epoch [2443/10000] Avg train loss: 0.119551\n",
      "Epoch [2444/10000] Avg train loss: 0.119502\n",
      "Epoch [2445/10000] Avg train loss: 0.119453\n",
      "Epoch [2446/10000] Avg train loss: 0.119405\n",
      "Epoch [2447/10000] Avg train loss: 0.119356\n",
      "Epoch [2448/10000] Avg train loss: 0.119308\n",
      "Epoch [2449/10000] Avg train loss: 0.119260\n",
      "Epoch [2450/10000] Avg train loss: 0.119211\n",
      "Epoch [2451/10000] Avg train loss: 0.119163\n",
      "Epoch [2452/10000] Avg train loss: 0.119114\n",
      "Epoch [2453/10000] Avg train loss: 0.119066\n",
      "Epoch [2454/10000] Avg train loss: 0.119018\n",
      "Epoch [2455/10000] Avg train loss: 0.118970\n",
      "Epoch [2456/10000] Avg train loss: 0.118921\n",
      "Epoch [2457/10000] Avg train loss: 0.118873\n",
      "Epoch [2458/10000] Avg train loss: 0.118825\n",
      "Epoch [2459/10000] Avg train loss: 0.118777\n",
      "Epoch [2460/10000] Avg train loss: 0.118729\n",
      "Epoch [2461/10000] Avg train loss: 0.118681\n",
      "Epoch [2462/10000] Avg train loss: 0.118633\n",
      "Epoch [2463/10000] Avg train loss: 0.118585\n",
      "Epoch [2464/10000] Avg train loss: 0.118537\n",
      "Epoch [2465/10000] Avg train loss: 0.118489\n",
      "Epoch [2466/10000] Avg train loss: 0.118442\n",
      "Epoch [2467/10000] Avg train loss: 0.118394\n",
      "Epoch [2468/10000] Avg train loss: 0.118346\n",
      "Epoch [2469/10000] Avg train loss: 0.118298\n",
      "Epoch [2470/10000] Avg train loss: 0.118251\n",
      "Epoch [2471/10000] Avg train loss: 0.118203\n",
      "Epoch [2472/10000] Avg train loss: 0.118156\n",
      "Epoch [2473/10000] Avg train loss: 0.118108\n",
      "Epoch [2474/10000] Avg train loss: 0.118061\n",
      "Epoch [2475/10000] Avg train loss: 0.118013\n",
      "Epoch [2476/10000] Avg train loss: 0.117966\n",
      "Epoch [2477/10000] Avg train loss: 0.117918\n",
      "Epoch [2478/10000] Avg train loss: 0.117871\n",
      "Epoch [2479/10000] Avg train loss: 0.117824\n",
      "Epoch [2480/10000] Avg train loss: 0.117777\n",
      "Epoch [2481/10000] Avg train loss: 0.117729\n",
      "Epoch [2482/10000] Avg train loss: 0.117682\n",
      "Epoch [2483/10000] Avg train loss: 0.117635\n",
      "Epoch [2484/10000] Avg train loss: 0.117588\n",
      "Epoch [2485/10000] Avg train loss: 0.117541\n",
      "Epoch [2486/10000] Avg train loss: 0.117494\n",
      "Epoch [2487/10000] Avg train loss: 0.117447\n",
      "Epoch [2488/10000] Avg train loss: 0.117400\n",
      "Epoch [2489/10000] Avg train loss: 0.117353\n",
      "Epoch [2490/10000] Avg train loss: 0.117306\n",
      "Epoch [2491/10000] Avg train loss: 0.117259\n",
      "Epoch [2492/10000] Avg train loss: 0.117212\n",
      "Epoch [2493/10000] Avg train loss: 0.117166\n",
      "Epoch [2494/10000] Avg train loss: 0.117119\n",
      "Epoch [2495/10000] Avg train loss: 0.117072\n",
      "Epoch [2496/10000] Avg train loss: 0.117025\n",
      "Epoch [2497/10000] Avg train loss: 0.116979\n",
      "Epoch [2498/10000] Avg train loss: 0.116932\n",
      "Epoch [2499/10000] Avg train loss: 0.116886\n",
      "Epoch [2500/10000] Avg train loss: 0.116839\n",
      "Epoch [2501/10000] Avg train loss: 0.116793\n",
      "Epoch [2502/10000] Avg train loss: 0.116746\n",
      "Epoch [2503/10000] Avg train loss: 0.116700\n",
      "Epoch [2504/10000] Avg train loss: 0.116653\n",
      "Epoch [2505/10000] Avg train loss: 0.116607\n",
      "Epoch [2506/10000] Avg train loss: 0.116561\n",
      "Epoch [2507/10000] Avg train loss: 0.116515\n",
      "Epoch [2508/10000] Avg train loss: 0.116468\n",
      "Epoch [2509/10000] Avg train loss: 0.116422\n",
      "Epoch [2510/10000] Avg train loss: 0.116376\n",
      "Epoch [2511/10000] Avg train loss: 0.116330\n",
      "Epoch [2512/10000] Avg train loss: 0.116284\n",
      "Epoch [2513/10000] Avg train loss: 0.116238\n",
      "Epoch [2514/10000] Avg train loss: 0.116192\n",
      "Epoch [2515/10000] Avg train loss: 0.116146\n",
      "Epoch [2516/10000] Avg train loss: 0.116100\n",
      "Epoch [2517/10000] Avg train loss: 0.116054\n",
      "Epoch [2518/10000] Avg train loss: 0.116008\n",
      "Epoch [2519/10000] Avg train loss: 0.115962\n",
      "Epoch [2520/10000] Avg train loss: 0.115917\n",
      "Epoch [2521/10000] Avg train loss: 0.115871\n",
      "Epoch [2522/10000] Avg train loss: 0.115825\n",
      "Epoch [2523/10000] Avg train loss: 0.115780\n",
      "Epoch [2524/10000] Avg train loss: 0.115734\n",
      "Epoch [2525/10000] Avg train loss: 0.115688\n",
      "Epoch [2526/10000] Avg train loss: 0.115643\n",
      "Epoch [2527/10000] Avg train loss: 0.115597\n",
      "Epoch [2528/10000] Avg train loss: 0.115552\n",
      "Epoch [2529/10000] Avg train loss: 0.115507\n",
      "Epoch [2530/10000] Avg train loss: 0.115461\n",
      "Epoch [2531/10000] Avg train loss: 0.115416\n",
      "Epoch [2532/10000] Avg train loss: 0.115370\n",
      "Epoch [2533/10000] Avg train loss: 0.115325\n",
      "Epoch [2534/10000] Avg train loss: 0.115280\n",
      "Epoch [2535/10000] Avg train loss: 0.115235\n",
      "Epoch [2536/10000] Avg train loss: 0.115189\n",
      "Epoch [2537/10000] Avg train loss: 0.115144\n",
      "Epoch [2538/10000] Avg train loss: 0.115099\n",
      "Epoch [2539/10000] Avg train loss: 0.115054\n",
      "Epoch [2540/10000] Avg train loss: 0.115009\n",
      "Epoch [2541/10000] Avg train loss: 0.114964\n",
      "Epoch [2542/10000] Avg train loss: 0.114919\n",
      "Epoch [2543/10000] Avg train loss: 0.114874\n",
      "Epoch [2544/10000] Avg train loss: 0.114829\n",
      "Epoch [2545/10000] Avg train loss: 0.114784\n",
      "Epoch [2546/10000] Avg train loss: 0.114739\n",
      "Epoch [2547/10000] Avg train loss: 0.114694\n",
      "Epoch [2548/10000] Avg train loss: 0.114649\n",
      "Epoch [2549/10000] Avg train loss: 0.114605\n",
      "Epoch [2550/10000] Avg train loss: 0.114560\n",
      "Epoch [2551/10000] Avg train loss: 0.114515\n",
      "Epoch [2552/10000] Avg train loss: 0.114471\n",
      "Epoch [2553/10000] Avg train loss: 0.114426\n",
      "Epoch [2554/10000] Avg train loss: 0.114381\n",
      "Epoch [2555/10000] Avg train loss: 0.114337\n",
      "Epoch [2556/10000] Avg train loss: 0.114292\n",
      "Epoch [2557/10000] Avg train loss: 0.114248\n",
      "Epoch [2558/10000] Avg train loss: 0.114203\n",
      "Epoch [2559/10000] Avg train loss: 0.114159\n",
      "Epoch [2560/10000] Avg train loss: 0.114115\n",
      "Epoch [2561/10000] Avg train loss: 0.114070\n",
      "Epoch [2562/10000] Avg train loss: 0.114026\n",
      "Epoch [2563/10000] Avg train loss: 0.113982\n",
      "Epoch [2564/10000] Avg train loss: 0.113937\n",
      "Epoch [2565/10000] Avg train loss: 0.113893\n",
      "Epoch [2566/10000] Avg train loss: 0.113849\n",
      "Epoch [2567/10000] Avg train loss: 0.113805\n",
      "Epoch [2568/10000] Avg train loss: 0.113761\n",
      "Epoch [2569/10000] Avg train loss: 0.113717\n",
      "Epoch [2570/10000] Avg train loss: 0.113673\n",
      "Epoch [2571/10000] Avg train loss: 0.113629\n",
      "Epoch [2572/10000] Avg train loss: 0.113585\n",
      "Epoch [2573/10000] Avg train loss: 0.113541\n",
      "Epoch [2574/10000] Avg train loss: 0.113497\n",
      "Epoch [2575/10000] Avg train loss: 0.113453\n",
      "Epoch [2576/10000] Avg train loss: 0.113409\n",
      "Epoch [2577/10000] Avg train loss: 0.113366\n",
      "Epoch [2578/10000] Avg train loss: 0.113322\n",
      "Epoch [2579/10000] Avg train loss: 0.113278\n",
      "Epoch [2580/10000] Avg train loss: 0.113235\n",
      "Epoch [2581/10000] Avg train loss: 0.113191\n",
      "Epoch [2582/10000] Avg train loss: 0.113147\n",
      "Epoch [2583/10000] Avg train loss: 0.113104\n",
      "Epoch [2584/10000] Avg train loss: 0.113060\n",
      "Epoch [2585/10000] Avg train loss: 0.113017\n",
      "Epoch [2586/10000] Avg train loss: 0.112973\n",
      "Epoch [2587/10000] Avg train loss: 0.112930\n",
      "Epoch [2588/10000] Avg train loss: 0.112886\n",
      "Epoch [2589/10000] Avg train loss: 0.112843\n",
      "Epoch [2590/10000] Avg train loss: 0.112800\n",
      "Epoch [2591/10000] Avg train loss: 0.112756\n",
      "Epoch [2592/10000] Avg train loss: 0.112713\n",
      "Epoch [2593/10000] Avg train loss: 0.112670\n",
      "Epoch [2594/10000] Avg train loss: 0.112627\n",
      "Epoch [2595/10000] Avg train loss: 0.112583\n",
      "Epoch [2596/10000] Avg train loss: 0.112540\n",
      "Epoch [2597/10000] Avg train loss: 0.112497\n",
      "Epoch [2598/10000] Avg train loss: 0.112454\n",
      "Epoch [2599/10000] Avg train loss: 0.112411\n",
      "Epoch [2600/10000] Avg train loss: 0.112368\n",
      "Epoch [2601/10000] Avg train loss: 0.112325\n",
      "Epoch [2602/10000] Avg train loss: 0.112282\n",
      "Epoch [2603/10000] Avg train loss: 0.112239\n",
      "Epoch [2604/10000] Avg train loss: 0.112196\n",
      "Epoch [2605/10000] Avg train loss: 0.112153\n",
      "Epoch [2606/10000] Avg train loss: 0.112111\n",
      "Epoch [2607/10000] Avg train loss: 0.112068\n",
      "Epoch [2608/10000] Avg train loss: 0.112025\n",
      "Epoch [2609/10000] Avg train loss: 0.111982\n",
      "Epoch [2610/10000] Avg train loss: 0.111940\n",
      "Epoch [2611/10000] Avg train loss: 0.111897\n",
      "Epoch [2612/10000] Avg train loss: 0.111854\n",
      "Epoch [2613/10000] Avg train loss: 0.111812\n",
      "Epoch [2614/10000] Avg train loss: 0.111769\n",
      "Epoch [2615/10000] Avg train loss: 0.111727\n",
      "Epoch [2616/10000] Avg train loss: 0.111684\n",
      "Epoch [2617/10000] Avg train loss: 0.111642\n",
      "Epoch [2618/10000] Avg train loss: 0.111599\n",
      "Epoch [2619/10000] Avg train loss: 0.111557\n",
      "Epoch [2620/10000] Avg train loss: 0.111515\n",
      "Epoch [2621/10000] Avg train loss: 0.111472\n",
      "Epoch [2622/10000] Avg train loss: 0.111430\n",
      "Epoch [2623/10000] Avg train loss: 0.111388\n",
      "Epoch [2624/10000] Avg train loss: 0.111345\n",
      "Epoch [2625/10000] Avg train loss: 0.111303\n",
      "Epoch [2626/10000] Avg train loss: 0.111261\n",
      "Epoch [2627/10000] Avg train loss: 0.111219\n",
      "Epoch [2628/10000] Avg train loss: 0.111177\n",
      "Epoch [2629/10000] Avg train loss: 0.111135\n",
      "Epoch [2630/10000] Avg train loss: 0.111093\n",
      "Epoch [2631/10000] Avg train loss: 0.111051\n",
      "Epoch [2632/10000] Avg train loss: 0.111009\n",
      "Epoch [2633/10000] Avg train loss: 0.110967\n",
      "Epoch [2634/10000] Avg train loss: 0.110925\n",
      "Epoch [2635/10000] Avg train loss: 0.110883\n",
      "Epoch [2636/10000] Avg train loss: 0.110841\n",
      "Epoch [2637/10000] Avg train loss: 0.110799\n",
      "Epoch [2638/10000] Avg train loss: 0.110758\n",
      "Epoch [2639/10000] Avg train loss: 0.110716\n",
      "Epoch [2640/10000] Avg train loss: 0.110674\n",
      "Epoch [2641/10000] Avg train loss: 0.110632\n",
      "Epoch [2642/10000] Avg train loss: 0.110591\n",
      "Epoch [2643/10000] Avg train loss: 0.110549\n",
      "Epoch [2644/10000] Avg train loss: 0.110508\n",
      "Epoch [2645/10000] Avg train loss: 0.110466\n",
      "Epoch [2646/10000] Avg train loss: 0.110424\n",
      "Epoch [2647/10000] Avg train loss: 0.110383\n",
      "Epoch [2648/10000] Avg train loss: 0.110341\n",
      "Epoch [2649/10000] Avg train loss: 0.110300\n",
      "Epoch [2650/10000] Avg train loss: 0.110259\n",
      "Epoch [2651/10000] Avg train loss: 0.110217\n",
      "Epoch [2652/10000] Avg train loss: 0.110176\n",
      "Epoch [2653/10000] Avg train loss: 0.110134\n",
      "Epoch [2654/10000] Avg train loss: 0.110093\n",
      "Epoch [2655/10000] Avg train loss: 0.110052\n",
      "Epoch [2656/10000] Avg train loss: 0.110011\n",
      "Epoch [2657/10000] Avg train loss: 0.109969\n",
      "Epoch [2658/10000] Avg train loss: 0.109928\n",
      "Epoch [2659/10000] Avg train loss: 0.109887\n",
      "Epoch [2660/10000] Avg train loss: 0.109846\n",
      "Epoch [2661/10000] Avg train loss: 0.109805\n",
      "Epoch [2662/10000] Avg train loss: 0.109764\n",
      "Epoch [2663/10000] Avg train loss: 0.109723\n",
      "Epoch [2664/10000] Avg train loss: 0.109682\n",
      "Epoch [2665/10000] Avg train loss: 0.109641\n",
      "Epoch [2666/10000] Avg train loss: 0.109600\n",
      "Epoch [2667/10000] Avg train loss: 0.109559\n",
      "Epoch [2668/10000] Avg train loss: 0.109518\n",
      "Epoch [2669/10000] Avg train loss: 0.109477\n",
      "Epoch [2670/10000] Avg train loss: 0.109437\n",
      "Epoch [2671/10000] Avg train loss: 0.109396\n",
      "Epoch [2672/10000] Avg train loss: 0.109355\n",
      "Epoch [2673/10000] Avg train loss: 0.109314\n",
      "Epoch [2674/10000] Avg train loss: 0.109274\n",
      "Epoch [2675/10000] Avg train loss: 0.109233\n",
      "Epoch [2676/10000] Avg train loss: 0.109192\n",
      "Epoch [2677/10000] Avg train loss: 0.109152\n",
      "Epoch [2678/10000] Avg train loss: 0.109111\n",
      "Epoch [2679/10000] Avg train loss: 0.109071\n",
      "Epoch [2680/10000] Avg train loss: 0.109030\n",
      "Epoch [2681/10000] Avg train loss: 0.108990\n",
      "Epoch [2682/10000] Avg train loss: 0.108949\n",
      "Epoch [2683/10000] Avg train loss: 0.108909\n",
      "Epoch [2684/10000] Avg train loss: 0.108869\n",
      "Epoch [2685/10000] Avg train loss: 0.108828\n",
      "Epoch [2686/10000] Avg train loss: 0.108788\n",
      "Epoch [2687/10000] Avg train loss: 0.108748\n",
      "Epoch [2688/10000] Avg train loss: 0.108707\n",
      "Epoch [2689/10000] Avg train loss: 0.108667\n",
      "Epoch [2690/10000] Avg train loss: 0.108627\n",
      "Epoch [2691/10000] Avg train loss: 0.108587\n",
      "Epoch [2692/10000] Avg train loss: 0.108547\n",
      "Epoch [2693/10000] Avg train loss: 0.108507\n",
      "Epoch [2694/10000] Avg train loss: 0.108466\n",
      "Epoch [2695/10000] Avg train loss: 0.108426\n",
      "Epoch [2696/10000] Avg train loss: 0.108386\n",
      "Epoch [2697/10000] Avg train loss: 0.108346\n",
      "Epoch [2698/10000] Avg train loss: 0.108306\n",
      "Epoch [2699/10000] Avg train loss: 0.108267\n",
      "Epoch [2700/10000] Avg train loss: 0.108227\n",
      "Epoch [2701/10000] Avg train loss: 0.108187\n",
      "Epoch [2702/10000] Avg train loss: 0.108147\n",
      "Epoch [2703/10000] Avg train loss: 0.108107\n",
      "Epoch [2704/10000] Avg train loss: 0.108067\n",
      "Epoch [2705/10000] Avg train loss: 0.108028\n",
      "Epoch [2706/10000] Avg train loss: 0.107988\n",
      "Epoch [2707/10000] Avg train loss: 0.107948\n",
      "Epoch [2708/10000] Avg train loss: 0.107908\n",
      "Epoch [2709/10000] Avg train loss: 0.107869\n",
      "Epoch [2710/10000] Avg train loss: 0.107829\n",
      "Epoch [2711/10000] Avg train loss: 0.107790\n",
      "Epoch [2712/10000] Avg train loss: 0.107750\n",
      "Epoch [2713/10000] Avg train loss: 0.107710\n",
      "Epoch [2714/10000] Avg train loss: 0.107671\n",
      "Epoch [2715/10000] Avg train loss: 0.107631\n",
      "Epoch [2716/10000] Avg train loss: 0.107592\n",
      "Epoch [2717/10000] Avg train loss: 0.107553\n",
      "Epoch [2718/10000] Avg train loss: 0.107513\n",
      "Epoch [2719/10000] Avg train loss: 0.107474\n",
      "Epoch [2720/10000] Avg train loss: 0.107435\n",
      "Epoch [2721/10000] Avg train loss: 0.107395\n",
      "Epoch [2722/10000] Avg train loss: 0.107356\n",
      "Epoch [2723/10000] Avg train loss: 0.107317\n",
      "Epoch [2724/10000] Avg train loss: 0.107278\n",
      "Epoch [2725/10000] Avg train loss: 0.107238\n",
      "Epoch [2726/10000] Avg train loss: 0.107199\n",
      "Epoch [2727/10000] Avg train loss: 0.107160\n",
      "Epoch [2728/10000] Avg train loss: 0.107121\n",
      "Epoch [2729/10000] Avg train loss: 0.107082\n",
      "Epoch [2730/10000] Avg train loss: 0.107043\n",
      "Epoch [2731/10000] Avg train loss: 0.107004\n",
      "Epoch [2732/10000] Avg train loss: 0.106965\n",
      "Epoch [2733/10000] Avg train loss: 0.106926\n",
      "Epoch [2734/10000] Avg train loss: 0.106887\n",
      "Epoch [2735/10000] Avg train loss: 0.106848\n",
      "Epoch [2736/10000] Avg train loss: 0.106809\n",
      "Epoch [2737/10000] Avg train loss: 0.106770\n",
      "Epoch [2738/10000] Avg train loss: 0.106732\n",
      "Epoch [2739/10000] Avg train loss: 0.106693\n",
      "Epoch [2740/10000] Avg train loss: 0.106654\n",
      "Epoch [2741/10000] Avg train loss: 0.106615\n",
      "Epoch [2742/10000] Avg train loss: 0.106577\n",
      "Epoch [2743/10000] Avg train loss: 0.106538\n",
      "Epoch [2744/10000] Avg train loss: 0.106499\n",
      "Epoch [2745/10000] Avg train loss: 0.106461\n",
      "Epoch [2746/10000] Avg train loss: 0.106422\n",
      "Epoch [2747/10000] Avg train loss: 0.106384\n",
      "Epoch [2748/10000] Avg train loss: 0.106345\n",
      "Epoch [2749/10000] Avg train loss: 0.106307\n",
      "Epoch [2750/10000] Avg train loss: 0.106268\n",
      "Epoch [2751/10000] Avg train loss: 0.106230\n",
      "Epoch [2752/10000] Avg train loss: 0.106191\n",
      "Epoch [2753/10000] Avg train loss: 0.106153\n",
      "Epoch [2754/10000] Avg train loss: 0.106114\n",
      "Epoch [2755/10000] Avg train loss: 0.106076\n",
      "Epoch [2756/10000] Avg train loss: 0.106038\n",
      "Epoch [2757/10000] Avg train loss: 0.105999\n",
      "Epoch [2758/10000] Avg train loss: 0.105961\n",
      "Epoch [2759/10000] Avg train loss: 0.105923\n",
      "Epoch [2760/10000] Avg train loss: 0.105885\n",
      "Epoch [2761/10000] Avg train loss: 0.105847\n",
      "Epoch [2762/10000] Avg train loss: 0.105809\n",
      "Epoch [2763/10000] Avg train loss: 0.105770\n",
      "Epoch [2764/10000] Avg train loss: 0.105732\n",
      "Epoch [2765/10000] Avg train loss: 0.105694\n",
      "Epoch [2766/10000] Avg train loss: 0.105656\n",
      "Epoch [2767/10000] Avg train loss: 0.105618\n",
      "Epoch [2768/10000] Avg train loss: 0.105580\n",
      "Epoch [2769/10000] Avg train loss: 0.105542\n",
      "Epoch [2770/10000] Avg train loss: 0.105504\n",
      "Epoch [2771/10000] Avg train loss: 0.105466\n",
      "Epoch [2772/10000] Avg train loss: 0.105429\n",
      "Epoch [2773/10000] Avg train loss: 0.105391\n",
      "Epoch [2774/10000] Avg train loss: 0.105353\n",
      "Epoch [2775/10000] Avg train loss: 0.105315\n",
      "Epoch [2776/10000] Avg train loss: 0.105277\n",
      "Epoch [2777/10000] Avg train loss: 0.105240\n",
      "Epoch [2778/10000] Avg train loss: 0.105202\n",
      "Epoch [2779/10000] Avg train loss: 0.105164\n",
      "Epoch [2780/10000] Avg train loss: 0.105127\n",
      "Epoch [2781/10000] Avg train loss: 0.105089\n",
      "Epoch [2782/10000] Avg train loss: 0.105051\n",
      "Epoch [2783/10000] Avg train loss: 0.105014\n",
      "Epoch [2784/10000] Avg train loss: 0.104976\n",
      "Epoch [2785/10000] Avg train loss: 0.104939\n",
      "Epoch [2786/10000] Avg train loss: 0.104901\n",
      "Epoch [2787/10000] Avg train loss: 0.104864\n",
      "Epoch [2788/10000] Avg train loss: 0.104826\n",
      "Epoch [2789/10000] Avg train loss: 0.104789\n",
      "Epoch [2790/10000] Avg train loss: 0.104752\n",
      "Epoch [2791/10000] Avg train loss: 0.104714\n",
      "Epoch [2792/10000] Avg train loss: 0.104677\n",
      "Epoch [2793/10000] Avg train loss: 0.104640\n",
      "Epoch [2794/10000] Avg train loss: 0.104602\n",
      "Epoch [2795/10000] Avg train loss: 0.104565\n",
      "Epoch [2796/10000] Avg train loss: 0.104528\n",
      "Epoch [2797/10000] Avg train loss: 0.104491\n",
      "Epoch [2798/10000] Avg train loss: 0.104453\n",
      "Epoch [2799/10000] Avg train loss: 0.104416\n",
      "Epoch [2800/10000] Avg train loss: 0.104379\n",
      "Epoch [2801/10000] Avg train loss: 0.104342\n",
      "Epoch [2802/10000] Avg train loss: 0.104305\n",
      "Epoch [2803/10000] Avg train loss: 0.104268\n",
      "Epoch [2804/10000] Avg train loss: 0.104231\n",
      "Epoch [2805/10000] Avg train loss: 0.104194\n",
      "Epoch [2806/10000] Avg train loss: 0.104157\n",
      "Epoch [2807/10000] Avg train loss: 0.104120\n",
      "Epoch [2808/10000] Avg train loss: 0.104083\n",
      "Epoch [2809/10000] Avg train loss: 0.104047\n",
      "Epoch [2810/10000] Avg train loss: 0.104010\n",
      "Epoch [2811/10000] Avg train loss: 0.103973\n",
      "Epoch [2812/10000] Avg train loss: 0.103936\n",
      "Epoch [2813/10000] Avg train loss: 0.103899\n",
      "Epoch [2814/10000] Avg train loss: 0.103862\n",
      "Epoch [2815/10000] Avg train loss: 0.103826\n",
      "Epoch [2816/10000] Avg train loss: 0.103789\n",
      "Epoch [2817/10000] Avg train loss: 0.103752\n",
      "Epoch [2818/10000] Avg train loss: 0.103716\n",
      "Epoch [2819/10000] Avg train loss: 0.103679\n",
      "Epoch [2820/10000] Avg train loss: 0.103643\n",
      "Epoch [2821/10000] Avg train loss: 0.103606\n",
      "Epoch [2822/10000] Avg train loss: 0.103570\n",
      "Epoch [2823/10000] Avg train loss: 0.103533\n",
      "Epoch [2824/10000] Avg train loss: 0.103497\n",
      "Epoch [2825/10000] Avg train loss: 0.103460\n",
      "Epoch [2826/10000] Avg train loss: 0.103424\n",
      "Epoch [2827/10000] Avg train loss: 0.103387\n",
      "Epoch [2828/10000] Avg train loss: 0.103351\n",
      "Epoch [2829/10000] Avg train loss: 0.103314\n",
      "Epoch [2830/10000] Avg train loss: 0.103278\n",
      "Epoch [2831/10000] Avg train loss: 0.103242\n",
      "Epoch [2832/10000] Avg train loss: 0.103206\n",
      "Epoch [2833/10000] Avg train loss: 0.103169\n",
      "Epoch [2834/10000] Avg train loss: 0.103133\n",
      "Epoch [2835/10000] Avg train loss: 0.103097\n",
      "Epoch [2836/10000] Avg train loss: 0.103061\n",
      "Epoch [2837/10000] Avg train loss: 0.103025\n",
      "Epoch [2838/10000] Avg train loss: 0.102988\n",
      "Epoch [2839/10000] Avg train loss: 0.102952\n",
      "Epoch [2840/10000] Avg train loss: 0.102916\n",
      "Epoch [2841/10000] Avg train loss: 0.102880\n",
      "Epoch [2842/10000] Avg train loss: 0.102844\n",
      "Epoch [2843/10000] Avg train loss: 0.102808\n",
      "Epoch [2844/10000] Avg train loss: 0.102772\n",
      "Epoch [2845/10000] Avg train loss: 0.102736\n",
      "Epoch [2846/10000] Avg train loss: 0.102700\n",
      "Epoch [2847/10000] Avg train loss: 0.102664\n",
      "Epoch [2848/10000] Avg train loss: 0.102629\n",
      "Epoch [2849/10000] Avg train loss: 0.102593\n",
      "Epoch [2850/10000] Avg train loss: 0.102557\n",
      "Epoch [2851/10000] Avg train loss: 0.102521\n",
      "Epoch [2852/10000] Avg train loss: 0.102485\n",
      "Epoch [2853/10000] Avg train loss: 0.102450\n",
      "Epoch [2854/10000] Avg train loss: 0.102414\n",
      "Epoch [2855/10000] Avg train loss: 0.102378\n",
      "Epoch [2856/10000] Avg train loss: 0.102342\n",
      "Epoch [2857/10000] Avg train loss: 0.102307\n",
      "Epoch [2858/10000] Avg train loss: 0.102271\n",
      "Epoch [2859/10000] Avg train loss: 0.102236\n",
      "Epoch [2860/10000] Avg train loss: 0.102200\n",
      "Epoch [2861/10000] Avg train loss: 0.102164\n",
      "Epoch [2862/10000] Avg train loss: 0.102129\n",
      "Epoch [2863/10000] Avg train loss: 0.102093\n",
      "Epoch [2864/10000] Avg train loss: 0.102058\n",
      "Epoch [2865/10000] Avg train loss: 0.102022\n",
      "Epoch [2866/10000] Avg train loss: 0.101987\n",
      "Epoch [2867/10000] Avg train loss: 0.101952\n",
      "Epoch [2868/10000] Avg train loss: 0.101916\n",
      "Epoch [2869/10000] Avg train loss: 0.101881\n",
      "Epoch [2870/10000] Avg train loss: 0.101846\n",
      "Epoch [2871/10000] Avg train loss: 0.101810\n",
      "Epoch [2872/10000] Avg train loss: 0.101775\n",
      "Epoch [2873/10000] Avg train loss: 0.101740\n",
      "Epoch [2874/10000] Avg train loss: 0.101704\n",
      "Epoch [2875/10000] Avg train loss: 0.101669\n",
      "Epoch [2876/10000] Avg train loss: 0.101634\n",
      "Epoch [2877/10000] Avg train loss: 0.101599\n",
      "Epoch [2878/10000] Avg train loss: 0.101564\n",
      "Epoch [2879/10000] Avg train loss: 0.101529\n",
      "Epoch [2880/10000] Avg train loss: 0.101494\n",
      "Epoch [2881/10000] Avg train loss: 0.101458\n",
      "Epoch [2882/10000] Avg train loss: 0.101423\n",
      "Epoch [2883/10000] Avg train loss: 0.101388\n",
      "Epoch [2884/10000] Avg train loss: 0.101353\n",
      "Epoch [2885/10000] Avg train loss: 0.101318\n",
      "Epoch [2886/10000] Avg train loss: 0.101284\n",
      "Epoch [2887/10000] Avg train loss: 0.101249\n",
      "Epoch [2888/10000] Avg train loss: 0.101214\n",
      "Epoch [2889/10000] Avg train loss: 0.101179\n",
      "Epoch [2890/10000] Avg train loss: 0.101144\n",
      "Epoch [2891/10000] Avg train loss: 0.101109\n",
      "Epoch [2892/10000] Avg train loss: 0.101074\n",
      "Epoch [2893/10000] Avg train loss: 0.101040\n",
      "Epoch [2894/10000] Avg train loss: 0.101005\n",
      "Epoch [2895/10000] Avg train loss: 0.100970\n",
      "Epoch [2896/10000] Avg train loss: 0.100935\n",
      "Epoch [2897/10000] Avg train loss: 0.100901\n",
      "Epoch [2898/10000] Avg train loss: 0.100866\n",
      "Epoch [2899/10000] Avg train loss: 0.100831\n",
      "Epoch [2900/10000] Avg train loss: 0.100797\n",
      "Epoch [2901/10000] Avg train loss: 0.100762\n",
      "Epoch [2902/10000] Avg train loss: 0.100728\n",
      "Epoch [2903/10000] Avg train loss: 0.100693\n",
      "Epoch [2904/10000] Avg train loss: 0.100659\n",
      "Epoch [2905/10000] Avg train loss: 0.100624\n",
      "Epoch [2906/10000] Avg train loss: 0.100590\n",
      "Epoch [2907/10000] Avg train loss: 0.100555\n",
      "Epoch [2908/10000] Avg train loss: 0.100521\n",
      "Epoch [2909/10000] Avg train loss: 0.100486\n",
      "Epoch [2910/10000] Avg train loss: 0.100452\n",
      "Epoch [2911/10000] Avg train loss: 0.100418\n",
      "Epoch [2912/10000] Avg train loss: 0.100383\n",
      "Epoch [2913/10000] Avg train loss: 0.100349\n",
      "Epoch [2914/10000] Avg train loss: 0.100315\n",
      "Epoch [2915/10000] Avg train loss: 0.100281\n",
      "Epoch [2916/10000] Avg train loss: 0.100246\n",
      "Epoch [2917/10000] Avg train loss: 0.100212\n",
      "Epoch [2918/10000] Avg train loss: 0.100178\n",
      "Epoch [2919/10000] Avg train loss: 0.100144\n",
      "Epoch [2920/10000] Avg train loss: 0.100110\n",
      "Epoch [2921/10000] Avg train loss: 0.100076\n",
      "Epoch [2922/10000] Avg train loss: 0.100042\n",
      "Epoch [2923/10000] Avg train loss: 0.100007\n",
      "Epoch [2924/10000] Avg train loss: 0.099973\n",
      "Epoch [2925/10000] Avg train loss: 0.099939\n",
      "Epoch [2926/10000] Avg train loss: 0.099905\n",
      "Epoch [2927/10000] Avg train loss: 0.099872\n",
      "Epoch [2928/10000] Avg train loss: 0.099838\n",
      "Epoch [2929/10000] Avg train loss: 0.099804\n",
      "Epoch [2930/10000] Avg train loss: 0.099770\n",
      "Epoch [2931/10000] Avg train loss: 0.099736\n",
      "Epoch [2932/10000] Avg train loss: 0.099702\n",
      "Epoch [2933/10000] Avg train loss: 0.099668\n",
      "Epoch [2934/10000] Avg train loss: 0.099634\n",
      "Epoch [2935/10000] Avg train loss: 0.099600\n",
      "Epoch [2936/10000] Avg train loss: 0.099567\n",
      "Epoch [2937/10000] Avg train loss: 0.099533\n",
      "Epoch [2938/10000] Avg train loss: 0.099499\n",
      "Epoch [2939/10000] Avg train loss: 0.099466\n",
      "Epoch [2940/10000] Avg train loss: 0.099432\n",
      "Epoch [2941/10000] Avg train loss: 0.099398\n",
      "Epoch [2942/10000] Avg train loss: 0.099365\n",
      "Epoch [2943/10000] Avg train loss: 0.099331\n",
      "Epoch [2944/10000] Avg train loss: 0.099297\n",
      "Epoch [2945/10000] Avg train loss: 0.099264\n",
      "Epoch [2946/10000] Avg train loss: 0.099230\n",
      "Epoch [2947/10000] Avg train loss: 0.099197\n",
      "Epoch [2948/10000] Avg train loss: 0.099163\n",
      "Epoch [2949/10000] Avg train loss: 0.099130\n",
      "Epoch [2950/10000] Avg train loss: 0.099096\n",
      "Epoch [2951/10000] Avg train loss: 0.099063\n",
      "Epoch [2952/10000] Avg train loss: 0.099030\n",
      "Epoch [2953/10000] Avg train loss: 0.098996\n",
      "Epoch [2954/10000] Avg train loss: 0.098963\n",
      "Epoch [2955/10000] Avg train loss: 0.098930\n",
      "Epoch [2956/10000] Avg train loss: 0.098896\n",
      "Epoch [2957/10000] Avg train loss: 0.098863\n",
      "Epoch [2958/10000] Avg train loss: 0.098830\n",
      "Epoch [2959/10000] Avg train loss: 0.098796\n",
      "Epoch [2960/10000] Avg train loss: 0.098763\n",
      "Epoch [2961/10000] Avg train loss: 0.098730\n",
      "Epoch [2962/10000] Avg train loss: 0.098697\n",
      "Epoch [2963/10000] Avg train loss: 0.098664\n",
      "Epoch [2964/10000] Avg train loss: 0.098630\n",
      "Epoch [2965/10000] Avg train loss: 0.098597\n",
      "Epoch [2966/10000] Avg train loss: 0.098564\n",
      "Epoch [2967/10000] Avg train loss: 0.098531\n",
      "Epoch [2968/10000] Avg train loss: 0.098498\n",
      "Epoch [2969/10000] Avg train loss: 0.098465\n",
      "Epoch [2970/10000] Avg train loss: 0.098432\n",
      "Epoch [2971/10000] Avg train loss: 0.098399\n",
      "Epoch [2972/10000] Avg train loss: 0.098366\n",
      "Epoch [2973/10000] Avg train loss: 0.098333\n",
      "Epoch [2974/10000] Avg train loss: 0.098300\n",
      "Epoch [2975/10000] Avg train loss: 0.098267\n",
      "Epoch [2976/10000] Avg train loss: 0.098235\n",
      "Epoch [2977/10000] Avg train loss: 0.098202\n",
      "Epoch [2978/10000] Avg train loss: 0.098169\n",
      "Epoch [2979/10000] Avg train loss: 0.098136\n",
      "Epoch [2980/10000] Avg train loss: 0.098103\n",
      "Epoch [2981/10000] Avg train loss: 0.098070\n",
      "Epoch [2982/10000] Avg train loss: 0.098038\n",
      "Epoch [2983/10000] Avg train loss: 0.098005\n",
      "Epoch [2984/10000] Avg train loss: 0.097972\n",
      "Epoch [2985/10000] Avg train loss: 0.097940\n",
      "Epoch [2986/10000] Avg train loss: 0.097907\n",
      "Epoch [2987/10000] Avg train loss: 0.097874\n",
      "Epoch [2988/10000] Avg train loss: 0.097842\n",
      "Epoch [2989/10000] Avg train loss: 0.097809\n",
      "Epoch [2990/10000] Avg train loss: 0.097777\n",
      "Epoch [2991/10000] Avg train loss: 0.097744\n",
      "Epoch [2992/10000] Avg train loss: 0.097711\n",
      "Epoch [2993/10000] Avg train loss: 0.097679\n",
      "Epoch [2994/10000] Avg train loss: 0.097646\n",
      "Epoch [2995/10000] Avg train loss: 0.097614\n",
      "Epoch [2996/10000] Avg train loss: 0.097582\n",
      "Epoch [2997/10000] Avg train loss: 0.097549\n",
      "Epoch [2998/10000] Avg train loss: 0.097517\n",
      "Epoch [2999/10000] Avg train loss: 0.097484\n",
      "Epoch [3000/10000] Avg train loss: 0.097452\n",
      "Epoch [3001/10000] Avg train loss: 0.097420\n",
      "Epoch [3002/10000] Avg train loss: 0.097387\n",
      "Epoch [3003/10000] Avg train loss: 0.097355\n",
      "Epoch [3004/10000] Avg train loss: 0.097323\n",
      "Epoch [3005/10000] Avg train loss: 0.097291\n",
      "Epoch [3006/10000] Avg train loss: 0.097258\n",
      "Epoch [3007/10000] Avg train loss: 0.097226\n",
      "Epoch [3008/10000] Avg train loss: 0.097194\n",
      "Epoch [3009/10000] Avg train loss: 0.097162\n",
      "Epoch [3010/10000] Avg train loss: 0.097130\n",
      "Epoch [3011/10000] Avg train loss: 0.097098\n",
      "Epoch [3012/10000] Avg train loss: 0.097066\n",
      "Epoch [3013/10000] Avg train loss: 0.097034\n",
      "Epoch [3014/10000] Avg train loss: 0.097001\n",
      "Epoch [3015/10000] Avg train loss: 0.096969\n",
      "Epoch [3016/10000] Avg train loss: 0.096937\n",
      "Epoch [3017/10000] Avg train loss: 0.096905\n",
      "Epoch [3018/10000] Avg train loss: 0.096873\n",
      "Epoch [3019/10000] Avg train loss: 0.096842\n",
      "Epoch [3020/10000] Avg train loss: 0.096810\n",
      "Epoch [3021/10000] Avg train loss: 0.096778\n",
      "Epoch [3022/10000] Avg train loss: 0.096746\n",
      "Epoch [3023/10000] Avg train loss: 0.096714\n",
      "Epoch [3024/10000] Avg train loss: 0.096682\n",
      "Epoch [3025/10000] Avg train loss: 0.096650\n",
      "Epoch [3026/10000] Avg train loss: 0.096619\n",
      "Epoch [3027/10000] Avg train loss: 0.096587\n",
      "Epoch [3028/10000] Avg train loss: 0.096555\n",
      "Epoch [3029/10000] Avg train loss: 0.096523\n",
      "Epoch [3030/10000] Avg train loss: 0.096491\n",
      "Epoch [3031/10000] Avg train loss: 0.096460\n",
      "Epoch [3032/10000] Avg train loss: 0.096428\n",
      "Epoch [3033/10000] Avg train loss: 0.096396\n",
      "Epoch [3034/10000] Avg train loss: 0.096365\n",
      "Epoch [3035/10000] Avg train loss: 0.096333\n",
      "Epoch [3036/10000] Avg train loss: 0.096302\n",
      "Epoch [3037/10000] Avg train loss: 0.096270\n",
      "Epoch [3038/10000] Avg train loss: 0.096239\n",
      "Epoch [3039/10000] Avg train loss: 0.096207\n",
      "Epoch [3040/10000] Avg train loss: 0.096175\n",
      "Epoch [3041/10000] Avg train loss: 0.096144\n",
      "Epoch [3042/10000] Avg train loss: 0.096112\n",
      "Epoch [3043/10000] Avg train loss: 0.096081\n",
      "Epoch [3044/10000] Avg train loss: 0.096050\n",
      "Epoch [3045/10000] Avg train loss: 0.096018\n",
      "Epoch [3046/10000] Avg train loss: 0.095987\n",
      "Epoch [3047/10000] Avg train loss: 0.095955\n",
      "Epoch [3048/10000] Avg train loss: 0.095924\n",
      "Epoch [3049/10000] Avg train loss: 0.095893\n",
      "Epoch [3050/10000] Avg train loss: 0.095862\n",
      "Epoch [3051/10000] Avg train loss: 0.095830\n",
      "Epoch [3052/10000] Avg train loss: 0.095799\n",
      "Epoch [3053/10000] Avg train loss: 0.095768\n",
      "Epoch [3054/10000] Avg train loss: 0.095737\n",
      "Epoch [3055/10000] Avg train loss: 0.095705\n",
      "Epoch [3056/10000] Avg train loss: 0.095674\n",
      "Epoch [3057/10000] Avg train loss: 0.095643\n",
      "Epoch [3058/10000] Avg train loss: 0.095612\n",
      "Epoch [3059/10000] Avg train loss: 0.095581\n",
      "Epoch [3060/10000] Avg train loss: 0.095550\n",
      "Epoch [3061/10000] Avg train loss: 0.095519\n",
      "Epoch [3062/10000] Avg train loss: 0.095488\n",
      "Epoch [3063/10000] Avg train loss: 0.095457\n",
      "Epoch [3064/10000] Avg train loss: 0.095425\n",
      "Epoch [3065/10000] Avg train loss: 0.095394\n",
      "Epoch [3066/10000] Avg train loss: 0.095363\n",
      "Epoch [3067/10000] Avg train loss: 0.095333\n",
      "Epoch [3068/10000] Avg train loss: 0.095302\n",
      "Epoch [3069/10000] Avg train loss: 0.095271\n",
      "Epoch [3070/10000] Avg train loss: 0.095240\n",
      "Epoch [3071/10000] Avg train loss: 0.095209\n",
      "Epoch [3072/10000] Avg train loss: 0.095178\n",
      "Epoch [3073/10000] Avg train loss: 0.095147\n",
      "Epoch [3074/10000] Avg train loss: 0.095116\n",
      "Epoch [3075/10000] Avg train loss: 0.095086\n",
      "Epoch [3076/10000] Avg train loss: 0.095055\n",
      "Epoch [3077/10000] Avg train loss: 0.095024\n",
      "Epoch [3078/10000] Avg train loss: 0.094993\n",
      "Epoch [3079/10000] Avg train loss: 0.094963\n",
      "Epoch [3080/10000] Avg train loss: 0.094932\n",
      "Epoch [3081/10000] Avg train loss: 0.094901\n",
      "Epoch [3082/10000] Avg train loss: 0.094871\n",
      "Epoch [3083/10000] Avg train loss: 0.094840\n",
      "Epoch [3084/10000] Avg train loss: 0.094809\n",
      "Epoch [3085/10000] Avg train loss: 0.094779\n",
      "Epoch [3086/10000] Avg train loss: 0.094748\n",
      "Epoch [3087/10000] Avg train loss: 0.094718\n",
      "Epoch [3088/10000] Avg train loss: 0.094687\n",
      "Epoch [3089/10000] Avg train loss: 0.094657\n",
      "Epoch [3090/10000] Avg train loss: 0.094626\n",
      "Epoch [3091/10000] Avg train loss: 0.094596\n",
      "Epoch [3092/10000] Avg train loss: 0.094565\n",
      "Epoch [3093/10000] Avg train loss: 0.094535\n",
      "Epoch [3094/10000] Avg train loss: 0.094504\n",
      "Epoch [3095/10000] Avg train loss: 0.094474\n",
      "Epoch [3096/10000] Avg train loss: 0.094444\n",
      "Epoch [3097/10000] Avg train loss: 0.094413\n",
      "Epoch [3098/10000] Avg train loss: 0.094383\n",
      "Epoch [3099/10000] Avg train loss: 0.094352\n",
      "Epoch [3100/10000] Avg train loss: 0.094322\n",
      "Epoch [3101/10000] Avg train loss: 0.094292\n",
      "Epoch [3102/10000] Avg train loss: 0.094262\n",
      "Epoch [3103/10000] Avg train loss: 0.094231\n",
      "Epoch [3104/10000] Avg train loss: 0.094201\n",
      "Epoch [3105/10000] Avg train loss: 0.094171\n",
      "Epoch [3106/10000] Avg train loss: 0.094141\n",
      "Epoch [3107/10000] Avg train loss: 0.094111\n",
      "Epoch [3108/10000] Avg train loss: 0.094080\n",
      "Epoch [3109/10000] Avg train loss: 0.094050\n",
      "Epoch [3110/10000] Avg train loss: 0.094020\n",
      "Epoch [3111/10000] Avg train loss: 0.093990\n",
      "Epoch [3112/10000] Avg train loss: 0.093960\n",
      "Epoch [3113/10000] Avg train loss: 0.093930\n",
      "Epoch [3114/10000] Avg train loss: 0.093900\n",
      "Epoch [3115/10000] Avg train loss: 0.093870\n",
      "Epoch [3116/10000] Avg train loss: 0.093840\n",
      "Epoch [3117/10000] Avg train loss: 0.093810\n",
      "Epoch [3118/10000] Avg train loss: 0.093780\n",
      "Epoch [3119/10000] Avg train loss: 0.093750\n",
      "Epoch [3120/10000] Avg train loss: 0.093720\n",
      "Epoch [3121/10000] Avg train loss: 0.093690\n",
      "Epoch [3122/10000] Avg train loss: 0.093660\n",
      "Epoch [3123/10000] Avg train loss: 0.093630\n",
      "Epoch [3124/10000] Avg train loss: 0.093601\n",
      "Epoch [3125/10000] Avg train loss: 0.093571\n",
      "Epoch [3126/10000] Avg train loss: 0.093541\n",
      "Epoch [3127/10000] Avg train loss: 0.093511\n",
      "Epoch [3128/10000] Avg train loss: 0.093481\n",
      "Epoch [3129/10000] Avg train loss: 0.093452\n",
      "Epoch [3130/10000] Avg train loss: 0.093422\n",
      "Epoch [3131/10000] Avg train loss: 0.093392\n",
      "Epoch [3132/10000] Avg train loss: 0.093363\n",
      "Epoch [3133/10000] Avg train loss: 0.093333\n",
      "Epoch [3134/10000] Avg train loss: 0.093303\n",
      "Epoch [3135/10000] Avg train loss: 0.093274\n",
      "Epoch [3136/10000] Avg train loss: 0.093244\n",
      "Epoch [3137/10000] Avg train loss: 0.093214\n",
      "Epoch [3138/10000] Avg train loss: 0.093185\n",
      "Epoch [3139/10000] Avg train loss: 0.093155\n",
      "Epoch [3140/10000] Avg train loss: 0.093126\n",
      "Epoch [3141/10000] Avg train loss: 0.093096\n",
      "Epoch [3142/10000] Avg train loss: 0.093067\n",
      "Epoch [3143/10000] Avg train loss: 0.093037\n",
      "Epoch [3144/10000] Avg train loss: 0.093008\n",
      "Epoch [3145/10000] Avg train loss: 0.092978\n",
      "Epoch [3146/10000] Avg train loss: 0.092949\n",
      "Epoch [3147/10000] Avg train loss: 0.092919\n",
      "Epoch [3148/10000] Avg train loss: 0.092890\n",
      "Epoch [3149/10000] Avg train loss: 0.092861\n",
      "Epoch [3150/10000] Avg train loss: 0.092831\n",
      "Epoch [3151/10000] Avg train loss: 0.092802\n",
      "Epoch [3152/10000] Avg train loss: 0.092773\n",
      "Epoch [3153/10000] Avg train loss: 0.092743\n",
      "Epoch [3154/10000] Avg train loss: 0.092714\n",
      "Epoch [3155/10000] Avg train loss: 0.092685\n",
      "Epoch [3156/10000] Avg train loss: 0.092656\n",
      "Epoch [3157/10000] Avg train loss: 0.092626\n",
      "Epoch [3158/10000] Avg train loss: 0.092597\n",
      "Epoch [3159/10000] Avg train loss: 0.092568\n",
      "Epoch [3160/10000] Avg train loss: 0.092539\n",
      "Epoch [3161/10000] Avg train loss: 0.092510\n",
      "Epoch [3162/10000] Avg train loss: 0.092480\n",
      "Epoch [3163/10000] Avg train loss: 0.092451\n",
      "Epoch [3164/10000] Avg train loss: 0.092422\n",
      "Epoch [3165/10000] Avg train loss: 0.092393\n",
      "Epoch [3166/10000] Avg train loss: 0.092364\n",
      "Epoch [3167/10000] Avg train loss: 0.092335\n",
      "Epoch [3168/10000] Avg train loss: 0.092306\n",
      "Epoch [3169/10000] Avg train loss: 0.092277\n",
      "Epoch [3170/10000] Avg train loss: 0.092248\n",
      "Epoch [3171/10000] Avg train loss: 0.092219\n",
      "Epoch [3172/10000] Avg train loss: 0.092190\n",
      "Epoch [3173/10000] Avg train loss: 0.092161\n",
      "Epoch [3174/10000] Avg train loss: 0.092132\n",
      "Epoch [3175/10000] Avg train loss: 0.092103\n",
      "Epoch [3176/10000] Avg train loss: 0.092074\n",
      "Epoch [3177/10000] Avg train loss: 0.092046\n",
      "Epoch [3178/10000] Avg train loss: 0.092017\n",
      "Epoch [3179/10000] Avg train loss: 0.091988\n",
      "Epoch [3180/10000] Avg train loss: 0.091959\n",
      "Epoch [3181/10000] Avg train loss: 0.091930\n",
      "Epoch [3182/10000] Avg train loss: 0.091902\n",
      "Epoch [3183/10000] Avg train loss: 0.091873\n",
      "Epoch [3184/10000] Avg train loss: 0.091844\n",
      "Epoch [3185/10000] Avg train loss: 0.091815\n",
      "Epoch [3186/10000] Avg train loss: 0.091787\n",
      "Epoch [3187/10000] Avg train loss: 0.091758\n",
      "Epoch [3188/10000] Avg train loss: 0.091729\n",
      "Epoch [3189/10000] Avg train loss: 0.091701\n",
      "Epoch [3190/10000] Avg train loss: 0.091672\n",
      "Epoch [3191/10000] Avg train loss: 0.091644\n",
      "Epoch [3192/10000] Avg train loss: 0.091615\n",
      "Epoch [3193/10000] Avg train loss: 0.091586\n",
      "Epoch [3194/10000] Avg train loss: 0.091558\n",
      "Epoch [3195/10000] Avg train loss: 0.091529\n",
      "Epoch [3196/10000] Avg train loss: 0.091501\n",
      "Epoch [3197/10000] Avg train loss: 0.091472\n",
      "Epoch [3198/10000] Avg train loss: 0.091444\n",
      "Epoch [3199/10000] Avg train loss: 0.091415\n",
      "Epoch [3200/10000] Avg train loss: 0.091387\n",
      "Epoch [3201/10000] Avg train loss: 0.091358\n",
      "Epoch [3202/10000] Avg train loss: 0.091330\n",
      "Epoch [3203/10000] Avg train loss: 0.091302\n",
      "Epoch [3204/10000] Avg train loss: 0.091273\n",
      "Epoch [3205/10000] Avg train loss: 0.091245\n",
      "Epoch [3206/10000] Avg train loss: 0.091217\n",
      "Epoch [3207/10000] Avg train loss: 0.091188\n",
      "Epoch [3208/10000] Avg train loss: 0.091160\n",
      "Epoch [3209/10000] Avg train loss: 0.091132\n",
      "Epoch [3210/10000] Avg train loss: 0.091103\n",
      "Epoch [3211/10000] Avg train loss: 0.091075\n",
      "Epoch [3212/10000] Avg train loss: 0.091047\n",
      "Epoch [3213/10000] Avg train loss: 0.091019\n",
      "Epoch [3214/10000] Avg train loss: 0.090991\n",
      "Epoch [3215/10000] Avg train loss: 0.090962\n",
      "Epoch [3216/10000] Avg train loss: 0.090934\n",
      "Epoch [3217/10000] Avg train loss: 0.090906\n",
      "Epoch [3218/10000] Avg train loss: 0.090878\n",
      "Epoch [3219/10000] Avg train loss: 0.090850\n",
      "Epoch [3220/10000] Avg train loss: 0.090822\n",
      "Epoch [3221/10000] Avg train loss: 0.090794\n",
      "Epoch [3222/10000] Avg train loss: 0.090766\n",
      "Epoch [3223/10000] Avg train loss: 0.090738\n",
      "Epoch [3224/10000] Avg train loss: 0.090710\n",
      "Epoch [3225/10000] Avg train loss: 0.090682\n",
      "Epoch [3226/10000] Avg train loss: 0.090654\n",
      "Epoch [3227/10000] Avg train loss: 0.090626\n",
      "Epoch [3228/10000] Avg train loss: 0.090598\n",
      "Epoch [3229/10000] Avg train loss: 0.090570\n",
      "Epoch [3230/10000] Avg train loss: 0.090542\n",
      "Epoch [3231/10000] Avg train loss: 0.090514\n",
      "Epoch [3232/10000] Avg train loss: 0.090486\n",
      "Epoch [3233/10000] Avg train loss: 0.090458\n",
      "Epoch [3234/10000] Avg train loss: 0.090430\n",
      "Epoch [3235/10000] Avg train loss: 0.090402\n",
      "Epoch [3236/10000] Avg train loss: 0.090375\n",
      "Epoch [3237/10000] Avg train loss: 0.090347\n",
      "Epoch [3238/10000] Avg train loss: 0.090319\n",
      "Epoch [3239/10000] Avg train loss: 0.090291\n",
      "Epoch [3240/10000] Avg train loss: 0.090263\n",
      "Epoch [3241/10000] Avg train loss: 0.090236\n",
      "Epoch [3242/10000] Avg train loss: 0.090208\n",
      "Epoch [3243/10000] Avg train loss: 0.090180\n",
      "Epoch [3244/10000] Avg train loss: 0.090153\n",
      "Epoch [3245/10000] Avg train loss: 0.090125\n",
      "Epoch [3246/10000] Avg train loss: 0.090097\n",
      "Epoch [3247/10000] Avg train loss: 0.090070\n",
      "Epoch [3248/10000] Avg train loss: 0.090042\n",
      "Epoch [3249/10000] Avg train loss: 0.090015\n",
      "Epoch [3250/10000] Avg train loss: 0.089987\n",
      "Epoch [3251/10000] Avg train loss: 0.089959\n",
      "Epoch [3252/10000] Avg train loss: 0.089932\n",
      "Epoch [3253/10000] Avg train loss: 0.089904\n",
      "Epoch [3254/10000] Avg train loss: 0.089877\n",
      "Epoch [3255/10000] Avg train loss: 0.089849\n",
      "Epoch [3256/10000] Avg train loss: 0.089822\n",
      "Epoch [3257/10000] Avg train loss: 0.089794\n",
      "Epoch [3258/10000] Avg train loss: 0.089767\n",
      "Epoch [3259/10000] Avg train loss: 0.089740\n",
      "Epoch [3260/10000] Avg train loss: 0.089712\n",
      "Epoch [3261/10000] Avg train loss: 0.089685\n",
      "Epoch [3262/10000] Avg train loss: 0.089657\n",
      "Epoch [3263/10000] Avg train loss: 0.089630\n",
      "Epoch [3264/10000] Avg train loss: 0.089603\n",
      "Epoch [3265/10000] Avg train loss: 0.089575\n",
      "Epoch [3266/10000] Avg train loss: 0.089548\n",
      "Epoch [3267/10000] Avg train loss: 0.089521\n",
      "Epoch [3268/10000] Avg train loss: 0.089493\n",
      "Epoch [3269/10000] Avg train loss: 0.089466\n",
      "Epoch [3270/10000] Avg train loss: 0.089439\n",
      "Epoch [3271/10000] Avg train loss: 0.089412\n",
      "Epoch [3272/10000] Avg train loss: 0.089384\n",
      "Epoch [3273/10000] Avg train loss: 0.089357\n",
      "Epoch [3274/10000] Avg train loss: 0.089330\n",
      "Epoch [3275/10000] Avg train loss: 0.089303\n",
      "Epoch [3276/10000] Avg train loss: 0.089276\n",
      "Epoch [3277/10000] Avg train loss: 0.089249\n",
      "Epoch [3278/10000] Avg train loss: 0.089222\n",
      "Epoch [3279/10000] Avg train loss: 0.089194\n",
      "Epoch [3280/10000] Avg train loss: 0.089167\n",
      "Epoch [3281/10000] Avg train loss: 0.089140\n",
      "Epoch [3282/10000] Avg train loss: 0.089113\n",
      "Epoch [3283/10000] Avg train loss: 0.089086\n",
      "Epoch [3284/10000] Avg train loss: 0.089059\n",
      "Epoch [3285/10000] Avg train loss: 0.089032\n",
      "Epoch [3286/10000] Avg train loss: 0.089005\n",
      "Epoch [3287/10000] Avg train loss: 0.088978\n",
      "Epoch [3288/10000] Avg train loss: 0.088951\n",
      "Epoch [3289/10000] Avg train loss: 0.088924\n",
      "Epoch [3290/10000] Avg train loss: 0.088897\n",
      "Epoch [3291/10000] Avg train loss: 0.088871\n",
      "Epoch [3292/10000] Avg train loss: 0.088844\n",
      "Epoch [3293/10000] Avg train loss: 0.088817\n",
      "Epoch [3294/10000] Avg train loss: 0.088790\n",
      "Epoch [3295/10000] Avg train loss: 0.088763\n",
      "Epoch [3296/10000] Avg train loss: 0.088736\n",
      "Epoch [3297/10000] Avg train loss: 0.088709\n",
      "Epoch [3298/10000] Avg train loss: 0.088683\n",
      "Epoch [3299/10000] Avg train loss: 0.088656\n",
      "Epoch [3300/10000] Avg train loss: 0.088629\n",
      "Epoch [3301/10000] Avg train loss: 0.088602\n",
      "Epoch [3302/10000] Avg train loss: 0.088576\n",
      "Epoch [3303/10000] Avg train loss: 0.088549\n",
      "Epoch [3304/10000] Avg train loss: 0.088522\n",
      "Epoch [3305/10000] Avg train loss: 0.088496\n",
      "Epoch [3306/10000] Avg train loss: 0.088469\n",
      "Epoch [3307/10000] Avg train loss: 0.088442\n",
      "Epoch [3308/10000] Avg train loss: 0.088416\n",
      "Epoch [3309/10000] Avg train loss: 0.088389\n",
      "Epoch [3310/10000] Avg train loss: 0.088363\n",
      "Epoch [3311/10000] Avg train loss: 0.088336\n",
      "Epoch [3312/10000] Avg train loss: 0.088309\n",
      "Epoch [3313/10000] Avg train loss: 0.088283\n",
      "Epoch [3314/10000] Avg train loss: 0.088256\n",
      "Epoch [3315/10000] Avg train loss: 0.088230\n",
      "Epoch [3316/10000] Avg train loss: 0.088203\n",
      "Epoch [3317/10000] Avg train loss: 0.088177\n",
      "Epoch [3318/10000] Avg train loss: 0.088150\n",
      "Epoch [3319/10000] Avg train loss: 0.088124\n",
      "Epoch [3320/10000] Avg train loss: 0.088098\n",
      "Epoch [3321/10000] Avg train loss: 0.088071\n",
      "Epoch [3322/10000] Avg train loss: 0.088045\n",
      "Epoch [3323/10000] Avg train loss: 0.088018\n",
      "Epoch [3324/10000] Avg train loss: 0.087992\n",
      "Epoch [3325/10000] Avg train loss: 0.087966\n",
      "Epoch [3326/10000] Avg train loss: 0.087939\n",
      "Epoch [3327/10000] Avg train loss: 0.087913\n",
      "Epoch [3328/10000] Avg train loss: 0.087887\n",
      "Epoch [3329/10000] Avg train loss: 0.087860\n",
      "Epoch [3330/10000] Avg train loss: 0.087834\n",
      "Epoch [3331/10000] Avg train loss: 0.087808\n",
      "Epoch [3332/10000] Avg train loss: 0.087782\n",
      "Epoch [3333/10000] Avg train loss: 0.087755\n",
      "Epoch [3334/10000] Avg train loss: 0.087729\n",
      "Epoch [3335/10000] Avg train loss: 0.087703\n",
      "Epoch [3336/10000] Avg train loss: 0.087677\n",
      "Epoch [3337/10000] Avg train loss: 0.087651\n",
      "Epoch [3338/10000] Avg train loss: 0.087624\n",
      "Epoch [3339/10000] Avg train loss: 0.087598\n",
      "Epoch [3340/10000] Avg train loss: 0.087572\n",
      "Epoch [3341/10000] Avg train loss: 0.087546\n",
      "Epoch [3342/10000] Avg train loss: 0.087520\n",
      "Epoch [3343/10000] Avg train loss: 0.087494\n",
      "Epoch [3344/10000] Avg train loss: 0.087468\n",
      "Epoch [3345/10000] Avg train loss: 0.087442\n",
      "Epoch [3346/10000] Avg train loss: 0.087416\n",
      "Epoch [3347/10000] Avg train loss: 0.087390\n",
      "Epoch [3348/10000] Avg train loss: 0.087364\n",
      "Epoch [3349/10000] Avg train loss: 0.087338\n",
      "Epoch [3350/10000] Avg train loss: 0.087312\n",
      "Epoch [3351/10000] Avg train loss: 0.087286\n",
      "Epoch [3352/10000] Avg train loss: 0.087260\n",
      "Epoch [3353/10000] Avg train loss: 0.087234\n",
      "Epoch [3354/10000] Avg train loss: 0.087208\n",
      "Epoch [3355/10000] Avg train loss: 0.087182\n",
      "Epoch [3356/10000] Avg train loss: 0.087156\n",
      "Epoch [3357/10000] Avg train loss: 0.087131\n",
      "Epoch [3358/10000] Avg train loss: 0.087105\n",
      "Epoch [3359/10000] Avg train loss: 0.087079\n",
      "Epoch [3360/10000] Avg train loss: 0.087053\n",
      "Epoch [3361/10000] Avg train loss: 0.087027\n",
      "Epoch [3362/10000] Avg train loss: 0.087002\n",
      "Epoch [3363/10000] Avg train loss: 0.086976\n",
      "Epoch [3364/10000] Avg train loss: 0.086950\n",
      "Epoch [3365/10000] Avg train loss: 0.086924\n",
      "Epoch [3366/10000] Avg train loss: 0.086899\n",
      "Epoch [3367/10000] Avg train loss: 0.086873\n",
      "Epoch [3368/10000] Avg train loss: 0.086847\n",
      "Epoch [3369/10000] Avg train loss: 0.086822\n",
      "Epoch [3370/10000] Avg train loss: 0.086796\n",
      "Epoch [3371/10000] Avg train loss: 0.086770\n",
      "Epoch [3372/10000] Avg train loss: 0.086745\n",
      "Epoch [3373/10000] Avg train loss: 0.086719\n",
      "Epoch [3374/10000] Avg train loss: 0.086693\n",
      "Epoch [3375/10000] Avg train loss: 0.086668\n",
      "Epoch [3376/10000] Avg train loss: 0.086642\n",
      "Epoch [3377/10000] Avg train loss: 0.086617\n",
      "Epoch [3378/10000] Avg train loss: 0.086591\n",
      "Epoch [3379/10000] Avg train loss: 0.086566\n",
      "Epoch [3380/10000] Avg train loss: 0.086540\n",
      "Epoch [3381/10000] Avg train loss: 0.086515\n",
      "Epoch [3382/10000] Avg train loss: 0.086489\n",
      "Epoch [3383/10000] Avg train loss: 0.086464\n",
      "Epoch [3384/10000] Avg train loss: 0.086438\n",
      "Epoch [3385/10000] Avg train loss: 0.086413\n",
      "Epoch [3386/10000] Avg train loss: 0.086387\n",
      "Epoch [3387/10000] Avg train loss: 0.086362\n",
      "Epoch [3388/10000] Avg train loss: 0.086337\n",
      "Epoch [3389/10000] Avg train loss: 0.086311\n",
      "Epoch [3390/10000] Avg train loss: 0.086286\n",
      "Epoch [3391/10000] Avg train loss: 0.086261\n",
      "Epoch [3392/10000] Avg train loss: 0.086235\n",
      "Epoch [3393/10000] Avg train loss: 0.086210\n",
      "Epoch [3394/10000] Avg train loss: 0.086185\n",
      "Epoch [3395/10000] Avg train loss: 0.086159\n",
      "Epoch [3396/10000] Avg train loss: 0.086134\n",
      "Epoch [3397/10000] Avg train loss: 0.086109\n",
      "Epoch [3398/10000] Avg train loss: 0.086084\n",
      "Epoch [3399/10000] Avg train loss: 0.086058\n",
      "Epoch [3400/10000] Avg train loss: 0.086033\n",
      "Epoch [3401/10000] Avg train loss: 0.086008\n",
      "Epoch [3402/10000] Avg train loss: 0.085983\n",
      "Epoch [3403/10000] Avg train loss: 0.085958\n",
      "Epoch [3404/10000] Avg train loss: 0.085932\n",
      "Epoch [3405/10000] Avg train loss: 0.085907\n",
      "Epoch [3406/10000] Avg train loss: 0.085882\n",
      "Epoch [3407/10000] Avg train loss: 0.085857\n",
      "Epoch [3408/10000] Avg train loss: 0.085832\n",
      "Epoch [3409/10000] Avg train loss: 0.085807\n",
      "Epoch [3410/10000] Avg train loss: 0.085782\n",
      "Epoch [3411/10000] Avg train loss: 0.085757\n",
      "Epoch [3412/10000] Avg train loss: 0.085732\n",
      "Epoch [3413/10000] Avg train loss: 0.085707\n",
      "Epoch [3414/10000] Avg train loss: 0.085682\n",
      "Epoch [3415/10000] Avg train loss: 0.085657\n",
      "Epoch [3416/10000] Avg train loss: 0.085632\n",
      "Epoch [3417/10000] Avg train loss: 0.085607\n",
      "Epoch [3418/10000] Avg train loss: 0.085582\n",
      "Epoch [3419/10000] Avg train loss: 0.085557\n",
      "Epoch [3420/10000] Avg train loss: 0.085532\n",
      "Epoch [3421/10000] Avg train loss: 0.085507\n",
      "Epoch [3422/10000] Avg train loss: 0.085482\n",
      "Epoch [3423/10000] Avg train loss: 0.085457\n",
      "Epoch [3424/10000] Avg train loss: 0.085433\n",
      "Epoch [3425/10000] Avg train loss: 0.085408\n",
      "Epoch [3426/10000] Avg train loss: 0.085383\n",
      "Epoch [3427/10000] Avg train loss: 0.085358\n",
      "Epoch [3428/10000] Avg train loss: 0.085333\n",
      "Epoch [3429/10000] Avg train loss: 0.085308\n",
      "Epoch [3430/10000] Avg train loss: 0.085284\n",
      "Epoch [3431/10000] Avg train loss: 0.085259\n",
      "Epoch [3432/10000] Avg train loss: 0.085234\n",
      "Epoch [3433/10000] Avg train loss: 0.085209\n",
      "Epoch [3434/10000] Avg train loss: 0.085185\n",
      "Epoch [3435/10000] Avg train loss: 0.085160\n",
      "Epoch [3436/10000] Avg train loss: 0.085135\n",
      "Epoch [3437/10000] Avg train loss: 0.085111\n",
      "Epoch [3438/10000] Avg train loss: 0.085086\n",
      "Epoch [3439/10000] Avg train loss: 0.085061\n",
      "Epoch [3440/10000] Avg train loss: 0.085037\n",
      "Epoch [3441/10000] Avg train loss: 0.085012\n",
      "Epoch [3442/10000] Avg train loss: 0.084988\n",
      "Epoch [3443/10000] Avg train loss: 0.084963\n",
      "Epoch [3444/10000] Avg train loss: 0.084938\n",
      "Epoch [3445/10000] Avg train loss: 0.084914\n",
      "Epoch [3446/10000] Avg train loss: 0.084889\n",
      "Epoch [3447/10000] Avg train loss: 0.084865\n",
      "Epoch [3448/10000] Avg train loss: 0.084840\n",
      "Epoch [3449/10000] Avg train loss: 0.084816\n",
      "Epoch [3450/10000] Avg train loss: 0.084791\n",
      "Epoch [3451/10000] Avg train loss: 0.084767\n",
      "Epoch [3452/10000] Avg train loss: 0.084743\n",
      "Epoch [3453/10000] Avg train loss: 0.084718\n",
      "Epoch [3454/10000] Avg train loss: 0.084694\n",
      "Epoch [3455/10000] Avg train loss: 0.084669\n",
      "Epoch [3456/10000] Avg train loss: 0.084645\n",
      "Epoch [3457/10000] Avg train loss: 0.084620\n",
      "Epoch [3458/10000] Avg train loss: 0.084596\n",
      "Epoch [3459/10000] Avg train loss: 0.084572\n",
      "Epoch [3460/10000] Avg train loss: 0.084547\n",
      "Epoch [3461/10000] Avg train loss: 0.084523\n",
      "Epoch [3462/10000] Avg train loss: 0.084499\n",
      "Epoch [3463/10000] Avg train loss: 0.084474\n",
      "Epoch [3464/10000] Avg train loss: 0.084450\n",
      "Epoch [3465/10000] Avg train loss: 0.084426\n",
      "Epoch [3466/10000] Avg train loss: 0.084402\n",
      "Epoch [3467/10000] Avg train loss: 0.084377\n",
      "Epoch [3468/10000] Avg train loss: 0.084353\n",
      "Epoch [3469/10000] Avg train loss: 0.084329\n",
      "Epoch [3470/10000] Avg train loss: 0.084305\n",
      "Epoch [3471/10000] Avg train loss: 0.084281\n",
      "Epoch [3472/10000] Avg train loss: 0.084256\n",
      "Epoch [3473/10000] Avg train loss: 0.084232\n",
      "Epoch [3474/10000] Avg train loss: 0.084208\n",
      "Epoch [3475/10000] Avg train loss: 0.084184\n",
      "Epoch [3476/10000] Avg train loss: 0.084160\n",
      "Epoch [3477/10000] Avg train loss: 0.084136\n",
      "Epoch [3478/10000] Avg train loss: 0.084112\n",
      "Epoch [3479/10000] Avg train loss: 0.084087\n",
      "Epoch [3480/10000] Avg train loss: 0.084063\n",
      "Epoch [3481/10000] Avg train loss: 0.084039\n",
      "Epoch [3482/10000] Avg train loss: 0.084015\n",
      "Epoch [3483/10000] Avg train loss: 0.083991\n",
      "Epoch [3484/10000] Avg train loss: 0.083967\n",
      "Epoch [3485/10000] Avg train loss: 0.083943\n",
      "Epoch [3486/10000] Avg train loss: 0.083919\n",
      "Epoch [3487/10000] Avg train loss: 0.083895\n",
      "Epoch [3488/10000] Avg train loss: 0.083871\n",
      "Epoch [3489/10000] Avg train loss: 0.083847\n",
      "Epoch [3490/10000] Avg train loss: 0.083823\n",
      "Epoch [3491/10000] Avg train loss: 0.083800\n",
      "Epoch [3492/10000] Avg train loss: 0.083776\n",
      "Epoch [3493/10000] Avg train loss: 0.083752\n",
      "Epoch [3494/10000] Avg train loss: 0.083728\n",
      "Epoch [3495/10000] Avg train loss: 0.083704\n",
      "Epoch [3496/10000] Avg train loss: 0.083680\n",
      "Epoch [3497/10000] Avg train loss: 0.083656\n",
      "Epoch [3498/10000] Avg train loss: 0.083632\n",
      "Epoch [3499/10000] Avg train loss: 0.083609\n",
      "Epoch [3500/10000] Avg train loss: 0.083585\n",
      "Epoch [3501/10000] Avg train loss: 0.083561\n",
      "Epoch [3502/10000] Avg train loss: 0.083537\n",
      "Epoch [3503/10000] Avg train loss: 0.083514\n",
      "Epoch [3504/10000] Avg train loss: 0.083490\n",
      "Epoch [3505/10000] Avg train loss: 0.083466\n",
      "Epoch [3506/10000] Avg train loss: 0.083442\n",
      "Epoch [3507/10000] Avg train loss: 0.083419\n",
      "Epoch [3508/10000] Avg train loss: 0.083395\n",
      "Epoch [3509/10000] Avg train loss: 0.083371\n",
      "Epoch [3510/10000] Avg train loss: 0.083348\n",
      "Epoch [3511/10000] Avg train loss: 0.083324\n",
      "Epoch [3512/10000] Avg train loss: 0.083300\n",
      "Epoch [3513/10000] Avg train loss: 0.083277\n",
      "Epoch [3514/10000] Avg train loss: 0.083253\n",
      "Epoch [3515/10000] Avg train loss: 0.083230\n",
      "Epoch [3516/10000] Avg train loss: 0.083206\n",
      "Epoch [3517/10000] Avg train loss: 0.083182\n",
      "Epoch [3518/10000] Avg train loss: 0.083159\n",
      "Epoch [3519/10000] Avg train loss: 0.083135\n",
      "Epoch [3520/10000] Avg train loss: 0.083112\n",
      "Epoch [3521/10000] Avg train loss: 0.083088\n",
      "Epoch [3522/10000] Avg train loss: 0.083065\n",
      "Epoch [3523/10000] Avg train loss: 0.083041\n",
      "Epoch [3524/10000] Avg train loss: 0.083018\n",
      "Epoch [3525/10000] Avg train loss: 0.082994\n",
      "Epoch [3526/10000] Avg train loss: 0.082971\n",
      "Epoch [3527/10000] Avg train loss: 0.082948\n",
      "Epoch [3528/10000] Avg train loss: 0.082924\n",
      "Epoch [3529/10000] Avg train loss: 0.082901\n",
      "Epoch [3530/10000] Avg train loss: 0.082877\n",
      "Epoch [3531/10000] Avg train loss: 0.082854\n",
      "Epoch [3532/10000] Avg train loss: 0.082831\n",
      "Epoch [3533/10000] Avg train loss: 0.082807\n",
      "Epoch [3534/10000] Avg train loss: 0.082784\n",
      "Epoch [3535/10000] Avg train loss: 0.082761\n",
      "Epoch [3536/10000] Avg train loss: 0.082737\n",
      "Epoch [3537/10000] Avg train loss: 0.082714\n",
      "Epoch [3538/10000] Avg train loss: 0.082691\n",
      "Epoch [3539/10000] Avg train loss: 0.082667\n",
      "Epoch [3540/10000] Avg train loss: 0.082644\n",
      "Epoch [3541/10000] Avg train loss: 0.082621\n",
      "Epoch [3542/10000] Avg train loss: 0.082598\n",
      "Epoch [3543/10000] Avg train loss: 0.082574\n",
      "Epoch [3544/10000] Avg train loss: 0.082551\n",
      "Epoch [3545/10000] Avg train loss: 0.082528\n",
      "Epoch [3546/10000] Avg train loss: 0.082505\n",
      "Epoch [3547/10000] Avg train loss: 0.082482\n",
      "Epoch [3548/10000] Avg train loss: 0.082459\n",
      "Epoch [3549/10000] Avg train loss: 0.082435\n",
      "Epoch [3550/10000] Avg train loss: 0.082412\n",
      "Epoch [3551/10000] Avg train loss: 0.082389\n",
      "Epoch [3552/10000] Avg train loss: 0.082366\n",
      "Epoch [3553/10000] Avg train loss: 0.082343\n",
      "Epoch [3554/10000] Avg train loss: 0.082320\n",
      "Epoch [3555/10000] Avg train loss: 0.082297\n",
      "Epoch [3556/10000] Avg train loss: 0.082274\n",
      "Epoch [3557/10000] Avg train loss: 0.082251\n",
      "Epoch [3558/10000] Avg train loss: 0.082228\n",
      "Epoch [3559/10000] Avg train loss: 0.082205\n",
      "Epoch [3560/10000] Avg train loss: 0.082182\n",
      "Epoch [3561/10000] Avg train loss: 0.082159\n",
      "Epoch [3562/10000] Avg train loss: 0.082136\n",
      "Epoch [3563/10000] Avg train loss: 0.082113\n",
      "Epoch [3564/10000] Avg train loss: 0.082090\n",
      "Epoch [3565/10000] Avg train loss: 0.082067\n",
      "Epoch [3566/10000] Avg train loss: 0.082044\n",
      "Epoch [3567/10000] Avg train loss: 0.082021\n",
      "Epoch [3568/10000] Avg train loss: 0.081998\n",
      "Epoch [3569/10000] Avg train loss: 0.081975\n",
      "Epoch [3570/10000] Avg train loss: 0.081952\n",
      "Epoch [3571/10000] Avg train loss: 0.081930\n",
      "Epoch [3572/10000] Avg train loss: 0.081907\n",
      "Epoch [3573/10000] Avg train loss: 0.081884\n",
      "Epoch [3574/10000] Avg train loss: 0.081861\n",
      "Epoch [3575/10000] Avg train loss: 0.081838\n",
      "Epoch [3576/10000] Avg train loss: 0.081815\n",
      "Epoch [3577/10000] Avg train loss: 0.081793\n",
      "Epoch [3578/10000] Avg train loss: 0.081770\n",
      "Epoch [3579/10000] Avg train loss: 0.081747\n",
      "Epoch [3580/10000] Avg train loss: 0.081724\n",
      "Epoch [3581/10000] Avg train loss: 0.081702\n",
      "Epoch [3582/10000] Avg train loss: 0.081679\n",
      "Epoch [3583/10000] Avg train loss: 0.081656\n",
      "Epoch [3584/10000] Avg train loss: 0.081634\n",
      "Epoch [3585/10000] Avg train loss: 0.081611\n",
      "Epoch [3586/10000] Avg train loss: 0.081588\n",
      "Epoch [3587/10000] Avg train loss: 0.081566\n",
      "Epoch [3588/10000] Avg train loss: 0.081543\n",
      "Epoch [3589/10000] Avg train loss: 0.081520\n",
      "Epoch [3590/10000] Avg train loss: 0.081498\n",
      "Epoch [3591/10000] Avg train loss: 0.081475\n",
      "Epoch [3592/10000] Avg train loss: 0.081452\n",
      "Epoch [3593/10000] Avg train loss: 0.081430\n",
      "Epoch [3594/10000] Avg train loss: 0.081407\n",
      "Epoch [3595/10000] Avg train loss: 0.081385\n",
      "Epoch [3596/10000] Avg train loss: 0.081362\n",
      "Epoch [3597/10000] Avg train loss: 0.081340\n",
      "Epoch [3598/10000] Avg train loss: 0.081317\n",
      "Epoch [3599/10000] Avg train loss: 0.081295\n",
      "Epoch [3600/10000] Avg train loss: 0.081272\n",
      "Epoch [3601/10000] Avg train loss: 0.081250\n",
      "Epoch [3602/10000] Avg train loss: 0.081227\n",
      "Epoch [3603/10000] Avg train loss: 0.081205\n",
      "Epoch [3604/10000] Avg train loss: 0.081182\n",
      "Epoch [3605/10000] Avg train loss: 0.081160\n",
      "Epoch [3606/10000] Avg train loss: 0.081137\n",
      "Epoch [3607/10000] Avg train loss: 0.081115\n",
      "Epoch [3608/10000] Avg train loss: 0.081093\n",
      "Epoch [3609/10000] Avg train loss: 0.081070\n",
      "Epoch [3610/10000] Avg train loss: 0.081048\n",
      "Epoch [3611/10000] Avg train loss: 0.081025\n",
      "Epoch [3612/10000] Avg train loss: 0.081003\n",
      "Epoch [3613/10000] Avg train loss: 0.080981\n",
      "Epoch [3614/10000] Avg train loss: 0.080958\n",
      "Epoch [3615/10000] Avg train loss: 0.080936\n",
      "Epoch [3616/10000] Avg train loss: 0.080914\n",
      "Epoch [3617/10000] Avg train loss: 0.080892\n",
      "Epoch [3618/10000] Avg train loss: 0.080869\n",
      "Epoch [3619/10000] Avg train loss: 0.080847\n",
      "Epoch [3620/10000] Avg train loss: 0.080825\n",
      "Epoch [3621/10000] Avg train loss: 0.080803\n",
      "Epoch [3622/10000] Avg train loss: 0.080780\n",
      "Epoch [3623/10000] Avg train loss: 0.080758\n",
      "Epoch [3624/10000] Avg train loss: 0.080736\n",
      "Epoch [3625/10000] Avg train loss: 0.080714\n",
      "Epoch [3626/10000] Avg train loss: 0.080692\n",
      "Epoch [3627/10000] Avg train loss: 0.080669\n",
      "Epoch [3628/10000] Avg train loss: 0.080647\n",
      "Epoch [3629/10000] Avg train loss: 0.080625\n",
      "Epoch [3630/10000] Avg train loss: 0.080603\n",
      "Epoch [3631/10000] Avg train loss: 0.080581\n",
      "Epoch [3632/10000] Avg train loss: 0.080559\n",
      "Epoch [3633/10000] Avg train loss: 0.080537\n",
      "Epoch [3634/10000] Avg train loss: 0.080515\n",
      "Epoch [3635/10000] Avg train loss: 0.080493\n",
      "Epoch [3636/10000] Avg train loss: 0.080471\n",
      "Epoch [3637/10000] Avg train loss: 0.080448\n",
      "Epoch [3638/10000] Avg train loss: 0.080426\n",
      "Epoch [3639/10000] Avg train loss: 0.080404\n",
      "Epoch [3640/10000] Avg train loss: 0.080382\n",
      "Epoch [3641/10000] Avg train loss: 0.080360\n",
      "Epoch [3642/10000] Avg train loss: 0.080338\n",
      "Epoch [3643/10000] Avg train loss: 0.080316\n",
      "Epoch [3644/10000] Avg train loss: 0.080294\n",
      "Epoch [3645/10000] Avg train loss: 0.080273\n",
      "Epoch [3646/10000] Avg train loss: 0.080251\n",
      "Epoch [3647/10000] Avg train loss: 0.080229\n",
      "Epoch [3648/10000] Avg train loss: 0.080207\n",
      "Epoch [3649/10000] Avg train loss: 0.080185\n",
      "Epoch [3650/10000] Avg train loss: 0.080163\n",
      "Epoch [3651/10000] Avg train loss: 0.080141\n",
      "Epoch [3652/10000] Avg train loss: 0.080119\n",
      "Epoch [3653/10000] Avg train loss: 0.080097\n",
      "Epoch [3654/10000] Avg train loss: 0.080076\n",
      "Epoch [3655/10000] Avg train loss: 0.080054\n",
      "Epoch [3656/10000] Avg train loss: 0.080032\n",
      "Epoch [3657/10000] Avg train loss: 0.080010\n",
      "Epoch [3658/10000] Avg train loss: 0.079988\n",
      "Epoch [3659/10000] Avg train loss: 0.079967\n",
      "Epoch [3660/10000] Avg train loss: 0.079945\n",
      "Epoch [3661/10000] Avg train loss: 0.079923\n",
      "Epoch [3662/10000] Avg train loss: 0.079901\n",
      "Epoch [3663/10000] Avg train loss: 0.079880\n",
      "Epoch [3664/10000] Avg train loss: 0.079858\n",
      "Epoch [3665/10000] Avg train loss: 0.079836\n",
      "Epoch [3666/10000] Avg train loss: 0.079814\n",
      "Epoch [3667/10000] Avg train loss: 0.079793\n",
      "Epoch [3668/10000] Avg train loss: 0.079771\n",
      "Epoch [3669/10000] Avg train loss: 0.079749\n",
      "Epoch [3670/10000] Avg train loss: 0.079728\n",
      "Epoch [3671/10000] Avg train loss: 0.079706\n",
      "Epoch [3672/10000] Avg train loss: 0.079685\n",
      "Epoch [3673/10000] Avg train loss: 0.079663\n",
      "Epoch [3674/10000] Avg train loss: 0.079641\n",
      "Epoch [3675/10000] Avg train loss: 0.079620\n",
      "Epoch [3676/10000] Avg train loss: 0.079598\n",
      "Epoch [3677/10000] Avg train loss: 0.079577\n",
      "Epoch [3678/10000] Avg train loss: 0.079555\n",
      "Epoch [3679/10000] Avg train loss: 0.079533\n",
      "Epoch [3680/10000] Avg train loss: 0.079512\n",
      "Epoch [3681/10000] Avg train loss: 0.079490\n",
      "Epoch [3682/10000] Avg train loss: 0.079469\n",
      "Epoch [3683/10000] Avg train loss: 0.079447\n",
      "Epoch [3684/10000] Avg train loss: 0.079426\n",
      "Epoch [3685/10000] Avg train loss: 0.079404\n",
      "Epoch [3686/10000] Avg train loss: 0.079383\n",
      "Epoch [3687/10000] Avg train loss: 0.079362\n",
      "Epoch [3688/10000] Avg train loss: 0.079340\n",
      "Epoch [3689/10000] Avg train loss: 0.079319\n",
      "Epoch [3690/10000] Avg train loss: 0.079297\n",
      "Epoch [3691/10000] Avg train loss: 0.079276\n",
      "Epoch [3692/10000] Avg train loss: 0.079255\n",
      "Epoch [3693/10000] Avg train loss: 0.079233\n",
      "Epoch [3694/10000] Avg train loss: 0.079212\n",
      "Epoch [3695/10000] Avg train loss: 0.079190\n",
      "Epoch [3696/10000] Avg train loss: 0.079169\n",
      "Epoch [3697/10000] Avg train loss: 0.079148\n",
      "Epoch [3698/10000] Avg train loss: 0.079126\n",
      "Epoch [3699/10000] Avg train loss: 0.079105\n",
      "Epoch [3700/10000] Avg train loss: 0.079084\n",
      "Epoch [3701/10000] Avg train loss: 0.079063\n",
      "Epoch [3702/10000] Avg train loss: 0.079041\n",
      "Epoch [3703/10000] Avg train loss: 0.079020\n",
      "Epoch [3704/10000] Avg train loss: 0.078999\n",
      "Epoch [3705/10000] Avg train loss: 0.078977\n",
      "Epoch [3706/10000] Avg train loss: 0.078956\n",
      "Epoch [3707/10000] Avg train loss: 0.078935\n",
      "Epoch [3708/10000] Avg train loss: 0.078914\n",
      "Epoch [3709/10000] Avg train loss: 0.078893\n",
      "Epoch [3710/10000] Avg train loss: 0.078871\n",
      "Epoch [3711/10000] Avg train loss: 0.078850\n",
      "Epoch [3712/10000] Avg train loss: 0.078829\n",
      "Epoch [3713/10000] Avg train loss: 0.078808\n",
      "Epoch [3714/10000] Avg train loss: 0.078787\n",
      "Epoch [3715/10000] Avg train loss: 0.078766\n",
      "Epoch [3716/10000] Avg train loss: 0.078745\n",
      "Epoch [3717/10000] Avg train loss: 0.078723\n",
      "Epoch [3718/10000] Avg train loss: 0.078702\n",
      "Epoch [3719/10000] Avg train loss: 0.078681\n",
      "Epoch [3720/10000] Avg train loss: 0.078660\n",
      "Epoch [3721/10000] Avg train loss: 0.078639\n",
      "Epoch [3722/10000] Avg train loss: 0.078618\n",
      "Epoch [3723/10000] Avg train loss: 0.078597\n",
      "Epoch [3724/10000] Avg train loss: 0.078576\n",
      "Epoch [3725/10000] Avg train loss: 0.078555\n",
      "Epoch [3726/10000] Avg train loss: 0.078534\n",
      "Epoch [3727/10000] Avg train loss: 0.078513\n",
      "Epoch [3728/10000] Avg train loss: 0.078492\n",
      "Epoch [3729/10000] Avg train loss: 0.078471\n",
      "Epoch [3730/10000] Avg train loss: 0.078450\n",
      "Epoch [3731/10000] Avg train loss: 0.078429\n",
      "Epoch [3732/10000] Avg train loss: 0.078408\n",
      "Epoch [3733/10000] Avg train loss: 0.078387\n",
      "Epoch [3734/10000] Avg train loss: 0.078366\n",
      "Epoch [3735/10000] Avg train loss: 0.078345\n",
      "Epoch [3736/10000] Avg train loss: 0.078325\n",
      "Epoch [3737/10000] Avg train loss: 0.078304\n",
      "Epoch [3738/10000] Avg train loss: 0.078283\n",
      "Epoch [3739/10000] Avg train loss: 0.078262\n",
      "Epoch [3740/10000] Avg train loss: 0.078241\n",
      "Epoch [3741/10000] Avg train loss: 0.078220\n",
      "Epoch [3742/10000] Avg train loss: 0.078199\n",
      "Epoch [3743/10000] Avg train loss: 0.078179\n",
      "Epoch [3744/10000] Avg train loss: 0.078158\n",
      "Epoch [3745/10000] Avg train loss: 0.078137\n",
      "Epoch [3746/10000] Avg train loss: 0.078116\n",
      "Epoch [3747/10000] Avg train loss: 0.078096\n",
      "Epoch [3748/10000] Avg train loss: 0.078075\n",
      "Epoch [3749/10000] Avg train loss: 0.078054\n",
      "Epoch [3750/10000] Avg train loss: 0.078033\n",
      "Epoch [3751/10000] Avg train loss: 0.078013\n",
      "Epoch [3752/10000] Avg train loss: 0.077992\n",
      "Epoch [3753/10000] Avg train loss: 0.077971\n",
      "Epoch [3754/10000] Avg train loss: 0.077951\n",
      "Epoch [3755/10000] Avg train loss: 0.077930\n",
      "Epoch [3756/10000] Avg train loss: 0.077909\n",
      "Epoch [3757/10000] Avg train loss: 0.077889\n",
      "Epoch [3758/10000] Avg train loss: 0.077868\n",
      "Epoch [3759/10000] Avg train loss: 0.077847\n",
      "Epoch [3760/10000] Avg train loss: 0.077827\n",
      "Epoch [3761/10000] Avg train loss: 0.077806\n",
      "Epoch [3762/10000] Avg train loss: 0.077785\n",
      "Epoch [3763/10000] Avg train loss: 0.077765\n",
      "Epoch [3764/10000] Avg train loss: 0.077744\n",
      "Epoch [3765/10000] Avg train loss: 0.077724\n",
      "Epoch [3766/10000] Avg train loss: 0.077703\n",
      "Epoch [3767/10000] Avg train loss: 0.077683\n",
      "Epoch [3768/10000] Avg train loss: 0.077662\n",
      "Epoch [3769/10000] Avg train loss: 0.077641\n",
      "Epoch [3770/10000] Avg train loss: 0.077621\n",
      "Epoch [3771/10000] Avg train loss: 0.077600\n",
      "Epoch [3772/10000] Avg train loss: 0.077580\n",
      "Epoch [3773/10000] Avg train loss: 0.077559\n",
      "Epoch [3774/10000] Avg train loss: 0.077539\n",
      "Epoch [3775/10000] Avg train loss: 0.077519\n",
      "Epoch [3776/10000] Avg train loss: 0.077498\n",
      "Epoch [3777/10000] Avg train loss: 0.077478\n",
      "Epoch [3778/10000] Avg train loss: 0.077457\n",
      "Epoch [3779/10000] Avg train loss: 0.077437\n",
      "Epoch [3780/10000] Avg train loss: 0.077416\n",
      "Epoch [3781/10000] Avg train loss: 0.077396\n",
      "Epoch [3782/10000] Avg train loss: 0.077376\n",
      "Epoch [3783/10000] Avg train loss: 0.077355\n",
      "Epoch [3784/10000] Avg train loss: 0.077335\n",
      "Epoch [3785/10000] Avg train loss: 0.077314\n",
      "Epoch [3786/10000] Avg train loss: 0.077294\n",
      "Epoch [3787/10000] Avg train loss: 0.077274\n",
      "Epoch [3788/10000] Avg train loss: 0.077253\n",
      "Epoch [3789/10000] Avg train loss: 0.077233\n",
      "Epoch [3790/10000] Avg train loss: 0.077213\n",
      "Epoch [3791/10000] Avg train loss: 0.077193\n",
      "Epoch [3792/10000] Avg train loss: 0.077172\n",
      "Epoch [3793/10000] Avg train loss: 0.077152\n",
      "Epoch [3794/10000] Avg train loss: 0.077132\n",
      "Epoch [3795/10000] Avg train loss: 0.077111\n",
      "Epoch [3796/10000] Avg train loss: 0.077091\n",
      "Epoch [3797/10000] Avg train loss: 0.077071\n",
      "Epoch [3798/10000] Avg train loss: 0.077051\n",
      "Epoch [3799/10000] Avg train loss: 0.077031\n",
      "Epoch [3800/10000] Avg train loss: 0.077010\n",
      "Epoch [3801/10000] Avg train loss: 0.076990\n",
      "Epoch [3802/10000] Avg train loss: 0.076970\n",
      "Epoch [3803/10000] Avg train loss: 0.076950\n",
      "Epoch [3804/10000] Avg train loss: 0.076930\n",
      "Epoch [3805/10000] Avg train loss: 0.076910\n",
      "Epoch [3806/10000] Avg train loss: 0.076889\n",
      "Epoch [3807/10000] Avg train loss: 0.076869\n",
      "Epoch [3808/10000] Avg train loss: 0.076849\n",
      "Epoch [3809/10000] Avg train loss: 0.076829\n",
      "Epoch [3810/10000] Avg train loss: 0.076809\n",
      "Epoch [3811/10000] Avg train loss: 0.076789\n",
      "Epoch [3812/10000] Avg train loss: 0.076769\n",
      "Epoch [3813/10000] Avg train loss: 0.076749\n",
      "Epoch [3814/10000] Avg train loss: 0.076729\n",
      "Epoch [3815/10000] Avg train loss: 0.076709\n",
      "Epoch [3816/10000] Avg train loss: 0.076689\n",
      "Epoch [3817/10000] Avg train loss: 0.076669\n",
      "Epoch [3818/10000] Avg train loss: 0.076649\n",
      "Epoch [3819/10000] Avg train loss: 0.076629\n",
      "Epoch [3820/10000] Avg train loss: 0.076609\n",
      "Epoch [3821/10000] Avg train loss: 0.076589\n",
      "Epoch [3822/10000] Avg train loss: 0.076569\n",
      "Epoch [3823/10000] Avg train loss: 0.076549\n",
      "Epoch [3824/10000] Avg train loss: 0.076529\n",
      "Epoch [3825/10000] Avg train loss: 0.076509\n",
      "Epoch [3826/10000] Avg train loss: 0.076489\n",
      "Epoch [3827/10000] Avg train loss: 0.076469\n",
      "Epoch [3828/10000] Avg train loss: 0.076449\n",
      "Epoch [3829/10000] Avg train loss: 0.076429\n",
      "Epoch [3830/10000] Avg train loss: 0.076409\n",
      "Epoch [3831/10000] Avg train loss: 0.076389\n",
      "Epoch [3832/10000] Avg train loss: 0.076370\n",
      "Epoch [3833/10000] Avg train loss: 0.076350\n",
      "Epoch [3834/10000] Avg train loss: 0.076330\n",
      "Epoch [3835/10000] Avg train loss: 0.076310\n",
      "Epoch [3836/10000] Avg train loss: 0.076290\n",
      "Epoch [3837/10000] Avg train loss: 0.076270\n",
      "Epoch [3838/10000] Avg train loss: 0.076251\n",
      "Epoch [3839/10000] Avg train loss: 0.076231\n",
      "Epoch [3840/10000] Avg train loss: 0.076211\n",
      "Epoch [3841/10000] Avg train loss: 0.076191\n",
      "Epoch [3842/10000] Avg train loss: 0.076172\n",
      "Epoch [3843/10000] Avg train loss: 0.076152\n",
      "Epoch [3844/10000] Avg train loss: 0.076132\n",
      "Epoch [3845/10000] Avg train loss: 0.076112\n",
      "Epoch [3846/10000] Avg train loss: 0.076093\n",
      "Epoch [3847/10000] Avg train loss: 0.076073\n",
      "Epoch [3848/10000] Avg train loss: 0.076053\n",
      "Epoch [3849/10000] Avg train loss: 0.076034\n",
      "Epoch [3850/10000] Avg train loss: 0.076014\n",
      "Epoch [3851/10000] Avg train loss: 0.075994\n",
      "Epoch [3852/10000] Avg train loss: 0.075975\n",
      "Epoch [3853/10000] Avg train loss: 0.075955\n",
      "Epoch [3854/10000] Avg train loss: 0.075935\n",
      "Epoch [3855/10000] Avg train loss: 0.075916\n",
      "Epoch [3856/10000] Avg train loss: 0.075896\n",
      "Epoch [3857/10000] Avg train loss: 0.075876\n",
      "Epoch [3858/10000] Avg train loss: 0.075857\n",
      "Epoch [3859/10000] Avg train loss: 0.075837\n",
      "Epoch [3860/10000] Avg train loss: 0.075818\n",
      "Epoch [3861/10000] Avg train loss: 0.075798\n",
      "Epoch [3862/10000] Avg train loss: 0.075779\n",
      "Epoch [3863/10000] Avg train loss: 0.075759\n",
      "Epoch [3864/10000] Avg train loss: 0.075739\n",
      "Epoch [3865/10000] Avg train loss: 0.075720\n",
      "Epoch [3866/10000] Avg train loss: 0.075700\n",
      "Epoch [3867/10000] Avg train loss: 0.075681\n",
      "Epoch [3868/10000] Avg train loss: 0.075661\n",
      "Epoch [3869/10000] Avg train loss: 0.075642\n",
      "Epoch [3870/10000] Avg train loss: 0.075622\n",
      "Epoch [3871/10000] Avg train loss: 0.075603\n",
      "Epoch [3872/10000] Avg train loss: 0.075584\n",
      "Epoch [3873/10000] Avg train loss: 0.075564\n",
      "Epoch [3874/10000] Avg train loss: 0.075545\n",
      "Epoch [3875/10000] Avg train loss: 0.075525\n",
      "Epoch [3876/10000] Avg train loss: 0.075506\n",
      "Epoch [3877/10000] Avg train loss: 0.075486\n",
      "Epoch [3878/10000] Avg train loss: 0.075467\n",
      "Epoch [3879/10000] Avg train loss: 0.075448\n",
      "Epoch [3880/10000] Avg train loss: 0.075428\n",
      "Epoch [3881/10000] Avg train loss: 0.075409\n",
      "Epoch [3882/10000] Avg train loss: 0.075390\n",
      "Epoch [3883/10000] Avg train loss: 0.075370\n",
      "Epoch [3884/10000] Avg train loss: 0.075351\n",
      "Epoch [3885/10000] Avg train loss: 0.075332\n",
      "Epoch [3886/10000] Avg train loss: 0.075312\n",
      "Epoch [3887/10000] Avg train loss: 0.075293\n",
      "Epoch [3888/10000] Avg train loss: 0.075274\n",
      "Epoch [3889/10000] Avg train loss: 0.075254\n",
      "Epoch [3890/10000] Avg train loss: 0.075235\n",
      "Epoch [3891/10000] Avg train loss: 0.075216\n",
      "Epoch [3892/10000] Avg train loss: 0.075197\n",
      "Epoch [3893/10000] Avg train loss: 0.075177\n",
      "Epoch [3894/10000] Avg train loss: 0.075158\n",
      "Epoch [3895/10000] Avg train loss: 0.075139\n",
      "Epoch [3896/10000] Avg train loss: 0.075120\n",
      "Epoch [3897/10000] Avg train loss: 0.075100\n",
      "Epoch [3898/10000] Avg train loss: 0.075081\n",
      "Epoch [3899/10000] Avg train loss: 0.075062\n",
      "Epoch [3900/10000] Avg train loss: 0.075043\n",
      "Epoch [3901/10000] Avg train loss: 0.075024\n",
      "Epoch [3902/10000] Avg train loss: 0.075004\n",
      "Epoch [3903/10000] Avg train loss: 0.074985\n",
      "Epoch [3904/10000] Avg train loss: 0.074966\n",
      "Epoch [3905/10000] Avg train loss: 0.074947\n",
      "Epoch [3906/10000] Avg train loss: 0.074928\n",
      "Epoch [3907/10000] Avg train loss: 0.074909\n",
      "Epoch [3908/10000] Avg train loss: 0.074890\n",
      "Epoch [3909/10000] Avg train loss: 0.074871\n",
      "Epoch [3910/10000] Avg train loss: 0.074852\n",
      "Epoch [3911/10000] Avg train loss: 0.074832\n",
      "Epoch [3912/10000] Avg train loss: 0.074813\n",
      "Epoch [3913/10000] Avg train loss: 0.074794\n",
      "Epoch [3914/10000] Avg train loss: 0.074775\n",
      "Epoch [3915/10000] Avg train loss: 0.074756\n",
      "Epoch [3916/10000] Avg train loss: 0.074737\n",
      "Epoch [3917/10000] Avg train loss: 0.074718\n",
      "Epoch [3918/10000] Avg train loss: 0.074699\n",
      "Epoch [3919/10000] Avg train loss: 0.074680\n",
      "Epoch [3920/10000] Avg train loss: 0.074661\n",
      "Epoch [3921/10000] Avg train loss: 0.074642\n",
      "Epoch [3922/10000] Avg train loss: 0.074623\n",
      "Epoch [3923/10000] Avg train loss: 0.074604\n",
      "Epoch [3924/10000] Avg train loss: 0.074585\n",
      "Epoch [3925/10000] Avg train loss: 0.074567\n",
      "Epoch [3926/10000] Avg train loss: 0.074548\n",
      "Epoch [3927/10000] Avg train loss: 0.074529\n",
      "Epoch [3928/10000] Avg train loss: 0.074510\n",
      "Epoch [3929/10000] Avg train loss: 0.074491\n",
      "Epoch [3930/10000] Avg train loss: 0.074472\n",
      "Epoch [3931/10000] Avg train loss: 0.074453\n",
      "Epoch [3932/10000] Avg train loss: 0.074434\n",
      "Epoch [3933/10000] Avg train loss: 0.074415\n",
      "Epoch [3934/10000] Avg train loss: 0.074397\n",
      "Epoch [3935/10000] Avg train loss: 0.074378\n",
      "Epoch [3936/10000] Avg train loss: 0.074359\n",
      "Epoch [3937/10000] Avg train loss: 0.074340\n",
      "Epoch [3938/10000] Avg train loss: 0.074321\n",
      "Epoch [3939/10000] Avg train loss: 0.074302\n",
      "Epoch [3940/10000] Avg train loss: 0.074284\n",
      "Epoch [3941/10000] Avg train loss: 0.074265\n",
      "Epoch [3942/10000] Avg train loss: 0.074246\n",
      "Epoch [3943/10000] Avg train loss: 0.074227\n",
      "Epoch [3944/10000] Avg train loss: 0.074209\n",
      "Epoch [3945/10000] Avg train loss: 0.074190\n",
      "Epoch [3946/10000] Avg train loss: 0.074171\n",
      "Epoch [3947/10000] Avg train loss: 0.074152\n",
      "Epoch [3948/10000] Avg train loss: 0.074134\n",
      "Epoch [3949/10000] Avg train loss: 0.074115\n",
      "Epoch [3950/10000] Avg train loss: 0.074096\n",
      "Epoch [3951/10000] Avg train loss: 0.074078\n",
      "Epoch [3952/10000] Avg train loss: 0.074059\n",
      "Epoch [3953/10000] Avg train loss: 0.074040\n",
      "Epoch [3954/10000] Avg train loss: 0.074022\n",
      "Epoch [3955/10000] Avg train loss: 0.074003\n",
      "Epoch [3956/10000] Avg train loss: 0.073984\n",
      "Epoch [3957/10000] Avg train loss: 0.073966\n",
      "Epoch [3958/10000] Avg train loss: 0.073947\n",
      "Epoch [3959/10000] Avg train loss: 0.073928\n",
      "Epoch [3960/10000] Avg train loss: 0.073910\n",
      "Epoch [3961/10000] Avg train loss: 0.073891\n",
      "Epoch [3962/10000] Avg train loss: 0.073873\n",
      "Epoch [3963/10000] Avg train loss: 0.073854\n",
      "Epoch [3964/10000] Avg train loss: 0.073836\n",
      "Epoch [3965/10000] Avg train loss: 0.073817\n",
      "Epoch [3966/10000] Avg train loss: 0.073798\n",
      "Epoch [3967/10000] Avg train loss: 0.073780\n",
      "Epoch [3968/10000] Avg train loss: 0.073761\n",
      "Epoch [3969/10000] Avg train loss: 0.073743\n",
      "Epoch [3970/10000] Avg train loss: 0.073724\n",
      "Epoch [3971/10000] Avg train loss: 0.073706\n",
      "Epoch [3972/10000] Avg train loss: 0.073687\n",
      "Epoch [3973/10000] Avg train loss: 0.073669\n",
      "Epoch [3974/10000] Avg train loss: 0.073650\n",
      "Epoch [3975/10000] Avg train loss: 0.073632\n",
      "Epoch [3976/10000] Avg train loss: 0.073614\n",
      "Epoch [3977/10000] Avg train loss: 0.073595\n",
      "Epoch [3978/10000] Avg train loss: 0.073577\n",
      "Epoch [3979/10000] Avg train loss: 0.073558\n",
      "Epoch [3980/10000] Avg train loss: 0.073540\n",
      "Epoch [3981/10000] Avg train loss: 0.073521\n",
      "Epoch [3982/10000] Avg train loss: 0.073503\n",
      "Epoch [3983/10000] Avg train loss: 0.073485\n",
      "Epoch [3984/10000] Avg train loss: 0.073466\n",
      "Epoch [3985/10000] Avg train loss: 0.073448\n",
      "Epoch [3986/10000] Avg train loss: 0.073429\n",
      "Epoch [3987/10000] Avg train loss: 0.073411\n",
      "Epoch [3988/10000] Avg train loss: 0.073393\n",
      "Epoch [3989/10000] Avg train loss: 0.073374\n",
      "Epoch [3990/10000] Avg train loss: 0.073356\n",
      "Epoch [3991/10000] Avg train loss: 0.073338\n",
      "Epoch [3992/10000] Avg train loss: 0.073319\n",
      "Epoch [3993/10000] Avg train loss: 0.073301\n",
      "Epoch [3994/10000] Avg train loss: 0.073283\n",
      "Epoch [3995/10000] Avg train loss: 0.073265\n",
      "Epoch [3996/10000] Avg train loss: 0.073246\n",
      "Epoch [3997/10000] Avg train loss: 0.073228\n",
      "Epoch [3998/10000] Avg train loss: 0.073210\n",
      "Epoch [3999/10000] Avg train loss: 0.073192\n",
      "Epoch [4000/10000] Avg train loss: 0.073173\n",
      "Epoch [4001/10000] Avg train loss: 0.073155\n",
      "Epoch [4002/10000] Avg train loss: 0.073137\n",
      "Epoch [4003/10000] Avg train loss: 0.073119\n",
      "Epoch [4004/10000] Avg train loss: 0.073101\n",
      "Epoch [4005/10000] Avg train loss: 0.073082\n",
      "Epoch [4006/10000] Avg train loss: 0.073064\n",
      "Epoch [4007/10000] Avg train loss: 0.073046\n",
      "Epoch [4008/10000] Avg train loss: 0.073028\n",
      "Epoch [4009/10000] Avg train loss: 0.073010\n",
      "Epoch [4010/10000] Avg train loss: 0.072992\n",
      "Epoch [4011/10000] Avg train loss: 0.072973\n",
      "Epoch [4012/10000] Avg train loss: 0.072955\n",
      "Epoch [4013/10000] Avg train loss: 0.072937\n",
      "Epoch [4014/10000] Avg train loss: 0.072919\n",
      "Epoch [4015/10000] Avg train loss: 0.072901\n",
      "Epoch [4016/10000] Avg train loss: 0.072883\n",
      "Epoch [4017/10000] Avg train loss: 0.072865\n",
      "Epoch [4018/10000] Avg train loss: 0.072847\n",
      "Epoch [4019/10000] Avg train loss: 0.072829\n",
      "Epoch [4020/10000] Avg train loss: 0.072811\n",
      "Epoch [4021/10000] Avg train loss: 0.072793\n",
      "Epoch [4022/10000] Avg train loss: 0.072775\n",
      "Epoch [4023/10000] Avg train loss: 0.072756\n",
      "Epoch [4024/10000] Avg train loss: 0.072738\n",
      "Epoch [4025/10000] Avg train loss: 0.072720\n",
      "Epoch [4026/10000] Avg train loss: 0.072702\n",
      "Epoch [4027/10000] Avg train loss: 0.072684\n",
      "Epoch [4028/10000] Avg train loss: 0.072667\n",
      "Epoch [4029/10000] Avg train loss: 0.072649\n",
      "Epoch [4030/10000] Avg train loss: 0.072631\n",
      "Epoch [4031/10000] Avg train loss: 0.072613\n",
      "Epoch [4032/10000] Avg train loss: 0.072595\n",
      "Epoch [4033/10000] Avg train loss: 0.072577\n",
      "Epoch [4034/10000] Avg train loss: 0.072559\n",
      "Epoch [4035/10000] Avg train loss: 0.072541\n",
      "Epoch [4036/10000] Avg train loss: 0.072523\n",
      "Epoch [4037/10000] Avg train loss: 0.072505\n",
      "Epoch [4038/10000] Avg train loss: 0.072487\n",
      "Epoch [4039/10000] Avg train loss: 0.072469\n",
      "Epoch [4040/10000] Avg train loss: 0.072451\n",
      "Epoch [4041/10000] Avg train loss: 0.072434\n",
      "Epoch [4042/10000] Avg train loss: 0.072416\n",
      "Epoch [4043/10000] Avg train loss: 0.072398\n",
      "Epoch [4044/10000] Avg train loss: 0.072380\n",
      "Epoch [4045/10000] Avg train loss: 0.072362\n",
      "Epoch [4046/10000] Avg train loss: 0.072344\n",
      "Epoch [4047/10000] Avg train loss: 0.072327\n",
      "Epoch [4048/10000] Avg train loss: 0.072309\n",
      "Epoch [4049/10000] Avg train loss: 0.072291\n",
      "Epoch [4050/10000] Avg train loss: 0.072273\n",
      "Epoch [4051/10000] Avg train loss: 0.072255\n",
      "Epoch [4052/10000] Avg train loss: 0.072238\n",
      "Epoch [4053/10000] Avg train loss: 0.072220\n",
      "Epoch [4054/10000] Avg train loss: 0.072202\n",
      "Epoch [4055/10000] Avg train loss: 0.072184\n",
      "Epoch [4056/10000] Avg train loss: 0.072167\n",
      "Epoch [4057/10000] Avg train loss: 0.072149\n",
      "Epoch [4058/10000] Avg train loss: 0.072131\n",
      "Epoch [4059/10000] Avg train loss: 0.072114\n",
      "Epoch [4060/10000] Avg train loss: 0.072096\n",
      "Epoch [4061/10000] Avg train loss: 0.072078\n",
      "Epoch [4062/10000] Avg train loss: 0.072060\n",
      "Epoch [4063/10000] Avg train loss: 0.072043\n",
      "Epoch [4064/10000] Avg train loss: 0.072025\n",
      "Epoch [4065/10000] Avg train loss: 0.072007\n",
      "Epoch [4066/10000] Avg train loss: 0.071990\n",
      "Epoch [4067/10000] Avg train loss: 0.071972\n",
      "Epoch [4068/10000] Avg train loss: 0.071955\n",
      "Epoch [4069/10000] Avg train loss: 0.071937\n",
      "Epoch [4070/10000] Avg train loss: 0.071919\n",
      "Epoch [4071/10000] Avg train loss: 0.071902\n",
      "Epoch [4072/10000] Avg train loss: 0.071884\n",
      "Epoch [4073/10000] Avg train loss: 0.071867\n",
      "Epoch [4074/10000] Avg train loss: 0.071849\n",
      "Epoch [4075/10000] Avg train loss: 0.071831\n",
      "Epoch [4076/10000] Avg train loss: 0.071814\n",
      "Epoch [4077/10000] Avg train loss: 0.071796\n",
      "Epoch [4078/10000] Avg train loss: 0.071779\n",
      "Epoch [4079/10000] Avg train loss: 0.071761\n",
      "Epoch [4080/10000] Avg train loss: 0.071744\n",
      "Epoch [4081/10000] Avg train loss: 0.071726\n",
      "Epoch [4082/10000] Avg train loss: 0.071709\n",
      "Epoch [4083/10000] Avg train loss: 0.071691\n",
      "Epoch [4084/10000] Avg train loss: 0.071674\n",
      "Epoch [4085/10000] Avg train loss: 0.071656\n",
      "Epoch [4086/10000] Avg train loss: 0.071639\n",
      "Epoch [4087/10000] Avg train loss: 0.071621\n",
      "Epoch [4088/10000] Avg train loss: 0.071604\n",
      "Epoch [4089/10000] Avg train loss: 0.071586\n",
      "Epoch [4090/10000] Avg train loss: 0.071569\n",
      "Epoch [4091/10000] Avg train loss: 0.071551\n",
      "Epoch [4092/10000] Avg train loss: 0.071534\n",
      "Epoch [4093/10000] Avg train loss: 0.071517\n",
      "Epoch [4094/10000] Avg train loss: 0.071499\n",
      "Epoch [4095/10000] Avg train loss: 0.071482\n",
      "Epoch [4096/10000] Avg train loss: 0.071464\n",
      "Epoch [4097/10000] Avg train loss: 0.071447\n",
      "Epoch [4098/10000] Avg train loss: 0.071430\n",
      "Epoch [4099/10000] Avg train loss: 0.071412\n",
      "Epoch [4100/10000] Avg train loss: 0.071395\n",
      "Epoch [4101/10000] Avg train loss: 0.071378\n",
      "Epoch [4102/10000] Avg train loss: 0.071360\n",
      "Epoch [4103/10000] Avg train loss: 0.071343\n",
      "Epoch [4104/10000] Avg train loss: 0.071326\n",
      "Epoch [4105/10000] Avg train loss: 0.071308\n",
      "Epoch [4106/10000] Avg train loss: 0.071291\n",
      "Epoch [4107/10000] Avg train loss: 0.071274\n",
      "Epoch [4108/10000] Avg train loss: 0.071256\n",
      "Epoch [4109/10000] Avg train loss: 0.071239\n",
      "Epoch [4110/10000] Avg train loss: 0.071222\n",
      "Epoch [4111/10000] Avg train loss: 0.071205\n",
      "Epoch [4112/10000] Avg train loss: 0.071187\n",
      "Epoch [4113/10000] Avg train loss: 0.071170\n",
      "Epoch [4114/10000] Avg train loss: 0.071153\n",
      "Epoch [4115/10000] Avg train loss: 0.071136\n",
      "Epoch [4116/10000] Avg train loss: 0.071118\n",
      "Epoch [4117/10000] Avg train loss: 0.071101\n",
      "Epoch [4118/10000] Avg train loss: 0.071084\n",
      "Epoch [4119/10000] Avg train loss: 0.071067\n",
      "Epoch [4120/10000] Avg train loss: 0.071050\n",
      "Epoch [4121/10000] Avg train loss: 0.071032\n",
      "Epoch [4122/10000] Avg train loss: 0.071015\n",
      "Epoch [4123/10000] Avg train loss: 0.070998\n",
      "Epoch [4124/10000] Avg train loss: 0.070981\n",
      "Epoch [4125/10000] Avg train loss: 0.070964\n",
      "Epoch [4126/10000] Avg train loss: 0.070947\n",
      "Epoch [4127/10000] Avg train loss: 0.070930\n",
      "Epoch [4128/10000] Avg train loss: 0.070912\n",
      "Epoch [4129/10000] Avg train loss: 0.070895\n",
      "Epoch [4130/10000] Avg train loss: 0.070878\n",
      "Epoch [4131/10000] Avg train loss: 0.070861\n",
      "Epoch [4132/10000] Avg train loss: 0.070844\n",
      "Epoch [4133/10000] Avg train loss: 0.070827\n",
      "Epoch [4134/10000] Avg train loss: 0.070810\n",
      "Epoch [4135/10000] Avg train loss: 0.070793\n",
      "Epoch [4136/10000] Avg train loss: 0.070776\n",
      "Epoch [4137/10000] Avg train loss: 0.070759\n",
      "Epoch [4138/10000] Avg train loss: 0.070742\n",
      "Epoch [4139/10000] Avg train loss: 0.070725\n",
      "Epoch [4140/10000] Avg train loss: 0.070708\n",
      "Epoch [4141/10000] Avg train loss: 0.070691\n",
      "Epoch [4142/10000] Avg train loss: 0.070674\n",
      "Epoch [4143/10000] Avg train loss: 0.070657\n",
      "Epoch [4144/10000] Avg train loss: 0.070640\n",
      "Epoch [4145/10000] Avg train loss: 0.070623\n",
      "Epoch [4146/10000] Avg train loss: 0.070606\n",
      "Epoch [4147/10000] Avg train loss: 0.070589\n",
      "Epoch [4148/10000] Avg train loss: 0.070572\n",
      "Epoch [4149/10000] Avg train loss: 0.070555\n",
      "Epoch [4150/10000] Avg train loss: 0.070538\n",
      "Epoch [4151/10000] Avg train loss: 0.070521\n",
      "Epoch [4152/10000] Avg train loss: 0.070504\n",
      "Epoch [4153/10000] Avg train loss: 0.070487\n",
      "Epoch [4154/10000] Avg train loss: 0.070470\n",
      "Epoch [4155/10000] Avg train loss: 0.070453\n",
      "Epoch [4156/10000] Avg train loss: 0.070436\n",
      "Epoch [4157/10000] Avg train loss: 0.070420\n",
      "Epoch [4158/10000] Avg train loss: 0.070403\n",
      "Epoch [4159/10000] Avg train loss: 0.070386\n",
      "Epoch [4160/10000] Avg train loss: 0.070369\n",
      "Epoch [4161/10000] Avg train loss: 0.070352\n",
      "Epoch [4162/10000] Avg train loss: 0.070335\n",
      "Epoch [4163/10000] Avg train loss: 0.070318\n",
      "Epoch [4164/10000] Avg train loss: 0.070302\n",
      "Epoch [4165/10000] Avg train loss: 0.070285\n",
      "Epoch [4166/10000] Avg train loss: 0.070268\n",
      "Epoch [4167/10000] Avg train loss: 0.070251\n",
      "Epoch [4168/10000] Avg train loss: 0.070234\n",
      "Epoch [4169/10000] Avg train loss: 0.070218\n",
      "Epoch [4170/10000] Avg train loss: 0.070201\n",
      "Epoch [4171/10000] Avg train loss: 0.070184\n",
      "Epoch [4172/10000] Avg train loss: 0.070167\n",
      "Epoch [4173/10000] Avg train loss: 0.070151\n",
      "Epoch [4174/10000] Avg train loss: 0.070134\n",
      "Epoch [4175/10000] Avg train loss: 0.070117\n",
      "Epoch [4176/10000] Avg train loss: 0.070100\n",
      "Epoch [4177/10000] Avg train loss: 0.070084\n",
      "Epoch [4178/10000] Avg train loss: 0.070067\n",
      "Epoch [4179/10000] Avg train loss: 0.070050\n",
      "Epoch [4180/10000] Avg train loss: 0.070033\n",
      "Epoch [4181/10000] Avg train loss: 0.070017\n",
      "Epoch [4182/10000] Avg train loss: 0.070000\n",
      "Epoch [4183/10000] Avg train loss: 0.069983\n",
      "Epoch [4184/10000] Avg train loss: 0.069967\n",
      "Epoch [4185/10000] Avg train loss: 0.069950\n",
      "Epoch [4186/10000] Avg train loss: 0.069933\n",
      "Epoch [4187/10000] Avg train loss: 0.069917\n",
      "Epoch [4188/10000] Avg train loss: 0.069900\n",
      "Epoch [4189/10000] Avg train loss: 0.069883\n",
      "Epoch [4190/10000] Avg train loss: 0.069867\n",
      "Epoch [4191/10000] Avg train loss: 0.069850\n",
      "Epoch [4192/10000] Avg train loss: 0.069834\n",
      "Epoch [4193/10000] Avg train loss: 0.069817\n",
      "Epoch [4194/10000] Avg train loss: 0.069800\n",
      "Epoch [4195/10000] Avg train loss: 0.069784\n",
      "Epoch [4196/10000] Avg train loss: 0.069767\n",
      "Epoch [4197/10000] Avg train loss: 0.069751\n",
      "Epoch [4198/10000] Avg train loss: 0.069734\n",
      "Epoch [4199/10000] Avg train loss: 0.069718\n",
      "Epoch [4200/10000] Avg train loss: 0.069701\n",
      "Epoch [4201/10000] Avg train loss: 0.069685\n",
      "Epoch [4202/10000] Avg train loss: 0.069668\n",
      "Epoch [4203/10000] Avg train loss: 0.069651\n",
      "Epoch [4204/10000] Avg train loss: 0.069635\n",
      "Epoch [4205/10000] Avg train loss: 0.069618\n",
      "Epoch [4206/10000] Avg train loss: 0.069602\n",
      "Epoch [4207/10000] Avg train loss: 0.069585\n",
      "Epoch [4208/10000] Avg train loss: 0.069569\n",
      "Epoch [4209/10000] Avg train loss: 0.069553\n",
      "Epoch [4210/10000] Avg train loss: 0.069536\n",
      "Epoch [4211/10000] Avg train loss: 0.069520\n",
      "Epoch [4212/10000] Avg train loss: 0.069503\n",
      "Epoch [4213/10000] Avg train loss: 0.069487\n",
      "Epoch [4214/10000] Avg train loss: 0.069470\n",
      "Epoch [4215/10000] Avg train loss: 0.069454\n",
      "Epoch [4216/10000] Avg train loss: 0.069437\n",
      "Epoch [4217/10000] Avg train loss: 0.069421\n",
      "Epoch [4218/10000] Avg train loss: 0.069405\n",
      "Epoch [4219/10000] Avg train loss: 0.069388\n",
      "Epoch [4220/10000] Avg train loss: 0.069372\n",
      "Epoch [4221/10000] Avg train loss: 0.069356\n",
      "Epoch [4222/10000] Avg train loss: 0.069339\n",
      "Epoch [4223/10000] Avg train loss: 0.069323\n",
      "Epoch [4224/10000] Avg train loss: 0.069306\n",
      "Epoch [4225/10000] Avg train loss: 0.069290\n",
      "Epoch [4226/10000] Avg train loss: 0.069274\n",
      "Epoch [4227/10000] Avg train loss: 0.069257\n",
      "Epoch [4228/10000] Avg train loss: 0.069241\n",
      "Epoch [4229/10000] Avg train loss: 0.069225\n",
      "Epoch [4230/10000] Avg train loss: 0.069208\n",
      "Epoch [4231/10000] Avg train loss: 0.069192\n",
      "Epoch [4232/10000] Avg train loss: 0.069176\n",
      "Epoch [4233/10000] Avg train loss: 0.069160\n",
      "Epoch [4234/10000] Avg train loss: 0.069143\n",
      "Epoch [4235/10000] Avg train loss: 0.069127\n",
      "Epoch [4236/10000] Avg train loss: 0.069111\n",
      "Epoch [4237/10000] Avg train loss: 0.069095\n",
      "Epoch [4238/10000] Avg train loss: 0.069078\n",
      "Epoch [4239/10000] Avg train loss: 0.069062\n",
      "Epoch [4240/10000] Avg train loss: 0.069046\n",
      "Epoch [4241/10000] Avg train loss: 0.069030\n",
      "Epoch [4242/10000] Avg train loss: 0.069013\n",
      "Epoch [4243/10000] Avg train loss: 0.068997\n",
      "Epoch [4244/10000] Avg train loss: 0.068981\n",
      "Epoch [4245/10000] Avg train loss: 0.068965\n",
      "Epoch [4246/10000] Avg train loss: 0.068949\n",
      "Epoch [4247/10000] Avg train loss: 0.068932\n",
      "Epoch [4248/10000] Avg train loss: 0.068916\n",
      "Epoch [4249/10000] Avg train loss: 0.068900\n",
      "Epoch [4250/10000] Avg train loss: 0.068884\n",
      "Epoch [4251/10000] Avg train loss: 0.068868\n",
      "Epoch [4252/10000] Avg train loss: 0.068852\n",
      "Epoch [4253/10000] Avg train loss: 0.068835\n",
      "Epoch [4254/10000] Avg train loss: 0.068819\n",
      "Epoch [4255/10000] Avg train loss: 0.068803\n",
      "Epoch [4256/10000] Avg train loss: 0.068787\n",
      "Epoch [4257/10000] Avg train loss: 0.068771\n",
      "Epoch [4258/10000] Avg train loss: 0.068755\n",
      "Epoch [4259/10000] Avg train loss: 0.068739\n",
      "Epoch [4260/10000] Avg train loss: 0.068723\n",
      "Epoch [4261/10000] Avg train loss: 0.068707\n",
      "Epoch [4262/10000] Avg train loss: 0.068691\n",
      "Epoch [4263/10000] Avg train loss: 0.068675\n",
      "Epoch [4264/10000] Avg train loss: 0.068658\n",
      "Epoch [4265/10000] Avg train loss: 0.068642\n",
      "Epoch [4266/10000] Avg train loss: 0.068626\n",
      "Epoch [4267/10000] Avg train loss: 0.068610\n",
      "Epoch [4268/10000] Avg train loss: 0.068594\n",
      "Epoch [4269/10000] Avg train loss: 0.068578\n",
      "Epoch [4270/10000] Avg train loss: 0.068562\n",
      "Epoch [4271/10000] Avg train loss: 0.068546\n",
      "Epoch [4272/10000] Avg train loss: 0.068530\n",
      "Epoch [4273/10000] Avg train loss: 0.068514\n",
      "Epoch [4274/10000] Avg train loss: 0.068498\n",
      "Epoch [4275/10000] Avg train loss: 0.068482\n",
      "Epoch [4276/10000] Avg train loss: 0.068466\n",
      "Epoch [4277/10000] Avg train loss: 0.068450\n",
      "Epoch [4278/10000] Avg train loss: 0.068435\n",
      "Epoch [4279/10000] Avg train loss: 0.068419\n",
      "Epoch [4280/10000] Avg train loss: 0.068403\n",
      "Epoch [4281/10000] Avg train loss: 0.068387\n",
      "Epoch [4282/10000] Avg train loss: 0.068371\n",
      "Epoch [4283/10000] Avg train loss: 0.068355\n",
      "Epoch [4284/10000] Avg train loss: 0.068339\n",
      "Epoch [4285/10000] Avg train loss: 0.068323\n",
      "Epoch [4286/10000] Avg train loss: 0.068307\n",
      "Epoch [4287/10000] Avg train loss: 0.068291\n",
      "Epoch [4288/10000] Avg train loss: 0.068275\n",
      "Epoch [4289/10000] Avg train loss: 0.068260\n",
      "Epoch [4290/10000] Avg train loss: 0.068244\n",
      "Epoch [4291/10000] Avg train loss: 0.068228\n",
      "Epoch [4292/10000] Avg train loss: 0.068212\n",
      "Epoch [4293/10000] Avg train loss: 0.068196\n",
      "Epoch [4294/10000] Avg train loss: 0.068180\n",
      "Epoch [4295/10000] Avg train loss: 0.068165\n",
      "Epoch [4296/10000] Avg train loss: 0.068149\n",
      "Epoch [4297/10000] Avg train loss: 0.068133\n",
      "Epoch [4298/10000] Avg train loss: 0.068117\n",
      "Epoch [4299/10000] Avg train loss: 0.068101\n",
      "Epoch [4300/10000] Avg train loss: 0.068086\n",
      "Epoch [4301/10000] Avg train loss: 0.068070\n",
      "Epoch [4302/10000] Avg train loss: 0.068054\n",
      "Epoch [4303/10000] Avg train loss: 0.068038\n",
      "Epoch [4304/10000] Avg train loss: 0.068022\n",
      "Epoch [4305/10000] Avg train loss: 0.068007\n",
      "Epoch [4306/10000] Avg train loss: 0.067991\n",
      "Epoch [4307/10000] Avg train loss: 0.067975\n",
      "Epoch [4308/10000] Avg train loss: 0.067960\n",
      "Epoch [4309/10000] Avg train loss: 0.067944\n",
      "Epoch [4310/10000] Avg train loss: 0.067928\n",
      "Epoch [4311/10000] Avg train loss: 0.067912\n",
      "Epoch [4312/10000] Avg train loss: 0.067897\n",
      "Epoch [4313/10000] Avg train loss: 0.067881\n",
      "Epoch [4314/10000] Avg train loss: 0.067865\n",
      "Epoch [4315/10000] Avg train loss: 0.067850\n",
      "Epoch [4316/10000] Avg train loss: 0.067834\n",
      "Epoch [4317/10000] Avg train loss: 0.067818\n",
      "Epoch [4318/10000] Avg train loss: 0.067803\n",
      "Epoch [4319/10000] Avg train loss: 0.067787\n",
      "Epoch [4320/10000] Avg train loss: 0.067771\n",
      "Epoch [4321/10000] Avg train loss: 0.067756\n",
      "Epoch [4322/10000] Avg train loss: 0.067740\n",
      "Epoch [4323/10000] Avg train loss: 0.067725\n",
      "Epoch [4324/10000] Avg train loss: 0.067709\n",
      "Epoch [4325/10000] Avg train loss: 0.067693\n",
      "Epoch [4326/10000] Avg train loss: 0.067678\n",
      "Epoch [4327/10000] Avg train loss: 0.067662\n",
      "Epoch [4328/10000] Avg train loss: 0.067647\n",
      "Epoch [4329/10000] Avg train loss: 0.067631\n",
      "Epoch [4330/10000] Avg train loss: 0.067616\n",
      "Epoch [4331/10000] Avg train loss: 0.067600\n",
      "Epoch [4332/10000] Avg train loss: 0.067584\n",
      "Epoch [4333/10000] Avg train loss: 0.067569\n",
      "Epoch [4334/10000] Avg train loss: 0.067553\n",
      "Epoch [4335/10000] Avg train loss: 0.067538\n",
      "Epoch [4336/10000] Avg train loss: 0.067522\n",
      "Epoch [4337/10000] Avg train loss: 0.067507\n",
      "Epoch [4338/10000] Avg train loss: 0.067491\n",
      "Epoch [4339/10000] Avg train loss: 0.067476\n",
      "Epoch [4340/10000] Avg train loss: 0.067460\n",
      "Epoch [4341/10000] Avg train loss: 0.067445\n",
      "Epoch [4342/10000] Avg train loss: 0.067429\n",
      "Epoch [4343/10000] Avg train loss: 0.067414\n",
      "Epoch [4344/10000] Avg train loss: 0.067398\n",
      "Epoch [4345/10000] Avg train loss: 0.067383\n",
      "Epoch [4346/10000] Avg train loss: 0.067367\n",
      "Epoch [4347/10000] Avg train loss: 0.067352\n",
      "Epoch [4348/10000] Avg train loss: 0.067337\n",
      "Epoch [4349/10000] Avg train loss: 0.067321\n",
      "Epoch [4350/10000] Avg train loss: 0.067306\n",
      "Epoch [4351/10000] Avg train loss: 0.067290\n",
      "Epoch [4352/10000] Avg train loss: 0.067275\n",
      "Epoch [4353/10000] Avg train loss: 0.067260\n",
      "Epoch [4354/10000] Avg train loss: 0.067244\n",
      "Epoch [4355/10000] Avg train loss: 0.067229\n",
      "Epoch [4356/10000] Avg train loss: 0.067213\n",
      "Epoch [4357/10000] Avg train loss: 0.067198\n",
      "Epoch [4358/10000] Avg train loss: 0.067183\n",
      "Epoch [4359/10000] Avg train loss: 0.067167\n",
      "Epoch [4360/10000] Avg train loss: 0.067152\n",
      "Epoch [4361/10000] Avg train loss: 0.067137\n",
      "Epoch [4362/10000] Avg train loss: 0.067121\n",
      "Epoch [4363/10000] Avg train loss: 0.067106\n",
      "Epoch [4364/10000] Avg train loss: 0.067091\n",
      "Epoch [4365/10000] Avg train loss: 0.067075\n",
      "Epoch [4366/10000] Avg train loss: 0.067060\n",
      "Epoch [4367/10000] Avg train loss: 0.067045\n",
      "Epoch [4368/10000] Avg train loss: 0.067029\n",
      "Epoch [4369/10000] Avg train loss: 0.067014\n",
      "Epoch [4370/10000] Avg train loss: 0.066999\n",
      "Epoch [4371/10000] Avg train loss: 0.066983\n",
      "Epoch [4372/10000] Avg train loss: 0.066968\n",
      "Epoch [4373/10000] Avg train loss: 0.066953\n",
      "Epoch [4374/10000] Avg train loss: 0.066938\n",
      "Epoch [4375/10000] Avg train loss: 0.066922\n",
      "Epoch [4376/10000] Avg train loss: 0.066907\n",
      "Epoch [4377/10000] Avg train loss: 0.066892\n",
      "Epoch [4378/10000] Avg train loss: 0.066877\n",
      "Epoch [4379/10000] Avg train loss: 0.066862\n",
      "Epoch [4380/10000] Avg train loss: 0.066846\n",
      "Epoch [4381/10000] Avg train loss: 0.066831\n",
      "Epoch [4382/10000] Avg train loss: 0.066816\n",
      "Epoch [4383/10000] Avg train loss: 0.066801\n",
      "Epoch [4384/10000] Avg train loss: 0.066785\n",
      "Epoch [4385/10000] Avg train loss: 0.066770\n",
      "Epoch [4386/10000] Avg train loss: 0.066755\n",
      "Epoch [4387/10000] Avg train loss: 0.066740\n",
      "Epoch [4388/10000] Avg train loss: 0.066725\n",
      "Epoch [4389/10000] Avg train loss: 0.066710\n",
      "Epoch [4390/10000] Avg train loss: 0.066695\n",
      "Epoch [4391/10000] Avg train loss: 0.066679\n",
      "Epoch [4392/10000] Avg train loss: 0.066664\n",
      "Epoch [4393/10000] Avg train loss: 0.066649\n",
      "Epoch [4394/10000] Avg train loss: 0.066634\n",
      "Epoch [4395/10000] Avg train loss: 0.066619\n",
      "Epoch [4396/10000] Avg train loss: 0.066604\n",
      "Epoch [4397/10000] Avg train loss: 0.066589\n",
      "Epoch [4398/10000] Avg train loss: 0.066574\n",
      "Epoch [4399/10000] Avg train loss: 0.066559\n",
      "Epoch [4400/10000] Avg train loss: 0.066543\n",
      "Epoch [4401/10000] Avg train loss: 0.066528\n",
      "Epoch [4402/10000] Avg train loss: 0.066513\n",
      "Epoch [4403/10000] Avg train loss: 0.066498\n",
      "Epoch [4404/10000] Avg train loss: 0.066483\n",
      "Epoch [4405/10000] Avg train loss: 0.066468\n",
      "Epoch [4406/10000] Avg train loss: 0.066453\n",
      "Epoch [4407/10000] Avg train loss: 0.066438\n",
      "Epoch [4408/10000] Avg train loss: 0.066423\n",
      "Epoch [4409/10000] Avg train loss: 0.066408\n",
      "Epoch [4410/10000] Avg train loss: 0.066393\n",
      "Epoch [4411/10000] Avg train loss: 0.066378\n",
      "Epoch [4412/10000] Avg train loss: 0.066363\n",
      "Epoch [4413/10000] Avg train loss: 0.066348\n",
      "Epoch [4414/10000] Avg train loss: 0.066333\n",
      "Epoch [4415/10000] Avg train loss: 0.066318\n",
      "Epoch [4416/10000] Avg train loss: 0.066303\n",
      "Epoch [4417/10000] Avg train loss: 0.066288\n",
      "Epoch [4418/10000] Avg train loss: 0.066273\n",
      "Epoch [4419/10000] Avg train loss: 0.066258\n",
      "Epoch [4420/10000] Avg train loss: 0.066243\n",
      "Epoch [4421/10000] Avg train loss: 0.066228\n",
      "Epoch [4422/10000] Avg train loss: 0.066213\n",
      "Epoch [4423/10000] Avg train loss: 0.066199\n",
      "Epoch [4424/10000] Avg train loss: 0.066184\n",
      "Epoch [4425/10000] Avg train loss: 0.066169\n",
      "Epoch [4426/10000] Avg train loss: 0.066154\n",
      "Epoch [4427/10000] Avg train loss: 0.066139\n",
      "Epoch [4428/10000] Avg train loss: 0.066124\n",
      "Epoch [4429/10000] Avg train loss: 0.066109\n",
      "Epoch [4430/10000] Avg train loss: 0.066094\n",
      "Epoch [4431/10000] Avg train loss: 0.066079\n",
      "Epoch [4432/10000] Avg train loss: 0.066065\n",
      "Epoch [4433/10000] Avg train loss: 0.066050\n",
      "Epoch [4434/10000] Avg train loss: 0.066035\n",
      "Epoch [4435/10000] Avg train loss: 0.066020\n",
      "Epoch [4436/10000] Avg train loss: 0.066005\n",
      "Epoch [4437/10000] Avg train loss: 0.065990\n",
      "Epoch [4438/10000] Avg train loss: 0.065976\n",
      "Epoch [4439/10000] Avg train loss: 0.065961\n",
      "Epoch [4440/10000] Avg train loss: 0.065946\n",
      "Epoch [4441/10000] Avg train loss: 0.065931\n",
      "Epoch [4442/10000] Avg train loss: 0.065916\n",
      "Epoch [4443/10000] Avg train loss: 0.065902\n",
      "Epoch [4444/10000] Avg train loss: 0.065887\n",
      "Epoch [4445/10000] Avg train loss: 0.065872\n",
      "Epoch [4446/10000] Avg train loss: 0.065857\n",
      "Epoch [4447/10000] Avg train loss: 0.065842\n",
      "Epoch [4448/10000] Avg train loss: 0.065828\n",
      "Epoch [4449/10000] Avg train loss: 0.065813\n",
      "Epoch [4450/10000] Avg train loss: 0.065798\n",
      "Epoch [4451/10000] Avg train loss: 0.065783\n",
      "Epoch [4452/10000] Avg train loss: 0.065769\n",
      "Epoch [4453/10000] Avg train loss: 0.065754\n",
      "Epoch [4454/10000] Avg train loss: 0.065739\n",
      "Epoch [4455/10000] Avg train loss: 0.065725\n",
      "Epoch [4456/10000] Avg train loss: 0.065710\n",
      "Epoch [4457/10000] Avg train loss: 0.065695\n",
      "Epoch [4458/10000] Avg train loss: 0.065681\n",
      "Epoch [4459/10000] Avg train loss: 0.065666\n",
      "Epoch [4460/10000] Avg train loss: 0.065651\n",
      "Epoch [4461/10000] Avg train loss: 0.065637\n",
      "Epoch [4462/10000] Avg train loss: 0.065622\n",
      "Epoch [4463/10000] Avg train loss: 0.065607\n",
      "Epoch [4464/10000] Avg train loss: 0.065593\n",
      "Epoch [4465/10000] Avg train loss: 0.065578\n",
      "Epoch [4466/10000] Avg train loss: 0.065563\n",
      "Epoch [4467/10000] Avg train loss: 0.065549\n",
      "Epoch [4468/10000] Avg train loss: 0.065534\n",
      "Epoch [4469/10000] Avg train loss: 0.065519\n",
      "Epoch [4470/10000] Avg train loss: 0.065505\n",
      "Epoch [4471/10000] Avg train loss: 0.065490\n",
      "Epoch [4472/10000] Avg train loss: 0.065476\n",
      "Epoch [4473/10000] Avg train loss: 0.065461\n",
      "Epoch [4474/10000] Avg train loss: 0.065446\n",
      "Epoch [4475/10000] Avg train loss: 0.065432\n",
      "Epoch [4476/10000] Avg train loss: 0.065417\n",
      "Epoch [4477/10000] Avg train loss: 0.065403\n",
      "Epoch [4478/10000] Avg train loss: 0.065388\n",
      "Epoch [4479/10000] Avg train loss: 0.065374\n",
      "Epoch [4480/10000] Avg train loss: 0.065359\n",
      "Epoch [4481/10000] Avg train loss: 0.065345\n",
      "Epoch [4482/10000] Avg train loss: 0.065330\n",
      "Epoch [4483/10000] Avg train loss: 0.065315\n",
      "Epoch [4484/10000] Avg train loss: 0.065301\n",
      "Epoch [4485/10000] Avg train loss: 0.065286\n",
      "Epoch [4486/10000] Avg train loss: 0.065272\n",
      "Epoch [4487/10000] Avg train loss: 0.065257\n",
      "Epoch [4488/10000] Avg train loss: 0.065243\n",
      "Epoch [4489/10000] Avg train loss: 0.065229\n",
      "Epoch [4490/10000] Avg train loss: 0.065214\n",
      "Epoch [4491/10000] Avg train loss: 0.065200\n",
      "Epoch [4492/10000] Avg train loss: 0.065185\n",
      "Epoch [4493/10000] Avg train loss: 0.065171\n",
      "Epoch [4494/10000] Avg train loss: 0.065156\n",
      "Epoch [4495/10000] Avg train loss: 0.065142\n",
      "Epoch [4496/10000] Avg train loss: 0.065127\n",
      "Epoch [4497/10000] Avg train loss: 0.065113\n",
      "Epoch [4498/10000] Avg train loss: 0.065098\n",
      "Epoch [4499/10000] Avg train loss: 0.065084\n",
      "Epoch [4500/10000] Avg train loss: 0.065070\n",
      "Epoch [4501/10000] Avg train loss: 0.065055\n",
      "Epoch [4502/10000] Avg train loss: 0.065041\n",
      "Epoch [4503/10000] Avg train loss: 0.065026\n",
      "Epoch [4504/10000] Avg train loss: 0.065012\n",
      "Epoch [4505/10000] Avg train loss: 0.064998\n",
      "Epoch [4506/10000] Avg train loss: 0.064983\n",
      "Epoch [4507/10000] Avg train loss: 0.064969\n",
      "Epoch [4508/10000] Avg train loss: 0.064954\n",
      "Epoch [4509/10000] Avg train loss: 0.064940\n",
      "Epoch [4510/10000] Avg train loss: 0.064926\n",
      "Epoch [4511/10000] Avg train loss: 0.064911\n",
      "Epoch [4512/10000] Avg train loss: 0.064897\n",
      "Epoch [4513/10000] Avg train loss: 0.064883\n",
      "Epoch [4514/10000] Avg train loss: 0.064868\n",
      "Epoch [4515/10000] Avg train loss: 0.064854\n",
      "Epoch [4516/10000] Avg train loss: 0.064840\n",
      "Epoch [4517/10000] Avg train loss: 0.064825\n",
      "Epoch [4518/10000] Avg train loss: 0.064811\n",
      "Epoch [4519/10000] Avg train loss: 0.064797\n",
      "Epoch [4520/10000] Avg train loss: 0.064783\n",
      "Epoch [4521/10000] Avg train loss: 0.064768\n",
      "Epoch [4522/10000] Avg train loss: 0.064754\n",
      "Epoch [4523/10000] Avg train loss: 0.064740\n",
      "Epoch [4524/10000] Avg train loss: 0.064726\n",
      "Epoch [4525/10000] Avg train loss: 0.064711\n",
      "Epoch [4526/10000] Avg train loss: 0.064697\n",
      "Epoch [4527/10000] Avg train loss: 0.064683\n",
      "Epoch [4528/10000] Avg train loss: 0.064669\n",
      "Epoch [4529/10000] Avg train loss: 0.064654\n",
      "Epoch [4530/10000] Avg train loss: 0.064640\n",
      "Epoch [4531/10000] Avg train loss: 0.064626\n",
      "Epoch [4532/10000] Avg train loss: 0.064612\n",
      "Epoch [4533/10000] Avg train loss: 0.064597\n",
      "Epoch [4534/10000] Avg train loss: 0.064583\n",
      "Epoch [4535/10000] Avg train loss: 0.064569\n",
      "Epoch [4536/10000] Avg train loss: 0.064555\n",
      "Epoch [4537/10000] Avg train loss: 0.064541\n",
      "Epoch [4538/10000] Avg train loss: 0.064526\n",
      "Epoch [4539/10000] Avg train loss: 0.064512\n",
      "Epoch [4540/10000] Avg train loss: 0.064498\n",
      "Epoch [4541/10000] Avg train loss: 0.064484\n",
      "Epoch [4542/10000] Avg train loss: 0.064470\n",
      "Epoch [4543/10000] Avg train loss: 0.064456\n",
      "Epoch [4544/10000] Avg train loss: 0.064442\n",
      "Epoch [4545/10000] Avg train loss: 0.064427\n",
      "Epoch [4546/10000] Avg train loss: 0.064413\n",
      "Epoch [4547/10000] Avg train loss: 0.064399\n",
      "Epoch [4548/10000] Avg train loss: 0.064385\n",
      "Epoch [4549/10000] Avg train loss: 0.064371\n",
      "Epoch [4550/10000] Avg train loss: 0.064357\n",
      "Epoch [4551/10000] Avg train loss: 0.064343\n",
      "Epoch [4552/10000] Avg train loss: 0.064329\n",
      "Epoch [4553/10000] Avg train loss: 0.064315\n",
      "Epoch [4554/10000] Avg train loss: 0.064301\n",
      "Epoch [4555/10000] Avg train loss: 0.064287\n",
      "Epoch [4556/10000] Avg train loss: 0.064272\n",
      "Epoch [4557/10000] Avg train loss: 0.064258\n",
      "Epoch [4558/10000] Avg train loss: 0.064244\n",
      "Epoch [4559/10000] Avg train loss: 0.064230\n",
      "Epoch [4560/10000] Avg train loss: 0.064216\n",
      "Epoch [4561/10000] Avg train loss: 0.064202\n",
      "Epoch [4562/10000] Avg train loss: 0.064188\n",
      "Epoch [4563/10000] Avg train loss: 0.064174\n",
      "Epoch [4564/10000] Avg train loss: 0.064160\n",
      "Epoch [4565/10000] Avg train loss: 0.064146\n",
      "Epoch [4566/10000] Avg train loss: 0.064132\n",
      "Epoch [4567/10000] Avg train loss: 0.064118\n",
      "Epoch [4568/10000] Avg train loss: 0.064104\n",
      "Epoch [4569/10000] Avg train loss: 0.064090\n",
      "Epoch [4570/10000] Avg train loss: 0.064076\n",
      "Epoch [4571/10000] Avg train loss: 0.064062\n",
      "Epoch [4572/10000] Avg train loss: 0.064048\n",
      "Epoch [4573/10000] Avg train loss: 0.064034\n",
      "Epoch [4574/10000] Avg train loss: 0.064020\n",
      "Epoch [4575/10000] Avg train loss: 0.064006\n",
      "Epoch [4576/10000] Avg train loss: 0.063992\n",
      "Epoch [4577/10000] Avg train loss: 0.063979\n",
      "Epoch [4578/10000] Avg train loss: 0.063965\n",
      "Epoch [4579/10000] Avg train loss: 0.063951\n",
      "Epoch [4580/10000] Avg train loss: 0.063937\n",
      "Epoch [4581/10000] Avg train loss: 0.063923\n",
      "Epoch [4582/10000] Avg train loss: 0.063909\n",
      "Epoch [4583/10000] Avg train loss: 0.063895\n",
      "Epoch [4584/10000] Avg train loss: 0.063881\n",
      "Epoch [4585/10000] Avg train loss: 0.063867\n",
      "Epoch [4586/10000] Avg train loss: 0.063853\n",
      "Epoch [4587/10000] Avg train loss: 0.063840\n",
      "Epoch [4588/10000] Avg train loss: 0.063826\n",
      "Epoch [4589/10000] Avg train loss: 0.063812\n",
      "Epoch [4590/10000] Avg train loss: 0.063798\n",
      "Epoch [4591/10000] Avg train loss: 0.063784\n",
      "Epoch [4592/10000] Avg train loss: 0.063770\n",
      "Epoch [4593/10000] Avg train loss: 0.063756\n",
      "Epoch [4594/10000] Avg train loss: 0.063743\n",
      "Epoch [4595/10000] Avg train loss: 0.063729\n",
      "Epoch [4596/10000] Avg train loss: 0.063715\n",
      "Epoch [4597/10000] Avg train loss: 0.063701\n",
      "Epoch [4598/10000] Avg train loss: 0.063687\n",
      "Epoch [4599/10000] Avg train loss: 0.063674\n",
      "Epoch [4600/10000] Avg train loss: 0.063660\n",
      "Epoch [4601/10000] Avg train loss: 0.063646\n",
      "Epoch [4602/10000] Avg train loss: 0.063632\n",
      "Epoch [4603/10000] Avg train loss: 0.063618\n",
      "Epoch [4604/10000] Avg train loss: 0.063605\n",
      "Epoch [4605/10000] Avg train loss: 0.063591\n",
      "Epoch [4606/10000] Avg train loss: 0.063577\n",
      "Epoch [4607/10000] Avg train loss: 0.063563\n",
      "Epoch [4608/10000] Avg train loss: 0.063550\n",
      "Epoch [4609/10000] Avg train loss: 0.063536\n",
      "Epoch [4610/10000] Avg train loss: 0.063522\n",
      "Epoch [4611/10000] Avg train loss: 0.063508\n",
      "Epoch [4612/10000] Avg train loss: 0.063495\n",
      "Epoch [4613/10000] Avg train loss: 0.063481\n",
      "Epoch [4614/10000] Avg train loss: 0.063467\n",
      "Epoch [4615/10000] Avg train loss: 0.063454\n",
      "Epoch [4616/10000] Avg train loss: 0.063440\n",
      "Epoch [4617/10000] Avg train loss: 0.063426\n",
      "Epoch [4618/10000] Avg train loss: 0.063412\n",
      "Epoch [4619/10000] Avg train loss: 0.063399\n",
      "Epoch [4620/10000] Avg train loss: 0.063385\n",
      "Epoch [4621/10000] Avg train loss: 0.063371\n",
      "Epoch [4622/10000] Avg train loss: 0.063358\n",
      "Epoch [4623/10000] Avg train loss: 0.063344\n",
      "Epoch [4624/10000] Avg train loss: 0.063330\n",
      "Epoch [4625/10000] Avg train loss: 0.063317\n",
      "Epoch [4626/10000] Avg train loss: 0.063303\n",
      "Epoch [4627/10000] Avg train loss: 0.063290\n",
      "Epoch [4628/10000] Avg train loss: 0.063276\n",
      "Epoch [4629/10000] Avg train loss: 0.063262\n",
      "Epoch [4630/10000] Avg train loss: 0.063249\n",
      "Epoch [4631/10000] Avg train loss: 0.063235\n",
      "Epoch [4632/10000] Avg train loss: 0.063221\n",
      "Epoch [4633/10000] Avg train loss: 0.063208\n",
      "Epoch [4634/10000] Avg train loss: 0.063194\n",
      "Epoch [4635/10000] Avg train loss: 0.063181\n",
      "Epoch [4636/10000] Avg train loss: 0.063167\n",
      "Epoch [4637/10000] Avg train loss: 0.063153\n",
      "Epoch [4638/10000] Avg train loss: 0.063140\n",
      "Epoch [4639/10000] Avg train loss: 0.063126\n",
      "Epoch [4640/10000] Avg train loss: 0.063113\n",
      "Epoch [4641/10000] Avg train loss: 0.063099\n",
      "Epoch [4642/10000] Avg train loss: 0.063086\n",
      "Epoch [4643/10000] Avg train loss: 0.063072\n",
      "Epoch [4644/10000] Avg train loss: 0.063059\n",
      "Epoch [4645/10000] Avg train loss: 0.063045\n",
      "Epoch [4646/10000] Avg train loss: 0.063032\n",
      "Epoch [4647/10000] Avg train loss: 0.063018\n",
      "Epoch [4648/10000] Avg train loss: 0.063005\n",
      "Epoch [4649/10000] Avg train loss: 0.062991\n",
      "Epoch [4650/10000] Avg train loss: 0.062977\n",
      "Epoch [4651/10000] Avg train loss: 0.062964\n",
      "Epoch [4652/10000] Avg train loss: 0.062951\n",
      "Epoch [4653/10000] Avg train loss: 0.062937\n",
      "Epoch [4654/10000] Avg train loss: 0.062924\n",
      "Epoch [4655/10000] Avg train loss: 0.062910\n",
      "Epoch [4656/10000] Avg train loss: 0.062897\n",
      "Epoch [4657/10000] Avg train loss: 0.062883\n",
      "Epoch [4658/10000] Avg train loss: 0.062870\n",
      "Epoch [4659/10000] Avg train loss: 0.062856\n",
      "Epoch [4660/10000] Avg train loss: 0.062843\n",
      "Epoch [4661/10000] Avg train loss: 0.062829\n",
      "Epoch [4662/10000] Avg train loss: 0.062816\n",
      "Epoch [4663/10000] Avg train loss: 0.062802\n",
      "Epoch [4664/10000] Avg train loss: 0.062789\n",
      "Epoch [4665/10000] Avg train loss: 0.062776\n",
      "Epoch [4666/10000] Avg train loss: 0.062762\n",
      "Epoch [4667/10000] Avg train loss: 0.062749\n",
      "Epoch [4668/10000] Avg train loss: 0.062735\n",
      "Epoch [4669/10000] Avg train loss: 0.062722\n",
      "Epoch [4670/10000] Avg train loss: 0.062709\n",
      "Epoch [4671/10000] Avg train loss: 0.062695\n",
      "Epoch [4672/10000] Avg train loss: 0.062682\n",
      "Epoch [4673/10000] Avg train loss: 0.062669\n",
      "Epoch [4674/10000] Avg train loss: 0.062655\n",
      "Epoch [4675/10000] Avg train loss: 0.062642\n",
      "Epoch [4676/10000] Avg train loss: 0.062628\n",
      "Epoch [4677/10000] Avg train loss: 0.062615\n",
      "Epoch [4678/10000] Avg train loss: 0.062602\n",
      "Epoch [4679/10000] Avg train loss: 0.062588\n",
      "Epoch [4680/10000] Avg train loss: 0.062575\n",
      "Epoch [4681/10000] Avg train loss: 0.062562\n",
      "Epoch [4682/10000] Avg train loss: 0.062548\n",
      "Epoch [4683/10000] Avg train loss: 0.062535\n",
      "Epoch [4684/10000] Avg train loss: 0.062522\n",
      "Epoch [4685/10000] Avg train loss: 0.062509\n",
      "Epoch [4686/10000] Avg train loss: 0.062495\n",
      "Epoch [4687/10000] Avg train loss: 0.062482\n",
      "Epoch [4688/10000] Avg train loss: 0.062469\n",
      "Epoch [4689/10000] Avg train loss: 0.062455\n",
      "Epoch [4690/10000] Avg train loss: 0.062442\n",
      "Epoch [4691/10000] Avg train loss: 0.062429\n",
      "Epoch [4692/10000] Avg train loss: 0.062416\n",
      "Epoch [4693/10000] Avg train loss: 0.062402\n",
      "Epoch [4694/10000] Avg train loss: 0.062389\n",
      "Epoch [4695/10000] Avg train loss: 0.062376\n",
      "Epoch [4696/10000] Avg train loss: 0.062363\n",
      "Epoch [4697/10000] Avg train loss: 0.062349\n",
      "Epoch [4698/10000] Avg train loss: 0.062336\n",
      "Epoch [4699/10000] Avg train loss: 0.062323\n",
      "Epoch [4700/10000] Avg train loss: 0.062310\n",
      "Epoch [4701/10000] Avg train loss: 0.062297\n",
      "Epoch [4702/10000] Avg train loss: 0.062283\n",
      "Epoch [4703/10000] Avg train loss: 0.062270\n",
      "Epoch [4704/10000] Avg train loss: 0.062257\n",
      "Epoch [4705/10000] Avg train loss: 0.062244\n",
      "Epoch [4706/10000] Avg train loss: 0.062231\n",
      "Epoch [4707/10000] Avg train loss: 0.062217\n",
      "Epoch [4708/10000] Avg train loss: 0.062204\n",
      "Epoch [4709/10000] Avg train loss: 0.062191\n",
      "Epoch [4710/10000] Avg train loss: 0.062178\n",
      "Epoch [4711/10000] Avg train loss: 0.062165\n",
      "Epoch [4712/10000] Avg train loss: 0.062152\n",
      "Epoch [4713/10000] Avg train loss: 0.062138\n",
      "Epoch [4714/10000] Avg train loss: 0.062125\n",
      "Epoch [4715/10000] Avg train loss: 0.062112\n",
      "Epoch [4716/10000] Avg train loss: 0.062099\n",
      "Epoch [4717/10000] Avg train loss: 0.062086\n",
      "Epoch [4718/10000] Avg train loss: 0.062073\n",
      "Epoch [4719/10000] Avg train loss: 0.062060\n",
      "Epoch [4720/10000] Avg train loss: 0.062047\n",
      "Epoch [4721/10000] Avg train loss: 0.062034\n",
      "Epoch [4722/10000] Avg train loss: 0.062020\n",
      "Epoch [4723/10000] Avg train loss: 0.062007\n",
      "Epoch [4724/10000] Avg train loss: 0.061994\n",
      "Epoch [4725/10000] Avg train loss: 0.061981\n",
      "Epoch [4726/10000] Avg train loss: 0.061968\n",
      "Epoch [4727/10000] Avg train loss: 0.061955\n",
      "Epoch [4728/10000] Avg train loss: 0.061942\n",
      "Epoch [4729/10000] Avg train loss: 0.061929\n",
      "Epoch [4730/10000] Avg train loss: 0.061916\n",
      "Epoch [4731/10000] Avg train loss: 0.061903\n",
      "Epoch [4732/10000] Avg train loss: 0.061890\n",
      "Epoch [4733/10000] Avg train loss: 0.061877\n",
      "Epoch [4734/10000] Avg train loss: 0.061864\n",
      "Epoch [4735/10000] Avg train loss: 0.061851\n",
      "Epoch [4736/10000] Avg train loss: 0.061838\n",
      "Epoch [4737/10000] Avg train loss: 0.061825\n",
      "Epoch [4738/10000] Avg train loss: 0.061812\n",
      "Epoch [4739/10000] Avg train loss: 0.061799\n",
      "Epoch [4740/10000] Avg train loss: 0.061786\n",
      "Epoch [4741/10000] Avg train loss: 0.061773\n",
      "Epoch [4742/10000] Avg train loss: 0.061760\n",
      "Epoch [4743/10000] Avg train loss: 0.061747\n",
      "Epoch [4744/10000] Avg train loss: 0.061734\n",
      "Epoch [4745/10000] Avg train loss: 0.061721\n",
      "Epoch [4746/10000] Avg train loss: 0.061708\n",
      "Epoch [4747/10000] Avg train loss: 0.061695\n",
      "Epoch [4748/10000] Avg train loss: 0.061682\n",
      "Epoch [4749/10000] Avg train loss: 0.061669\n",
      "Epoch [4750/10000] Avg train loss: 0.061656\n",
      "Epoch [4751/10000] Avg train loss: 0.061643\n",
      "Epoch [4752/10000] Avg train loss: 0.061630\n",
      "Epoch [4753/10000] Avg train loss: 0.061617\n",
      "Epoch [4754/10000] Avg train loss: 0.061604\n",
      "Epoch [4755/10000] Avg train loss: 0.061591\n",
      "Epoch [4756/10000] Avg train loss: 0.061579\n",
      "Epoch [4757/10000] Avg train loss: 0.061566\n",
      "Epoch [4758/10000] Avg train loss: 0.061553\n",
      "Epoch [4759/10000] Avg train loss: 0.061540\n",
      "Epoch [4760/10000] Avg train loss: 0.061527\n",
      "Epoch [4761/10000] Avg train loss: 0.061514\n",
      "Epoch [4762/10000] Avg train loss: 0.061501\n",
      "Epoch [4763/10000] Avg train loss: 0.061488\n",
      "Epoch [4764/10000] Avg train loss: 0.061475\n",
      "Epoch [4765/10000] Avg train loss: 0.061463\n",
      "Epoch [4766/10000] Avg train loss: 0.061450\n",
      "Epoch [4767/10000] Avg train loss: 0.061437\n",
      "Epoch [4768/10000] Avg train loss: 0.061424\n",
      "Epoch [4769/10000] Avg train loss: 0.061411\n",
      "Epoch [4770/10000] Avg train loss: 0.061398\n",
      "Epoch [4771/10000] Avg train loss: 0.061386\n",
      "Epoch [4772/10000] Avg train loss: 0.061373\n",
      "Epoch [4773/10000] Avg train loss: 0.061360\n",
      "Epoch [4774/10000] Avg train loss: 0.061347\n",
      "Epoch [4775/10000] Avg train loss: 0.061334\n",
      "Epoch [4776/10000] Avg train loss: 0.061321\n",
      "Epoch [4777/10000] Avg train loss: 0.061309\n",
      "Epoch [4778/10000] Avg train loss: 0.061296\n",
      "Epoch [4779/10000] Avg train loss: 0.061283\n",
      "Epoch [4780/10000] Avg train loss: 0.061270\n",
      "Epoch [4781/10000] Avg train loss: 0.061258\n",
      "Epoch [4782/10000] Avg train loss: 0.061245\n",
      "Epoch [4783/10000] Avg train loss: 0.061232\n",
      "Epoch [4784/10000] Avg train loss: 0.061219\n",
      "Epoch [4785/10000] Avg train loss: 0.061207\n",
      "Epoch [4786/10000] Avg train loss: 0.061194\n",
      "Epoch [4787/10000] Avg train loss: 0.061181\n",
      "Epoch [4788/10000] Avg train loss: 0.061168\n",
      "Epoch [4789/10000] Avg train loss: 0.061156\n",
      "Epoch [4790/10000] Avg train loss: 0.061143\n",
      "Epoch [4791/10000] Avg train loss: 0.061130\n",
      "Epoch [4792/10000] Avg train loss: 0.061117\n",
      "Epoch [4793/10000] Avg train loss: 0.061105\n",
      "Epoch [4794/10000] Avg train loss: 0.061092\n",
      "Epoch [4795/10000] Avg train loss: 0.061079\n",
      "Epoch [4796/10000] Avg train loss: 0.061067\n",
      "Epoch [4797/10000] Avg train loss: 0.061054\n",
      "Epoch [4798/10000] Avg train loss: 0.061041\n",
      "Epoch [4799/10000] Avg train loss: 0.061029\n",
      "Epoch [4800/10000] Avg train loss: 0.061016\n",
      "Epoch [4801/10000] Avg train loss: 0.061003\n",
      "Epoch [4802/10000] Avg train loss: 0.060991\n",
      "Epoch [4803/10000] Avg train loss: 0.060978\n",
      "Epoch [4804/10000] Avg train loss: 0.060965\n",
      "Epoch [4805/10000] Avg train loss: 0.060953\n",
      "Epoch [4806/10000] Avg train loss: 0.060940\n",
      "Epoch [4807/10000] Avg train loss: 0.060927\n",
      "Epoch [4808/10000] Avg train loss: 0.060915\n",
      "Epoch [4809/10000] Avg train loss: 0.060902\n",
      "Epoch [4810/10000] Avg train loss: 0.060889\n",
      "Epoch [4811/10000] Avg train loss: 0.060877\n",
      "Epoch [4812/10000] Avg train loss: 0.060864\n",
      "Epoch [4813/10000] Avg train loss: 0.060852\n",
      "Epoch [4814/10000] Avg train loss: 0.060839\n",
      "Epoch [4815/10000] Avg train loss: 0.060826\n",
      "Epoch [4816/10000] Avg train loss: 0.060814\n",
      "Epoch [4817/10000] Avg train loss: 0.060801\n",
      "Epoch [4818/10000] Avg train loss: 0.060789\n",
      "Epoch [4819/10000] Avg train loss: 0.060776\n",
      "Epoch [4820/10000] Avg train loss: 0.060764\n",
      "Epoch [4821/10000] Avg train loss: 0.060751\n",
      "Epoch [4822/10000] Avg train loss: 0.060738\n",
      "Epoch [4823/10000] Avg train loss: 0.060726\n",
      "Epoch [4824/10000] Avg train loss: 0.060713\n",
      "Epoch [4825/10000] Avg train loss: 0.060701\n",
      "Epoch [4826/10000] Avg train loss: 0.060688\n",
      "Epoch [4827/10000] Avg train loss: 0.060676\n",
      "Epoch [4828/10000] Avg train loss: 0.060663\n",
      "Epoch [4829/10000] Avg train loss: 0.060651\n",
      "Epoch [4830/10000] Avg train loss: 0.060638\n",
      "Epoch [4831/10000] Avg train loss: 0.060626\n",
      "Epoch [4832/10000] Avg train loss: 0.060613\n",
      "Epoch [4833/10000] Avg train loss: 0.060601\n",
      "Epoch [4834/10000] Avg train loss: 0.060588\n",
      "Epoch [4835/10000] Avg train loss: 0.060576\n",
      "Epoch [4836/10000] Avg train loss: 0.060563\n",
      "Epoch [4837/10000] Avg train loss: 0.060551\n",
      "Epoch [4838/10000] Avg train loss: 0.060538\n",
      "Epoch [4839/10000] Avg train loss: 0.060526\n",
      "Epoch [4840/10000] Avg train loss: 0.060513\n",
      "Epoch [4841/10000] Avg train loss: 0.060501\n",
      "Epoch [4842/10000] Avg train loss: 0.060488\n",
      "Epoch [4843/10000] Avg train loss: 0.060476\n",
      "Epoch [4844/10000] Avg train loss: 0.060463\n",
      "Epoch [4845/10000] Avg train loss: 0.060451\n",
      "Epoch [4846/10000] Avg train loss: 0.060439\n",
      "Epoch [4847/10000] Avg train loss: 0.060426\n",
      "Epoch [4848/10000] Avg train loss: 0.060414\n",
      "Epoch [4849/10000] Avg train loss: 0.060401\n",
      "Epoch [4850/10000] Avg train loss: 0.060389\n",
      "Epoch [4851/10000] Avg train loss: 0.060376\n",
      "Epoch [4852/10000] Avg train loss: 0.060364\n",
      "Epoch [4853/10000] Avg train loss: 0.060352\n",
      "Epoch [4854/10000] Avg train loss: 0.060339\n",
      "Epoch [4855/10000] Avg train loss: 0.060327\n",
      "Epoch [4856/10000] Avg train loss: 0.060314\n",
      "Epoch [4857/10000] Avg train loss: 0.060302\n",
      "Epoch [4858/10000] Avg train loss: 0.060290\n",
      "Epoch [4859/10000] Avg train loss: 0.060277\n",
      "Epoch [4860/10000] Avg train loss: 0.060265\n",
      "Epoch [4861/10000] Avg train loss: 0.060253\n",
      "Epoch [4862/10000] Avg train loss: 0.060240\n",
      "Epoch [4863/10000] Avg train loss: 0.060228\n",
      "Epoch [4864/10000] Avg train loss: 0.060216\n",
      "Epoch [4865/10000] Avg train loss: 0.060203\n",
      "Epoch [4866/10000] Avg train loss: 0.060191\n",
      "Epoch [4867/10000] Avg train loss: 0.060179\n",
      "Epoch [4868/10000] Avg train loss: 0.060166\n",
      "Epoch [4869/10000] Avg train loss: 0.060154\n",
      "Epoch [4870/10000] Avg train loss: 0.060142\n",
      "Epoch [4871/10000] Avg train loss: 0.060129\n",
      "Epoch [4872/10000] Avg train loss: 0.060117\n",
      "Epoch [4873/10000] Avg train loss: 0.060105\n",
      "Epoch [4874/10000] Avg train loss: 0.060092\n",
      "Epoch [4875/10000] Avg train loss: 0.060080\n",
      "Epoch [4876/10000] Avg train loss: 0.060068\n",
      "Epoch [4877/10000] Avg train loss: 0.060056\n",
      "Epoch [4878/10000] Avg train loss: 0.060043\n",
      "Epoch [4879/10000] Avg train loss: 0.060031\n",
      "Epoch [4880/10000] Avg train loss: 0.060019\n",
      "Epoch [4881/10000] Avg train loss: 0.060007\n",
      "Epoch [4882/10000] Avg train loss: 0.059994\n",
      "Epoch [4883/10000] Avg train loss: 0.059982\n",
      "Epoch [4884/10000] Avg train loss: 0.059970\n",
      "Epoch [4885/10000] Avg train loss: 0.059958\n",
      "Epoch [4886/10000] Avg train loss: 0.059945\n",
      "Epoch [4887/10000] Avg train loss: 0.059933\n",
      "Epoch [4888/10000] Avg train loss: 0.059921\n",
      "Epoch [4889/10000] Avg train loss: 0.059909\n",
      "Epoch [4890/10000] Avg train loss: 0.059896\n",
      "Epoch [4891/10000] Avg train loss: 0.059884\n",
      "Epoch [4892/10000] Avg train loss: 0.059872\n",
      "Epoch [4893/10000] Avg train loss: 0.059860\n",
      "Epoch [4894/10000] Avg train loss: 0.059848\n",
      "Epoch [4895/10000] Avg train loss: 0.059835\n",
      "Epoch [4896/10000] Avg train loss: 0.059823\n",
      "Epoch [4897/10000] Avg train loss: 0.059811\n",
      "Epoch [4898/10000] Avg train loss: 0.059799\n",
      "Epoch [4899/10000] Avg train loss: 0.059787\n",
      "Epoch [4900/10000] Avg train loss: 0.059775\n",
      "Epoch [4901/10000] Avg train loss: 0.059762\n",
      "Epoch [4902/10000] Avg train loss: 0.059750\n",
      "Epoch [4903/10000] Avg train loss: 0.059738\n",
      "Epoch [4904/10000] Avg train loss: 0.059726\n",
      "Epoch [4905/10000] Avg train loss: 0.059714\n",
      "Epoch [4906/10000] Avg train loss: 0.059702\n",
      "Epoch [4907/10000] Avg train loss: 0.059690\n",
      "Epoch [4908/10000] Avg train loss: 0.059677\n",
      "Epoch [4909/10000] Avg train loss: 0.059665\n",
      "Epoch [4910/10000] Avg train loss: 0.059653\n",
      "Epoch [4911/10000] Avg train loss: 0.059641\n",
      "Epoch [4912/10000] Avg train loss: 0.059629\n",
      "Epoch [4913/10000] Avg train loss: 0.059617\n",
      "Epoch [4914/10000] Avg train loss: 0.059605\n",
      "Epoch [4915/10000] Avg train loss: 0.059593\n",
      "Epoch [4916/10000] Avg train loss: 0.059581\n",
      "Epoch [4917/10000] Avg train loss: 0.059569\n",
      "Epoch [4918/10000] Avg train loss: 0.059556\n",
      "Epoch [4919/10000] Avg train loss: 0.059544\n",
      "Epoch [4920/10000] Avg train loss: 0.059532\n",
      "Epoch [4921/10000] Avg train loss: 0.059520\n",
      "Epoch [4922/10000] Avg train loss: 0.059508\n",
      "Epoch [4923/10000] Avg train loss: 0.059496\n",
      "Epoch [4924/10000] Avg train loss: 0.059484\n",
      "Epoch [4925/10000] Avg train loss: 0.059472\n",
      "Epoch [4926/10000] Avg train loss: 0.059460\n",
      "Epoch [4927/10000] Avg train loss: 0.059448\n",
      "Epoch [4928/10000] Avg train loss: 0.059436\n",
      "Epoch [4929/10000] Avg train loss: 0.059424\n",
      "Epoch [4930/10000] Avg train loss: 0.059412\n",
      "Epoch [4931/10000] Avg train loss: 0.059400\n",
      "Epoch [4932/10000] Avg train loss: 0.059388\n",
      "Epoch [4933/10000] Avg train loss: 0.059376\n",
      "Epoch [4934/10000] Avg train loss: 0.059364\n",
      "Epoch [4935/10000] Avg train loss: 0.059352\n",
      "Epoch [4936/10000] Avg train loss: 0.059340\n",
      "Epoch [4937/10000] Avg train loss: 0.059328\n",
      "Epoch [4938/10000] Avg train loss: 0.059316\n",
      "Epoch [4939/10000] Avg train loss: 0.059304\n",
      "Epoch [4940/10000] Avg train loss: 0.059292\n",
      "Epoch [4941/10000] Avg train loss: 0.059280\n",
      "Epoch [4942/10000] Avg train loss: 0.059268\n",
      "Epoch [4943/10000] Avg train loss: 0.059256\n",
      "Epoch [4944/10000] Avg train loss: 0.059244\n",
      "Epoch [4945/10000] Avg train loss: 0.059232\n",
      "Epoch [4946/10000] Avg train loss: 0.059220\n",
      "Epoch [4947/10000] Avg train loss: 0.059208\n",
      "Epoch [4948/10000] Avg train loss: 0.059197\n",
      "Epoch [4949/10000] Avg train loss: 0.059185\n",
      "Epoch [4950/10000] Avg train loss: 0.059173\n",
      "Epoch [4951/10000] Avg train loss: 0.059161\n",
      "Epoch [4952/10000] Avg train loss: 0.059149\n",
      "Epoch [4953/10000] Avg train loss: 0.059137\n",
      "Epoch [4954/10000] Avg train loss: 0.059125\n",
      "Epoch [4955/10000] Avg train loss: 0.059113\n",
      "Epoch [4956/10000] Avg train loss: 0.059101\n",
      "Epoch [4957/10000] Avg train loss: 0.059089\n",
      "Epoch [4958/10000] Avg train loss: 0.059078\n",
      "Epoch [4959/10000] Avg train loss: 0.059066\n",
      "Epoch [4960/10000] Avg train loss: 0.059054\n",
      "Epoch [4961/10000] Avg train loss: 0.059042\n",
      "Epoch [4962/10000] Avg train loss: 0.059030\n",
      "Epoch [4963/10000] Avg train loss: 0.059018\n",
      "Epoch [4964/10000] Avg train loss: 0.059006\n",
      "Epoch [4965/10000] Avg train loss: 0.058994\n",
      "Epoch [4966/10000] Avg train loss: 0.058983\n",
      "Epoch [4967/10000] Avg train loss: 0.058971\n",
      "Epoch [4968/10000] Avg train loss: 0.058959\n",
      "Epoch [4969/10000] Avg train loss: 0.058947\n",
      "Epoch [4970/10000] Avg train loss: 0.058935\n",
      "Epoch [4971/10000] Avg train loss: 0.058924\n",
      "Epoch [4972/10000] Avg train loss: 0.058912\n",
      "Epoch [4973/10000] Avg train loss: 0.058900\n",
      "Epoch [4974/10000] Avg train loss: 0.058888\n",
      "Epoch [4975/10000] Avg train loss: 0.058876\n",
      "Epoch [4976/10000] Avg train loss: 0.058865\n",
      "Epoch [4977/10000] Avg train loss: 0.058853\n",
      "Epoch [4978/10000] Avg train loss: 0.058841\n",
      "Epoch [4979/10000] Avg train loss: 0.058829\n",
      "Epoch [4980/10000] Avg train loss: 0.058817\n",
      "Epoch [4981/10000] Avg train loss: 0.058806\n",
      "Epoch [4982/10000] Avg train loss: 0.058794\n",
      "Epoch [4983/10000] Avg train loss: 0.058782\n",
      "Epoch [4984/10000] Avg train loss: 0.058770\n",
      "Epoch [4985/10000] Avg train loss: 0.058759\n",
      "Epoch [4986/10000] Avg train loss: 0.058747\n",
      "Epoch [4987/10000] Avg train loss: 0.058735\n",
      "Epoch [4988/10000] Avg train loss: 0.058723\n",
      "Epoch [4989/10000] Avg train loss: 0.058712\n",
      "Epoch [4990/10000] Avg train loss: 0.058700\n",
      "Epoch [4991/10000] Avg train loss: 0.058688\n",
      "Epoch [4992/10000] Avg train loss: 0.058676\n",
      "Epoch [4993/10000] Avg train loss: 0.058665\n",
      "Epoch [4994/10000] Avg train loss: 0.058653\n",
      "Epoch [4995/10000] Avg train loss: 0.058641\n",
      "Epoch [4996/10000] Avg train loss: 0.058630\n",
      "Epoch [4997/10000] Avg train loss: 0.058618\n",
      "Epoch [4998/10000] Avg train loss: 0.058606\n",
      "Epoch [4999/10000] Avg train loss: 0.058595\n",
      "Epoch [5000/10000] Avg train loss: 0.058583\n",
      "Epoch [5001/10000] Avg train loss: 0.058571\n",
      "Epoch [5002/10000] Avg train loss: 0.058560\n",
      "Epoch [5003/10000] Avg train loss: 0.058548\n",
      "Epoch [5004/10000] Avg train loss: 0.058536\n",
      "Epoch [5005/10000] Avg train loss: 0.058525\n",
      "Epoch [5006/10000] Avg train loss: 0.058513\n",
      "Epoch [5007/10000] Avg train loss: 0.058501\n",
      "Epoch [5008/10000] Avg train loss: 0.058490\n",
      "Epoch [5009/10000] Avg train loss: 0.058478\n",
      "Epoch [5010/10000] Avg train loss: 0.058466\n",
      "Epoch [5011/10000] Avg train loss: 0.058455\n",
      "Epoch [5012/10000] Avg train loss: 0.058443\n",
      "Epoch [5013/10000] Avg train loss: 0.058431\n",
      "Epoch [5014/10000] Avg train loss: 0.058420\n",
      "Epoch [5015/10000] Avg train loss: 0.058408\n",
      "Epoch [5016/10000] Avg train loss: 0.058397\n",
      "Epoch [5017/10000] Avg train loss: 0.058385\n",
      "Epoch [5018/10000] Avg train loss: 0.058373\n",
      "Epoch [5019/10000] Avg train loss: 0.058362\n",
      "Epoch [5020/10000] Avg train loss: 0.058350\n",
      "Epoch [5021/10000] Avg train loss: 0.058339\n",
      "Epoch [5022/10000] Avg train loss: 0.058327\n",
      "Epoch [5023/10000] Avg train loss: 0.058315\n",
      "Epoch [5024/10000] Avg train loss: 0.058304\n",
      "Epoch [5025/10000] Avg train loss: 0.058292\n",
      "Epoch [5026/10000] Avg train loss: 0.058281\n",
      "Epoch [5027/10000] Avg train loss: 0.058269\n",
      "Epoch [5028/10000] Avg train loss: 0.058258\n",
      "Epoch [5029/10000] Avg train loss: 0.058246\n",
      "Epoch [5030/10000] Avg train loss: 0.058235\n",
      "Epoch [5031/10000] Avg train loss: 0.058223\n",
      "Epoch [5032/10000] Avg train loss: 0.058211\n",
      "Epoch [5033/10000] Avg train loss: 0.058200\n",
      "Epoch [5034/10000] Avg train loss: 0.058188\n",
      "Epoch [5035/10000] Avg train loss: 0.058177\n",
      "Epoch [5036/10000] Avg train loss: 0.058165\n",
      "Epoch [5037/10000] Avg train loss: 0.058154\n",
      "Epoch [5038/10000] Avg train loss: 0.058142\n",
      "Epoch [5039/10000] Avg train loss: 0.058131\n",
      "Epoch [5040/10000] Avg train loss: 0.058119\n",
      "Epoch [5041/10000] Avg train loss: 0.058108\n",
      "Epoch [5042/10000] Avg train loss: 0.058096\n",
      "Epoch [5043/10000] Avg train loss: 0.058085\n",
      "Epoch [5044/10000] Avg train loss: 0.058073\n",
      "Epoch [5045/10000] Avg train loss: 0.058062\n",
      "Epoch [5046/10000] Avg train loss: 0.058050\n",
      "Epoch [5047/10000] Avg train loss: 0.058039\n",
      "Epoch [5048/10000] Avg train loss: 0.058028\n",
      "Epoch [5049/10000] Avg train loss: 0.058016\n",
      "Epoch [5050/10000] Avg train loss: 0.058005\n",
      "Epoch [5051/10000] Avg train loss: 0.057993\n",
      "Epoch [5052/10000] Avg train loss: 0.057982\n",
      "Epoch [5053/10000] Avg train loss: 0.057970\n",
      "Epoch [5054/10000] Avg train loss: 0.057959\n",
      "Epoch [5055/10000] Avg train loss: 0.057947\n",
      "Epoch [5056/10000] Avg train loss: 0.057936\n",
      "Epoch [5057/10000] Avg train loss: 0.057925\n",
      "Epoch [5058/10000] Avg train loss: 0.057913\n",
      "Epoch [5059/10000] Avg train loss: 0.057902\n",
      "Epoch [5060/10000] Avg train loss: 0.057890\n",
      "Epoch [5061/10000] Avg train loss: 0.057879\n",
      "Epoch [5062/10000] Avg train loss: 0.057868\n",
      "Epoch [5063/10000] Avg train loss: 0.057856\n",
      "Epoch [5064/10000] Avg train loss: 0.057845\n",
      "Epoch [5065/10000] Avg train loss: 0.057833\n",
      "Epoch [5066/10000] Avg train loss: 0.057822\n",
      "Epoch [5067/10000] Avg train loss: 0.057811\n",
      "Epoch [5068/10000] Avg train loss: 0.057799\n",
      "Epoch [5069/10000] Avg train loss: 0.057788\n",
      "Epoch [5070/10000] Avg train loss: 0.057777\n",
      "Epoch [5071/10000] Avg train loss: 0.057765\n",
      "Epoch [5072/10000] Avg train loss: 0.057754\n",
      "Epoch [5073/10000] Avg train loss: 0.057742\n",
      "Epoch [5074/10000] Avg train loss: 0.057731\n",
      "Epoch [5075/10000] Avg train loss: 0.057720\n",
      "Epoch [5076/10000] Avg train loss: 0.057708\n",
      "Epoch [5077/10000] Avg train loss: 0.057697\n",
      "Epoch [5078/10000] Avg train loss: 0.057686\n",
      "Epoch [5079/10000] Avg train loss: 0.057675\n",
      "Epoch [5080/10000] Avg train loss: 0.057663\n",
      "Epoch [5081/10000] Avg train loss: 0.057652\n",
      "Epoch [5082/10000] Avg train loss: 0.057641\n",
      "Epoch [5083/10000] Avg train loss: 0.057629\n",
      "Epoch [5084/10000] Avg train loss: 0.057618\n",
      "Epoch [5085/10000] Avg train loss: 0.057607\n",
      "Epoch [5086/10000] Avg train loss: 0.057595\n",
      "Epoch [5087/10000] Avg train loss: 0.057584\n",
      "Epoch [5088/10000] Avg train loss: 0.057573\n",
      "Epoch [5089/10000] Avg train loss: 0.057562\n",
      "Epoch [5090/10000] Avg train loss: 0.057550\n",
      "Epoch [5091/10000] Avg train loss: 0.057539\n",
      "Epoch [5092/10000] Avg train loss: 0.057528\n",
      "Epoch [5093/10000] Avg train loss: 0.057516\n",
      "Epoch [5094/10000] Avg train loss: 0.057505\n",
      "Epoch [5095/10000] Avg train loss: 0.057494\n",
      "Epoch [5096/10000] Avg train loss: 0.057483\n",
      "Epoch [5097/10000] Avg train loss: 0.057471\n",
      "Epoch [5098/10000] Avg train loss: 0.057460\n",
      "Epoch [5099/10000] Avg train loss: 0.057449\n",
      "Epoch [5100/10000] Avg train loss: 0.057438\n",
      "Epoch [5101/10000] Avg train loss: 0.057427\n",
      "Epoch [5102/10000] Avg train loss: 0.057415\n",
      "Epoch [5103/10000] Avg train loss: 0.057404\n",
      "Epoch [5104/10000] Avg train loss: 0.057393\n",
      "Epoch [5105/10000] Avg train loss: 0.057382\n",
      "Epoch [5106/10000] Avg train loss: 0.057370\n",
      "Epoch [5107/10000] Avg train loss: 0.057359\n",
      "Epoch [5108/10000] Avg train loss: 0.057348\n",
      "Epoch [5109/10000] Avg train loss: 0.057337\n",
      "Epoch [5110/10000] Avg train loss: 0.057326\n",
      "Epoch [5111/10000] Avg train loss: 0.057315\n",
      "Epoch [5112/10000] Avg train loss: 0.057303\n",
      "Epoch [5113/10000] Avg train loss: 0.057292\n",
      "Epoch [5114/10000] Avg train loss: 0.057281\n",
      "Epoch [5115/10000] Avg train loss: 0.057270\n",
      "Epoch [5116/10000] Avg train loss: 0.057259\n",
      "Epoch [5117/10000] Avg train loss: 0.057248\n",
      "Epoch [5118/10000] Avg train loss: 0.057236\n",
      "Epoch [5119/10000] Avg train loss: 0.057225\n",
      "Epoch [5120/10000] Avg train loss: 0.057214\n",
      "Epoch [5121/10000] Avg train loss: 0.057203\n",
      "Epoch [5122/10000] Avg train loss: 0.057192\n",
      "Epoch [5123/10000] Avg train loss: 0.057181\n",
      "Epoch [5124/10000] Avg train loss: 0.057170\n",
      "Epoch [5125/10000] Avg train loss: 0.057158\n",
      "Epoch [5126/10000] Avg train loss: 0.057147\n",
      "Epoch [5127/10000] Avg train loss: 0.057136\n",
      "Epoch [5128/10000] Avg train loss: 0.057125\n",
      "Epoch [5129/10000] Avg train loss: 0.057114\n",
      "Epoch [5130/10000] Avg train loss: 0.057103\n",
      "Epoch [5131/10000] Avg train loss: 0.057092\n",
      "Epoch [5132/10000] Avg train loss: 0.057081\n",
      "Epoch [5133/10000] Avg train loss: 0.057070\n",
      "Epoch [5134/10000] Avg train loss: 0.057059\n",
      "Epoch [5135/10000] Avg train loss: 0.057047\n",
      "Epoch [5136/10000] Avg train loss: 0.057036\n",
      "Epoch [5137/10000] Avg train loss: 0.057025\n",
      "Epoch [5138/10000] Avg train loss: 0.057014\n",
      "Epoch [5139/10000] Avg train loss: 0.057003\n",
      "Epoch [5140/10000] Avg train loss: 0.056992\n",
      "Epoch [5141/10000] Avg train loss: 0.056981\n",
      "Epoch [5142/10000] Avg train loss: 0.056970\n",
      "Epoch [5143/10000] Avg train loss: 0.056959\n",
      "Epoch [5144/10000] Avg train loss: 0.056948\n",
      "Epoch [5145/10000] Avg train loss: 0.056937\n",
      "Epoch [5146/10000] Avg train loss: 0.056926\n",
      "Epoch [5147/10000] Avg train loss: 0.056915\n",
      "Epoch [5148/10000] Avg train loss: 0.056904\n",
      "Epoch [5149/10000] Avg train loss: 0.056893\n",
      "Epoch [5150/10000] Avg train loss: 0.056882\n",
      "Epoch [5151/10000] Avg train loss: 0.056871\n",
      "Epoch [5152/10000] Avg train loss: 0.056860\n",
      "Epoch [5153/10000] Avg train loss: 0.056849\n",
      "Epoch [5154/10000] Avg train loss: 0.056838\n",
      "Epoch [5155/10000] Avg train loss: 0.056827\n",
      "Epoch [5156/10000] Avg train loss: 0.056816\n",
      "Epoch [5157/10000] Avg train loss: 0.056805\n",
      "Epoch [5158/10000] Avg train loss: 0.056794\n",
      "Epoch [5159/10000] Avg train loss: 0.056783\n",
      "Epoch [5160/10000] Avg train loss: 0.056772\n",
      "Epoch [5161/10000] Avg train loss: 0.056761\n",
      "Epoch [5162/10000] Avg train loss: 0.056750\n",
      "Epoch [5163/10000] Avg train loss: 0.056739\n",
      "Epoch [5164/10000] Avg train loss: 0.056728\n",
      "Epoch [5165/10000] Avg train loss: 0.056717\n",
      "Epoch [5166/10000] Avg train loss: 0.056706\n",
      "Epoch [5167/10000] Avg train loss: 0.056695\n",
      "Epoch [5168/10000] Avg train loss: 0.056684\n",
      "Epoch [5169/10000] Avg train loss: 0.056673\n",
      "Epoch [5170/10000] Avg train loss: 0.056662\n",
      "Epoch [5171/10000] Avg train loss: 0.056652\n",
      "Epoch [5172/10000] Avg train loss: 0.056641\n",
      "Epoch [5173/10000] Avg train loss: 0.056630\n",
      "Epoch [5174/10000] Avg train loss: 0.056619\n",
      "Epoch [5175/10000] Avg train loss: 0.056608\n",
      "Epoch [5176/10000] Avg train loss: 0.056597\n",
      "Epoch [5177/10000] Avg train loss: 0.056586\n",
      "Epoch [5178/10000] Avg train loss: 0.056575\n",
      "Epoch [5179/10000] Avg train loss: 0.056564\n",
      "Epoch [5180/10000] Avg train loss: 0.056553\n",
      "Epoch [5181/10000] Avg train loss: 0.056543\n",
      "Epoch [5182/10000] Avg train loss: 0.056532\n",
      "Epoch [5183/10000] Avg train loss: 0.056521\n",
      "Epoch [5184/10000] Avg train loss: 0.056510\n",
      "Epoch [5185/10000] Avg train loss: 0.056499\n",
      "Epoch [5186/10000] Avg train loss: 0.056488\n",
      "Epoch [5187/10000] Avg train loss: 0.056477\n",
      "Epoch [5188/10000] Avg train loss: 0.056467\n",
      "Epoch [5189/10000] Avg train loss: 0.056456\n",
      "Epoch [5190/10000] Avg train loss: 0.056445\n",
      "Epoch [5191/10000] Avg train loss: 0.056434\n",
      "Epoch [5192/10000] Avg train loss: 0.056423\n",
      "Epoch [5193/10000] Avg train loss: 0.056412\n",
      "Epoch [5194/10000] Avg train loss: 0.056402\n",
      "Epoch [5195/10000] Avg train loss: 0.056391\n",
      "Epoch [5196/10000] Avg train loss: 0.056380\n",
      "Epoch [5197/10000] Avg train loss: 0.056369\n",
      "Epoch [5198/10000] Avg train loss: 0.056358\n",
      "Epoch [5199/10000] Avg train loss: 0.056347\n",
      "Epoch [5200/10000] Avg train loss: 0.056337\n",
      "Epoch [5201/10000] Avg train loss: 0.056326\n",
      "Epoch [5202/10000] Avg train loss: 0.056315\n",
      "Epoch [5203/10000] Avg train loss: 0.056304\n",
      "Epoch [5204/10000] Avg train loss: 0.056293\n",
      "Epoch [5205/10000] Avg train loss: 0.056283\n",
      "Epoch [5206/10000] Avg train loss: 0.056272\n",
      "Epoch [5207/10000] Avg train loss: 0.056261\n",
      "Epoch [5208/10000] Avg train loss: 0.056250\n",
      "Epoch [5209/10000] Avg train loss: 0.056240\n",
      "Epoch [5210/10000] Avg train loss: 0.056229\n",
      "Epoch [5211/10000] Avg train loss: 0.056218\n",
      "Epoch [5212/10000] Avg train loss: 0.056207\n",
      "Epoch [5213/10000] Avg train loss: 0.056197\n",
      "Epoch [5214/10000] Avg train loss: 0.056186\n",
      "Epoch [5215/10000] Avg train loss: 0.056175\n",
      "Epoch [5216/10000] Avg train loss: 0.056164\n",
      "Epoch [5217/10000] Avg train loss: 0.056154\n",
      "Epoch [5218/10000] Avg train loss: 0.056143\n",
      "Epoch [5219/10000] Avg train loss: 0.056132\n",
      "Epoch [5220/10000] Avg train loss: 0.056121\n",
      "Epoch [5221/10000] Avg train loss: 0.056111\n",
      "Epoch [5222/10000] Avg train loss: 0.056100\n",
      "Epoch [5223/10000] Avg train loss: 0.056089\n",
      "Epoch [5224/10000] Avg train loss: 0.056079\n",
      "Epoch [5225/10000] Avg train loss: 0.056068\n",
      "Epoch [5226/10000] Avg train loss: 0.056057\n",
      "Epoch [5227/10000] Avg train loss: 0.056047\n",
      "Epoch [5228/10000] Avg train loss: 0.056036\n",
      "Epoch [5229/10000] Avg train loss: 0.056025\n",
      "Epoch [5230/10000] Avg train loss: 0.056014\n",
      "Epoch [5231/10000] Avg train loss: 0.056004\n",
      "Epoch [5232/10000] Avg train loss: 0.055993\n",
      "Epoch [5233/10000] Avg train loss: 0.055982\n",
      "Epoch [5234/10000] Avg train loss: 0.055972\n",
      "Epoch [5235/10000] Avg train loss: 0.055961\n",
      "Epoch [5236/10000] Avg train loss: 0.055950\n",
      "Epoch [5237/10000] Avg train loss: 0.055940\n",
      "Epoch [5238/10000] Avg train loss: 0.055929\n",
      "Epoch [5239/10000] Avg train loss: 0.055919\n",
      "Epoch [5240/10000] Avg train loss: 0.055908\n",
      "Epoch [5241/10000] Avg train loss: 0.055897\n",
      "Epoch [5242/10000] Avg train loss: 0.055887\n",
      "Epoch [5243/10000] Avg train loss: 0.055876\n",
      "Epoch [5244/10000] Avg train loss: 0.055865\n",
      "Epoch [5245/10000] Avg train loss: 0.055855\n",
      "Epoch [5246/10000] Avg train loss: 0.055844\n",
      "Epoch [5247/10000] Avg train loss: 0.055834\n",
      "Epoch [5248/10000] Avg train loss: 0.055823\n",
      "Epoch [5249/10000] Avg train loss: 0.055812\n",
      "Epoch [5250/10000] Avg train loss: 0.055802\n",
      "Epoch [5251/10000] Avg train loss: 0.055791\n",
      "Epoch [5252/10000] Avg train loss: 0.055781\n",
      "Epoch [5253/10000] Avg train loss: 0.055770\n",
      "Epoch [5254/10000] Avg train loss: 0.055759\n",
      "Epoch [5255/10000] Avg train loss: 0.055749\n",
      "Epoch [5256/10000] Avg train loss: 0.055738\n",
      "Epoch [5257/10000] Avg train loss: 0.055728\n",
      "Epoch [5258/10000] Avg train loss: 0.055717\n",
      "Epoch [5259/10000] Avg train loss: 0.055707\n",
      "Epoch [5260/10000] Avg train loss: 0.055696\n",
      "Epoch [5261/10000] Avg train loss: 0.055685\n",
      "Epoch [5262/10000] Avg train loss: 0.055675\n",
      "Epoch [5263/10000] Avg train loss: 0.055664\n",
      "Epoch [5264/10000] Avg train loss: 0.055654\n",
      "Epoch [5265/10000] Avg train loss: 0.055643\n",
      "Epoch [5266/10000] Avg train loss: 0.055633\n",
      "Epoch [5267/10000] Avg train loss: 0.055622\n",
      "Epoch [5268/10000] Avg train loss: 0.055612\n",
      "Epoch [5269/10000] Avg train loss: 0.055601\n",
      "Epoch [5270/10000] Avg train loss: 0.055591\n",
      "Epoch [5271/10000] Avg train loss: 0.055580\n",
      "Epoch [5272/10000] Avg train loss: 0.055570\n",
      "Epoch [5273/10000] Avg train loss: 0.055559\n",
      "Epoch [5274/10000] Avg train loss: 0.055549\n",
      "Epoch [5275/10000] Avg train loss: 0.055538\n",
      "Epoch [5276/10000] Avg train loss: 0.055528\n",
      "Epoch [5277/10000] Avg train loss: 0.055517\n",
      "Epoch [5278/10000] Avg train loss: 0.055507\n",
      "Epoch [5279/10000] Avg train loss: 0.055496\n",
      "Epoch [5280/10000] Avg train loss: 0.055486\n",
      "Epoch [5281/10000] Avg train loss: 0.055475\n",
      "Epoch [5282/10000] Avg train loss: 0.055465\n",
      "Epoch [5283/10000] Avg train loss: 0.055454\n",
      "Epoch [5284/10000] Avg train loss: 0.055444\n",
      "Epoch [5285/10000] Avg train loss: 0.055433\n",
      "Epoch [5286/10000] Avg train loss: 0.055423\n",
      "Epoch [5287/10000] Avg train loss: 0.055412\n",
      "Epoch [5288/10000] Avg train loss: 0.055402\n",
      "Epoch [5289/10000] Avg train loss: 0.055392\n",
      "Epoch [5290/10000] Avg train loss: 0.055381\n",
      "Epoch [5291/10000] Avg train loss: 0.055371\n",
      "Epoch [5292/10000] Avg train loss: 0.055360\n",
      "Epoch [5293/10000] Avg train loss: 0.055350\n",
      "Epoch [5294/10000] Avg train loss: 0.055339\n",
      "Epoch [5295/10000] Avg train loss: 0.055329\n",
      "Epoch [5296/10000] Avg train loss: 0.055319\n",
      "Epoch [5297/10000] Avg train loss: 0.055308\n",
      "Epoch [5298/10000] Avg train loss: 0.055298\n",
      "Epoch [5299/10000] Avg train loss: 0.055287\n",
      "Epoch [5300/10000] Avg train loss: 0.055277\n",
      "Epoch [5301/10000] Avg train loss: 0.055267\n",
      "Epoch [5302/10000] Avg train loss: 0.055256\n",
      "Epoch [5303/10000] Avg train loss: 0.055246\n",
      "Epoch [5304/10000] Avg train loss: 0.055235\n",
      "Epoch [5305/10000] Avg train loss: 0.055225\n",
      "Epoch [5306/10000] Avg train loss: 0.055215\n",
      "Epoch [5307/10000] Avg train loss: 0.055204\n",
      "Epoch [5308/10000] Avg train loss: 0.055194\n",
      "Epoch [5309/10000] Avg train loss: 0.055183\n",
      "Epoch [5310/10000] Avg train loss: 0.055173\n",
      "Epoch [5311/10000] Avg train loss: 0.055163\n",
      "Epoch [5312/10000] Avg train loss: 0.055152\n",
      "Epoch [5313/10000] Avg train loss: 0.055142\n",
      "Epoch [5314/10000] Avg train loss: 0.055132\n",
      "Epoch [5315/10000] Avg train loss: 0.055121\n",
      "Epoch [5316/10000] Avg train loss: 0.055111\n",
      "Epoch [5317/10000] Avg train loss: 0.055101\n",
      "Epoch [5318/10000] Avg train loss: 0.055090\n",
      "Epoch [5319/10000] Avg train loss: 0.055080\n",
      "Epoch [5320/10000] Avg train loss: 0.055070\n",
      "Epoch [5321/10000] Avg train loss: 0.055059\n",
      "Epoch [5322/10000] Avg train loss: 0.055049\n",
      "Epoch [5323/10000] Avg train loss: 0.055039\n",
      "Epoch [5324/10000] Avg train loss: 0.055028\n",
      "Epoch [5325/10000] Avg train loss: 0.055018\n",
      "Epoch [5326/10000] Avg train loss: 0.055008\n",
      "Epoch [5327/10000] Avg train loss: 0.054998\n",
      "Epoch [5328/10000] Avg train loss: 0.054987\n",
      "Epoch [5329/10000] Avg train loss: 0.054977\n",
      "Epoch [5330/10000] Avg train loss: 0.054967\n",
      "Epoch [5331/10000] Avg train loss: 0.054956\n",
      "Epoch [5332/10000] Avg train loss: 0.054946\n",
      "Epoch [5333/10000] Avg train loss: 0.054936\n",
      "Epoch [5334/10000] Avg train loss: 0.054926\n",
      "Epoch [5335/10000] Avg train loss: 0.054915\n",
      "Epoch [5336/10000] Avg train loss: 0.054905\n",
      "Epoch [5337/10000] Avg train loss: 0.054895\n",
      "Epoch [5338/10000] Avg train loss: 0.054885\n",
      "Epoch [5339/10000] Avg train loss: 0.054874\n",
      "Epoch [5340/10000] Avg train loss: 0.054864\n",
      "Epoch [5341/10000] Avg train loss: 0.054854\n",
      "Epoch [5342/10000] Avg train loss: 0.054844\n",
      "Epoch [5343/10000] Avg train loss: 0.054833\n",
      "Epoch [5344/10000] Avg train loss: 0.054823\n",
      "Epoch [5345/10000] Avg train loss: 0.054813\n",
      "Epoch [5346/10000] Avg train loss: 0.054803\n",
      "Epoch [5347/10000] Avg train loss: 0.054792\n",
      "Epoch [5348/10000] Avg train loss: 0.054782\n",
      "Epoch [5349/10000] Avg train loss: 0.054772\n",
      "Epoch [5350/10000] Avg train loss: 0.054762\n",
      "Epoch [5351/10000] Avg train loss: 0.054752\n",
      "Epoch [5352/10000] Avg train loss: 0.054741\n",
      "Epoch [5353/10000] Avg train loss: 0.054731\n",
      "Epoch [5354/10000] Avg train loss: 0.054721\n",
      "Epoch [5355/10000] Avg train loss: 0.054711\n",
      "Epoch [5356/10000] Avg train loss: 0.054701\n",
      "Epoch [5357/10000] Avg train loss: 0.054691\n",
      "Epoch [5358/10000] Avg train loss: 0.054680\n",
      "Epoch [5359/10000] Avg train loss: 0.054670\n",
      "Epoch [5360/10000] Avg train loss: 0.054660\n",
      "Epoch [5361/10000] Avg train loss: 0.054650\n",
      "Epoch [5362/10000] Avg train loss: 0.054640\n",
      "Epoch [5363/10000] Avg train loss: 0.054629\n",
      "Epoch [5364/10000] Avg train loss: 0.054619\n",
      "Epoch [5365/10000] Avg train loss: 0.054609\n",
      "Epoch [5366/10000] Avg train loss: 0.054599\n",
      "Epoch [5367/10000] Avg train loss: 0.054589\n",
      "Epoch [5368/10000] Avg train loss: 0.054579\n",
      "Epoch [5369/10000] Avg train loss: 0.054569\n",
      "Epoch [5370/10000] Avg train loss: 0.054558\n",
      "Epoch [5371/10000] Avg train loss: 0.054548\n",
      "Epoch [5372/10000] Avg train loss: 0.054538\n",
      "Epoch [5373/10000] Avg train loss: 0.054528\n",
      "Epoch [5374/10000] Avg train loss: 0.054518\n",
      "Epoch [5375/10000] Avg train loss: 0.054508\n",
      "Epoch [5376/10000] Avg train loss: 0.054498\n",
      "Epoch [5377/10000] Avg train loss: 0.054488\n",
      "Epoch [5378/10000] Avg train loss: 0.054478\n",
      "Epoch [5379/10000] Avg train loss: 0.054467\n",
      "Epoch [5380/10000] Avg train loss: 0.054457\n",
      "Epoch [5381/10000] Avg train loss: 0.054447\n",
      "Epoch [5382/10000] Avg train loss: 0.054437\n",
      "Epoch [5383/10000] Avg train loss: 0.054427\n",
      "Epoch [5384/10000] Avg train loss: 0.054417\n",
      "Epoch [5385/10000] Avg train loss: 0.054407\n",
      "Epoch [5386/10000] Avg train loss: 0.054397\n",
      "Epoch [5387/10000] Avg train loss: 0.054387\n",
      "Epoch [5388/10000] Avg train loss: 0.054377\n",
      "Epoch [5389/10000] Avg train loss: 0.054367\n",
      "Epoch [5390/10000] Avg train loss: 0.054357\n",
      "Epoch [5391/10000] Avg train loss: 0.054347\n",
      "Epoch [5392/10000] Avg train loss: 0.054337\n",
      "Epoch [5393/10000] Avg train loss: 0.054326\n",
      "Epoch [5394/10000] Avg train loss: 0.054316\n",
      "Epoch [5395/10000] Avg train loss: 0.054306\n",
      "Epoch [5396/10000] Avg train loss: 0.054296\n",
      "Epoch [5397/10000] Avg train loss: 0.054286\n",
      "Epoch [5398/10000] Avg train loss: 0.054276\n",
      "Epoch [5399/10000] Avg train loss: 0.054266\n",
      "Epoch [5400/10000] Avg train loss: 0.054256\n",
      "Epoch [5401/10000] Avg train loss: 0.054246\n",
      "Epoch [5402/10000] Avg train loss: 0.054236\n",
      "Epoch [5403/10000] Avg train loss: 0.054226\n",
      "Epoch [5404/10000] Avg train loss: 0.054216\n",
      "Epoch [5405/10000] Avg train loss: 0.054206\n",
      "Epoch [5406/10000] Avg train loss: 0.054196\n",
      "Epoch [5407/10000] Avg train loss: 0.054186\n",
      "Epoch [5408/10000] Avg train loss: 0.054176\n",
      "Epoch [5409/10000] Avg train loss: 0.054166\n",
      "Epoch [5410/10000] Avg train loss: 0.054156\n",
      "Epoch [5411/10000] Avg train loss: 0.054146\n",
      "Epoch [5412/10000] Avg train loss: 0.054136\n",
      "Epoch [5413/10000] Avg train loss: 0.054126\n",
      "Epoch [5414/10000] Avg train loss: 0.054116\n",
      "Epoch [5415/10000] Avg train loss: 0.054106\n",
      "Epoch [5416/10000] Avg train loss: 0.054096\n",
      "Epoch [5417/10000] Avg train loss: 0.054086\n",
      "Epoch [5418/10000] Avg train loss: 0.054077\n",
      "Epoch [5419/10000] Avg train loss: 0.054067\n",
      "Epoch [5420/10000] Avg train loss: 0.054057\n",
      "Epoch [5421/10000] Avg train loss: 0.054047\n",
      "Epoch [5422/10000] Avg train loss: 0.054037\n",
      "Epoch [5423/10000] Avg train loss: 0.054027\n",
      "Epoch [5424/10000] Avg train loss: 0.054017\n",
      "Epoch [5425/10000] Avg train loss: 0.054007\n",
      "Epoch [5426/10000] Avg train loss: 0.053997\n",
      "Epoch [5427/10000] Avg train loss: 0.053987\n",
      "Epoch [5428/10000] Avg train loss: 0.053977\n",
      "Epoch [5429/10000] Avg train loss: 0.053967\n",
      "Epoch [5430/10000] Avg train loss: 0.053957\n",
      "Epoch [5431/10000] Avg train loss: 0.053947\n",
      "Epoch [5432/10000] Avg train loss: 0.053938\n",
      "Epoch [5433/10000] Avg train loss: 0.053928\n",
      "Epoch [5434/10000] Avg train loss: 0.053918\n",
      "Epoch [5435/10000] Avg train loss: 0.053908\n",
      "Epoch [5436/10000] Avg train loss: 0.053898\n",
      "Epoch [5437/10000] Avg train loss: 0.053888\n",
      "Epoch [5438/10000] Avg train loss: 0.053878\n",
      "Epoch [5439/10000] Avg train loss: 0.053868\n",
      "Epoch [5440/10000] Avg train loss: 0.053859\n",
      "Epoch [5441/10000] Avg train loss: 0.053849\n",
      "Epoch [5442/10000] Avg train loss: 0.053839\n",
      "Epoch [5443/10000] Avg train loss: 0.053829\n",
      "Epoch [5444/10000] Avg train loss: 0.053819\n",
      "Epoch [5445/10000] Avg train loss: 0.053809\n",
      "Epoch [5446/10000] Avg train loss: 0.053799\n",
      "Epoch [5447/10000] Avg train loss: 0.053789\n",
      "Epoch [5448/10000] Avg train loss: 0.053780\n",
      "Epoch [5449/10000] Avg train loss: 0.053770\n",
      "Epoch [5450/10000] Avg train loss: 0.053760\n",
      "Epoch [5451/10000] Avg train loss: 0.053750\n",
      "Epoch [5452/10000] Avg train loss: 0.053740\n",
      "Epoch [5453/10000] Avg train loss: 0.053730\n",
      "Epoch [5454/10000] Avg train loss: 0.053721\n",
      "Epoch [5455/10000] Avg train loss: 0.053711\n",
      "Epoch [5456/10000] Avg train loss: 0.053701\n",
      "Epoch [5457/10000] Avg train loss: 0.053691\n",
      "Epoch [5458/10000] Avg train loss: 0.053681\n",
      "Epoch [5459/10000] Avg train loss: 0.053672\n",
      "Epoch [5460/10000] Avg train loss: 0.053662\n",
      "Epoch [5461/10000] Avg train loss: 0.053652\n",
      "Epoch [5462/10000] Avg train loss: 0.053642\n",
      "Epoch [5463/10000] Avg train loss: 0.053632\n",
      "Epoch [5464/10000] Avg train loss: 0.053623\n",
      "Epoch [5465/10000] Avg train loss: 0.053613\n",
      "Epoch [5466/10000] Avg train loss: 0.053603\n",
      "Epoch [5467/10000] Avg train loss: 0.053593\n",
      "Epoch [5468/10000] Avg train loss: 0.053584\n",
      "Epoch [5469/10000] Avg train loss: 0.053574\n",
      "Epoch [5470/10000] Avg train loss: 0.053564\n",
      "Epoch [5471/10000] Avg train loss: 0.053554\n",
      "Epoch [5472/10000] Avg train loss: 0.053545\n",
      "Epoch [5473/10000] Avg train loss: 0.053535\n",
      "Epoch [5474/10000] Avg train loss: 0.053525\n",
      "Epoch [5475/10000] Avg train loss: 0.053515\n",
      "Epoch [5476/10000] Avg train loss: 0.053506\n",
      "Epoch [5477/10000] Avg train loss: 0.053496\n",
      "Epoch [5478/10000] Avg train loss: 0.053486\n",
      "Epoch [5479/10000] Avg train loss: 0.053476\n",
      "Epoch [5480/10000] Avg train loss: 0.053467\n",
      "Epoch [5481/10000] Avg train loss: 0.053457\n",
      "Epoch [5482/10000] Avg train loss: 0.053447\n",
      "Epoch [5483/10000] Avg train loss: 0.053437\n",
      "Epoch [5484/10000] Avg train loss: 0.053428\n",
      "Epoch [5485/10000] Avg train loss: 0.053418\n",
      "Epoch [5486/10000] Avg train loss: 0.053408\n",
      "Epoch [5487/10000] Avg train loss: 0.053399\n",
      "Epoch [5488/10000] Avg train loss: 0.053389\n",
      "Epoch [5489/10000] Avg train loss: 0.053379\n",
      "Epoch [5490/10000] Avg train loss: 0.053370\n",
      "Epoch [5491/10000] Avg train loss: 0.053360\n",
      "Epoch [5492/10000] Avg train loss: 0.053350\n",
      "Epoch [5493/10000] Avg train loss: 0.053340\n",
      "Epoch [5494/10000] Avg train loss: 0.053331\n",
      "Epoch [5495/10000] Avg train loss: 0.053321\n",
      "Epoch [5496/10000] Avg train loss: 0.053311\n",
      "Epoch [5497/10000] Avg train loss: 0.053302\n",
      "Epoch [5498/10000] Avg train loss: 0.053292\n",
      "Epoch [5499/10000] Avg train loss: 0.053282\n",
      "Epoch [5500/10000] Avg train loss: 0.053273\n",
      "Epoch [5501/10000] Avg train loss: 0.053263\n",
      "Epoch [5502/10000] Avg train loss: 0.053253\n",
      "Epoch [5503/10000] Avg train loss: 0.053244\n",
      "Epoch [5504/10000] Avg train loss: 0.053234\n",
      "Epoch [5505/10000] Avg train loss: 0.053225\n",
      "Epoch [5506/10000] Avg train loss: 0.053215\n",
      "Epoch [5507/10000] Avg train loss: 0.053205\n",
      "Epoch [5508/10000] Avg train loss: 0.053196\n",
      "Epoch [5509/10000] Avg train loss: 0.053186\n",
      "Epoch [5510/10000] Avg train loss: 0.053176\n",
      "Epoch [5511/10000] Avg train loss: 0.053167\n",
      "Epoch [5512/10000] Avg train loss: 0.053157\n",
      "Epoch [5513/10000] Avg train loss: 0.053147\n",
      "Epoch [5514/10000] Avg train loss: 0.053138\n",
      "Epoch [5515/10000] Avg train loss: 0.053128\n",
      "Epoch [5516/10000] Avg train loss: 0.053119\n",
      "Epoch [5517/10000] Avg train loss: 0.053109\n",
      "Epoch [5518/10000] Avg train loss: 0.053099\n",
      "Epoch [5519/10000] Avg train loss: 0.053090\n",
      "Epoch [5520/10000] Avg train loss: 0.053080\n",
      "Epoch [5521/10000] Avg train loss: 0.053071\n",
      "Epoch [5522/10000] Avg train loss: 0.053061\n",
      "Epoch [5523/10000] Avg train loss: 0.053052\n",
      "Epoch [5524/10000] Avg train loss: 0.053042\n",
      "Epoch [5525/10000] Avg train loss: 0.053032\n",
      "Epoch [5526/10000] Avg train loss: 0.053023\n",
      "Epoch [5527/10000] Avg train loss: 0.053013\n",
      "Epoch [5528/10000] Avg train loss: 0.053004\n",
      "Epoch [5529/10000] Avg train loss: 0.052994\n",
      "Epoch [5530/10000] Avg train loss: 0.052985\n",
      "Epoch [5531/10000] Avg train loss: 0.052975\n",
      "Epoch [5532/10000] Avg train loss: 0.052965\n",
      "Epoch [5533/10000] Avg train loss: 0.052956\n",
      "Epoch [5534/10000] Avg train loss: 0.052946\n",
      "Epoch [5535/10000] Avg train loss: 0.052937\n",
      "Epoch [5536/10000] Avg train loss: 0.052927\n",
      "Epoch [5537/10000] Avg train loss: 0.052918\n",
      "Epoch [5538/10000] Avg train loss: 0.052908\n",
      "Epoch [5539/10000] Avg train loss: 0.052899\n",
      "Epoch [5540/10000] Avg train loss: 0.052889\n",
      "Epoch [5541/10000] Avg train loss: 0.052880\n",
      "Epoch [5542/10000] Avg train loss: 0.052870\n",
      "Epoch [5543/10000] Avg train loss: 0.052861\n",
      "Epoch [5544/10000] Avg train loss: 0.052851\n",
      "Epoch [5545/10000] Avg train loss: 0.052842\n",
      "Epoch [5546/10000] Avg train loss: 0.052832\n",
      "Epoch [5547/10000] Avg train loss: 0.052823\n",
      "Epoch [5548/10000] Avg train loss: 0.052813\n",
      "Epoch [5549/10000] Avg train loss: 0.052804\n",
      "Epoch [5550/10000] Avg train loss: 0.052794\n",
      "Epoch [5551/10000] Avg train loss: 0.052785\n",
      "Epoch [5552/10000] Avg train loss: 0.052775\n",
      "Epoch [5553/10000] Avg train loss: 0.052766\n",
      "Epoch [5554/10000] Avg train loss: 0.052756\n",
      "Epoch [5555/10000] Avg train loss: 0.052747\n",
      "Epoch [5556/10000] Avg train loss: 0.052737\n",
      "Epoch [5557/10000] Avg train loss: 0.052728\n",
      "Epoch [5558/10000] Avg train loss: 0.052718\n",
      "Epoch [5559/10000] Avg train loss: 0.052709\n",
      "Epoch [5560/10000] Avg train loss: 0.052700\n",
      "Epoch [5561/10000] Avg train loss: 0.052690\n",
      "Epoch [5562/10000] Avg train loss: 0.052681\n",
      "Epoch [5563/10000] Avg train loss: 0.052671\n",
      "Epoch [5564/10000] Avg train loss: 0.052662\n",
      "Epoch [5565/10000] Avg train loss: 0.052652\n",
      "Epoch [5566/10000] Avg train loss: 0.052643\n",
      "Epoch [5567/10000] Avg train loss: 0.052633\n",
      "Epoch [5568/10000] Avg train loss: 0.052624\n",
      "Epoch [5569/10000] Avg train loss: 0.052615\n",
      "Epoch [5570/10000] Avg train loss: 0.052605\n",
      "Epoch [5571/10000] Avg train loss: 0.052596\n",
      "Epoch [5572/10000] Avg train loss: 0.052586\n",
      "Epoch [5573/10000] Avg train loss: 0.052577\n",
      "Epoch [5574/10000] Avg train loss: 0.052568\n",
      "Epoch [5575/10000] Avg train loss: 0.052558\n",
      "Epoch [5576/10000] Avg train loss: 0.052549\n",
      "Epoch [5577/10000] Avg train loss: 0.052539\n",
      "Epoch [5578/10000] Avg train loss: 0.052530\n",
      "Epoch [5579/10000] Avg train loss: 0.052521\n",
      "Epoch [5580/10000] Avg train loss: 0.052511\n",
      "Epoch [5581/10000] Avg train loss: 0.052502\n",
      "Epoch [5582/10000] Avg train loss: 0.052492\n",
      "Epoch [5583/10000] Avg train loss: 0.052483\n",
      "Epoch [5584/10000] Avg train loss: 0.052474\n",
      "Epoch [5585/10000] Avg train loss: 0.052464\n",
      "Epoch [5586/10000] Avg train loss: 0.052455\n",
      "Epoch [5587/10000] Avg train loss: 0.052446\n",
      "Epoch [5588/10000] Avg train loss: 0.052436\n",
      "Epoch [5589/10000] Avg train loss: 0.052427\n",
      "Epoch [5590/10000] Avg train loss: 0.052418\n",
      "Epoch [5591/10000] Avg train loss: 0.052408\n",
      "Epoch [5592/10000] Avg train loss: 0.052399\n",
      "Epoch [5593/10000] Avg train loss: 0.052390\n",
      "Epoch [5594/10000] Avg train loss: 0.052380\n",
      "Epoch [5595/10000] Avg train loss: 0.052371\n",
      "Epoch [5596/10000] Avg train loss: 0.052362\n",
      "Epoch [5597/10000] Avg train loss: 0.052352\n",
      "Epoch [5598/10000] Avg train loss: 0.052343\n",
      "Epoch [5599/10000] Avg train loss: 0.052334\n",
      "Epoch [5600/10000] Avg train loss: 0.052324\n",
      "Epoch [5601/10000] Avg train loss: 0.052315\n",
      "Epoch [5602/10000] Avg train loss: 0.052306\n",
      "Epoch [5603/10000] Avg train loss: 0.052296\n",
      "Epoch [5604/10000] Avg train loss: 0.052287\n",
      "Epoch [5605/10000] Avg train loss: 0.052278\n",
      "Epoch [5606/10000] Avg train loss: 0.052268\n",
      "Epoch [5607/10000] Avg train loss: 0.052259\n",
      "Epoch [5608/10000] Avg train loss: 0.052250\n",
      "Epoch [5609/10000] Avg train loss: 0.052241\n",
      "Epoch [5610/10000] Avg train loss: 0.052231\n",
      "Epoch [5611/10000] Avg train loss: 0.052222\n",
      "Epoch [5612/10000] Avg train loss: 0.052213\n",
      "Epoch [5613/10000] Avg train loss: 0.052203\n",
      "Epoch [5614/10000] Avg train loss: 0.052194\n",
      "Epoch [5615/10000] Avg train loss: 0.052185\n",
      "Epoch [5616/10000] Avg train loss: 0.052176\n",
      "Epoch [5617/10000] Avg train loss: 0.052166\n",
      "Epoch [5618/10000] Avg train loss: 0.052157\n",
      "Epoch [5619/10000] Avg train loss: 0.052148\n",
      "Epoch [5620/10000] Avg train loss: 0.052139\n",
      "Epoch [5621/10000] Avg train loss: 0.052129\n",
      "Epoch [5622/10000] Avg train loss: 0.052120\n",
      "Epoch [5623/10000] Avg train loss: 0.052111\n",
      "Epoch [5624/10000] Avg train loss: 0.052102\n",
      "Epoch [5625/10000] Avg train loss: 0.052092\n",
      "Epoch [5626/10000] Avg train loss: 0.052083\n",
      "Epoch [5627/10000] Avg train loss: 0.052074\n",
      "Epoch [5628/10000] Avg train loss: 0.052065\n",
      "Epoch [5629/10000] Avg train loss: 0.052055\n",
      "Epoch [5630/10000] Avg train loss: 0.052046\n",
      "Epoch [5631/10000] Avg train loss: 0.052037\n",
      "Epoch [5632/10000] Avg train loss: 0.052028\n",
      "Epoch [5633/10000] Avg train loss: 0.052019\n",
      "Epoch [5634/10000] Avg train loss: 0.052009\n",
      "Epoch [5635/10000] Avg train loss: 0.052000\n",
      "Epoch [5636/10000] Avg train loss: 0.051991\n",
      "Epoch [5637/10000] Avg train loss: 0.051982\n",
      "Epoch [5638/10000] Avg train loss: 0.051973\n",
      "Epoch [5639/10000] Avg train loss: 0.051963\n",
      "Epoch [5640/10000] Avg train loss: 0.051954\n",
      "Epoch [5641/10000] Avg train loss: 0.051945\n",
      "Epoch [5642/10000] Avg train loss: 0.051936\n",
      "Epoch [5643/10000] Avg train loss: 0.051927\n",
      "Epoch [5644/10000] Avg train loss: 0.051918\n",
      "Epoch [5645/10000] Avg train loss: 0.051908\n",
      "Epoch [5646/10000] Avg train loss: 0.051899\n",
      "Epoch [5647/10000] Avg train loss: 0.051890\n",
      "Epoch [5648/10000] Avg train loss: 0.051881\n",
      "Epoch [5649/10000] Avg train loss: 0.051872\n",
      "Epoch [5650/10000] Avg train loss: 0.051863\n",
      "Epoch [5651/10000] Avg train loss: 0.051853\n",
      "Epoch [5652/10000] Avg train loss: 0.051844\n",
      "Epoch [5653/10000] Avg train loss: 0.051835\n",
      "Epoch [5654/10000] Avg train loss: 0.051826\n",
      "Epoch [5655/10000] Avg train loss: 0.051817\n",
      "Epoch [5656/10000] Avg train loss: 0.051808\n",
      "Epoch [5657/10000] Avg train loss: 0.051799\n",
      "Epoch [5658/10000] Avg train loss: 0.051789\n",
      "Epoch [5659/10000] Avg train loss: 0.051780\n",
      "Epoch [5660/10000] Avg train loss: 0.051771\n",
      "Epoch [5661/10000] Avg train loss: 0.051762\n",
      "Epoch [5662/10000] Avg train loss: 0.051753\n",
      "Epoch [5663/10000] Avg train loss: 0.051744\n",
      "Epoch [5664/10000] Avg train loss: 0.051735\n",
      "Epoch [5665/10000] Avg train loss: 0.051726\n",
      "Epoch [5666/10000] Avg train loss: 0.051717\n",
      "Epoch [5667/10000] Avg train loss: 0.051707\n",
      "Epoch [5668/10000] Avg train loss: 0.051698\n",
      "Epoch [5669/10000] Avg train loss: 0.051689\n",
      "Epoch [5670/10000] Avg train loss: 0.051680\n",
      "Epoch [5671/10000] Avg train loss: 0.051671\n",
      "Epoch [5672/10000] Avg train loss: 0.051662\n",
      "Epoch [5673/10000] Avg train loss: 0.051653\n",
      "Epoch [5674/10000] Avg train loss: 0.051644\n",
      "Epoch [5675/10000] Avg train loss: 0.051635\n",
      "Epoch [5676/10000] Avg train loss: 0.051626\n",
      "Epoch [5677/10000] Avg train loss: 0.051617\n",
      "Epoch [5678/10000] Avg train loss: 0.051608\n",
      "Epoch [5679/10000] Avg train loss: 0.051598\n",
      "Epoch [5680/10000] Avg train loss: 0.051589\n",
      "Epoch [5681/10000] Avg train loss: 0.051580\n",
      "Epoch [5682/10000] Avg train loss: 0.051571\n",
      "Epoch [5683/10000] Avg train loss: 0.051562\n",
      "Epoch [5684/10000] Avg train loss: 0.051553\n",
      "Epoch [5685/10000] Avg train loss: 0.051544\n",
      "Epoch [5686/10000] Avg train loss: 0.051535\n",
      "Epoch [5687/10000] Avg train loss: 0.051526\n",
      "Epoch [5688/10000] Avg train loss: 0.051517\n",
      "Epoch [5689/10000] Avg train loss: 0.051508\n",
      "Epoch [5690/10000] Avg train loss: 0.051499\n",
      "Epoch [5691/10000] Avg train loss: 0.051490\n",
      "Epoch [5692/10000] Avg train loss: 0.051481\n",
      "Epoch [5693/10000] Avg train loss: 0.051472\n",
      "Epoch [5694/10000] Avg train loss: 0.051463\n",
      "Epoch [5695/10000] Avg train loss: 0.051454\n",
      "Epoch [5696/10000] Avg train loss: 0.051445\n",
      "Epoch [5697/10000] Avg train loss: 0.051436\n",
      "Epoch [5698/10000] Avg train loss: 0.051427\n",
      "Epoch [5699/10000] Avg train loss: 0.051418\n",
      "Epoch [5700/10000] Avg train loss: 0.051409\n",
      "Epoch [5701/10000] Avg train loss: 0.051400\n",
      "Epoch [5702/10000] Avg train loss: 0.051391\n",
      "Epoch [5703/10000] Avg train loss: 0.051382\n",
      "Epoch [5704/10000] Avg train loss: 0.051373\n",
      "Epoch [5705/10000] Avg train loss: 0.051364\n",
      "Epoch [5706/10000] Avg train loss: 0.051355\n",
      "Epoch [5707/10000] Avg train loss: 0.051346\n",
      "Epoch [5708/10000] Avg train loss: 0.051337\n",
      "Epoch [5709/10000] Avg train loss: 0.051328\n",
      "Epoch [5710/10000] Avg train loss: 0.051319\n",
      "Epoch [5711/10000] Avg train loss: 0.051310\n",
      "Epoch [5712/10000] Avg train loss: 0.051301\n",
      "Epoch [5713/10000] Avg train loss: 0.051292\n",
      "Epoch [5714/10000] Avg train loss: 0.051283\n",
      "Epoch [5715/10000] Avg train loss: 0.051274\n",
      "Epoch [5716/10000] Avg train loss: 0.051265\n",
      "Epoch [5717/10000] Avg train loss: 0.051257\n",
      "Epoch [5718/10000] Avg train loss: 0.051248\n",
      "Epoch [5719/10000] Avg train loss: 0.051239\n",
      "Epoch [5720/10000] Avg train loss: 0.051230\n",
      "Epoch [5721/10000] Avg train loss: 0.051221\n",
      "Epoch [5722/10000] Avg train loss: 0.051212\n",
      "Epoch [5723/10000] Avg train loss: 0.051203\n",
      "Epoch [5724/10000] Avg train loss: 0.051194\n",
      "Epoch [5725/10000] Avg train loss: 0.051185\n",
      "Epoch [5726/10000] Avg train loss: 0.051176\n",
      "Epoch [5727/10000] Avg train loss: 0.051167\n",
      "Epoch [5728/10000] Avg train loss: 0.051158\n",
      "Epoch [5729/10000] Avg train loss: 0.051150\n",
      "Epoch [5730/10000] Avg train loss: 0.051141\n",
      "Epoch [5731/10000] Avg train loss: 0.051132\n",
      "Epoch [5732/10000] Avg train loss: 0.051123\n",
      "Epoch [5733/10000] Avg train loss: 0.051114\n",
      "Epoch [5734/10000] Avg train loss: 0.051105\n",
      "Epoch [5735/10000] Avg train loss: 0.051096\n",
      "Epoch [5736/10000] Avg train loss: 0.051087\n",
      "Epoch [5737/10000] Avg train loss: 0.051078\n",
      "Epoch [5738/10000] Avg train loss: 0.051070\n",
      "Epoch [5739/10000] Avg train loss: 0.051061\n",
      "Epoch [5740/10000] Avg train loss: 0.051052\n",
      "Epoch [5741/10000] Avg train loss: 0.051043\n",
      "Epoch [5742/10000] Avg train loss: 0.051034\n",
      "Epoch [5743/10000] Avg train loss: 0.051025\n",
      "Epoch [5744/10000] Avg train loss: 0.051016\n",
      "Epoch [5745/10000] Avg train loss: 0.051008\n",
      "Epoch [5746/10000] Avg train loss: 0.050999\n",
      "Epoch [5747/10000] Avg train loss: 0.050990\n",
      "Epoch [5748/10000] Avg train loss: 0.050981\n",
      "Epoch [5749/10000] Avg train loss: 0.050972\n",
      "Epoch [5750/10000] Avg train loss: 0.050963\n",
      "Epoch [5751/10000] Avg train loss: 0.050954\n",
      "Epoch [5752/10000] Avg train loss: 0.050946\n",
      "Epoch [5753/10000] Avg train loss: 0.050937\n",
      "Epoch [5754/10000] Avg train loss: 0.050928\n",
      "Epoch [5755/10000] Avg train loss: 0.050919\n",
      "Epoch [5756/10000] Avg train loss: 0.050910\n",
      "Epoch [5757/10000] Avg train loss: 0.050901\n",
      "Epoch [5758/10000] Avg train loss: 0.050893\n",
      "Epoch [5759/10000] Avg train loss: 0.050884\n",
      "Epoch [5760/10000] Avg train loss: 0.050875\n",
      "Epoch [5761/10000] Avg train loss: 0.050866\n",
      "Epoch [5762/10000] Avg train loss: 0.050857\n",
      "Epoch [5763/10000] Avg train loss: 0.050849\n",
      "Epoch [5764/10000] Avg train loss: 0.050840\n",
      "Epoch [5765/10000] Avg train loss: 0.050831\n",
      "Epoch [5766/10000] Avg train loss: 0.050822\n",
      "Epoch [5767/10000] Avg train loss: 0.050813\n",
      "Epoch [5768/10000] Avg train loss: 0.050805\n",
      "Epoch [5769/10000] Avg train loss: 0.050796\n",
      "Epoch [5770/10000] Avg train loss: 0.050787\n",
      "Epoch [5771/10000] Avg train loss: 0.050778\n",
      "Epoch [5772/10000] Avg train loss: 0.050770\n",
      "Epoch [5773/10000] Avg train loss: 0.050761\n",
      "Epoch [5774/10000] Avg train loss: 0.050752\n",
      "Epoch [5775/10000] Avg train loss: 0.050743\n",
      "Epoch [5776/10000] Avg train loss: 0.050735\n",
      "Epoch [5777/10000] Avg train loss: 0.050726\n",
      "Epoch [5778/10000] Avg train loss: 0.050717\n",
      "Epoch [5779/10000] Avg train loss: 0.050708\n",
      "Epoch [5780/10000] Avg train loss: 0.050700\n",
      "Epoch [5781/10000] Avg train loss: 0.050691\n",
      "Epoch [5782/10000] Avg train loss: 0.050682\n",
      "Epoch [5783/10000] Avg train loss: 0.050673\n",
      "Epoch [5784/10000] Avg train loss: 0.050665\n",
      "Epoch [5785/10000] Avg train loss: 0.050656\n",
      "Epoch [5786/10000] Avg train loss: 0.050647\n",
      "Epoch [5787/10000] Avg train loss: 0.050638\n",
      "Epoch [5788/10000] Avg train loss: 0.050630\n",
      "Epoch [5789/10000] Avg train loss: 0.050621\n",
      "Epoch [5790/10000] Avg train loss: 0.050612\n",
      "Epoch [5791/10000] Avg train loss: 0.050604\n",
      "Epoch [5792/10000] Avg train loss: 0.050595\n",
      "Epoch [5793/10000] Avg train loss: 0.050586\n",
      "Epoch [5794/10000] Avg train loss: 0.050577\n",
      "Epoch [5795/10000] Avg train loss: 0.050569\n",
      "Epoch [5796/10000] Avg train loss: 0.050560\n",
      "Epoch [5797/10000] Avg train loss: 0.050551\n",
      "Epoch [5798/10000] Avg train loss: 0.050543\n",
      "Epoch [5799/10000] Avg train loss: 0.050534\n",
      "Epoch [5800/10000] Avg train loss: 0.050525\n",
      "Epoch [5801/10000] Avg train loss: 0.050517\n",
      "Epoch [5802/10000] Avg train loss: 0.050508\n",
      "Epoch [5803/10000] Avg train loss: 0.050499\n",
      "Epoch [5804/10000] Avg train loss: 0.050491\n",
      "Epoch [5805/10000] Avg train loss: 0.050482\n",
      "Epoch [5806/10000] Avg train loss: 0.050473\n",
      "Epoch [5807/10000] Avg train loss: 0.050465\n",
      "Epoch [5808/10000] Avg train loss: 0.050456\n",
      "Epoch [5809/10000] Avg train loss: 0.050447\n",
      "Epoch [5810/10000] Avg train loss: 0.050439\n",
      "Epoch [5811/10000] Avg train loss: 0.050430\n",
      "Epoch [5812/10000] Avg train loss: 0.050421\n",
      "Epoch [5813/10000] Avg train loss: 0.050413\n",
      "Epoch [5814/10000] Avg train loss: 0.050404\n",
      "Epoch [5815/10000] Avg train loss: 0.050395\n",
      "Epoch [5816/10000] Avg train loss: 0.050387\n",
      "Epoch [5817/10000] Avg train loss: 0.050378\n",
      "Epoch [5818/10000] Avg train loss: 0.050369\n",
      "Epoch [5819/10000] Avg train loss: 0.050361\n",
      "Epoch [5820/10000] Avg train loss: 0.050352\n",
      "Epoch [5821/10000] Avg train loss: 0.050343\n",
      "Epoch [5822/10000] Avg train loss: 0.050335\n",
      "Epoch [5823/10000] Avg train loss: 0.050326\n",
      "Epoch [5824/10000] Avg train loss: 0.050318\n",
      "Epoch [5825/10000] Avg train loss: 0.050309\n",
      "Epoch [5826/10000] Avg train loss: 0.050300\n",
      "Epoch [5827/10000] Avg train loss: 0.050292\n",
      "Epoch [5828/10000] Avg train loss: 0.050283\n",
      "Epoch [5829/10000] Avg train loss: 0.050275\n",
      "Epoch [5830/10000] Avg train loss: 0.050266\n",
      "Epoch [5831/10000] Avg train loss: 0.050257\n",
      "Epoch [5832/10000] Avg train loss: 0.050249\n",
      "Epoch [5833/10000] Avg train loss: 0.050240\n",
      "Epoch [5834/10000] Avg train loss: 0.050232\n",
      "Epoch [5835/10000] Avg train loss: 0.050223\n",
      "Epoch [5836/10000] Avg train loss: 0.050214\n",
      "Epoch [5837/10000] Avg train loss: 0.050206\n",
      "Epoch [5838/10000] Avg train loss: 0.050197\n",
      "Epoch [5839/10000] Avg train loss: 0.050189\n",
      "Epoch [5840/10000] Avg train loss: 0.050180\n",
      "Epoch [5841/10000] Avg train loss: 0.050172\n",
      "Epoch [5842/10000] Avg train loss: 0.050163\n",
      "Epoch [5843/10000] Avg train loss: 0.050154\n",
      "Epoch [5844/10000] Avg train loss: 0.050146\n",
      "Epoch [5845/10000] Avg train loss: 0.050137\n",
      "Epoch [5846/10000] Avg train loss: 0.050129\n",
      "Epoch [5847/10000] Avg train loss: 0.050120\n",
      "Epoch [5848/10000] Avg train loss: 0.050112\n",
      "Epoch [5849/10000] Avg train loss: 0.050103\n",
      "Epoch [5850/10000] Avg train loss: 0.050095\n",
      "Epoch [5851/10000] Avg train loss: 0.050086\n",
      "Epoch [5852/10000] Avg train loss: 0.050078\n",
      "Epoch [5853/10000] Avg train loss: 0.050069\n",
      "Epoch [5854/10000] Avg train loss: 0.050061\n",
      "Epoch [5855/10000] Avg train loss: 0.050052\n",
      "Epoch [5856/10000] Avg train loss: 0.050043\n",
      "Epoch [5857/10000] Avg train loss: 0.050035\n",
      "Epoch [5858/10000] Avg train loss: 0.050026\n",
      "Epoch [5859/10000] Avg train loss: 0.050018\n",
      "Epoch [5860/10000] Avg train loss: 0.050009\n",
      "Epoch [5861/10000] Avg train loss: 0.050001\n",
      "Epoch [5862/10000] Avg train loss: 0.049992\n",
      "Epoch [5863/10000] Avg train loss: 0.049984\n",
      "Epoch [5864/10000] Avg train loss: 0.049975\n",
      "Epoch [5865/10000] Avg train loss: 0.049967\n",
      "Epoch [5866/10000] Avg train loss: 0.049958\n",
      "Epoch [5867/10000] Avg train loss: 0.049950\n",
      "Epoch [5868/10000] Avg train loss: 0.049941\n",
      "Epoch [5869/10000] Avg train loss: 0.049933\n",
      "Epoch [5870/10000] Avg train loss: 0.049924\n",
      "Epoch [5871/10000] Avg train loss: 0.049916\n",
      "Epoch [5872/10000] Avg train loss: 0.049908\n",
      "Epoch [5873/10000] Avg train loss: 0.049899\n",
      "Epoch [5874/10000] Avg train loss: 0.049891\n",
      "Epoch [5875/10000] Avg train loss: 0.049882\n",
      "Epoch [5876/10000] Avg train loss: 0.049874\n",
      "Epoch [5877/10000] Avg train loss: 0.049865\n",
      "Epoch [5878/10000] Avg train loss: 0.049857\n",
      "Epoch [5879/10000] Avg train loss: 0.049848\n",
      "Epoch [5880/10000] Avg train loss: 0.049840\n",
      "Epoch [5881/10000] Avg train loss: 0.049831\n",
      "Epoch [5882/10000] Avg train loss: 0.049823\n",
      "Epoch [5883/10000] Avg train loss: 0.049814\n",
      "Epoch [5884/10000] Avg train loss: 0.049806\n",
      "Epoch [5885/10000] Avg train loss: 0.049798\n",
      "Epoch [5886/10000] Avg train loss: 0.049789\n",
      "Epoch [5887/10000] Avg train loss: 0.049781\n",
      "Epoch [5888/10000] Avg train loss: 0.049772\n",
      "Epoch [5889/10000] Avg train loss: 0.049764\n",
      "Epoch [5890/10000] Avg train loss: 0.049755\n",
      "Epoch [5891/10000] Avg train loss: 0.049747\n",
      "Epoch [5892/10000] Avg train loss: 0.049739\n",
      "Epoch [5893/10000] Avg train loss: 0.049730\n",
      "Epoch [5894/10000] Avg train loss: 0.049722\n",
      "Epoch [5895/10000] Avg train loss: 0.049713\n",
      "Epoch [5896/10000] Avg train loss: 0.049705\n",
      "Epoch [5897/10000] Avg train loss: 0.049697\n",
      "Epoch [5898/10000] Avg train loss: 0.049688\n",
      "Epoch [5899/10000] Avg train loss: 0.049680\n",
      "Epoch [5900/10000] Avg train loss: 0.049671\n",
      "Epoch [5901/10000] Avg train loss: 0.049663\n",
      "Epoch [5902/10000] Avg train loss: 0.049655\n",
      "Epoch [5903/10000] Avg train loss: 0.049646\n",
      "Epoch [5904/10000] Avg train loss: 0.049638\n",
      "Epoch [5905/10000] Avg train loss: 0.049629\n",
      "Epoch [5906/10000] Avg train loss: 0.049621\n",
      "Epoch [5907/10000] Avg train loss: 0.049613\n",
      "Epoch [5908/10000] Avg train loss: 0.049604\n",
      "Epoch [5909/10000] Avg train loss: 0.049596\n",
      "Epoch [5910/10000] Avg train loss: 0.049588\n",
      "Epoch [5911/10000] Avg train loss: 0.049579\n",
      "Epoch [5912/10000] Avg train loss: 0.049571\n",
      "Epoch [5913/10000] Avg train loss: 0.049562\n",
      "Epoch [5914/10000] Avg train loss: 0.049554\n",
      "Epoch [5915/10000] Avg train loss: 0.049546\n",
      "Epoch [5916/10000] Avg train loss: 0.049537\n",
      "Epoch [5917/10000] Avg train loss: 0.049529\n",
      "Epoch [5918/10000] Avg train loss: 0.049521\n",
      "Epoch [5919/10000] Avg train loss: 0.049512\n",
      "Epoch [5920/10000] Avg train loss: 0.049504\n",
      "Epoch [5921/10000] Avg train loss: 0.049496\n",
      "Epoch [5922/10000] Avg train loss: 0.049487\n",
      "Epoch [5923/10000] Avg train loss: 0.049479\n",
      "Epoch [5924/10000] Avg train loss: 0.049471\n",
      "Epoch [5925/10000] Avg train loss: 0.049462\n",
      "Epoch [5926/10000] Avg train loss: 0.049454\n",
      "Epoch [5927/10000] Avg train loss: 0.049446\n",
      "Epoch [5928/10000] Avg train loss: 0.049437\n",
      "Epoch [5929/10000] Avg train loss: 0.049429\n",
      "Epoch [5930/10000] Avg train loss: 0.049421\n",
      "Epoch [5931/10000] Avg train loss: 0.049413\n",
      "Epoch [5932/10000] Avg train loss: 0.049404\n",
      "Epoch [5933/10000] Avg train loss: 0.049396\n",
      "Epoch [5934/10000] Avg train loss: 0.049388\n",
      "Epoch [5935/10000] Avg train loss: 0.049379\n",
      "Epoch [5936/10000] Avg train loss: 0.049371\n",
      "Epoch [5937/10000] Avg train loss: 0.049363\n",
      "Epoch [5938/10000] Avg train loss: 0.049354\n",
      "Epoch [5939/10000] Avg train loss: 0.049346\n",
      "Epoch [5940/10000] Avg train loss: 0.049338\n",
      "Epoch [5941/10000] Avg train loss: 0.049330\n",
      "Epoch [5942/10000] Avg train loss: 0.049321\n",
      "Epoch [5943/10000] Avg train loss: 0.049313\n",
      "Epoch [5944/10000] Avg train loss: 0.049305\n",
      "Epoch [5945/10000] Avg train loss: 0.049296\n",
      "Epoch [5946/10000] Avg train loss: 0.049288\n",
      "Epoch [5947/10000] Avg train loss: 0.049280\n",
      "Epoch [5948/10000] Avg train loss: 0.049272\n",
      "Epoch [5949/10000] Avg train loss: 0.049263\n",
      "Epoch [5950/10000] Avg train loss: 0.049255\n",
      "Epoch [5951/10000] Avg train loss: 0.049247\n",
      "Epoch [5952/10000] Avg train loss: 0.049239\n",
      "Epoch [5953/10000] Avg train loss: 0.049230\n",
      "Epoch [5954/10000] Avg train loss: 0.049222\n",
      "Epoch [5955/10000] Avg train loss: 0.049214\n",
      "Epoch [5956/10000] Avg train loss: 0.049206\n",
      "Epoch [5957/10000] Avg train loss: 0.049197\n",
      "Epoch [5958/10000] Avg train loss: 0.049189\n",
      "Epoch [5959/10000] Avg train loss: 0.049181\n",
      "Epoch [5960/10000] Avg train loss: 0.049173\n",
      "Epoch [5961/10000] Avg train loss: 0.049165\n",
      "Epoch [5962/10000] Avg train loss: 0.049156\n",
      "Epoch [5963/10000] Avg train loss: 0.049148\n",
      "Epoch [5964/10000] Avg train loss: 0.049140\n",
      "Epoch [5965/10000] Avg train loss: 0.049132\n",
      "Epoch [5966/10000] Avg train loss: 0.049123\n",
      "Epoch [5967/10000] Avg train loss: 0.049115\n",
      "Epoch [5968/10000] Avg train loss: 0.049107\n",
      "Epoch [5969/10000] Avg train loss: 0.049099\n",
      "Epoch [5970/10000] Avg train loss: 0.049091\n",
      "Epoch [5971/10000] Avg train loss: 0.049082\n",
      "Epoch [5972/10000] Avg train loss: 0.049074\n",
      "Epoch [5973/10000] Avg train loss: 0.049066\n",
      "Epoch [5974/10000] Avg train loss: 0.049058\n",
      "Epoch [5975/10000] Avg train loss: 0.049050\n",
      "Epoch [5976/10000] Avg train loss: 0.049041\n",
      "Epoch [5977/10000] Avg train loss: 0.049033\n",
      "Epoch [5978/10000] Avg train loss: 0.049025\n",
      "Epoch [5979/10000] Avg train loss: 0.049017\n",
      "Epoch [5980/10000] Avg train loss: 0.049009\n",
      "Epoch [5981/10000] Avg train loss: 0.049001\n",
      "Epoch [5982/10000] Avg train loss: 0.048992\n",
      "Epoch [5983/10000] Avg train loss: 0.048984\n",
      "Epoch [5984/10000] Avg train loss: 0.048976\n",
      "Epoch [5985/10000] Avg train loss: 0.048968\n",
      "Epoch [5986/10000] Avg train loss: 0.048960\n",
      "Epoch [5987/10000] Avg train loss: 0.048952\n",
      "Epoch [5988/10000] Avg train loss: 0.048943\n",
      "Epoch [5989/10000] Avg train loss: 0.048935\n",
      "Epoch [5990/10000] Avg train loss: 0.048927\n",
      "Epoch [5991/10000] Avg train loss: 0.048919\n",
      "Epoch [5992/10000] Avg train loss: 0.048911\n",
      "Epoch [5993/10000] Avg train loss: 0.048903\n",
      "Epoch [5994/10000] Avg train loss: 0.048895\n",
      "Epoch [5995/10000] Avg train loss: 0.048887\n",
      "Epoch [5996/10000] Avg train loss: 0.048878\n",
      "Epoch [5997/10000] Avg train loss: 0.048870\n",
      "Epoch [5998/10000] Avg train loss: 0.048862\n",
      "Epoch [5999/10000] Avg train loss: 0.048854\n",
      "Epoch [6000/10000] Avg train loss: 0.048846\n",
      "Epoch [6001/10000] Avg train loss: 0.048838\n",
      "Epoch [6002/10000] Avg train loss: 0.048830\n",
      "Epoch [6003/10000] Avg train loss: 0.048822\n",
      "Epoch [6004/10000] Avg train loss: 0.048813\n",
      "Epoch [6005/10000] Avg train loss: 0.048805\n",
      "Epoch [6006/10000] Avg train loss: 0.048797\n",
      "Epoch [6007/10000] Avg train loss: 0.048789\n",
      "Epoch [6008/10000] Avg train loss: 0.048781\n",
      "Epoch [6009/10000] Avg train loss: 0.048773\n",
      "Epoch [6010/10000] Avg train loss: 0.048765\n",
      "Epoch [6011/10000] Avg train loss: 0.048757\n",
      "Epoch [6012/10000] Avg train loss: 0.048749\n",
      "Epoch [6013/10000] Avg train loss: 0.048741\n",
      "Epoch [6014/10000] Avg train loss: 0.048732\n",
      "Epoch [6015/10000] Avg train loss: 0.048724\n",
      "Epoch [6016/10000] Avg train loss: 0.048716\n",
      "Epoch [6017/10000] Avg train loss: 0.048708\n",
      "Epoch [6018/10000] Avg train loss: 0.048700\n",
      "Epoch [6019/10000] Avg train loss: 0.048692\n",
      "Epoch [6020/10000] Avg train loss: 0.048684\n",
      "Epoch [6021/10000] Avg train loss: 0.048676\n",
      "Epoch [6022/10000] Avg train loss: 0.048668\n",
      "Epoch [6023/10000] Avg train loss: 0.048660\n",
      "Epoch [6024/10000] Avg train loss: 0.048652\n",
      "Epoch [6025/10000] Avg train loss: 0.048644\n",
      "Epoch [6026/10000] Avg train loss: 0.048636\n",
      "Epoch [6027/10000] Avg train loss: 0.048628\n",
      "Epoch [6028/10000] Avg train loss: 0.048620\n",
      "Epoch [6029/10000] Avg train loss: 0.048612\n",
      "Epoch [6030/10000] Avg train loss: 0.048604\n",
      "Epoch [6031/10000] Avg train loss: 0.048595\n",
      "Epoch [6032/10000] Avg train loss: 0.048587\n",
      "Epoch [6033/10000] Avg train loss: 0.048579\n",
      "Epoch [6034/10000] Avg train loss: 0.048571\n",
      "Epoch [6035/10000] Avg train loss: 0.048563\n",
      "Epoch [6036/10000] Avg train loss: 0.048555\n",
      "Epoch [6037/10000] Avg train loss: 0.048547\n",
      "Epoch [6038/10000] Avg train loss: 0.048539\n",
      "Epoch [6039/10000] Avg train loss: 0.048531\n",
      "Epoch [6040/10000] Avg train loss: 0.048523\n",
      "Epoch [6041/10000] Avg train loss: 0.048515\n",
      "Epoch [6042/10000] Avg train loss: 0.048507\n",
      "Epoch [6043/10000] Avg train loss: 0.048499\n",
      "Epoch [6044/10000] Avg train loss: 0.048491\n",
      "Epoch [6045/10000] Avg train loss: 0.048483\n",
      "Epoch [6046/10000] Avg train loss: 0.048475\n",
      "Epoch [6047/10000] Avg train loss: 0.048467\n",
      "Epoch [6048/10000] Avg train loss: 0.048459\n",
      "Epoch [6049/10000] Avg train loss: 0.048451\n",
      "Epoch [6050/10000] Avg train loss: 0.048443\n",
      "Epoch [6051/10000] Avg train loss: 0.048435\n",
      "Epoch [6052/10000] Avg train loss: 0.048427\n",
      "Epoch [6053/10000] Avg train loss: 0.048419\n",
      "Epoch [6054/10000] Avg train loss: 0.048411\n",
      "Epoch [6055/10000] Avg train loss: 0.048403\n",
      "Epoch [6056/10000] Avg train loss: 0.048396\n",
      "Epoch [6057/10000] Avg train loss: 0.048388\n",
      "Epoch [6058/10000] Avg train loss: 0.048380\n",
      "Epoch [6059/10000] Avg train loss: 0.048372\n",
      "Epoch [6060/10000] Avg train loss: 0.048364\n",
      "Epoch [6061/10000] Avg train loss: 0.048356\n",
      "Epoch [6062/10000] Avg train loss: 0.048348\n",
      "Epoch [6063/10000] Avg train loss: 0.048340\n",
      "Epoch [6064/10000] Avg train loss: 0.048332\n",
      "Epoch [6065/10000] Avg train loss: 0.048324\n",
      "Epoch [6066/10000] Avg train loss: 0.048316\n",
      "Epoch [6067/10000] Avg train loss: 0.048308\n",
      "Epoch [6068/10000] Avg train loss: 0.048300\n",
      "Epoch [6069/10000] Avg train loss: 0.048292\n",
      "Epoch [6070/10000] Avg train loss: 0.048284\n",
      "Epoch [6071/10000] Avg train loss: 0.048276\n",
      "Epoch [6072/10000] Avg train loss: 0.048268\n",
      "Epoch [6073/10000] Avg train loss: 0.048260\n",
      "Epoch [6074/10000] Avg train loss: 0.048252\n",
      "Epoch [6075/10000] Avg train loss: 0.048245\n",
      "Epoch [6076/10000] Avg train loss: 0.048237\n",
      "Epoch [6077/10000] Avg train loss: 0.048229\n",
      "Epoch [6078/10000] Avg train loss: 0.048221\n",
      "Epoch [6079/10000] Avg train loss: 0.048213\n",
      "Epoch [6080/10000] Avg train loss: 0.048205\n",
      "Epoch [6081/10000] Avg train loss: 0.048197\n",
      "Epoch [6082/10000] Avg train loss: 0.048189\n",
      "Epoch [6083/10000] Avg train loss: 0.048181\n",
      "Epoch [6084/10000] Avg train loss: 0.048173\n",
      "Epoch [6085/10000] Avg train loss: 0.048165\n",
      "Epoch [6086/10000] Avg train loss: 0.048158\n",
      "Epoch [6087/10000] Avg train loss: 0.048150\n",
      "Epoch [6088/10000] Avg train loss: 0.048142\n",
      "Epoch [6089/10000] Avg train loss: 0.048134\n",
      "Epoch [6090/10000] Avg train loss: 0.048126\n",
      "Epoch [6091/10000] Avg train loss: 0.048118\n",
      "Epoch [6092/10000] Avg train loss: 0.048110\n",
      "Epoch [6093/10000] Avg train loss: 0.048102\n",
      "Epoch [6094/10000] Avg train loss: 0.048095\n",
      "Epoch [6095/10000] Avg train loss: 0.048087\n",
      "Epoch [6096/10000] Avg train loss: 0.048079\n",
      "Epoch [6097/10000] Avg train loss: 0.048071\n",
      "Epoch [6098/10000] Avg train loss: 0.048063\n",
      "Epoch [6099/10000] Avg train loss: 0.048055\n",
      "Epoch [6100/10000] Avg train loss: 0.048047\n",
      "Epoch [6101/10000] Avg train loss: 0.048040\n",
      "Epoch [6102/10000] Avg train loss: 0.048032\n",
      "Epoch [6103/10000] Avg train loss: 0.048024\n",
      "Epoch [6104/10000] Avg train loss: 0.048016\n",
      "Epoch [6105/10000] Avg train loss: 0.048008\n",
      "Epoch [6106/10000] Avg train loss: 0.048000\n",
      "Epoch [6107/10000] Avg train loss: 0.047992\n",
      "Epoch [6108/10000] Avg train loss: 0.047985\n",
      "Epoch [6109/10000] Avg train loss: 0.047977\n",
      "Epoch [6110/10000] Avg train loss: 0.047969\n",
      "Epoch [6111/10000] Avg train loss: 0.047961\n",
      "Epoch [6112/10000] Avg train loss: 0.047953\n",
      "Epoch [6113/10000] Avg train loss: 0.047946\n",
      "Epoch [6114/10000] Avg train loss: 0.047938\n",
      "Epoch [6115/10000] Avg train loss: 0.047930\n",
      "Epoch [6116/10000] Avg train loss: 0.047922\n",
      "Epoch [6117/10000] Avg train loss: 0.047914\n",
      "Epoch [6118/10000] Avg train loss: 0.047906\n",
      "Epoch [6119/10000] Avg train loss: 0.047899\n",
      "Epoch [6120/10000] Avg train loss: 0.047891\n",
      "Epoch [6121/10000] Avg train loss: 0.047883\n",
      "Epoch [6122/10000] Avg train loss: 0.047875\n",
      "Epoch [6123/10000] Avg train loss: 0.047867\n",
      "Epoch [6124/10000] Avg train loss: 0.047860\n",
      "Epoch [6125/10000] Avg train loss: 0.047852\n",
      "Epoch [6126/10000] Avg train loss: 0.047844\n",
      "Epoch [6127/10000] Avg train loss: 0.047836\n",
      "Epoch [6128/10000] Avg train loss: 0.047828\n",
      "Epoch [6129/10000] Avg train loss: 0.047821\n",
      "Epoch [6130/10000] Avg train loss: 0.047813\n",
      "Epoch [6131/10000] Avg train loss: 0.047805\n",
      "Epoch [6132/10000] Avg train loss: 0.047797\n",
      "Epoch [6133/10000] Avg train loss: 0.047790\n",
      "Epoch [6134/10000] Avg train loss: 0.047782\n",
      "Epoch [6135/10000] Avg train loss: 0.047774\n",
      "Epoch [6136/10000] Avg train loss: 0.047766\n",
      "Epoch [6137/10000] Avg train loss: 0.047759\n",
      "Epoch [6138/10000] Avg train loss: 0.047751\n",
      "Epoch [6139/10000] Avg train loss: 0.047743\n",
      "Epoch [6140/10000] Avg train loss: 0.047735\n",
      "Epoch [6141/10000] Avg train loss: 0.047728\n",
      "Epoch [6142/10000] Avg train loss: 0.047720\n",
      "Epoch [6143/10000] Avg train loss: 0.047712\n",
      "Epoch [6144/10000] Avg train loss: 0.047704\n",
      "Epoch [6145/10000] Avg train loss: 0.047697\n",
      "Epoch [6146/10000] Avg train loss: 0.047689\n",
      "Epoch [6147/10000] Avg train loss: 0.047681\n",
      "Epoch [6148/10000] Avg train loss: 0.047673\n",
      "Epoch [6149/10000] Avg train loss: 0.047666\n",
      "Epoch [6150/10000] Avg train loss: 0.047658\n",
      "Epoch [6151/10000] Avg train loss: 0.047650\n",
      "Epoch [6152/10000] Avg train loss: 0.047642\n",
      "Epoch [6153/10000] Avg train loss: 0.047635\n",
      "Epoch [6154/10000] Avg train loss: 0.047627\n",
      "Epoch [6155/10000] Avg train loss: 0.047619\n",
      "Epoch [6156/10000] Avg train loss: 0.047612\n",
      "Epoch [6157/10000] Avg train loss: 0.047604\n",
      "Epoch [6158/10000] Avg train loss: 0.047596\n",
      "Epoch [6159/10000] Avg train loss: 0.047588\n",
      "Epoch [6160/10000] Avg train loss: 0.047581\n",
      "Epoch [6161/10000] Avg train loss: 0.047573\n",
      "Epoch [6162/10000] Avg train loss: 0.047565\n",
      "Epoch [6163/10000] Avg train loss: 0.047558\n",
      "Epoch [6164/10000] Avg train loss: 0.047550\n",
      "Epoch [6165/10000] Avg train loss: 0.047542\n",
      "Epoch [6166/10000] Avg train loss: 0.047535\n",
      "Epoch [6167/10000] Avg train loss: 0.047527\n",
      "Epoch [6168/10000] Avg train loss: 0.047519\n",
      "Epoch [6169/10000] Avg train loss: 0.047512\n",
      "Epoch [6170/10000] Avg train loss: 0.047504\n",
      "Epoch [6171/10000] Avg train loss: 0.047496\n",
      "Epoch [6172/10000] Avg train loss: 0.047488\n",
      "Epoch [6173/10000] Avg train loss: 0.047481\n",
      "Epoch [6174/10000] Avg train loss: 0.047473\n",
      "Epoch [6175/10000] Avg train loss: 0.047465\n",
      "Epoch [6176/10000] Avg train loss: 0.047458\n",
      "Epoch [6177/10000] Avg train loss: 0.047450\n",
      "Epoch [6178/10000] Avg train loss: 0.047443\n",
      "Epoch [6179/10000] Avg train loss: 0.047435\n",
      "Epoch [6180/10000] Avg train loss: 0.047427\n",
      "Epoch [6181/10000] Avg train loss: 0.047420\n",
      "Epoch [6182/10000] Avg train loss: 0.047412\n",
      "Epoch [6183/10000] Avg train loss: 0.047404\n",
      "Epoch [6184/10000] Avg train loss: 0.047397\n",
      "Epoch [6185/10000] Avg train loss: 0.047389\n",
      "Epoch [6186/10000] Avg train loss: 0.047381\n",
      "Epoch [6187/10000] Avg train loss: 0.047374\n",
      "Epoch [6188/10000] Avg train loss: 0.047366\n",
      "Epoch [6189/10000] Avg train loss: 0.047358\n",
      "Epoch [6190/10000] Avg train loss: 0.047351\n",
      "Epoch [6191/10000] Avg train loss: 0.047343\n",
      "Epoch [6192/10000] Avg train loss: 0.047336\n",
      "Epoch [6193/10000] Avg train loss: 0.047328\n",
      "Epoch [6194/10000] Avg train loss: 0.047320\n",
      "Epoch [6195/10000] Avg train loss: 0.047313\n",
      "Epoch [6196/10000] Avg train loss: 0.047305\n",
      "Epoch [6197/10000] Avg train loss: 0.047297\n",
      "Epoch [6198/10000] Avg train loss: 0.047290\n",
      "Epoch [6199/10000] Avg train loss: 0.047282\n",
      "Epoch [6200/10000] Avg train loss: 0.047275\n",
      "Epoch [6201/10000] Avg train loss: 0.047267\n",
      "Epoch [6202/10000] Avg train loss: 0.047259\n",
      "Epoch [6203/10000] Avg train loss: 0.047252\n",
      "Epoch [6204/10000] Avg train loss: 0.047244\n",
      "Epoch [6205/10000] Avg train loss: 0.047237\n",
      "Epoch [6206/10000] Avg train loss: 0.047229\n",
      "Epoch [6207/10000] Avg train loss: 0.047221\n",
      "Epoch [6208/10000] Avg train loss: 0.047214\n",
      "Epoch [6209/10000] Avg train loss: 0.047206\n",
      "Epoch [6210/10000] Avg train loss: 0.047199\n",
      "Epoch [6211/10000] Avg train loss: 0.047191\n",
      "Epoch [6212/10000] Avg train loss: 0.047184\n",
      "Epoch [6213/10000] Avg train loss: 0.047176\n",
      "Epoch [6214/10000] Avg train loss: 0.047168\n",
      "Epoch [6215/10000] Avg train loss: 0.047161\n",
      "Epoch [6216/10000] Avg train loss: 0.047153\n",
      "Epoch [6217/10000] Avg train loss: 0.047146\n",
      "Epoch [6218/10000] Avg train loss: 0.047138\n",
      "Epoch [6219/10000] Avg train loss: 0.047131\n",
      "Epoch [6220/10000] Avg train loss: 0.047123\n",
      "Epoch [6221/10000] Avg train loss: 0.047115\n",
      "Epoch [6222/10000] Avg train loss: 0.047108\n",
      "Epoch [6223/10000] Avg train loss: 0.047100\n",
      "Epoch [6224/10000] Avg train loss: 0.047093\n",
      "Epoch [6225/10000] Avg train loss: 0.047085\n",
      "Epoch [6226/10000] Avg train loss: 0.047078\n",
      "Epoch [6227/10000] Avg train loss: 0.047070\n",
      "Epoch [6228/10000] Avg train loss: 0.047063\n",
      "Epoch [6229/10000] Avg train loss: 0.047055\n",
      "Epoch [6230/10000] Avg train loss: 0.047048\n",
      "Epoch [6231/10000] Avg train loss: 0.047040\n",
      "Epoch [6232/10000] Avg train loss: 0.047033\n",
      "Epoch [6233/10000] Avg train loss: 0.047025\n",
      "Epoch [6234/10000] Avg train loss: 0.047018\n",
      "Epoch [6235/10000] Avg train loss: 0.047010\n",
      "Epoch [6236/10000] Avg train loss: 0.047002\n",
      "Epoch [6237/10000] Avg train loss: 0.046995\n",
      "Epoch [6238/10000] Avg train loss: 0.046987\n",
      "Epoch [6239/10000] Avg train loss: 0.046980\n",
      "Epoch [6240/10000] Avg train loss: 0.046972\n",
      "Epoch [6241/10000] Avg train loss: 0.046965\n",
      "Epoch [6242/10000] Avg train loss: 0.046957\n",
      "Epoch [6243/10000] Avg train loss: 0.046950\n",
      "Epoch [6244/10000] Avg train loss: 0.046942\n",
      "Epoch [6245/10000] Avg train loss: 0.046935\n",
      "Epoch [6246/10000] Avg train loss: 0.046927\n",
      "Epoch [6247/10000] Avg train loss: 0.046920\n",
      "Epoch [6248/10000] Avg train loss: 0.046912\n",
      "Epoch [6249/10000] Avg train loss: 0.046905\n",
      "Epoch [6250/10000] Avg train loss: 0.046898\n",
      "Epoch [6251/10000] Avg train loss: 0.046890\n",
      "Epoch [6252/10000] Avg train loss: 0.046883\n",
      "Epoch [6253/10000] Avg train loss: 0.046875\n",
      "Epoch [6254/10000] Avg train loss: 0.046868\n",
      "Epoch [6255/10000] Avg train loss: 0.046860\n",
      "Epoch [6256/10000] Avg train loss: 0.046853\n",
      "Epoch [6257/10000] Avg train loss: 0.046845\n",
      "Epoch [6258/10000] Avg train loss: 0.046838\n",
      "Epoch [6259/10000] Avg train loss: 0.046830\n",
      "Epoch [6260/10000] Avg train loss: 0.046823\n",
      "Epoch [6261/10000] Avg train loss: 0.046815\n",
      "Epoch [6262/10000] Avg train loss: 0.046808\n",
      "Epoch [6263/10000] Avg train loss: 0.046800\n",
      "Epoch [6264/10000] Avg train loss: 0.046793\n",
      "Epoch [6265/10000] Avg train loss: 0.046786\n",
      "Epoch [6266/10000] Avg train loss: 0.046778\n",
      "Epoch [6267/10000] Avg train loss: 0.046771\n",
      "Epoch [6268/10000] Avg train loss: 0.046763\n",
      "Epoch [6269/10000] Avg train loss: 0.046756\n",
      "Epoch [6270/10000] Avg train loss: 0.046748\n",
      "Epoch [6271/10000] Avg train loss: 0.046741\n",
      "Epoch [6272/10000] Avg train loss: 0.046733\n",
      "Epoch [6273/10000] Avg train loss: 0.046726\n",
      "Epoch [6274/10000] Avg train loss: 0.046719\n",
      "Epoch [6275/10000] Avg train loss: 0.046711\n",
      "Epoch [6276/10000] Avg train loss: 0.046704\n",
      "Epoch [6277/10000] Avg train loss: 0.046696\n",
      "Epoch [6278/10000] Avg train loss: 0.046689\n",
      "Epoch [6279/10000] Avg train loss: 0.046682\n",
      "Epoch [6280/10000] Avg train loss: 0.046674\n",
      "Epoch [6281/10000] Avg train loss: 0.046667\n",
      "Epoch [6282/10000] Avg train loss: 0.046659\n",
      "Epoch [6283/10000] Avg train loss: 0.046652\n",
      "Epoch [6284/10000] Avg train loss: 0.046644\n",
      "Epoch [6285/10000] Avg train loss: 0.046637\n",
      "Epoch [6286/10000] Avg train loss: 0.046630\n",
      "Epoch [6287/10000] Avg train loss: 0.046622\n",
      "Epoch [6288/10000] Avg train loss: 0.046615\n",
      "Epoch [6289/10000] Avg train loss: 0.046607\n",
      "Epoch [6290/10000] Avg train loss: 0.046600\n",
      "Epoch [6291/10000] Avg train loss: 0.046593\n",
      "Epoch [6292/10000] Avg train loss: 0.046585\n",
      "Epoch [6293/10000] Avg train loss: 0.046578\n",
      "Epoch [6294/10000] Avg train loss: 0.046571\n",
      "Epoch [6295/10000] Avg train loss: 0.046563\n",
      "Epoch [6296/10000] Avg train loss: 0.046556\n",
      "Epoch [6297/10000] Avg train loss: 0.046548\n",
      "Epoch [6298/10000] Avg train loss: 0.046541\n",
      "Epoch [6299/10000] Avg train loss: 0.046534\n",
      "Epoch [6300/10000] Avg train loss: 0.046526\n",
      "Epoch [6301/10000] Avg train loss: 0.046519\n",
      "Epoch [6302/10000] Avg train loss: 0.046512\n",
      "Epoch [6303/10000] Avg train loss: 0.046504\n",
      "Epoch [6304/10000] Avg train loss: 0.046497\n",
      "Epoch [6305/10000] Avg train loss: 0.046490\n",
      "Epoch [6306/10000] Avg train loss: 0.046482\n",
      "Epoch [6307/10000] Avg train loss: 0.046475\n",
      "Epoch [6308/10000] Avg train loss: 0.046468\n",
      "Epoch [6309/10000] Avg train loss: 0.046460\n",
      "Epoch [6310/10000] Avg train loss: 0.046453\n",
      "Epoch [6311/10000] Avg train loss: 0.046445\n",
      "Epoch [6312/10000] Avg train loss: 0.046438\n",
      "Epoch [6313/10000] Avg train loss: 0.046431\n",
      "Epoch [6314/10000] Avg train loss: 0.046423\n",
      "Epoch [6315/10000] Avg train loss: 0.046416\n",
      "Epoch [6316/10000] Avg train loss: 0.046409\n",
      "Epoch [6317/10000] Avg train loss: 0.046401\n",
      "Epoch [6318/10000] Avg train loss: 0.046394\n",
      "Epoch [6319/10000] Avg train loss: 0.046387\n",
      "Epoch [6320/10000] Avg train loss: 0.046380\n",
      "Epoch [6321/10000] Avg train loss: 0.046372\n",
      "Epoch [6322/10000] Avg train loss: 0.046365\n",
      "Epoch [6323/10000] Avg train loss: 0.046358\n",
      "Epoch [6324/10000] Avg train loss: 0.046350\n",
      "Epoch [6325/10000] Avg train loss: 0.046343\n",
      "Epoch [6326/10000] Avg train loss: 0.046336\n",
      "Epoch [6327/10000] Avg train loss: 0.046328\n",
      "Epoch [6328/10000] Avg train loss: 0.046321\n",
      "Epoch [6329/10000] Avg train loss: 0.046314\n",
      "Epoch [6330/10000] Avg train loss: 0.046306\n",
      "Epoch [6331/10000] Avg train loss: 0.046299\n",
      "Epoch [6332/10000] Avg train loss: 0.046292\n",
      "Epoch [6333/10000] Avg train loss: 0.046285\n",
      "Epoch [6334/10000] Avg train loss: 0.046277\n",
      "Epoch [6335/10000] Avg train loss: 0.046270\n",
      "Epoch [6336/10000] Avg train loss: 0.046263\n",
      "Epoch [6337/10000] Avg train loss: 0.046255\n",
      "Epoch [6338/10000] Avg train loss: 0.046248\n",
      "Epoch [6339/10000] Avg train loss: 0.046241\n",
      "Epoch [6340/10000] Avg train loss: 0.046234\n",
      "Epoch [6341/10000] Avg train loss: 0.046226\n",
      "Epoch [6342/10000] Avg train loss: 0.046219\n",
      "Epoch [6343/10000] Avg train loss: 0.046212\n",
      "Epoch [6344/10000] Avg train loss: 0.046205\n",
      "Epoch [6345/10000] Avg train loss: 0.046197\n",
      "Epoch [6346/10000] Avg train loss: 0.046190\n",
      "Epoch [6347/10000] Avg train loss: 0.046183\n",
      "Epoch [6348/10000] Avg train loss: 0.046176\n",
      "Epoch [6349/10000] Avg train loss: 0.046168\n",
      "Epoch [6350/10000] Avg train loss: 0.046161\n",
      "Epoch [6351/10000] Avg train loss: 0.046154\n",
      "Epoch [6352/10000] Avg train loss: 0.046147\n",
      "Epoch [6353/10000] Avg train loss: 0.046139\n",
      "Epoch [6354/10000] Avg train loss: 0.046132\n",
      "Epoch [6355/10000] Avg train loss: 0.046125\n",
      "Epoch [6356/10000] Avg train loss: 0.046118\n",
      "Epoch [6357/10000] Avg train loss: 0.046110\n",
      "Epoch [6358/10000] Avg train loss: 0.046103\n",
      "Epoch [6359/10000] Avg train loss: 0.046096\n",
      "Epoch [6360/10000] Avg train loss: 0.046089\n",
      "Epoch [6361/10000] Avg train loss: 0.046081\n",
      "Epoch [6362/10000] Avg train loss: 0.046074\n",
      "Epoch [6363/10000] Avg train loss: 0.046067\n",
      "Epoch [6364/10000] Avg train loss: 0.046060\n",
      "Epoch [6365/10000] Avg train loss: 0.046053\n",
      "Epoch [6366/10000] Avg train loss: 0.046045\n",
      "Epoch [6367/10000] Avg train loss: 0.046038\n",
      "Epoch [6368/10000] Avg train loss: 0.046031\n",
      "Epoch [6369/10000] Avg train loss: 0.046024\n",
      "Epoch [6370/10000] Avg train loss: 0.046016\n",
      "Epoch [6371/10000] Avg train loss: 0.046009\n",
      "Epoch [6372/10000] Avg train loss: 0.046002\n",
      "Epoch [6373/10000] Avg train loss: 0.045995\n",
      "Epoch [6374/10000] Avg train loss: 0.045988\n",
      "Epoch [6375/10000] Avg train loss: 0.045980\n",
      "Epoch [6376/10000] Avg train loss: 0.045973\n",
      "Epoch [6377/10000] Avg train loss: 0.045966\n",
      "Epoch [6378/10000] Avg train loss: 0.045959\n",
      "Epoch [6379/10000] Avg train loss: 0.045952\n",
      "Epoch [6380/10000] Avg train loss: 0.045945\n",
      "Epoch [6381/10000] Avg train loss: 0.045937\n",
      "Epoch [6382/10000] Avg train loss: 0.045930\n",
      "Epoch [6383/10000] Avg train loss: 0.045923\n",
      "Epoch [6384/10000] Avg train loss: 0.045916\n",
      "Epoch [6385/10000] Avg train loss: 0.045909\n",
      "Epoch [6386/10000] Avg train loss: 0.045902\n",
      "Epoch [6387/10000] Avg train loss: 0.045894\n",
      "Epoch [6388/10000] Avg train loss: 0.045887\n",
      "Epoch [6389/10000] Avg train loss: 0.045880\n",
      "Epoch [6390/10000] Avg train loss: 0.045873\n",
      "Epoch [6391/10000] Avg train loss: 0.045866\n",
      "Epoch [6392/10000] Avg train loss: 0.045859\n",
      "Epoch [6393/10000] Avg train loss: 0.045851\n",
      "Epoch [6394/10000] Avg train loss: 0.045844\n",
      "Epoch [6395/10000] Avg train loss: 0.045837\n",
      "Epoch [6396/10000] Avg train loss: 0.045830\n",
      "Epoch [6397/10000] Avg train loss: 0.045823\n",
      "Epoch [6398/10000] Avg train loss: 0.045816\n",
      "Epoch [6399/10000] Avg train loss: 0.045809\n",
      "Epoch [6400/10000] Avg train loss: 0.045801\n",
      "Epoch [6401/10000] Avg train loss: 0.045794\n",
      "Epoch [6402/10000] Avg train loss: 0.045787\n",
      "Epoch [6403/10000] Avg train loss: 0.045780\n",
      "Epoch [6404/10000] Avg train loss: 0.045773\n",
      "Epoch [6405/10000] Avg train loss: 0.045766\n",
      "Epoch [6406/10000] Avg train loss: 0.045759\n",
      "Epoch [6407/10000] Avg train loss: 0.045751\n",
      "Epoch [6408/10000] Avg train loss: 0.045744\n",
      "Epoch [6409/10000] Avg train loss: 0.045737\n",
      "Epoch [6410/10000] Avg train loss: 0.045730\n",
      "Epoch [6411/10000] Avg train loss: 0.045723\n",
      "Epoch [6412/10000] Avg train loss: 0.045716\n",
      "Epoch [6413/10000] Avg train loss: 0.045709\n",
      "Epoch [6414/10000] Avg train loss: 0.045702\n",
      "Epoch [6415/10000] Avg train loss: 0.045695\n",
      "Epoch [6416/10000] Avg train loss: 0.045688\n",
      "Epoch [6417/10000] Avg train loss: 0.045680\n",
      "Epoch [6418/10000] Avg train loss: 0.045673\n",
      "Epoch [6419/10000] Avg train loss: 0.045666\n",
      "Epoch [6420/10000] Avg train loss: 0.045659\n",
      "Epoch [6421/10000] Avg train loss: 0.045652\n",
      "Epoch [6422/10000] Avg train loss: 0.045645\n",
      "Epoch [6423/10000] Avg train loss: 0.045638\n",
      "Epoch [6424/10000] Avg train loss: 0.045631\n",
      "Epoch [6425/10000] Avg train loss: 0.045624\n",
      "Epoch [6426/10000] Avg train loss: 0.045617\n",
      "Epoch [6427/10000] Avg train loss: 0.045610\n",
      "Epoch [6428/10000] Avg train loss: 0.045602\n",
      "Epoch [6429/10000] Avg train loss: 0.045595\n",
      "Epoch [6430/10000] Avg train loss: 0.045588\n",
      "Epoch [6431/10000] Avg train loss: 0.045581\n",
      "Epoch [6432/10000] Avg train loss: 0.045574\n",
      "Epoch [6433/10000] Avg train loss: 0.045567\n",
      "Epoch [6434/10000] Avg train loss: 0.045560\n",
      "Epoch [6435/10000] Avg train loss: 0.045553\n",
      "Epoch [6436/10000] Avg train loss: 0.045546\n",
      "Epoch [6437/10000] Avg train loss: 0.045539\n",
      "Epoch [6438/10000] Avg train loss: 0.045532\n",
      "Epoch [6439/10000] Avg train loss: 0.045525\n",
      "Epoch [6440/10000] Avg train loss: 0.045518\n",
      "Epoch [6441/10000] Avg train loss: 0.045511\n",
      "Epoch [6442/10000] Avg train loss: 0.045504\n",
      "Epoch [6443/10000] Avg train loss: 0.045497\n",
      "Epoch [6444/10000] Avg train loss: 0.045490\n",
      "Epoch [6445/10000] Avg train loss: 0.045483\n",
      "Epoch [6446/10000] Avg train loss: 0.045475\n",
      "Epoch [6447/10000] Avg train loss: 0.045468\n",
      "Epoch [6448/10000] Avg train loss: 0.045461\n",
      "Epoch [6449/10000] Avg train loss: 0.045454\n",
      "Epoch [6450/10000] Avg train loss: 0.045447\n",
      "Epoch [6451/10000] Avg train loss: 0.045440\n",
      "Epoch [6452/10000] Avg train loss: 0.045433\n",
      "Epoch [6453/10000] Avg train loss: 0.045426\n",
      "Epoch [6454/10000] Avg train loss: 0.045419\n",
      "Epoch [6455/10000] Avg train loss: 0.045412\n",
      "Epoch [6456/10000] Avg train loss: 0.045405\n",
      "Epoch [6457/10000] Avg train loss: 0.045398\n",
      "Epoch [6458/10000] Avg train loss: 0.045391\n",
      "Epoch [6459/10000] Avg train loss: 0.045384\n",
      "Epoch [6460/10000] Avg train loss: 0.045377\n",
      "Epoch [6461/10000] Avg train loss: 0.045370\n",
      "Epoch [6462/10000] Avg train loss: 0.045363\n",
      "Epoch [6463/10000] Avg train loss: 0.045356\n",
      "Epoch [6464/10000] Avg train loss: 0.045349\n",
      "Epoch [6465/10000] Avg train loss: 0.045342\n",
      "Epoch [6466/10000] Avg train loss: 0.045335\n",
      "Epoch [6467/10000] Avg train loss: 0.045328\n",
      "Epoch [6468/10000] Avg train loss: 0.045321\n",
      "Epoch [6469/10000] Avg train loss: 0.045314\n",
      "Epoch [6470/10000] Avg train loss: 0.045307\n",
      "Epoch [6471/10000] Avg train loss: 0.045300\n",
      "Epoch [6472/10000] Avg train loss: 0.045293\n",
      "Epoch [6473/10000] Avg train loss: 0.045286\n",
      "Epoch [6474/10000] Avg train loss: 0.045279\n",
      "Epoch [6475/10000] Avg train loss: 0.045272\n",
      "Epoch [6476/10000] Avg train loss: 0.045265\n",
      "Epoch [6477/10000] Avg train loss: 0.045258\n",
      "Epoch [6478/10000] Avg train loss: 0.045251\n",
      "Epoch [6479/10000] Avg train loss: 0.045244\n",
      "Epoch [6480/10000] Avg train loss: 0.045237\n",
      "Epoch [6481/10000] Avg train loss: 0.045231\n",
      "Epoch [6482/10000] Avg train loss: 0.045224\n",
      "Epoch [6483/10000] Avg train loss: 0.045217\n",
      "Epoch [6484/10000] Avg train loss: 0.045210\n",
      "Epoch [6485/10000] Avg train loss: 0.045203\n",
      "Epoch [6486/10000] Avg train loss: 0.045196\n",
      "Epoch [6487/10000] Avg train loss: 0.045189\n",
      "Epoch [6488/10000] Avg train loss: 0.045182\n",
      "Epoch [6489/10000] Avg train loss: 0.045175\n",
      "Epoch [6490/10000] Avg train loss: 0.045168\n",
      "Epoch [6491/10000] Avg train loss: 0.045161\n",
      "Epoch [6492/10000] Avg train loss: 0.045154\n",
      "Epoch [6493/10000] Avg train loss: 0.045147\n",
      "Epoch [6494/10000] Avg train loss: 0.045140\n",
      "Epoch [6495/10000] Avg train loss: 0.045133\n",
      "Epoch [6496/10000] Avg train loss: 0.045126\n",
      "Epoch [6497/10000] Avg train loss: 0.045119\n",
      "Epoch [6498/10000] Avg train loss: 0.045113\n",
      "Epoch [6499/10000] Avg train loss: 0.045106\n",
      "Epoch [6500/10000] Avg train loss: 0.045099\n",
      "Epoch [6501/10000] Avg train loss: 0.045092\n",
      "Epoch [6502/10000] Avg train loss: 0.045085\n",
      "Epoch [6503/10000] Avg train loss: 0.045078\n",
      "Epoch [6504/10000] Avg train loss: 0.045071\n",
      "Epoch [6505/10000] Avg train loss: 0.045064\n",
      "Epoch [6506/10000] Avg train loss: 0.045057\n",
      "Epoch [6507/10000] Avg train loss: 0.045050\n",
      "Epoch [6508/10000] Avg train loss: 0.045043\n",
      "Epoch [6509/10000] Avg train loss: 0.045037\n",
      "Epoch [6510/10000] Avg train loss: 0.045030\n",
      "Epoch [6511/10000] Avg train loss: 0.045023\n",
      "Epoch [6512/10000] Avg train loss: 0.045016\n",
      "Epoch [6513/10000] Avg train loss: 0.045009\n",
      "Epoch [6514/10000] Avg train loss: 0.045002\n",
      "Epoch [6515/10000] Avg train loss: 0.044995\n",
      "Epoch [6516/10000] Avg train loss: 0.044988\n",
      "Epoch [6517/10000] Avg train loss: 0.044981\n",
      "Epoch [6518/10000] Avg train loss: 0.044974\n",
      "Epoch [6519/10000] Avg train loss: 0.044968\n",
      "Epoch [6520/10000] Avg train loss: 0.044961\n",
      "Epoch [6521/10000] Avg train loss: 0.044954\n",
      "Epoch [6522/10000] Avg train loss: 0.044947\n",
      "Epoch [6523/10000] Avg train loss: 0.044940\n",
      "Epoch [6524/10000] Avg train loss: 0.044933\n",
      "Epoch [6525/10000] Avg train loss: 0.044926\n",
      "Epoch [6526/10000] Avg train loss: 0.044920\n",
      "Epoch [6527/10000] Avg train loss: 0.044913\n",
      "Epoch [6528/10000] Avg train loss: 0.044906\n",
      "Epoch [6529/10000] Avg train loss: 0.044899\n",
      "Epoch [6530/10000] Avg train loss: 0.044892\n",
      "Epoch [6531/10000] Avg train loss: 0.044885\n",
      "Epoch [6532/10000] Avg train loss: 0.044878\n",
      "Epoch [6533/10000] Avg train loss: 0.044872\n",
      "Epoch [6534/10000] Avg train loss: 0.044865\n",
      "Epoch [6535/10000] Avg train loss: 0.044858\n",
      "Epoch [6536/10000] Avg train loss: 0.044851\n",
      "Epoch [6537/10000] Avg train loss: 0.044844\n",
      "Epoch [6538/10000] Avg train loss: 0.044837\n",
      "Epoch [6539/10000] Avg train loss: 0.044830\n",
      "Epoch [6540/10000] Avg train loss: 0.044824\n",
      "Epoch [6541/10000] Avg train loss: 0.044817\n",
      "Epoch [6542/10000] Avg train loss: 0.044810\n",
      "Epoch [6543/10000] Avg train loss: 0.044803\n",
      "Epoch [6544/10000] Avg train loss: 0.044796\n",
      "Epoch [6545/10000] Avg train loss: 0.044789\n",
      "Epoch [6546/10000] Avg train loss: 0.044783\n",
      "Epoch [6547/10000] Avg train loss: 0.044776\n",
      "Epoch [6548/10000] Avg train loss: 0.044769\n",
      "Epoch [6549/10000] Avg train loss: 0.044762\n",
      "Epoch [6550/10000] Avg train loss: 0.044755\n",
      "Epoch [6551/10000] Avg train loss: 0.044749\n",
      "Epoch [6552/10000] Avg train loss: 0.044742\n",
      "Epoch [6553/10000] Avg train loss: 0.044735\n",
      "Epoch [6554/10000] Avg train loss: 0.044728\n",
      "Epoch [6555/10000] Avg train loss: 0.044721\n",
      "Epoch [6556/10000] Avg train loss: 0.044715\n",
      "Epoch [6557/10000] Avg train loss: 0.044708\n",
      "Epoch [6558/10000] Avg train loss: 0.044701\n",
      "Epoch [6559/10000] Avg train loss: 0.044694\n",
      "Epoch [6560/10000] Avg train loss: 0.044687\n",
      "Epoch [6561/10000] Avg train loss: 0.044681\n",
      "Epoch [6562/10000] Avg train loss: 0.044674\n",
      "Epoch [6563/10000] Avg train loss: 0.044667\n",
      "Epoch [6564/10000] Avg train loss: 0.044660\n",
      "Epoch [6565/10000] Avg train loss: 0.044653\n",
      "Epoch [6566/10000] Avg train loss: 0.044647\n",
      "Epoch [6567/10000] Avg train loss: 0.044640\n",
      "Epoch [6568/10000] Avg train loss: 0.044633\n",
      "Epoch [6569/10000] Avg train loss: 0.044626\n",
      "Epoch [6570/10000] Avg train loss: 0.044619\n",
      "Epoch [6571/10000] Avg train loss: 0.044613\n",
      "Epoch [6572/10000] Avg train loss: 0.044606\n",
      "Epoch [6573/10000] Avg train loss: 0.044599\n",
      "Epoch [6574/10000] Avg train loss: 0.044592\n",
      "Epoch [6575/10000] Avg train loss: 0.044586\n",
      "Epoch [6576/10000] Avg train loss: 0.044579\n",
      "Epoch [6577/10000] Avg train loss: 0.044572\n",
      "Epoch [6578/10000] Avg train loss: 0.044565\n",
      "Epoch [6579/10000] Avg train loss: 0.044559\n",
      "Epoch [6580/10000] Avg train loss: 0.044552\n",
      "Epoch [6581/10000] Avg train loss: 0.044545\n",
      "Epoch [6582/10000] Avg train loss: 0.044538\n",
      "Epoch [6583/10000] Avg train loss: 0.044532\n",
      "Epoch [6584/10000] Avg train loss: 0.044525\n",
      "Epoch [6585/10000] Avg train loss: 0.044518\n",
      "Epoch [6586/10000] Avg train loss: 0.044511\n",
      "Epoch [6587/10000] Avg train loss: 0.044505\n",
      "Epoch [6588/10000] Avg train loss: 0.044498\n",
      "Epoch [6589/10000] Avg train loss: 0.044491\n",
      "Epoch [6590/10000] Avg train loss: 0.044484\n",
      "Epoch [6591/10000] Avg train loss: 0.044478\n",
      "Epoch [6592/10000] Avg train loss: 0.044471\n",
      "Epoch [6593/10000] Avg train loss: 0.044464\n",
      "Epoch [6594/10000] Avg train loss: 0.044458\n",
      "Epoch [6595/10000] Avg train loss: 0.044451\n",
      "Epoch [6596/10000] Avg train loss: 0.044444\n",
      "Epoch [6597/10000] Avg train loss: 0.044437\n",
      "Epoch [6598/10000] Avg train loss: 0.044431\n",
      "Epoch [6599/10000] Avg train loss: 0.044424\n",
      "Epoch [6600/10000] Avg train loss: 0.044417\n",
      "Epoch [6601/10000] Avg train loss: 0.044411\n",
      "Epoch [6602/10000] Avg train loss: 0.044404\n",
      "Epoch [6603/10000] Avg train loss: 0.044397\n",
      "Epoch [6604/10000] Avg train loss: 0.044390\n",
      "Epoch [6605/10000] Avg train loss: 0.044384\n",
      "Epoch [6606/10000] Avg train loss: 0.044377\n",
      "Epoch [6607/10000] Avg train loss: 0.044370\n",
      "Epoch [6608/10000] Avg train loss: 0.044364\n",
      "Epoch [6609/10000] Avg train loss: 0.044357\n",
      "Epoch [6610/10000] Avg train loss: 0.044350\n",
      "Epoch [6611/10000] Avg train loss: 0.044344\n",
      "Epoch [6612/10000] Avg train loss: 0.044337\n",
      "Epoch [6613/10000] Avg train loss: 0.044330\n",
      "Epoch [6614/10000] Avg train loss: 0.044323\n",
      "Epoch [6615/10000] Avg train loss: 0.044317\n",
      "Epoch [6616/10000] Avg train loss: 0.044310\n",
      "Epoch [6617/10000] Avg train loss: 0.044303\n",
      "Epoch [6618/10000] Avg train loss: 0.044297\n",
      "Epoch [6619/10000] Avg train loss: 0.044290\n",
      "Epoch [6620/10000] Avg train loss: 0.044283\n",
      "Epoch [6621/10000] Avg train loss: 0.044277\n",
      "Epoch [6622/10000] Avg train loss: 0.044270\n",
      "Epoch [6623/10000] Avg train loss: 0.044263\n",
      "Epoch [6624/10000] Avg train loss: 0.044257\n",
      "Epoch [6625/10000] Avg train loss: 0.044250\n",
      "Epoch [6626/10000] Avg train loss: 0.044243\n",
      "Epoch [6627/10000] Avg train loss: 0.044237\n",
      "Epoch [6628/10000] Avg train loss: 0.044230\n",
      "Epoch [6629/10000] Avg train loss: 0.044223\n",
      "Epoch [6630/10000] Avg train loss: 0.044217\n",
      "Epoch [6631/10000] Avg train loss: 0.044210\n",
      "Epoch [6632/10000] Avg train loss: 0.044203\n",
      "Epoch [6633/10000] Avg train loss: 0.044197\n",
      "Epoch [6634/10000] Avg train loss: 0.044190\n",
      "Epoch [6635/10000] Avg train loss: 0.044184\n",
      "Epoch [6636/10000] Avg train loss: 0.044177\n",
      "Epoch [6637/10000] Avg train loss: 0.044170\n",
      "Epoch [6638/10000] Avg train loss: 0.044164\n",
      "Epoch [6639/10000] Avg train loss: 0.044157\n",
      "Epoch [6640/10000] Avg train loss: 0.044150\n",
      "Epoch [6641/10000] Avg train loss: 0.044144\n",
      "Epoch [6642/10000] Avg train loss: 0.044137\n",
      "Epoch [6643/10000] Avg train loss: 0.044131\n",
      "Epoch [6644/10000] Avg train loss: 0.044124\n",
      "Epoch [6645/10000] Avg train loss: 0.044117\n",
      "Epoch [6646/10000] Avg train loss: 0.044111\n",
      "Epoch [6647/10000] Avg train loss: 0.044104\n",
      "Epoch [6648/10000] Avg train loss: 0.044097\n",
      "Epoch [6649/10000] Avg train loss: 0.044091\n",
      "Epoch [6650/10000] Avg train loss: 0.044084\n",
      "Epoch [6651/10000] Avg train loss: 0.044078\n",
      "Epoch [6652/10000] Avg train loss: 0.044071\n",
      "Epoch [6653/10000] Avg train loss: 0.044064\n",
      "Epoch [6654/10000] Avg train loss: 0.044058\n",
      "Epoch [6655/10000] Avg train loss: 0.044051\n",
      "Epoch [6656/10000] Avg train loss: 0.044045\n",
      "Epoch [6657/10000] Avg train loss: 0.044038\n",
      "Epoch [6658/10000] Avg train loss: 0.044031\n",
      "Epoch [6659/10000] Avg train loss: 0.044025\n",
      "Epoch [6660/10000] Avg train loss: 0.044018\n",
      "Epoch [6661/10000] Avg train loss: 0.044012\n",
      "Epoch [6662/10000] Avg train loss: 0.044005\n",
      "Epoch [6663/10000] Avg train loss: 0.043998\n",
      "Epoch [6664/10000] Avg train loss: 0.043992\n",
      "Epoch [6665/10000] Avg train loss: 0.043985\n",
      "Epoch [6666/10000] Avg train loss: 0.043979\n",
      "Epoch [6667/10000] Avg train loss: 0.043972\n",
      "Epoch [6668/10000] Avg train loss: 0.043966\n",
      "Epoch [6669/10000] Avg train loss: 0.043959\n",
      "Epoch [6670/10000] Avg train loss: 0.043952\n",
      "Epoch [6671/10000] Avg train loss: 0.043946\n",
      "Epoch [6672/10000] Avg train loss: 0.043939\n",
      "Epoch [6673/10000] Avg train loss: 0.043933\n",
      "Epoch [6674/10000] Avg train loss: 0.043926\n",
      "Epoch [6675/10000] Avg train loss: 0.043920\n",
      "Epoch [6676/10000] Avg train loss: 0.043913\n",
      "Epoch [6677/10000] Avg train loss: 0.043906\n",
      "Epoch [6678/10000] Avg train loss: 0.043900\n",
      "Epoch [6679/10000] Avg train loss: 0.043893\n",
      "Epoch [6680/10000] Avg train loss: 0.043887\n",
      "Epoch [6681/10000] Avg train loss: 0.043880\n",
      "Epoch [6682/10000] Avg train loss: 0.043874\n",
      "Epoch [6683/10000] Avg train loss: 0.043867\n",
      "Epoch [6684/10000] Avg train loss: 0.043861\n",
      "Epoch [6685/10000] Avg train loss: 0.043854\n",
      "Epoch [6686/10000] Avg train loss: 0.043847\n",
      "Epoch [6687/10000] Avg train loss: 0.043841\n",
      "Epoch [6688/10000] Avg train loss: 0.043834\n",
      "Epoch [6689/10000] Avg train loss: 0.043828\n",
      "Epoch [6690/10000] Avg train loss: 0.043821\n",
      "Epoch [6691/10000] Avg train loss: 0.043815\n",
      "Epoch [6692/10000] Avg train loss: 0.043808\n",
      "Epoch [6693/10000] Avg train loss: 0.043802\n",
      "Epoch [6694/10000] Avg train loss: 0.043795\n",
      "Epoch [6695/10000] Avg train loss: 0.043789\n",
      "Epoch [6696/10000] Avg train loss: 0.043782\n",
      "Epoch [6697/10000] Avg train loss: 0.043776\n",
      "Epoch [6698/10000] Avg train loss: 0.043769\n",
      "Epoch [6699/10000] Avg train loss: 0.043763\n",
      "Epoch [6700/10000] Avg train loss: 0.043756\n",
      "Epoch [6701/10000] Avg train loss: 0.043750\n",
      "Epoch [6702/10000] Avg train loss: 0.043743\n",
      "Epoch [6703/10000] Avg train loss: 0.043737\n",
      "Epoch [6704/10000] Avg train loss: 0.043730\n",
      "Epoch [6705/10000] Avg train loss: 0.043724\n",
      "Epoch [6706/10000] Avg train loss: 0.043717\n",
      "Epoch [6707/10000] Avg train loss: 0.043711\n",
      "Epoch [6708/10000] Avg train loss: 0.043704\n",
      "Epoch [6709/10000] Avg train loss: 0.043698\n",
      "Epoch [6710/10000] Avg train loss: 0.043691\n",
      "Epoch [6711/10000] Avg train loss: 0.043685\n",
      "Epoch [6712/10000] Avg train loss: 0.043678\n",
      "Epoch [6713/10000] Avg train loss: 0.043672\n",
      "Epoch [6714/10000] Avg train loss: 0.043665\n",
      "Epoch [6715/10000] Avg train loss: 0.043659\n",
      "Epoch [6716/10000] Avg train loss: 0.043652\n",
      "Epoch [6717/10000] Avg train loss: 0.043646\n",
      "Epoch [6718/10000] Avg train loss: 0.043639\n",
      "Epoch [6719/10000] Avg train loss: 0.043633\n",
      "Epoch [6720/10000] Avg train loss: 0.043626\n",
      "Epoch [6721/10000] Avg train loss: 0.043620\n",
      "Epoch [6722/10000] Avg train loss: 0.043613\n",
      "Epoch [6723/10000] Avg train loss: 0.043607\n",
      "Epoch [6724/10000] Avg train loss: 0.043600\n",
      "Epoch [6725/10000] Avg train loss: 0.043594\n",
      "Epoch [6726/10000] Avg train loss: 0.043587\n",
      "Epoch [6727/10000] Avg train loss: 0.043581\n",
      "Epoch [6728/10000] Avg train loss: 0.043575\n",
      "Epoch [6729/10000] Avg train loss: 0.043568\n",
      "Epoch [6730/10000] Avg train loss: 0.043562\n",
      "Epoch [6731/10000] Avg train loss: 0.043555\n",
      "Epoch [6732/10000] Avg train loss: 0.043549\n",
      "Epoch [6733/10000] Avg train loss: 0.043542\n",
      "Epoch [6734/10000] Avg train loss: 0.043536\n",
      "Epoch [6735/10000] Avg train loss: 0.043529\n",
      "Epoch [6736/10000] Avg train loss: 0.043523\n",
      "Epoch [6737/10000] Avg train loss: 0.043516\n",
      "Epoch [6738/10000] Avg train loss: 0.043510\n",
      "Epoch [6739/10000] Avg train loss: 0.043504\n",
      "Epoch [6740/10000] Avg train loss: 0.043497\n",
      "Epoch [6741/10000] Avg train loss: 0.043491\n",
      "Epoch [6742/10000] Avg train loss: 0.043484\n",
      "Epoch [6743/10000] Avg train loss: 0.043478\n",
      "Epoch [6744/10000] Avg train loss: 0.043471\n",
      "Epoch [6745/10000] Avg train loss: 0.043465\n",
      "Epoch [6746/10000] Avg train loss: 0.043459\n",
      "Epoch [6747/10000] Avg train loss: 0.043452\n",
      "Epoch [6748/10000] Avg train loss: 0.043446\n",
      "Epoch [6749/10000] Avg train loss: 0.043439\n",
      "Epoch [6750/10000] Avg train loss: 0.043433\n",
      "Epoch [6751/10000] Avg train loss: 0.043426\n",
      "Epoch [6752/10000] Avg train loss: 0.043420\n",
      "Epoch [6753/10000] Avg train loss: 0.043414\n",
      "Epoch [6754/10000] Avg train loss: 0.043407\n",
      "Epoch [6755/10000] Avg train loss: 0.043401\n",
      "Epoch [6756/10000] Avg train loss: 0.043394\n",
      "Epoch [6757/10000] Avg train loss: 0.043388\n",
      "Epoch [6758/10000] Avg train loss: 0.043382\n",
      "Epoch [6759/10000] Avg train loss: 0.043375\n",
      "Epoch [6760/10000] Avg train loss: 0.043369\n",
      "Epoch [6761/10000] Avg train loss: 0.043362\n",
      "Epoch [6762/10000] Avg train loss: 0.043356\n",
      "Epoch [6763/10000] Avg train loss: 0.043350\n",
      "Epoch [6764/10000] Avg train loss: 0.043343\n",
      "Epoch [6765/10000] Avg train loss: 0.043337\n",
      "Epoch [6766/10000] Avg train loss: 0.043330\n",
      "Epoch [6767/10000] Avg train loss: 0.043324\n",
      "Epoch [6768/10000] Avg train loss: 0.043318\n",
      "Epoch [6769/10000] Avg train loss: 0.043311\n",
      "Epoch [6770/10000] Avg train loss: 0.043305\n",
      "Epoch [6771/10000] Avg train loss: 0.043299\n",
      "Epoch [6772/10000] Avg train loss: 0.043292\n",
      "Epoch [6773/10000] Avg train loss: 0.043286\n",
      "Epoch [6774/10000] Avg train loss: 0.043279\n",
      "Epoch [6775/10000] Avg train loss: 0.043273\n",
      "Epoch [6776/10000] Avg train loss: 0.043267\n",
      "Epoch [6777/10000] Avg train loss: 0.043260\n",
      "Epoch [6778/10000] Avg train loss: 0.043254\n",
      "Epoch [6779/10000] Avg train loss: 0.043248\n",
      "Epoch [6780/10000] Avg train loss: 0.043241\n",
      "Epoch [6781/10000] Avg train loss: 0.043235\n",
      "Epoch [6782/10000] Avg train loss: 0.043228\n",
      "Epoch [6783/10000] Avg train loss: 0.043222\n",
      "Epoch [6784/10000] Avg train loss: 0.043216\n",
      "Epoch [6785/10000] Avg train loss: 0.043209\n",
      "Epoch [6786/10000] Avg train loss: 0.043203\n",
      "Epoch [6787/10000] Avg train loss: 0.043197\n",
      "Epoch [6788/10000] Avg train loss: 0.043190\n",
      "Epoch [6789/10000] Avg train loss: 0.043184\n",
      "Epoch [6790/10000] Avg train loss: 0.043178\n",
      "Epoch [6791/10000] Avg train loss: 0.043171\n",
      "Epoch [6792/10000] Avg train loss: 0.043165\n",
      "Epoch [6793/10000] Avg train loss: 0.043159\n",
      "Epoch [6794/10000] Avg train loss: 0.043152\n",
      "Epoch [6795/10000] Avg train loss: 0.043146\n",
      "Epoch [6796/10000] Avg train loss: 0.043140\n",
      "Epoch [6797/10000] Avg train loss: 0.043133\n",
      "Epoch [6798/10000] Avg train loss: 0.043127\n",
      "Epoch [6799/10000] Avg train loss: 0.043121\n",
      "Epoch [6800/10000] Avg train loss: 0.043114\n",
      "Epoch [6801/10000] Avg train loss: 0.043108\n",
      "Epoch [6802/10000] Avg train loss: 0.043102\n",
      "Epoch [6803/10000] Avg train loss: 0.043095\n",
      "Epoch [6804/10000] Avg train loss: 0.043089\n",
      "Epoch [6805/10000] Avg train loss: 0.043083\n",
      "Epoch [6806/10000] Avg train loss: 0.043076\n",
      "Epoch [6807/10000] Avg train loss: 0.043070\n",
      "Epoch [6808/10000] Avg train loss: 0.043064\n",
      "Epoch [6809/10000] Avg train loss: 0.043058\n",
      "Epoch [6810/10000] Avg train loss: 0.043051\n",
      "Epoch [6811/10000] Avg train loss: 0.043045\n",
      "Epoch [6812/10000] Avg train loss: 0.043039\n",
      "Epoch [6813/10000] Avg train loss: 0.043032\n",
      "Epoch [6814/10000] Avg train loss: 0.043026\n",
      "Epoch [6815/10000] Avg train loss: 0.043020\n",
      "Epoch [6816/10000] Avg train loss: 0.043013\n",
      "Epoch [6817/10000] Avg train loss: 0.043007\n",
      "Epoch [6818/10000] Avg train loss: 0.043001\n",
      "Epoch [6819/10000] Avg train loss: 0.042995\n",
      "Epoch [6820/10000] Avg train loss: 0.042988\n",
      "Epoch [6821/10000] Avg train loss: 0.042982\n",
      "Epoch [6822/10000] Avg train loss: 0.042976\n",
      "Epoch [6823/10000] Avg train loss: 0.042969\n",
      "Epoch [6824/10000] Avg train loss: 0.042963\n",
      "Epoch [6825/10000] Avg train loss: 0.042957\n",
      "Epoch [6826/10000] Avg train loss: 0.042951\n",
      "Epoch [6827/10000] Avg train loss: 0.042944\n",
      "Epoch [6828/10000] Avg train loss: 0.042938\n",
      "Epoch [6829/10000] Avg train loss: 0.042932\n",
      "Epoch [6830/10000] Avg train loss: 0.042925\n",
      "Epoch [6831/10000] Avg train loss: 0.042919\n",
      "Epoch [6832/10000] Avg train loss: 0.042913\n",
      "Epoch [6833/10000] Avg train loss: 0.042907\n",
      "Epoch [6834/10000] Avg train loss: 0.042900\n",
      "Epoch [6835/10000] Avg train loss: 0.042894\n",
      "Epoch [6836/10000] Avg train loss: 0.042888\n",
      "Epoch [6837/10000] Avg train loss: 0.042882\n",
      "Epoch [6838/10000] Avg train loss: 0.042875\n",
      "Epoch [6839/10000] Avg train loss: 0.042869\n",
      "Epoch [6840/10000] Avg train loss: 0.042863\n",
      "Epoch [6841/10000] Avg train loss: 0.042857\n",
      "Epoch [6842/10000] Avg train loss: 0.042850\n",
      "Epoch [6843/10000] Avg train loss: 0.042844\n",
      "Epoch [6844/10000] Avg train loss: 0.042838\n",
      "Epoch [6845/10000] Avg train loss: 0.042832\n",
      "Epoch [6846/10000] Avg train loss: 0.042825\n",
      "Epoch [6847/10000] Avg train loss: 0.042819\n",
      "Epoch [6848/10000] Avg train loss: 0.042813\n",
      "Epoch [6849/10000] Avg train loss: 0.042807\n",
      "Epoch [6850/10000] Avg train loss: 0.042800\n",
      "Epoch [6851/10000] Avg train loss: 0.042794\n",
      "Epoch [6852/10000] Avg train loss: 0.042788\n",
      "Epoch [6853/10000] Avg train loss: 0.042782\n",
      "Epoch [6854/10000] Avg train loss: 0.042776\n",
      "Epoch [6855/10000] Avg train loss: 0.042769\n",
      "Epoch [6856/10000] Avg train loss: 0.042763\n",
      "Epoch [6857/10000] Avg train loss: 0.042757\n",
      "Epoch [6858/10000] Avg train loss: 0.042751\n",
      "Epoch [6859/10000] Avg train loss: 0.042744\n",
      "Epoch [6860/10000] Avg train loss: 0.042738\n",
      "Epoch [6861/10000] Avg train loss: 0.042732\n",
      "Epoch [6862/10000] Avg train loss: 0.042726\n",
      "Epoch [6863/10000] Avg train loss: 0.042720\n",
      "Epoch [6864/10000] Avg train loss: 0.042713\n",
      "Epoch [6865/10000] Avg train loss: 0.042707\n",
      "Epoch [6866/10000] Avg train loss: 0.042701\n",
      "Epoch [6867/10000] Avg train loss: 0.042695\n",
      "Epoch [6868/10000] Avg train loss: 0.042689\n",
      "Epoch [6869/10000] Avg train loss: 0.042682\n",
      "Epoch [6870/10000] Avg train loss: 0.042676\n",
      "Epoch [6871/10000] Avg train loss: 0.042670\n",
      "Epoch [6872/10000] Avg train loss: 0.042664\n",
      "Epoch [6873/10000] Avg train loss: 0.042658\n",
      "Epoch [6874/10000] Avg train loss: 0.042651\n",
      "Epoch [6875/10000] Avg train loss: 0.042645\n",
      "Epoch [6876/10000] Avg train loss: 0.042639\n",
      "Epoch [6877/10000] Avg train loss: 0.042633\n",
      "Epoch [6878/10000] Avg train loss: 0.042627\n",
      "Epoch [6879/10000] Avg train loss: 0.042621\n",
      "Epoch [6880/10000] Avg train loss: 0.042614\n",
      "Epoch [6881/10000] Avg train loss: 0.042608\n",
      "Epoch [6882/10000] Avg train loss: 0.042602\n",
      "Epoch [6883/10000] Avg train loss: 0.042596\n",
      "Epoch [6884/10000] Avg train loss: 0.042590\n",
      "Epoch [6885/10000] Avg train loss: 0.042583\n",
      "Epoch [6886/10000] Avg train loss: 0.042577\n",
      "Epoch [6887/10000] Avg train loss: 0.042571\n",
      "Epoch [6888/10000] Avg train loss: 0.042565\n",
      "Epoch [6889/10000] Avg train loss: 0.042559\n",
      "Epoch [6890/10000] Avg train loss: 0.042553\n",
      "Epoch [6891/10000] Avg train loss: 0.042547\n",
      "Epoch [6892/10000] Avg train loss: 0.042540\n",
      "Epoch [6893/10000] Avg train loss: 0.042534\n",
      "Epoch [6894/10000] Avg train loss: 0.042528\n",
      "Epoch [6895/10000] Avg train loss: 0.042522\n",
      "Epoch [6896/10000] Avg train loss: 0.042516\n",
      "Epoch [6897/10000] Avg train loss: 0.042510\n",
      "Epoch [6898/10000] Avg train loss: 0.042503\n",
      "Epoch [6899/10000] Avg train loss: 0.042497\n",
      "Epoch [6900/10000] Avg train loss: 0.042491\n",
      "Epoch [6901/10000] Avg train loss: 0.042485\n",
      "Epoch [6902/10000] Avg train loss: 0.042479\n",
      "Epoch [6903/10000] Avg train loss: 0.042473\n",
      "Epoch [6904/10000] Avg train loss: 0.042467\n",
      "Epoch [6905/10000] Avg train loss: 0.042461\n",
      "Epoch [6906/10000] Avg train loss: 0.042454\n",
      "Epoch [6907/10000] Avg train loss: 0.042448\n",
      "Epoch [6908/10000] Avg train loss: 0.042442\n",
      "Epoch [6909/10000] Avg train loss: 0.042436\n",
      "Epoch [6910/10000] Avg train loss: 0.042430\n",
      "Epoch [6911/10000] Avg train loss: 0.042424\n",
      "Epoch [6912/10000] Avg train loss: 0.042418\n",
      "Epoch [6913/10000] Avg train loss: 0.042412\n",
      "Epoch [6914/10000] Avg train loss: 0.042405\n",
      "Epoch [6915/10000] Avg train loss: 0.042399\n",
      "Epoch [6916/10000] Avg train loss: 0.042393\n",
      "Epoch [6917/10000] Avg train loss: 0.042387\n",
      "Epoch [6918/10000] Avg train loss: 0.042381\n",
      "Epoch [6919/10000] Avg train loss: 0.042375\n",
      "Epoch [6920/10000] Avg train loss: 0.042369\n",
      "Epoch [6921/10000] Avg train loss: 0.042363\n",
      "Epoch [6922/10000] Avg train loss: 0.042357\n",
      "Epoch [6923/10000] Avg train loss: 0.042350\n",
      "Epoch [6924/10000] Avg train loss: 0.042344\n",
      "Epoch [6925/10000] Avg train loss: 0.042338\n",
      "Epoch [6926/10000] Avg train loss: 0.042332\n",
      "Epoch [6927/10000] Avg train loss: 0.042326\n",
      "Epoch [6928/10000] Avg train loss: 0.042320\n",
      "Epoch [6929/10000] Avg train loss: 0.042314\n",
      "Epoch [6930/10000] Avg train loss: 0.042308\n",
      "Epoch [6931/10000] Avg train loss: 0.042302\n",
      "Epoch [6932/10000] Avg train loss: 0.042296\n",
      "Epoch [6933/10000] Avg train loss: 0.042289\n",
      "Epoch [6934/10000] Avg train loss: 0.042283\n",
      "Epoch [6935/10000] Avg train loss: 0.042277\n",
      "Epoch [6936/10000] Avg train loss: 0.042271\n",
      "Epoch [6937/10000] Avg train loss: 0.042265\n",
      "Epoch [6938/10000] Avg train loss: 0.042259\n",
      "Epoch [6939/10000] Avg train loss: 0.042253\n",
      "Epoch [6940/10000] Avg train loss: 0.042247\n",
      "Epoch [6941/10000] Avg train loss: 0.042241\n",
      "Epoch [6942/10000] Avg train loss: 0.042235\n",
      "Epoch [6943/10000] Avg train loss: 0.042229\n",
      "Epoch [6944/10000] Avg train loss: 0.042223\n",
      "Epoch [6945/10000] Avg train loss: 0.042217\n",
      "Epoch [6946/10000] Avg train loss: 0.042211\n",
      "Epoch [6947/10000] Avg train loss: 0.042205\n",
      "Epoch [6948/10000] Avg train loss: 0.042198\n",
      "Epoch [6949/10000] Avg train loss: 0.042192\n",
      "Epoch [6950/10000] Avg train loss: 0.042186\n",
      "Epoch [6951/10000] Avg train loss: 0.042180\n",
      "Epoch [6952/10000] Avg train loss: 0.042174\n",
      "Epoch [6953/10000] Avg train loss: 0.042168\n",
      "Epoch [6954/10000] Avg train loss: 0.042162\n",
      "Epoch [6955/10000] Avg train loss: 0.042156\n",
      "Epoch [6956/10000] Avg train loss: 0.042150\n",
      "Epoch [6957/10000] Avg train loss: 0.042144\n",
      "Epoch [6958/10000] Avg train loss: 0.042138\n",
      "Epoch [6959/10000] Avg train loss: 0.042132\n",
      "Epoch [6960/10000] Avg train loss: 0.042126\n",
      "Epoch [6961/10000] Avg train loss: 0.042120\n",
      "Epoch [6962/10000] Avg train loss: 0.042114\n",
      "Epoch [6963/10000] Avg train loss: 0.042108\n",
      "Epoch [6964/10000] Avg train loss: 0.042102\n",
      "Epoch [6965/10000] Avg train loss: 0.042096\n",
      "Epoch [6966/10000] Avg train loss: 0.042090\n",
      "Epoch [6967/10000] Avg train loss: 0.042084\n",
      "Epoch [6968/10000] Avg train loss: 0.042078\n",
      "Epoch [6969/10000] Avg train loss: 0.042072\n",
      "Epoch [6970/10000] Avg train loss: 0.042066\n",
      "Epoch [6971/10000] Avg train loss: 0.042060\n",
      "Epoch [6972/10000] Avg train loss: 0.042054\n",
      "Epoch [6973/10000] Avg train loss: 0.042048\n",
      "Epoch [6974/10000] Avg train loss: 0.042042\n",
      "Epoch [6975/10000] Avg train loss: 0.042036\n",
      "Epoch [6976/10000] Avg train loss: 0.042030\n",
      "Epoch [6977/10000] Avg train loss: 0.042024\n",
      "Epoch [6978/10000] Avg train loss: 0.042018\n",
      "Epoch [6979/10000] Avg train loss: 0.042012\n",
      "Epoch [6980/10000] Avg train loss: 0.042006\n",
      "Epoch [6981/10000] Avg train loss: 0.042000\n",
      "Epoch [6982/10000] Avg train loss: 0.041994\n",
      "Epoch [6983/10000] Avg train loss: 0.041988\n",
      "Epoch [6984/10000] Avg train loss: 0.041982\n",
      "Epoch [6985/10000] Avg train loss: 0.041976\n",
      "Epoch [6986/10000] Avg train loss: 0.041970\n",
      "Epoch [6987/10000] Avg train loss: 0.041964\n",
      "Epoch [6988/10000] Avg train loss: 0.041958\n",
      "Epoch [6989/10000] Avg train loss: 0.041952\n",
      "Epoch [6990/10000] Avg train loss: 0.041946\n",
      "Epoch [6991/10000] Avg train loss: 0.041940\n",
      "Epoch [6992/10000] Avg train loss: 0.041934\n",
      "Epoch [6993/10000] Avg train loss: 0.041928\n",
      "Epoch [6994/10000] Avg train loss: 0.041922\n",
      "Epoch [6995/10000] Avg train loss: 0.041916\n",
      "Epoch [6996/10000] Avg train loss: 0.041910\n",
      "Epoch [6997/10000] Avg train loss: 0.041904\n",
      "Epoch [6998/10000] Avg train loss: 0.041898\n",
      "Epoch [6999/10000] Avg train loss: 0.041892\n",
      "Epoch [7000/10000] Avg train loss: 0.041886\n",
      "Epoch [7001/10000] Avg train loss: 0.041880\n",
      "Epoch [7002/10000] Avg train loss: 0.041874\n",
      "Epoch [7003/10000] Avg train loss: 0.041868\n",
      "Epoch [7004/10000] Avg train loss: 0.041862\n",
      "Epoch [7005/10000] Avg train loss: 0.041856\n",
      "Epoch [7006/10000] Avg train loss: 0.041850\n",
      "Epoch [7007/10000] Avg train loss: 0.041844\n",
      "Epoch [7008/10000] Avg train loss: 0.041838\n",
      "Epoch [7009/10000] Avg train loss: 0.041832\n",
      "Epoch [7010/10000] Avg train loss: 0.041826\n",
      "Epoch [7011/10000] Avg train loss: 0.041820\n",
      "Epoch [7012/10000] Avg train loss: 0.041814\n",
      "Epoch [7013/10000] Avg train loss: 0.041808\n",
      "Epoch [7014/10000] Avg train loss: 0.041802\n",
      "Epoch [7015/10000] Avg train loss: 0.041796\n",
      "Epoch [7016/10000] Avg train loss: 0.041791\n",
      "Epoch [7017/10000] Avg train loss: 0.041785\n",
      "Epoch [7018/10000] Avg train loss: 0.041779\n",
      "Epoch [7019/10000] Avg train loss: 0.041773\n",
      "Epoch [7020/10000] Avg train loss: 0.041767\n",
      "Epoch [7021/10000] Avg train loss: 0.041761\n",
      "Epoch [7022/10000] Avg train loss: 0.041755\n",
      "Epoch [7023/10000] Avg train loss: 0.041749\n",
      "Epoch [7024/10000] Avg train loss: 0.041743\n",
      "Epoch [7025/10000] Avg train loss: 0.041737\n",
      "Epoch [7026/10000] Avg train loss: 0.041731\n",
      "Epoch [7027/10000] Avg train loss: 0.041725\n",
      "Epoch [7028/10000] Avg train loss: 0.041719\n",
      "Epoch [7029/10000] Avg train loss: 0.041713\n",
      "Epoch [7030/10000] Avg train loss: 0.041708\n",
      "Epoch [7031/10000] Avg train loss: 0.041702\n",
      "Epoch [7032/10000] Avg train loss: 0.041696\n",
      "Epoch [7033/10000] Avg train loss: 0.041690\n",
      "Epoch [7034/10000] Avg train loss: 0.041684\n",
      "Epoch [7035/10000] Avg train loss: 0.041678\n",
      "Epoch [7036/10000] Avg train loss: 0.041672\n",
      "Epoch [7037/10000] Avg train loss: 0.041666\n",
      "Epoch [7038/10000] Avg train loss: 0.041660\n",
      "Epoch [7039/10000] Avg train loss: 0.041654\n",
      "Epoch [7040/10000] Avg train loss: 0.041648\n",
      "Epoch [7041/10000] Avg train loss: 0.041643\n",
      "Epoch [7042/10000] Avg train loss: 0.041637\n",
      "Epoch [7043/10000] Avg train loss: 0.041631\n",
      "Epoch [7044/10000] Avg train loss: 0.041625\n",
      "Epoch [7045/10000] Avg train loss: 0.041619\n",
      "Epoch [7046/10000] Avg train loss: 0.041613\n",
      "Epoch [7047/10000] Avg train loss: 0.041607\n",
      "Epoch [7048/10000] Avg train loss: 0.041601\n",
      "Epoch [7049/10000] Avg train loss: 0.041595\n",
      "Epoch [7050/10000] Avg train loss: 0.041590\n",
      "Epoch [7051/10000] Avg train loss: 0.041584\n",
      "Epoch [7052/10000] Avg train loss: 0.041578\n",
      "Epoch [7053/10000] Avg train loss: 0.041572\n",
      "Epoch [7054/10000] Avg train loss: 0.041566\n",
      "Epoch [7055/10000] Avg train loss: 0.041560\n",
      "Epoch [7056/10000] Avg train loss: 0.041554\n",
      "Epoch [7057/10000] Avg train loss: 0.041548\n",
      "Epoch [7058/10000] Avg train loss: 0.041543\n",
      "Epoch [7059/10000] Avg train loss: 0.041537\n",
      "Epoch [7060/10000] Avg train loss: 0.041531\n",
      "Epoch [7061/10000] Avg train loss: 0.041525\n",
      "Epoch [7062/10000] Avg train loss: 0.041519\n",
      "Epoch [7063/10000] Avg train loss: 0.041513\n",
      "Epoch [7064/10000] Avg train loss: 0.041507\n",
      "Epoch [7065/10000] Avg train loss: 0.041501\n",
      "Epoch [7066/10000] Avg train loss: 0.041496\n",
      "Epoch [7067/10000] Avg train loss: 0.041490\n",
      "Epoch [7068/10000] Avg train loss: 0.041484\n",
      "Epoch [7069/10000] Avg train loss: 0.041478\n",
      "Epoch [7070/10000] Avg train loss: 0.041472\n",
      "Epoch [7071/10000] Avg train loss: 0.041466\n",
      "Epoch [7072/10000] Avg train loss: 0.041460\n",
      "Epoch [7073/10000] Avg train loss: 0.041455\n",
      "Epoch [7074/10000] Avg train loss: 0.041449\n",
      "Epoch [7075/10000] Avg train loss: 0.041443\n",
      "Epoch [7076/10000] Avg train loss: 0.041437\n",
      "Epoch [7077/10000] Avg train loss: 0.041431\n",
      "Epoch [7078/10000] Avg train loss: 0.041425\n",
      "Epoch [7079/10000] Avg train loss: 0.041420\n",
      "Epoch [7080/10000] Avg train loss: 0.041414\n",
      "Epoch [7081/10000] Avg train loss: 0.041408\n",
      "Epoch [7082/10000] Avg train loss: 0.041402\n",
      "Epoch [7083/10000] Avg train loss: 0.041396\n",
      "Epoch [7084/10000] Avg train loss: 0.041390\n",
      "Epoch [7085/10000] Avg train loss: 0.041385\n",
      "Epoch [7086/10000] Avg train loss: 0.041379\n",
      "Epoch [7087/10000] Avg train loss: 0.041373\n",
      "Epoch [7088/10000] Avg train loss: 0.041367\n",
      "Epoch [7089/10000] Avg train loss: 0.041361\n",
      "Epoch [7090/10000] Avg train loss: 0.041356\n",
      "Epoch [7091/10000] Avg train loss: 0.041350\n",
      "Epoch [7092/10000] Avg train loss: 0.041344\n",
      "Epoch [7093/10000] Avg train loss: 0.041338\n",
      "Epoch [7094/10000] Avg train loss: 0.041332\n",
      "Epoch [7095/10000] Avg train loss: 0.041326\n",
      "Epoch [7096/10000] Avg train loss: 0.041321\n",
      "Epoch [7097/10000] Avg train loss: 0.041315\n",
      "Epoch [7098/10000] Avg train loss: 0.041309\n",
      "Epoch [7099/10000] Avg train loss: 0.041303\n",
      "Epoch [7100/10000] Avg train loss: 0.041297\n",
      "Epoch [7101/10000] Avg train loss: 0.041292\n",
      "Epoch [7102/10000] Avg train loss: 0.041286\n",
      "Epoch [7103/10000] Avg train loss: 0.041280\n",
      "Epoch [7104/10000] Avg train loss: 0.041274\n",
      "Epoch [7105/10000] Avg train loss: 0.041268\n",
      "Epoch [7106/10000] Avg train loss: 0.041263\n",
      "Epoch [7107/10000] Avg train loss: 0.041257\n",
      "Epoch [7108/10000] Avg train loss: 0.041251\n",
      "Epoch [7109/10000] Avg train loss: 0.041245\n",
      "Epoch [7110/10000] Avg train loss: 0.041240\n",
      "Epoch [7111/10000] Avg train loss: 0.041234\n",
      "Epoch [7112/10000] Avg train loss: 0.041228\n",
      "Epoch [7113/10000] Avg train loss: 0.041222\n",
      "Epoch [7114/10000] Avg train loss: 0.041216\n",
      "Epoch [7115/10000] Avg train loss: 0.041211\n",
      "Epoch [7116/10000] Avg train loss: 0.041205\n",
      "Epoch [7117/10000] Avg train loss: 0.041199\n",
      "Epoch [7118/10000] Avg train loss: 0.041193\n",
      "Epoch [7119/10000] Avg train loss: 0.041187\n",
      "Epoch [7120/10000] Avg train loss: 0.041182\n",
      "Epoch [7121/10000] Avg train loss: 0.041176\n",
      "Epoch [7122/10000] Avg train loss: 0.041170\n",
      "Epoch [7123/10000] Avg train loss: 0.041164\n",
      "Epoch [7124/10000] Avg train loss: 0.041159\n",
      "Epoch [7125/10000] Avg train loss: 0.041153\n",
      "Epoch [7126/10000] Avg train loss: 0.041147\n",
      "Epoch [7127/10000] Avg train loss: 0.041141\n",
      "Epoch [7128/10000] Avg train loss: 0.041136\n",
      "Epoch [7129/10000] Avg train loss: 0.041130\n",
      "Epoch [7130/10000] Avg train loss: 0.041124\n",
      "Epoch [7131/10000] Avg train loss: 0.041118\n",
      "Epoch [7132/10000] Avg train loss: 0.041113\n",
      "Epoch [7133/10000] Avg train loss: 0.041107\n",
      "Epoch [7134/10000] Avg train loss: 0.041101\n",
      "Epoch [7135/10000] Avg train loss: 0.041095\n",
      "Epoch [7136/10000] Avg train loss: 0.041090\n",
      "Epoch [7137/10000] Avg train loss: 0.041084\n",
      "Epoch [7138/10000] Avg train loss: 0.041078\n",
      "Epoch [7139/10000] Avg train loss: 0.041072\n",
      "Epoch [7140/10000] Avg train loss: 0.041067\n",
      "Epoch [7141/10000] Avg train loss: 0.041061\n",
      "Epoch [7142/10000] Avg train loss: 0.041055\n",
      "Epoch [7143/10000] Avg train loss: 0.041049\n",
      "Epoch [7144/10000] Avg train loss: 0.041044\n",
      "Epoch [7145/10000] Avg train loss: 0.041038\n",
      "Epoch [7146/10000] Avg train loss: 0.041032\n",
      "Epoch [7147/10000] Avg train loss: 0.041027\n",
      "Epoch [7148/10000] Avg train loss: 0.041021\n",
      "Epoch [7149/10000] Avg train loss: 0.041015\n",
      "Epoch [7150/10000] Avg train loss: 0.041009\n",
      "Epoch [7151/10000] Avg train loss: 0.041004\n",
      "Epoch [7152/10000] Avg train loss: 0.040998\n",
      "Epoch [7153/10000] Avg train loss: 0.040992\n",
      "Epoch [7154/10000] Avg train loss: 0.040987\n",
      "Epoch [7155/10000] Avg train loss: 0.040981\n",
      "Epoch [7156/10000] Avg train loss: 0.040975\n",
      "Epoch [7157/10000] Avg train loss: 0.040969\n",
      "Epoch [7158/10000] Avg train loss: 0.040964\n",
      "Epoch [7159/10000] Avg train loss: 0.040958\n",
      "Epoch [7160/10000] Avg train loss: 0.040952\n",
      "Epoch [7161/10000] Avg train loss: 0.040947\n",
      "Epoch [7162/10000] Avg train loss: 0.040941\n",
      "Epoch [7163/10000] Avg train loss: 0.040935\n",
      "Epoch [7164/10000] Avg train loss: 0.040929\n",
      "Epoch [7165/10000] Avg train loss: 0.040924\n",
      "Epoch [7166/10000] Avg train loss: 0.040918\n",
      "Epoch [7167/10000] Avg train loss: 0.040912\n",
      "Epoch [7168/10000] Avg train loss: 0.040907\n",
      "Epoch [7169/10000] Avg train loss: 0.040901\n",
      "Epoch [7170/10000] Avg train loss: 0.040895\n",
      "Epoch [7171/10000] Avg train loss: 0.040890\n",
      "Epoch [7172/10000] Avg train loss: 0.040884\n",
      "Epoch [7173/10000] Avg train loss: 0.040878\n",
      "Epoch [7174/10000] Avg train loss: 0.040873\n",
      "Epoch [7175/10000] Avg train loss: 0.040867\n",
      "Epoch [7176/10000] Avg train loss: 0.040861\n",
      "Epoch [7177/10000] Avg train loss: 0.040856\n",
      "Epoch [7178/10000] Avg train loss: 0.040850\n",
      "Epoch [7179/10000] Avg train loss: 0.040844\n",
      "Epoch [7180/10000] Avg train loss: 0.040838\n",
      "Epoch [7181/10000] Avg train loss: 0.040833\n",
      "Epoch [7182/10000] Avg train loss: 0.040827\n",
      "Epoch [7183/10000] Avg train loss: 0.040821\n",
      "Epoch [7184/10000] Avg train loss: 0.040816\n",
      "Epoch [7185/10000] Avg train loss: 0.040810\n",
      "Epoch [7186/10000] Avg train loss: 0.040804\n",
      "Epoch [7187/10000] Avg train loss: 0.040799\n",
      "Epoch [7188/10000] Avg train loss: 0.040793\n",
      "Epoch [7189/10000] Avg train loss: 0.040787\n",
      "Epoch [7190/10000] Avg train loss: 0.040782\n",
      "Epoch [7191/10000] Avg train loss: 0.040776\n",
      "Epoch [7192/10000] Avg train loss: 0.040771\n",
      "Epoch [7193/10000] Avg train loss: 0.040765\n",
      "Epoch [7194/10000] Avg train loss: 0.040759\n",
      "Epoch [7195/10000] Avg train loss: 0.040754\n",
      "Epoch [7196/10000] Avg train loss: 0.040748\n",
      "Epoch [7197/10000] Avg train loss: 0.040742\n",
      "Epoch [7198/10000] Avg train loss: 0.040737\n",
      "Epoch [7199/10000] Avg train loss: 0.040731\n",
      "Epoch [7200/10000] Avg train loss: 0.040725\n",
      "Epoch [7201/10000] Avg train loss: 0.040720\n",
      "Epoch [7202/10000] Avg train loss: 0.040714\n",
      "Epoch [7203/10000] Avg train loss: 0.040708\n",
      "Epoch [7204/10000] Avg train loss: 0.040703\n",
      "Epoch [7205/10000] Avg train loss: 0.040697\n",
      "Epoch [7206/10000] Avg train loss: 0.040692\n",
      "Epoch [7207/10000] Avg train loss: 0.040686\n",
      "Epoch [7208/10000] Avg train loss: 0.040680\n",
      "Epoch [7209/10000] Avg train loss: 0.040675\n",
      "Epoch [7210/10000] Avg train loss: 0.040669\n",
      "Epoch [7211/10000] Avg train loss: 0.040663\n",
      "Epoch [7212/10000] Avg train loss: 0.040658\n",
      "Epoch [7213/10000] Avg train loss: 0.040652\n",
      "Epoch [7214/10000] Avg train loss: 0.040646\n",
      "Epoch [7215/10000] Avg train loss: 0.040641\n",
      "Epoch [7216/10000] Avg train loss: 0.040635\n",
      "Epoch [7217/10000] Avg train loss: 0.040630\n",
      "Epoch [7218/10000] Avg train loss: 0.040624\n",
      "Epoch [7219/10000] Avg train loss: 0.040618\n",
      "Epoch [7220/10000] Avg train loss: 0.040613\n",
      "Epoch [7221/10000] Avg train loss: 0.040607\n",
      "Epoch [7222/10000] Avg train loss: 0.040602\n",
      "Epoch [7223/10000] Avg train loss: 0.040596\n",
      "Epoch [7224/10000] Avg train loss: 0.040590\n",
      "Epoch [7225/10000] Avg train loss: 0.040585\n",
      "Epoch [7226/10000] Avg train loss: 0.040579\n",
      "Epoch [7227/10000] Avg train loss: 0.040574\n",
      "Epoch [7228/10000] Avg train loss: 0.040568\n",
      "Epoch [7229/10000] Avg train loss: 0.040562\n",
      "Epoch [7230/10000] Avg train loss: 0.040557\n",
      "Epoch [7231/10000] Avg train loss: 0.040551\n",
      "Epoch [7232/10000] Avg train loss: 0.040546\n",
      "Epoch [7233/10000] Avg train loss: 0.040540\n",
      "Epoch [7234/10000] Avg train loss: 0.040534\n",
      "Epoch [7235/10000] Avg train loss: 0.040529\n",
      "Epoch [7236/10000] Avg train loss: 0.040523\n",
      "Epoch [7237/10000] Avg train loss: 0.040518\n",
      "Epoch [7238/10000] Avg train loss: 0.040512\n",
      "Epoch [7239/10000] Avg train loss: 0.040506\n",
      "Epoch [7240/10000] Avg train loss: 0.040501\n",
      "Epoch [7241/10000] Avg train loss: 0.040495\n",
      "Epoch [7242/10000] Avg train loss: 0.040490\n",
      "Epoch [7243/10000] Avg train loss: 0.040484\n",
      "Epoch [7244/10000] Avg train loss: 0.040479\n",
      "Epoch [7245/10000] Avg train loss: 0.040473\n",
      "Epoch [7246/10000] Avg train loss: 0.040467\n",
      "Epoch [7247/10000] Avg train loss: 0.040462\n",
      "Epoch [7248/10000] Avg train loss: 0.040456\n",
      "Epoch [7249/10000] Avg train loss: 0.040451\n",
      "Epoch [7250/10000] Avg train loss: 0.040445\n",
      "Epoch [7251/10000] Avg train loss: 0.040440\n",
      "Epoch [7252/10000] Avg train loss: 0.040434\n",
      "Epoch [7253/10000] Avg train loss: 0.040429\n",
      "Epoch [7254/10000] Avg train loss: 0.040423\n",
      "Epoch [7255/10000] Avg train loss: 0.040417\n",
      "Epoch [7256/10000] Avg train loss: 0.040412\n",
      "Epoch [7257/10000] Avg train loss: 0.040406\n",
      "Epoch [7258/10000] Avg train loss: 0.040401\n",
      "Epoch [7259/10000] Avg train loss: 0.040395\n",
      "Epoch [7260/10000] Avg train loss: 0.040390\n",
      "Epoch [7261/10000] Avg train loss: 0.040384\n",
      "Epoch [7262/10000] Avg train loss: 0.040379\n",
      "Epoch [7263/10000] Avg train loss: 0.040373\n",
      "Epoch [7264/10000] Avg train loss: 0.040367\n",
      "Epoch [7265/10000] Avg train loss: 0.040362\n",
      "Epoch [7266/10000] Avg train loss: 0.040356\n",
      "Epoch [7267/10000] Avg train loss: 0.040351\n",
      "Epoch [7268/10000] Avg train loss: 0.040345\n",
      "Epoch [7269/10000] Avg train loss: 0.040340\n",
      "Epoch [7270/10000] Avg train loss: 0.040334\n",
      "Epoch [7271/10000] Avg train loss: 0.040329\n",
      "Epoch [7272/10000] Avg train loss: 0.040323\n",
      "Epoch [7273/10000] Avg train loss: 0.040318\n",
      "Epoch [7274/10000] Avg train loss: 0.040312\n",
      "Epoch [7275/10000] Avg train loss: 0.040307\n",
      "Epoch [7276/10000] Avg train loss: 0.040301\n",
      "Epoch [7277/10000] Avg train loss: 0.040296\n",
      "Epoch [7278/10000] Avg train loss: 0.040290\n",
      "Epoch [7279/10000] Avg train loss: 0.040284\n",
      "Epoch [7280/10000] Avg train loss: 0.040279\n",
      "Epoch [7281/10000] Avg train loss: 0.040273\n",
      "Epoch [7282/10000] Avg train loss: 0.040268\n",
      "Epoch [7283/10000] Avg train loss: 0.040262\n",
      "Epoch [7284/10000] Avg train loss: 0.040257\n",
      "Epoch [7285/10000] Avg train loss: 0.040251\n",
      "Epoch [7286/10000] Avg train loss: 0.040246\n",
      "Epoch [7287/10000] Avg train loss: 0.040240\n",
      "Epoch [7288/10000] Avg train loss: 0.040235\n",
      "Epoch [7289/10000] Avg train loss: 0.040229\n",
      "Epoch [7290/10000] Avg train loss: 0.040224\n",
      "Epoch [7291/10000] Avg train loss: 0.040218\n",
      "Epoch [7292/10000] Avg train loss: 0.040213\n",
      "Epoch [7293/10000] Avg train loss: 0.040207\n",
      "Epoch [7294/10000] Avg train loss: 0.040202\n",
      "Epoch [7295/10000] Avg train loss: 0.040196\n",
      "Epoch [7296/10000] Avg train loss: 0.040191\n",
      "Epoch [7297/10000] Avg train loss: 0.040185\n",
      "Epoch [7298/10000] Avg train loss: 0.040180\n",
      "Epoch [7299/10000] Avg train loss: 0.040174\n",
      "Epoch [7300/10000] Avg train loss: 0.040169\n",
      "Epoch [7301/10000] Avg train loss: 0.040163\n",
      "Epoch [7302/10000] Avg train loss: 0.040158\n",
      "Epoch [7303/10000] Avg train loss: 0.040152\n",
      "Epoch [7304/10000] Avg train loss: 0.040147\n",
      "Epoch [7305/10000] Avg train loss: 0.040141\n",
      "Epoch [7306/10000] Avg train loss: 0.040136\n",
      "Epoch [7307/10000] Avg train loss: 0.040131\n",
      "Epoch [7308/10000] Avg train loss: 0.040125\n",
      "Epoch [7309/10000] Avg train loss: 0.040120\n",
      "Epoch [7310/10000] Avg train loss: 0.040114\n",
      "Epoch [7311/10000] Avg train loss: 0.040109\n",
      "Epoch [7312/10000] Avg train loss: 0.040103\n",
      "Epoch [7313/10000] Avg train loss: 0.040098\n",
      "Epoch [7314/10000] Avg train loss: 0.040092\n",
      "Epoch [7315/10000] Avg train loss: 0.040087\n",
      "Epoch [7316/10000] Avg train loss: 0.040081\n",
      "Epoch [7317/10000] Avg train loss: 0.040076\n",
      "Epoch [7318/10000] Avg train loss: 0.040070\n",
      "Epoch [7319/10000] Avg train loss: 0.040065\n",
      "Epoch [7320/10000] Avg train loss: 0.040059\n",
      "Epoch [7321/10000] Avg train loss: 0.040054\n",
      "Epoch [7322/10000] Avg train loss: 0.040049\n",
      "Epoch [7323/10000] Avg train loss: 0.040043\n",
      "Epoch [7324/10000] Avg train loss: 0.040038\n",
      "Epoch [7325/10000] Avg train loss: 0.040032\n",
      "Epoch [7326/10000] Avg train loss: 0.040027\n",
      "Epoch [7327/10000] Avg train loss: 0.040021\n",
      "Epoch [7328/10000] Avg train loss: 0.040016\n",
      "Epoch [7329/10000] Avg train loss: 0.040010\n",
      "Epoch [7330/10000] Avg train loss: 0.040005\n",
      "Epoch [7331/10000] Avg train loss: 0.040000\n",
      "Epoch [7332/10000] Avg train loss: 0.039994\n",
      "Epoch [7333/10000] Avg train loss: 0.039989\n",
      "Epoch [7334/10000] Avg train loss: 0.039983\n",
      "Epoch [7335/10000] Avg train loss: 0.039978\n",
      "Epoch [7336/10000] Avg train loss: 0.039972\n",
      "Epoch [7337/10000] Avg train loss: 0.039967\n",
      "Epoch [7338/10000] Avg train loss: 0.039961\n",
      "Epoch [7339/10000] Avg train loss: 0.039956\n",
      "Epoch [7340/10000] Avg train loss: 0.039951\n",
      "Epoch [7341/10000] Avg train loss: 0.039945\n",
      "Epoch [7342/10000] Avg train loss: 0.039940\n",
      "Epoch [7343/10000] Avg train loss: 0.039934\n",
      "Epoch [7344/10000] Avg train loss: 0.039929\n",
      "Epoch [7345/10000] Avg train loss: 0.039923\n",
      "Epoch [7346/10000] Avg train loss: 0.039918\n",
      "Epoch [7347/10000] Avg train loss: 0.039913\n",
      "Epoch [7348/10000] Avg train loss: 0.039907\n",
      "Epoch [7349/10000] Avg train loss: 0.039902\n",
      "Epoch [7350/10000] Avg train loss: 0.039896\n",
      "Epoch [7351/10000] Avg train loss: 0.039891\n",
      "Epoch [7352/10000] Avg train loss: 0.039886\n",
      "Epoch [7353/10000] Avg train loss: 0.039880\n",
      "Epoch [7354/10000] Avg train loss: 0.039875\n",
      "Epoch [7355/10000] Avg train loss: 0.039869\n",
      "Epoch [7356/10000] Avg train loss: 0.039864\n",
      "Epoch [7357/10000] Avg train loss: 0.039858\n",
      "Epoch [7358/10000] Avg train loss: 0.039853\n",
      "Epoch [7359/10000] Avg train loss: 0.039848\n",
      "Epoch [7360/10000] Avg train loss: 0.039842\n",
      "Epoch [7361/10000] Avg train loss: 0.039837\n",
      "Epoch [7362/10000] Avg train loss: 0.039831\n",
      "Epoch [7363/10000] Avg train loss: 0.039826\n",
      "Epoch [7364/10000] Avg train loss: 0.039821\n",
      "Epoch [7365/10000] Avg train loss: 0.039815\n",
      "Epoch [7366/10000] Avg train loss: 0.039810\n",
      "Epoch [7367/10000] Avg train loss: 0.039805\n",
      "Epoch [7368/10000] Avg train loss: 0.039799\n",
      "Epoch [7369/10000] Avg train loss: 0.039794\n",
      "Epoch [7370/10000] Avg train loss: 0.039788\n",
      "Epoch [7371/10000] Avg train loss: 0.039783\n",
      "Epoch [7372/10000] Avg train loss: 0.039778\n",
      "Epoch [7373/10000] Avg train loss: 0.039772\n",
      "Epoch [7374/10000] Avg train loss: 0.039767\n",
      "Epoch [7375/10000] Avg train loss: 0.039761\n",
      "Epoch [7376/10000] Avg train loss: 0.039756\n",
      "Epoch [7377/10000] Avg train loss: 0.039751\n",
      "Epoch [7378/10000] Avg train loss: 0.039745\n",
      "Epoch [7379/10000] Avg train loss: 0.039740\n",
      "Epoch [7380/10000] Avg train loss: 0.039735\n",
      "Epoch [7381/10000] Avg train loss: 0.039729\n",
      "Epoch [7382/10000] Avg train loss: 0.039724\n",
      "Epoch [7383/10000] Avg train loss: 0.039718\n",
      "Epoch [7384/10000] Avg train loss: 0.039713\n",
      "Epoch [7385/10000] Avg train loss: 0.039708\n",
      "Epoch [7386/10000] Avg train loss: 0.039702\n",
      "Epoch [7387/10000] Avg train loss: 0.039697\n",
      "Epoch [7388/10000] Avg train loss: 0.039692\n",
      "Epoch [7389/10000] Avg train loss: 0.039686\n",
      "Epoch [7390/10000] Avg train loss: 0.039681\n",
      "Epoch [7391/10000] Avg train loss: 0.039676\n",
      "Epoch [7392/10000] Avg train loss: 0.039670\n",
      "Epoch [7393/10000] Avg train loss: 0.039665\n",
      "Epoch [7394/10000] Avg train loss: 0.039660\n",
      "Epoch [7395/10000] Avg train loss: 0.039654\n",
      "Epoch [7396/10000] Avg train loss: 0.039649\n",
      "Epoch [7397/10000] Avg train loss: 0.039644\n",
      "Epoch [7398/10000] Avg train loss: 0.039638\n",
      "Epoch [7399/10000] Avg train loss: 0.039633\n",
      "Epoch [7400/10000] Avg train loss: 0.039627\n",
      "Epoch [7401/10000] Avg train loss: 0.039622\n",
      "Epoch [7402/10000] Avg train loss: 0.039617\n",
      "Epoch [7403/10000] Avg train loss: 0.039611\n",
      "Epoch [7404/10000] Avg train loss: 0.039606\n",
      "Epoch [7405/10000] Avg train loss: 0.039601\n",
      "Epoch [7406/10000] Avg train loss: 0.039595\n",
      "Epoch [7407/10000] Avg train loss: 0.039590\n",
      "Epoch [7408/10000] Avg train loss: 0.039585\n",
      "Epoch [7409/10000] Avg train loss: 0.039579\n",
      "Epoch [7410/10000] Avg train loss: 0.039574\n",
      "Epoch [7411/10000] Avg train loss: 0.039569\n",
      "Epoch [7412/10000] Avg train loss: 0.039563\n",
      "Epoch [7413/10000] Avg train loss: 0.039558\n",
      "Epoch [7414/10000] Avg train loss: 0.039553\n",
      "Epoch [7415/10000] Avg train loss: 0.039548\n",
      "Epoch [7416/10000] Avg train loss: 0.039542\n",
      "Epoch [7417/10000] Avg train loss: 0.039537\n",
      "Epoch [7418/10000] Avg train loss: 0.039532\n",
      "Epoch [7419/10000] Avg train loss: 0.039526\n",
      "Epoch [7420/10000] Avg train loss: 0.039521\n",
      "Epoch [7421/10000] Avg train loss: 0.039516\n",
      "Epoch [7422/10000] Avg train loss: 0.039510\n",
      "Epoch [7423/10000] Avg train loss: 0.039505\n",
      "Epoch [7424/10000] Avg train loss: 0.039500\n",
      "Epoch [7425/10000] Avg train loss: 0.039494\n",
      "Epoch [7426/10000] Avg train loss: 0.039489\n",
      "Epoch [7427/10000] Avg train loss: 0.039484\n",
      "Epoch [7428/10000] Avg train loss: 0.039478\n",
      "Epoch [7429/10000] Avg train loss: 0.039473\n",
      "Epoch [7430/10000] Avg train loss: 0.039468\n",
      "Epoch [7431/10000] Avg train loss: 0.039463\n",
      "Epoch [7432/10000] Avg train loss: 0.039457\n",
      "Epoch [7433/10000] Avg train loss: 0.039452\n",
      "Epoch [7434/10000] Avg train loss: 0.039447\n",
      "Epoch [7435/10000] Avg train loss: 0.039441\n",
      "Epoch [7436/10000] Avg train loss: 0.039436\n",
      "Epoch [7437/10000] Avg train loss: 0.039431\n",
      "Epoch [7438/10000] Avg train loss: 0.039426\n",
      "Epoch [7439/10000] Avg train loss: 0.039420\n",
      "Epoch [7440/10000] Avg train loss: 0.039415\n",
      "Epoch [7441/10000] Avg train loss: 0.039410\n",
      "Epoch [7442/10000] Avg train loss: 0.039404\n",
      "Epoch [7443/10000] Avg train loss: 0.039399\n",
      "Epoch [7444/10000] Avg train loss: 0.039394\n",
      "Epoch [7445/10000] Avg train loss: 0.039389\n",
      "Epoch [7446/10000] Avg train loss: 0.039383\n",
      "Epoch [7447/10000] Avg train loss: 0.039378\n",
      "Epoch [7448/10000] Avg train loss: 0.039373\n",
      "Epoch [7449/10000] Avg train loss: 0.039367\n",
      "Epoch [7450/10000] Avg train loss: 0.039362\n",
      "Epoch [7451/10000] Avg train loss: 0.039357\n",
      "Epoch [7452/10000] Avg train loss: 0.039352\n",
      "Epoch [7453/10000] Avg train loss: 0.039346\n",
      "Epoch [7454/10000] Avg train loss: 0.039341\n",
      "Epoch [7455/10000] Avg train loss: 0.039336\n",
      "Epoch [7456/10000] Avg train loss: 0.039331\n",
      "Epoch [7457/10000] Avg train loss: 0.039325\n",
      "Epoch [7458/10000] Avg train loss: 0.039320\n",
      "Epoch [7459/10000] Avg train loss: 0.039315\n",
      "Epoch [7460/10000] Avg train loss: 0.039310\n",
      "Epoch [7461/10000] Avg train loss: 0.039304\n",
      "Epoch [7462/10000] Avg train loss: 0.039299\n",
      "Epoch [7463/10000] Avg train loss: 0.039294\n",
      "Epoch [7464/10000] Avg train loss: 0.039289\n",
      "Epoch [7465/10000] Avg train loss: 0.039283\n",
      "Epoch [7466/10000] Avg train loss: 0.039278\n",
      "Epoch [7467/10000] Avg train loss: 0.039273\n",
      "Epoch [7468/10000] Avg train loss: 0.039268\n",
      "Epoch [7469/10000] Avg train loss: 0.039262\n",
      "Epoch [7470/10000] Avg train loss: 0.039257\n",
      "Epoch [7471/10000] Avg train loss: 0.039252\n",
      "Epoch [7472/10000] Avg train loss: 0.039247\n",
      "Epoch [7473/10000] Avg train loss: 0.039241\n",
      "Epoch [7474/10000] Avg train loss: 0.039236\n",
      "Epoch [7475/10000] Avg train loss: 0.039231\n",
      "Epoch [7476/10000] Avg train loss: 0.039226\n",
      "Epoch [7477/10000] Avg train loss: 0.039220\n",
      "Epoch [7478/10000] Avg train loss: 0.039215\n",
      "Epoch [7479/10000] Avg train loss: 0.039210\n",
      "Epoch [7480/10000] Avg train loss: 0.039205\n",
      "Epoch [7481/10000] Avg train loss: 0.039200\n",
      "Epoch [7482/10000] Avg train loss: 0.039194\n",
      "Epoch [7483/10000] Avg train loss: 0.039189\n",
      "Epoch [7484/10000] Avg train loss: 0.039184\n",
      "Epoch [7485/10000] Avg train loss: 0.039179\n",
      "Epoch [7486/10000] Avg train loss: 0.039173\n",
      "Epoch [7487/10000] Avg train loss: 0.039168\n",
      "Epoch [7488/10000] Avg train loss: 0.039163\n",
      "Epoch [7489/10000] Avg train loss: 0.039158\n",
      "Epoch [7490/10000] Avg train loss: 0.039153\n",
      "Epoch [7491/10000] Avg train loss: 0.039147\n",
      "Epoch [7492/10000] Avg train loss: 0.039142\n",
      "Epoch [7493/10000] Avg train loss: 0.039137\n",
      "Epoch [7494/10000] Avg train loss: 0.039132\n",
      "Epoch [7495/10000] Avg train loss: 0.039126\n",
      "Epoch [7496/10000] Avg train loss: 0.039121\n",
      "Epoch [7497/10000] Avg train loss: 0.039116\n",
      "Epoch [7498/10000] Avg train loss: 0.039111\n",
      "Epoch [7499/10000] Avg train loss: 0.039106\n",
      "Epoch [7500/10000] Avg train loss: 0.039100\n",
      "Epoch [7501/10000] Avg train loss: 0.039095\n",
      "Epoch [7502/10000] Avg train loss: 0.039090\n",
      "Epoch [7503/10000] Avg train loss: 0.039085\n",
      "Epoch [7504/10000] Avg train loss: 0.039080\n",
      "Epoch [7505/10000] Avg train loss: 0.039074\n",
      "Epoch [7506/10000] Avg train loss: 0.039069\n",
      "Epoch [7507/10000] Avg train loss: 0.039064\n",
      "Epoch [7508/10000] Avg train loss: 0.039059\n",
      "Epoch [7509/10000] Avg train loss: 0.039054\n",
      "Epoch [7510/10000] Avg train loss: 0.039049\n",
      "Epoch [7511/10000] Avg train loss: 0.039043\n",
      "Epoch [7512/10000] Avg train loss: 0.039038\n",
      "Epoch [7513/10000] Avg train loss: 0.039033\n",
      "Epoch [7514/10000] Avg train loss: 0.039028\n",
      "Epoch [7515/10000] Avg train loss: 0.039023\n",
      "Epoch [7516/10000] Avg train loss: 0.039017\n",
      "Epoch [7517/10000] Avg train loss: 0.039012\n",
      "Epoch [7518/10000] Avg train loss: 0.039007\n",
      "Epoch [7519/10000] Avg train loss: 0.039002\n",
      "Epoch [7520/10000] Avg train loss: 0.038997\n",
      "Epoch [7521/10000] Avg train loss: 0.038992\n",
      "Epoch [7522/10000] Avg train loss: 0.038986\n",
      "Epoch [7523/10000] Avg train loss: 0.038981\n",
      "Epoch [7524/10000] Avg train loss: 0.038976\n",
      "Epoch [7525/10000] Avg train loss: 0.038971\n",
      "Epoch [7526/10000] Avg train loss: 0.038966\n",
      "Epoch [7527/10000] Avg train loss: 0.038961\n",
      "Epoch [7528/10000] Avg train loss: 0.038955\n",
      "Epoch [7529/10000] Avg train loss: 0.038950\n",
      "Epoch [7530/10000] Avg train loss: 0.038945\n",
      "Epoch [7531/10000] Avg train loss: 0.038940\n",
      "Epoch [7532/10000] Avg train loss: 0.038935\n",
      "Epoch [7533/10000] Avg train loss: 0.038930\n",
      "Epoch [7534/10000] Avg train loss: 0.038924\n",
      "Epoch [7535/10000] Avg train loss: 0.038919\n",
      "Epoch [7536/10000] Avg train loss: 0.038914\n",
      "Epoch [7537/10000] Avg train loss: 0.038909\n",
      "Epoch [7538/10000] Avg train loss: 0.038904\n",
      "Epoch [7539/10000] Avg train loss: 0.038899\n",
      "Epoch [7540/10000] Avg train loss: 0.038894\n",
      "Epoch [7541/10000] Avg train loss: 0.038888\n",
      "Epoch [7542/10000] Avg train loss: 0.038883\n",
      "Epoch [7543/10000] Avg train loss: 0.038878\n",
      "Epoch [7544/10000] Avg train loss: 0.038873\n",
      "Epoch [7545/10000] Avg train loss: 0.038868\n",
      "Epoch [7546/10000] Avg train loss: 0.038863\n",
      "Epoch [7547/10000] Avg train loss: 0.038858\n",
      "Epoch [7548/10000] Avg train loss: 0.038852\n",
      "Epoch [7549/10000] Avg train loss: 0.038847\n",
      "Epoch [7550/10000] Avg train loss: 0.038842\n",
      "Epoch [7551/10000] Avg train loss: 0.038837\n",
      "Epoch [7552/10000] Avg train loss: 0.038832\n",
      "Epoch [7553/10000] Avg train loss: 0.038827\n",
      "Epoch [7554/10000] Avg train loss: 0.038822\n",
      "Epoch [7555/10000] Avg train loss: 0.038817\n",
      "Epoch [7556/10000] Avg train loss: 0.038811\n",
      "Epoch [7557/10000] Avg train loss: 0.038806\n",
      "Epoch [7558/10000] Avg train loss: 0.038801\n",
      "Epoch [7559/10000] Avg train loss: 0.038796\n",
      "Epoch [7560/10000] Avg train loss: 0.038791\n",
      "Epoch [7561/10000] Avg train loss: 0.038786\n",
      "Epoch [7562/10000] Avg train loss: 0.038781\n",
      "Epoch [7563/10000] Avg train loss: 0.038776\n",
      "Epoch [7564/10000] Avg train loss: 0.038770\n",
      "Epoch [7565/10000] Avg train loss: 0.038765\n",
      "Epoch [7566/10000] Avg train loss: 0.038760\n",
      "Epoch [7567/10000] Avg train loss: 0.038755\n",
      "Epoch [7568/10000] Avg train loss: 0.038750\n",
      "Epoch [7569/10000] Avg train loss: 0.038745\n",
      "Epoch [7570/10000] Avg train loss: 0.038740\n",
      "Epoch [7571/10000] Avg train loss: 0.038735\n",
      "Epoch [7572/10000] Avg train loss: 0.038730\n",
      "Epoch [7573/10000] Avg train loss: 0.038725\n",
      "Epoch [7574/10000] Avg train loss: 0.038719\n",
      "Epoch [7575/10000] Avg train loss: 0.038714\n",
      "Epoch [7576/10000] Avg train loss: 0.038709\n",
      "Epoch [7577/10000] Avg train loss: 0.038704\n",
      "Epoch [7578/10000] Avg train loss: 0.038699\n",
      "Epoch [7579/10000] Avg train loss: 0.038694\n",
      "Epoch [7580/10000] Avg train loss: 0.038689\n",
      "Epoch [7581/10000] Avg train loss: 0.038684\n",
      "Epoch [7582/10000] Avg train loss: 0.038679\n",
      "Epoch [7583/10000] Avg train loss: 0.038674\n",
      "Epoch [7584/10000] Avg train loss: 0.038668\n",
      "Epoch [7585/10000] Avg train loss: 0.038663\n",
      "Epoch [7586/10000] Avg train loss: 0.038658\n",
      "Epoch [7587/10000] Avg train loss: 0.038653\n",
      "Epoch [7588/10000] Avg train loss: 0.038648\n",
      "Epoch [7589/10000] Avg train loss: 0.038643\n",
      "Epoch [7590/10000] Avg train loss: 0.038638\n",
      "Epoch [7591/10000] Avg train loss: 0.038633\n",
      "Epoch [7592/10000] Avg train loss: 0.038628\n",
      "Epoch [7593/10000] Avg train loss: 0.038623\n",
      "Epoch [7594/10000] Avg train loss: 0.038618\n",
      "Epoch [7595/10000] Avg train loss: 0.038613\n",
      "Epoch [7596/10000] Avg train loss: 0.038608\n",
      "Epoch [7597/10000] Avg train loss: 0.038602\n",
      "Epoch [7598/10000] Avg train loss: 0.038597\n",
      "Epoch [7599/10000] Avg train loss: 0.038592\n",
      "Epoch [7600/10000] Avg train loss: 0.038587\n",
      "Epoch [7601/10000] Avg train loss: 0.038582\n",
      "Epoch [7602/10000] Avg train loss: 0.038577\n",
      "Epoch [7603/10000] Avg train loss: 0.038572\n",
      "Epoch [7604/10000] Avg train loss: 0.038567\n",
      "Epoch [7605/10000] Avg train loss: 0.038562\n",
      "Epoch [7606/10000] Avg train loss: 0.038557\n",
      "Epoch [7607/10000] Avg train loss: 0.038552\n",
      "Epoch [7608/10000] Avg train loss: 0.038547\n",
      "Epoch [7609/10000] Avg train loss: 0.038542\n",
      "Epoch [7610/10000] Avg train loss: 0.038537\n",
      "Epoch [7611/10000] Avg train loss: 0.038532\n",
      "Epoch [7612/10000] Avg train loss: 0.038527\n",
      "Epoch [7613/10000] Avg train loss: 0.038522\n",
      "Epoch [7614/10000] Avg train loss: 0.038517\n",
      "Epoch [7615/10000] Avg train loss: 0.038511\n",
      "Epoch [7616/10000] Avg train loss: 0.038506\n",
      "Epoch [7617/10000] Avg train loss: 0.038501\n",
      "Epoch [7618/10000] Avg train loss: 0.038496\n",
      "Epoch [7619/10000] Avg train loss: 0.038491\n",
      "Epoch [7620/10000] Avg train loss: 0.038486\n",
      "Epoch [7621/10000] Avg train loss: 0.038481\n",
      "Epoch [7622/10000] Avg train loss: 0.038476\n",
      "Epoch [7623/10000] Avg train loss: 0.038471\n",
      "Epoch [7624/10000] Avg train loss: 0.038466\n",
      "Epoch [7625/10000] Avg train loss: 0.038461\n",
      "Epoch [7626/10000] Avg train loss: 0.038456\n",
      "Epoch [7627/10000] Avg train loss: 0.038451\n",
      "Epoch [7628/10000] Avg train loss: 0.038446\n",
      "Epoch [7629/10000] Avg train loss: 0.038441\n",
      "Epoch [7630/10000] Avg train loss: 0.038436\n",
      "Epoch [7631/10000] Avg train loss: 0.038431\n",
      "Epoch [7632/10000] Avg train loss: 0.038426\n",
      "Epoch [7633/10000] Avg train loss: 0.038421\n",
      "Epoch [7634/10000] Avg train loss: 0.038416\n",
      "Epoch [7635/10000] Avg train loss: 0.038411\n",
      "Epoch [7636/10000] Avg train loss: 0.038406\n",
      "Epoch [7637/10000] Avg train loss: 0.038401\n",
      "Epoch [7638/10000] Avg train loss: 0.038396\n",
      "Epoch [7639/10000] Avg train loss: 0.038391\n",
      "Epoch [7640/10000] Avg train loss: 0.038386\n",
      "Epoch [7641/10000] Avg train loss: 0.038381\n",
      "Epoch [7642/10000] Avg train loss: 0.038376\n",
      "Epoch [7643/10000] Avg train loss: 0.038371\n",
      "Epoch [7644/10000] Avg train loss: 0.038366\n",
      "Epoch [7645/10000] Avg train loss: 0.038361\n",
      "Epoch [7646/10000] Avg train loss: 0.038356\n",
      "Epoch [7647/10000] Avg train loss: 0.038351\n",
      "Epoch [7648/10000] Avg train loss: 0.038346\n",
      "Epoch [7649/10000] Avg train loss: 0.038341\n",
      "Epoch [7650/10000] Avg train loss: 0.038336\n",
      "Epoch [7651/10000] Avg train loss: 0.038331\n",
      "Epoch [7652/10000] Avg train loss: 0.038326\n",
      "Epoch [7653/10000] Avg train loss: 0.038321\n",
      "Epoch [7654/10000] Avg train loss: 0.038316\n",
      "Epoch [7655/10000] Avg train loss: 0.038311\n",
      "Epoch [7656/10000] Avg train loss: 0.038306\n",
      "Epoch [7657/10000] Avg train loss: 0.038301\n",
      "Epoch [7658/10000] Avg train loss: 0.038296\n",
      "Epoch [7659/10000] Avg train loss: 0.038291\n",
      "Epoch [7660/10000] Avg train loss: 0.038286\n",
      "Epoch [7661/10000] Avg train loss: 0.038281\n",
      "Epoch [7662/10000] Avg train loss: 0.038276\n",
      "Epoch [7663/10000] Avg train loss: 0.038271\n",
      "Epoch [7664/10000] Avg train loss: 0.038266\n",
      "Epoch [7665/10000] Avg train loss: 0.038261\n",
      "Epoch [7666/10000] Avg train loss: 0.038256\n",
      "Epoch [7667/10000] Avg train loss: 0.038251\n",
      "Epoch [7668/10000] Avg train loss: 0.038246\n",
      "Epoch [7669/10000] Avg train loss: 0.038241\n",
      "Epoch [7670/10000] Avg train loss: 0.038236\n",
      "Epoch [7671/10000] Avg train loss: 0.038231\n",
      "Epoch [7672/10000] Avg train loss: 0.038226\n",
      "Epoch [7673/10000] Avg train loss: 0.038221\n",
      "Epoch [7674/10000] Avg train loss: 0.038216\n",
      "Epoch [7675/10000] Avg train loss: 0.038211\n",
      "Epoch [7676/10000] Avg train loss: 0.038206\n",
      "Epoch [7677/10000] Avg train loss: 0.038201\n",
      "Epoch [7678/10000] Avg train loss: 0.038196\n",
      "Epoch [7679/10000] Avg train loss: 0.038191\n",
      "Epoch [7680/10000] Avg train loss: 0.038186\n",
      "Epoch [7681/10000] Avg train loss: 0.038181\n",
      "Epoch [7682/10000] Avg train loss: 0.038176\n",
      "Epoch [7683/10000] Avg train loss: 0.038171\n",
      "Epoch [7684/10000] Avg train loss: 0.038167\n",
      "Epoch [7685/10000] Avg train loss: 0.038162\n",
      "Epoch [7686/10000] Avg train loss: 0.038157\n",
      "Epoch [7687/10000] Avg train loss: 0.038152\n",
      "Epoch [7688/10000] Avg train loss: 0.038147\n",
      "Epoch [7689/10000] Avg train loss: 0.038142\n",
      "Epoch [7690/10000] Avg train loss: 0.038137\n",
      "Epoch [7691/10000] Avg train loss: 0.038132\n",
      "Epoch [7692/10000] Avg train loss: 0.038127\n",
      "Epoch [7693/10000] Avg train loss: 0.038122\n",
      "Epoch [7694/10000] Avg train loss: 0.038117\n",
      "Epoch [7695/10000] Avg train loss: 0.038112\n",
      "Epoch [7696/10000] Avg train loss: 0.038107\n",
      "Epoch [7697/10000] Avg train loss: 0.038102\n",
      "Epoch [7698/10000] Avg train loss: 0.038097\n",
      "Epoch [7699/10000] Avg train loss: 0.038092\n",
      "Epoch [7700/10000] Avg train loss: 0.038087\n",
      "Epoch [7701/10000] Avg train loss: 0.038083\n",
      "Epoch [7702/10000] Avg train loss: 0.038078\n",
      "Epoch [7703/10000] Avg train loss: 0.038073\n",
      "Epoch [7704/10000] Avg train loss: 0.038068\n",
      "Epoch [7705/10000] Avg train loss: 0.038063\n",
      "Epoch [7706/10000] Avg train loss: 0.038058\n",
      "Epoch [7707/10000] Avg train loss: 0.038053\n",
      "Epoch [7708/10000] Avg train loss: 0.038048\n",
      "Epoch [7709/10000] Avg train loss: 0.038043\n",
      "Epoch [7710/10000] Avg train loss: 0.038038\n",
      "Epoch [7711/10000] Avg train loss: 0.038033\n",
      "Epoch [7712/10000] Avg train loss: 0.038028\n",
      "Epoch [7713/10000] Avg train loss: 0.038023\n",
      "Epoch [7714/10000] Avg train loss: 0.038018\n",
      "Epoch [7715/10000] Avg train loss: 0.038014\n",
      "Epoch [7716/10000] Avg train loss: 0.038009\n",
      "Epoch [7717/10000] Avg train loss: 0.038004\n",
      "Epoch [7718/10000] Avg train loss: 0.037999\n",
      "Epoch [7719/10000] Avg train loss: 0.037994\n",
      "Epoch [7720/10000] Avg train loss: 0.037989\n",
      "Epoch [7721/10000] Avg train loss: 0.037984\n",
      "Epoch [7722/10000] Avg train loss: 0.037979\n",
      "Epoch [7723/10000] Avg train loss: 0.037974\n",
      "Epoch [7724/10000] Avg train loss: 0.037969\n",
      "Epoch [7725/10000] Avg train loss: 0.037964\n",
      "Epoch [7726/10000] Avg train loss: 0.037960\n",
      "Epoch [7727/10000] Avg train loss: 0.037955\n",
      "Epoch [7728/10000] Avg train loss: 0.037950\n",
      "Epoch [7729/10000] Avg train loss: 0.037945\n",
      "Epoch [7730/10000] Avg train loss: 0.037940\n",
      "Epoch [7731/10000] Avg train loss: 0.037935\n",
      "Epoch [7732/10000] Avg train loss: 0.037930\n",
      "Epoch [7733/10000] Avg train loss: 0.037925\n",
      "Epoch [7734/10000] Avg train loss: 0.037920\n",
      "Epoch [7735/10000] Avg train loss: 0.037916\n",
      "Epoch [7736/10000] Avg train loss: 0.037911\n",
      "Epoch [7737/10000] Avg train loss: 0.037906\n",
      "Epoch [7738/10000] Avg train loss: 0.037901\n",
      "Epoch [7739/10000] Avg train loss: 0.037896\n",
      "Epoch [7740/10000] Avg train loss: 0.037891\n",
      "Epoch [7741/10000] Avg train loss: 0.037886\n",
      "Epoch [7742/10000] Avg train loss: 0.037881\n",
      "Epoch [7743/10000] Avg train loss: 0.037876\n",
      "Epoch [7744/10000] Avg train loss: 0.037872\n",
      "Epoch [7745/10000] Avg train loss: 0.037867\n",
      "Epoch [7746/10000] Avg train loss: 0.037862\n",
      "Epoch [7747/10000] Avg train loss: 0.037857\n",
      "Epoch [7748/10000] Avg train loss: 0.037852\n",
      "Epoch [7749/10000] Avg train loss: 0.037847\n",
      "Epoch [7750/10000] Avg train loss: 0.037842\n",
      "Epoch [7751/10000] Avg train loss: 0.037837\n",
      "Epoch [7752/10000] Avg train loss: 0.037833\n",
      "Epoch [7753/10000] Avg train loss: 0.037828\n",
      "Epoch [7754/10000] Avg train loss: 0.037823\n",
      "Epoch [7755/10000] Avg train loss: 0.037818\n",
      "Epoch [7756/10000] Avg train loss: 0.037813\n",
      "Epoch [7757/10000] Avg train loss: 0.037808\n",
      "Epoch [7758/10000] Avg train loss: 0.037803\n",
      "Epoch [7759/10000] Avg train loss: 0.037799\n",
      "Epoch [7760/10000] Avg train loss: 0.037794\n",
      "Epoch [7761/10000] Avg train loss: 0.037789\n",
      "Epoch [7762/10000] Avg train loss: 0.037784\n",
      "Epoch [7763/10000] Avg train loss: 0.037779\n",
      "Epoch [7764/10000] Avg train loss: 0.037774\n",
      "Epoch [7765/10000] Avg train loss: 0.037769\n",
      "Epoch [7766/10000] Avg train loss: 0.037765\n",
      "Epoch [7767/10000] Avg train loss: 0.037760\n",
      "Epoch [7768/10000] Avg train loss: 0.037755\n",
      "Epoch [7769/10000] Avg train loss: 0.037750\n",
      "Epoch [7770/10000] Avg train loss: 0.037745\n",
      "Epoch [7771/10000] Avg train loss: 0.037740\n",
      "Epoch [7772/10000] Avg train loss: 0.037735\n",
      "Epoch [7773/10000] Avg train loss: 0.037731\n",
      "Epoch [7774/10000] Avg train loss: 0.037726\n",
      "Epoch [7775/10000] Avg train loss: 0.037721\n",
      "Epoch [7776/10000] Avg train loss: 0.037716\n",
      "Epoch [7777/10000] Avg train loss: 0.037711\n",
      "Epoch [7778/10000] Avg train loss: 0.037706\n",
      "Epoch [7779/10000] Avg train loss: 0.037702\n",
      "Epoch [7780/10000] Avg train loss: 0.037697\n",
      "Epoch [7781/10000] Avg train loss: 0.037692\n",
      "Epoch [7782/10000] Avg train loss: 0.037687\n",
      "Epoch [7783/10000] Avg train loss: 0.037682\n",
      "Epoch [7784/10000] Avg train loss: 0.037677\n",
      "Epoch [7785/10000] Avg train loss: 0.037673\n",
      "Epoch [7786/10000] Avg train loss: 0.037668\n",
      "Epoch [7787/10000] Avg train loss: 0.037663\n",
      "Epoch [7788/10000] Avg train loss: 0.037658\n",
      "Epoch [7789/10000] Avg train loss: 0.037653\n",
      "Epoch [7790/10000] Avg train loss: 0.037649\n",
      "Epoch [7791/10000] Avg train loss: 0.037644\n",
      "Epoch [7792/10000] Avg train loss: 0.037639\n",
      "Epoch [7793/10000] Avg train loss: 0.037634\n",
      "Epoch [7794/10000] Avg train loss: 0.037629\n",
      "Epoch [7795/10000] Avg train loss: 0.037624\n",
      "Epoch [7796/10000] Avg train loss: 0.037620\n",
      "Epoch [7797/10000] Avg train loss: 0.037615\n",
      "Epoch [7798/10000] Avg train loss: 0.037610\n",
      "Epoch [7799/10000] Avg train loss: 0.037605\n",
      "Epoch [7800/10000] Avg train loss: 0.037600\n",
      "Epoch [7801/10000] Avg train loss: 0.037596\n",
      "Epoch [7802/10000] Avg train loss: 0.037591\n",
      "Epoch [7803/10000] Avg train loss: 0.037586\n",
      "Epoch [7804/10000] Avg train loss: 0.037581\n",
      "Epoch [7805/10000] Avg train loss: 0.037576\n",
      "Epoch [7806/10000] Avg train loss: 0.037572\n",
      "Epoch [7807/10000] Avg train loss: 0.037567\n",
      "Epoch [7808/10000] Avg train loss: 0.037562\n",
      "Epoch [7809/10000] Avg train loss: 0.037557\n",
      "Epoch [7810/10000] Avg train loss: 0.037552\n",
      "Epoch [7811/10000] Avg train loss: 0.037548\n",
      "Epoch [7812/10000] Avg train loss: 0.037543\n",
      "Epoch [7813/10000] Avg train loss: 0.037538\n",
      "Epoch [7814/10000] Avg train loss: 0.037533\n",
      "Epoch [7815/10000] Avg train loss: 0.037528\n",
      "Epoch [7816/10000] Avg train loss: 0.037524\n",
      "Epoch [7817/10000] Avg train loss: 0.037519\n",
      "Epoch [7818/10000] Avg train loss: 0.037514\n",
      "Epoch [7819/10000] Avg train loss: 0.037509\n",
      "Epoch [7820/10000] Avg train loss: 0.037504\n",
      "Epoch [7821/10000] Avg train loss: 0.037500\n",
      "Epoch [7822/10000] Avg train loss: 0.037495\n",
      "Epoch [7823/10000] Avg train loss: 0.037490\n",
      "Epoch [7824/10000] Avg train loss: 0.037485\n",
      "Epoch [7825/10000] Avg train loss: 0.037481\n",
      "Epoch [7826/10000] Avg train loss: 0.037476\n",
      "Epoch [7827/10000] Avg train loss: 0.037471\n",
      "Epoch [7828/10000] Avg train loss: 0.037466\n",
      "Epoch [7829/10000] Avg train loss: 0.037461\n",
      "Epoch [7830/10000] Avg train loss: 0.037457\n",
      "Epoch [7831/10000] Avg train loss: 0.037452\n",
      "Epoch [7832/10000] Avg train loss: 0.037447\n",
      "Epoch [7833/10000] Avg train loss: 0.037442\n",
      "Epoch [7834/10000] Avg train loss: 0.037438\n",
      "Epoch [7835/10000] Avg train loss: 0.037433\n",
      "Epoch [7836/10000] Avg train loss: 0.037428\n",
      "Epoch [7837/10000] Avg train loss: 0.037423\n",
      "Epoch [7838/10000] Avg train loss: 0.037419\n",
      "Epoch [7839/10000] Avg train loss: 0.037414\n",
      "Epoch [7840/10000] Avg train loss: 0.037409\n",
      "Epoch [7841/10000] Avg train loss: 0.037404\n",
      "Epoch [7842/10000] Avg train loss: 0.037400\n",
      "Epoch [7843/10000] Avg train loss: 0.037395\n",
      "Epoch [7844/10000] Avg train loss: 0.037390\n",
      "Epoch [7845/10000] Avg train loss: 0.037385\n",
      "Epoch [7846/10000] Avg train loss: 0.037380\n",
      "Epoch [7847/10000] Avg train loss: 0.037376\n",
      "Epoch [7848/10000] Avg train loss: 0.037371\n",
      "Epoch [7849/10000] Avg train loss: 0.037366\n",
      "Epoch [7850/10000] Avg train loss: 0.037361\n",
      "Epoch [7851/10000] Avg train loss: 0.037357\n",
      "Epoch [7852/10000] Avg train loss: 0.037352\n",
      "Epoch [7853/10000] Avg train loss: 0.037347\n",
      "Epoch [7854/10000] Avg train loss: 0.037343\n",
      "Epoch [7855/10000] Avg train loss: 0.037338\n",
      "Epoch [7856/10000] Avg train loss: 0.037333\n",
      "Epoch [7857/10000] Avg train loss: 0.037328\n",
      "Epoch [7858/10000] Avg train loss: 0.037324\n",
      "Epoch [7859/10000] Avg train loss: 0.037319\n",
      "Epoch [7860/10000] Avg train loss: 0.037314\n",
      "Epoch [7861/10000] Avg train loss: 0.037309\n",
      "Epoch [7862/10000] Avg train loss: 0.037305\n",
      "Epoch [7863/10000] Avg train loss: 0.037300\n",
      "Epoch [7864/10000] Avg train loss: 0.037295\n",
      "Epoch [7865/10000] Avg train loss: 0.037290\n",
      "Epoch [7866/10000] Avg train loss: 0.037286\n",
      "Epoch [7867/10000] Avg train loss: 0.037281\n",
      "Epoch [7868/10000] Avg train loss: 0.037276\n",
      "Epoch [7869/10000] Avg train loss: 0.037272\n",
      "Epoch [7870/10000] Avg train loss: 0.037267\n",
      "Epoch [7871/10000] Avg train loss: 0.037262\n",
      "Epoch [7872/10000] Avg train loss: 0.037257\n",
      "Epoch [7873/10000] Avg train loss: 0.037253\n",
      "Epoch [7874/10000] Avg train loss: 0.037248\n",
      "Epoch [7875/10000] Avg train loss: 0.037243\n",
      "Epoch [7876/10000] Avg train loss: 0.037238\n",
      "Epoch [7877/10000] Avg train loss: 0.037234\n",
      "Epoch [7878/10000] Avg train loss: 0.037229\n",
      "Epoch [7879/10000] Avg train loss: 0.037224\n",
      "Epoch [7880/10000] Avg train loss: 0.037220\n",
      "Epoch [7881/10000] Avg train loss: 0.037215\n",
      "Epoch [7882/10000] Avg train loss: 0.037210\n",
      "Epoch [7883/10000] Avg train loss: 0.037205\n",
      "Epoch [7884/10000] Avg train loss: 0.037201\n",
      "Epoch [7885/10000] Avg train loss: 0.037196\n",
      "Epoch [7886/10000] Avg train loss: 0.037191\n",
      "Epoch [7887/10000] Avg train loss: 0.037187\n",
      "Epoch [7888/10000] Avg train loss: 0.037182\n",
      "Epoch [7889/10000] Avg train loss: 0.037177\n",
      "Epoch [7890/10000] Avg train loss: 0.037173\n",
      "Epoch [7891/10000] Avg train loss: 0.037168\n",
      "Epoch [7892/10000] Avg train loss: 0.037163\n",
      "Epoch [7893/10000] Avg train loss: 0.037158\n",
      "Epoch [7894/10000] Avg train loss: 0.037154\n",
      "Epoch [7895/10000] Avg train loss: 0.037149\n",
      "Epoch [7896/10000] Avg train loss: 0.037144\n",
      "Epoch [7897/10000] Avg train loss: 0.037140\n",
      "Epoch [7898/10000] Avg train loss: 0.037135\n",
      "Epoch [7899/10000] Avg train loss: 0.037130\n",
      "Epoch [7900/10000] Avg train loss: 0.037126\n",
      "Epoch [7901/10000] Avg train loss: 0.037121\n",
      "Epoch [7902/10000] Avg train loss: 0.037116\n",
      "Epoch [7903/10000] Avg train loss: 0.037112\n",
      "Epoch [7904/10000] Avg train loss: 0.037107\n",
      "Epoch [7905/10000] Avg train loss: 0.037102\n",
      "Epoch [7906/10000] Avg train loss: 0.037098\n",
      "Epoch [7907/10000] Avg train loss: 0.037093\n",
      "Epoch [7908/10000] Avg train loss: 0.037088\n",
      "Epoch [7909/10000] Avg train loss: 0.037083\n",
      "Epoch [7910/10000] Avg train loss: 0.037079\n",
      "Epoch [7911/10000] Avg train loss: 0.037074\n",
      "Epoch [7912/10000] Avg train loss: 0.037069\n",
      "Epoch [7913/10000] Avg train loss: 0.037065\n",
      "Epoch [7914/10000] Avg train loss: 0.037060\n",
      "Epoch [7915/10000] Avg train loss: 0.037055\n",
      "Epoch [7916/10000] Avg train loss: 0.037051\n",
      "Epoch [7917/10000] Avg train loss: 0.037046\n",
      "Epoch [7918/10000] Avg train loss: 0.037041\n",
      "Epoch [7919/10000] Avg train loss: 0.037037\n",
      "Epoch [7920/10000] Avg train loss: 0.037032\n",
      "Epoch [7921/10000] Avg train loss: 0.037027\n",
      "Epoch [7922/10000] Avg train loss: 0.037023\n",
      "Epoch [7923/10000] Avg train loss: 0.037018\n",
      "Epoch [7924/10000] Avg train loss: 0.037013\n",
      "Epoch [7925/10000] Avg train loss: 0.037009\n",
      "Epoch [7926/10000] Avg train loss: 0.037004\n",
      "Epoch [7927/10000] Avg train loss: 0.036999\n",
      "Epoch [7928/10000] Avg train loss: 0.036995\n",
      "Epoch [7929/10000] Avg train loss: 0.036990\n",
      "Epoch [7930/10000] Avg train loss: 0.036986\n",
      "Epoch [7931/10000] Avg train loss: 0.036981\n",
      "Epoch [7932/10000] Avg train loss: 0.036976\n",
      "Epoch [7933/10000] Avg train loss: 0.036972\n",
      "Epoch [7934/10000] Avg train loss: 0.036967\n",
      "Epoch [7935/10000] Avg train loss: 0.036962\n",
      "Epoch [7936/10000] Avg train loss: 0.036958\n",
      "Epoch [7937/10000] Avg train loss: 0.036953\n",
      "Epoch [7938/10000] Avg train loss: 0.036948\n",
      "Epoch [7939/10000] Avg train loss: 0.036944\n",
      "Epoch [7940/10000] Avg train loss: 0.036939\n",
      "Epoch [7941/10000] Avg train loss: 0.036934\n",
      "Epoch [7942/10000] Avg train loss: 0.036930\n",
      "Epoch [7943/10000] Avg train loss: 0.036925\n",
      "Epoch [7944/10000] Avg train loss: 0.036921\n",
      "Epoch [7945/10000] Avg train loss: 0.036916\n",
      "Epoch [7946/10000] Avg train loss: 0.036911\n",
      "Epoch [7947/10000] Avg train loss: 0.036907\n",
      "Epoch [7948/10000] Avg train loss: 0.036902\n",
      "Epoch [7949/10000] Avg train loss: 0.036897\n",
      "Epoch [7950/10000] Avg train loss: 0.036893\n",
      "Epoch [7951/10000] Avg train loss: 0.036888\n",
      "Epoch [7952/10000] Avg train loss: 0.036883\n",
      "Epoch [7953/10000] Avg train loss: 0.036879\n",
      "Epoch [7954/10000] Avg train loss: 0.036874\n",
      "Epoch [7955/10000] Avg train loss: 0.036870\n",
      "Epoch [7956/10000] Avg train loss: 0.036865\n",
      "Epoch [7957/10000] Avg train loss: 0.036860\n",
      "Epoch [7958/10000] Avg train loss: 0.036856\n",
      "Epoch [7959/10000] Avg train loss: 0.036851\n",
      "Epoch [7960/10000] Avg train loss: 0.036847\n",
      "Epoch [7961/10000] Avg train loss: 0.036842\n",
      "Epoch [7962/10000] Avg train loss: 0.036837\n",
      "Epoch [7963/10000] Avg train loss: 0.036833\n",
      "Epoch [7964/10000] Avg train loss: 0.036828\n",
      "Epoch [7965/10000] Avg train loss: 0.036823\n",
      "Epoch [7966/10000] Avg train loss: 0.036819\n",
      "Epoch [7967/10000] Avg train loss: 0.036814\n",
      "Epoch [7968/10000] Avg train loss: 0.036810\n",
      "Epoch [7969/10000] Avg train loss: 0.036805\n",
      "Epoch [7970/10000] Avg train loss: 0.036800\n",
      "Epoch [7971/10000] Avg train loss: 0.036796\n",
      "Epoch [7972/10000] Avg train loss: 0.036791\n",
      "Epoch [7973/10000] Avg train loss: 0.036787\n",
      "Epoch [7974/10000] Avg train loss: 0.036782\n",
      "Epoch [7975/10000] Avg train loss: 0.036777\n",
      "Epoch [7976/10000] Avg train loss: 0.036773\n",
      "Epoch [7977/10000] Avg train loss: 0.036768\n",
      "Epoch [7978/10000] Avg train loss: 0.036764\n",
      "Epoch [7979/10000] Avg train loss: 0.036759\n",
      "Epoch [7980/10000] Avg train loss: 0.036754\n",
      "Epoch [7981/10000] Avg train loss: 0.036750\n",
      "Epoch [7982/10000] Avg train loss: 0.036745\n",
      "Epoch [7983/10000] Avg train loss: 0.036741\n",
      "Epoch [7984/10000] Avg train loss: 0.036736\n",
      "Epoch [7985/10000] Avg train loss: 0.036731\n",
      "Epoch [7986/10000] Avg train loss: 0.036727\n",
      "Epoch [7987/10000] Avg train loss: 0.036722\n",
      "Epoch [7988/10000] Avg train loss: 0.036718\n",
      "Epoch [7989/10000] Avg train loss: 0.036713\n",
      "Epoch [7990/10000] Avg train loss: 0.036709\n",
      "Epoch [7991/10000] Avg train loss: 0.036704\n",
      "Epoch [7992/10000] Avg train loss: 0.036699\n",
      "Epoch [7993/10000] Avg train loss: 0.036695\n",
      "Epoch [7994/10000] Avg train loss: 0.036690\n",
      "Epoch [7995/10000] Avg train loss: 0.036686\n",
      "Epoch [7996/10000] Avg train loss: 0.036681\n",
      "Epoch [7997/10000] Avg train loss: 0.036676\n",
      "Epoch [7998/10000] Avg train loss: 0.036672\n",
      "Epoch [7999/10000] Avg train loss: 0.036667\n",
      "Epoch [8000/10000] Avg train loss: 0.036663\n",
      "Epoch [8001/10000] Avg train loss: 0.036658\n",
      "Epoch [8002/10000] Avg train loss: 0.036654\n",
      "Epoch [8003/10000] Avg train loss: 0.036649\n",
      "Epoch [8004/10000] Avg train loss: 0.036644\n",
      "Epoch [8005/10000] Avg train loss: 0.036640\n",
      "Epoch [8006/10000] Avg train loss: 0.036635\n",
      "Epoch [8007/10000] Avg train loss: 0.036631\n",
      "Epoch [8008/10000] Avg train loss: 0.036626\n",
      "Epoch [8009/10000] Avg train loss: 0.036622\n",
      "Epoch [8010/10000] Avg train loss: 0.036617\n",
      "Epoch [8011/10000] Avg train loss: 0.036613\n",
      "Epoch [8012/10000] Avg train loss: 0.036608\n",
      "Epoch [8013/10000] Avg train loss: 0.036603\n",
      "Epoch [8014/10000] Avg train loss: 0.036599\n",
      "Epoch [8015/10000] Avg train loss: 0.036594\n",
      "Epoch [8016/10000] Avg train loss: 0.036590\n",
      "Epoch [8017/10000] Avg train loss: 0.036585\n",
      "Epoch [8018/10000] Avg train loss: 0.036581\n",
      "Epoch [8019/10000] Avg train loss: 0.036576\n",
      "Epoch [8020/10000] Avg train loss: 0.036572\n",
      "Epoch [8021/10000] Avg train loss: 0.036567\n",
      "Epoch [8022/10000] Avg train loss: 0.036562\n",
      "Epoch [8023/10000] Avg train loss: 0.036558\n",
      "Epoch [8024/10000] Avg train loss: 0.036553\n",
      "Epoch [8025/10000] Avg train loss: 0.036549\n",
      "Epoch [8026/10000] Avg train loss: 0.036544\n",
      "Epoch [8027/10000] Avg train loss: 0.036540\n",
      "Epoch [8028/10000] Avg train loss: 0.036535\n",
      "Epoch [8029/10000] Avg train loss: 0.036531\n",
      "Epoch [8030/10000] Avg train loss: 0.036526\n",
      "Epoch [8031/10000] Avg train loss: 0.036522\n",
      "Epoch [8032/10000] Avg train loss: 0.036517\n",
      "Epoch [8033/10000] Avg train loss: 0.036513\n",
      "Epoch [8034/10000] Avg train loss: 0.036508\n",
      "Epoch [8035/10000] Avg train loss: 0.036503\n",
      "Epoch [8036/10000] Avg train loss: 0.036499\n",
      "Epoch [8037/10000] Avg train loss: 0.036494\n",
      "Epoch [8038/10000] Avg train loss: 0.036490\n",
      "Epoch [8039/10000] Avg train loss: 0.036485\n",
      "Epoch [8040/10000] Avg train loss: 0.036481\n",
      "Epoch [8041/10000] Avg train loss: 0.036476\n",
      "Epoch [8042/10000] Avg train loss: 0.036472\n",
      "Epoch [8043/10000] Avg train loss: 0.036467\n",
      "Epoch [8044/10000] Avg train loss: 0.036463\n",
      "Epoch [8045/10000] Avg train loss: 0.036458\n",
      "Epoch [8046/10000] Avg train loss: 0.036454\n",
      "Epoch [8047/10000] Avg train loss: 0.036449\n",
      "Epoch [8048/10000] Avg train loss: 0.036445\n",
      "Epoch [8049/10000] Avg train loss: 0.036440\n",
      "Epoch [8050/10000] Avg train loss: 0.036436\n",
      "Epoch [8051/10000] Avg train loss: 0.036431\n",
      "Epoch [8052/10000] Avg train loss: 0.036427\n",
      "Epoch [8053/10000] Avg train loss: 0.036422\n",
      "Epoch [8054/10000] Avg train loss: 0.036418\n",
      "Epoch [8055/10000] Avg train loss: 0.036413\n",
      "Epoch [8056/10000] Avg train loss: 0.036409\n",
      "Epoch [8057/10000] Avg train loss: 0.036404\n",
      "Epoch [8058/10000] Avg train loss: 0.036400\n",
      "Epoch [8059/10000] Avg train loss: 0.036395\n",
      "Epoch [8060/10000] Avg train loss: 0.036391\n",
      "Epoch [8061/10000] Avg train loss: 0.036386\n",
      "Epoch [8062/10000] Avg train loss: 0.036382\n",
      "Epoch [8063/10000] Avg train loss: 0.036377\n",
      "Epoch [8064/10000] Avg train loss: 0.036373\n",
      "Epoch [8065/10000] Avg train loss: 0.036368\n",
      "Epoch [8066/10000] Avg train loss: 0.036364\n",
      "Epoch [8067/10000] Avg train loss: 0.036359\n",
      "Epoch [8068/10000] Avg train loss: 0.036355\n",
      "Epoch [8069/10000] Avg train loss: 0.036350\n",
      "Epoch [8070/10000] Avg train loss: 0.036346\n",
      "Epoch [8071/10000] Avg train loss: 0.036341\n",
      "Epoch [8072/10000] Avg train loss: 0.036337\n",
      "Epoch [8073/10000] Avg train loss: 0.036332\n",
      "Epoch [8074/10000] Avg train loss: 0.036328\n",
      "Epoch [8075/10000] Avg train loss: 0.036323\n",
      "Epoch [8076/10000] Avg train loss: 0.036319\n",
      "Epoch [8077/10000] Avg train loss: 0.036314\n",
      "Epoch [8078/10000] Avg train loss: 0.036310\n",
      "Epoch [8079/10000] Avg train loss: 0.036305\n",
      "Epoch [8080/10000] Avg train loss: 0.036301\n",
      "Epoch [8081/10000] Avg train loss: 0.036296\n",
      "Epoch [8082/10000] Avg train loss: 0.036292\n",
      "Epoch [8083/10000] Avg train loss: 0.036287\n",
      "Epoch [8084/10000] Avg train loss: 0.036283\n",
      "Epoch [8085/10000] Avg train loss: 0.036278\n",
      "Epoch [8086/10000] Avg train loss: 0.036274\n",
      "Epoch [8087/10000] Avg train loss: 0.036269\n",
      "Epoch [8088/10000] Avg train loss: 0.036265\n",
      "Epoch [8089/10000] Avg train loss: 0.036260\n",
      "Epoch [8090/10000] Avg train loss: 0.036256\n",
      "Epoch [8091/10000] Avg train loss: 0.036251\n",
      "Epoch [8092/10000] Avg train loss: 0.036247\n",
      "Epoch [8093/10000] Avg train loss: 0.036243\n",
      "Epoch [8094/10000] Avg train loss: 0.036238\n",
      "Epoch [8095/10000] Avg train loss: 0.036234\n",
      "Epoch [8096/10000] Avg train loss: 0.036229\n",
      "Epoch [8097/10000] Avg train loss: 0.036225\n",
      "Epoch [8098/10000] Avg train loss: 0.036220\n",
      "Epoch [8099/10000] Avg train loss: 0.036216\n",
      "Epoch [8100/10000] Avg train loss: 0.036211\n",
      "Epoch [8101/10000] Avg train loss: 0.036207\n",
      "Epoch [8102/10000] Avg train loss: 0.036202\n",
      "Epoch [8103/10000] Avg train loss: 0.036198\n",
      "Epoch [8104/10000] Avg train loss: 0.036193\n",
      "Epoch [8105/10000] Avg train loss: 0.036189\n",
      "Epoch [8106/10000] Avg train loss: 0.036185\n",
      "Epoch [8107/10000] Avg train loss: 0.036180\n",
      "Epoch [8108/10000] Avg train loss: 0.036176\n",
      "Epoch [8109/10000] Avg train loss: 0.036171\n",
      "Epoch [8110/10000] Avg train loss: 0.036167\n",
      "Epoch [8111/10000] Avg train loss: 0.036162\n",
      "Epoch [8112/10000] Avg train loss: 0.036158\n",
      "Epoch [8113/10000] Avg train loss: 0.036153\n",
      "Epoch [8114/10000] Avg train loss: 0.036149\n",
      "Epoch [8115/10000] Avg train loss: 0.036145\n",
      "Epoch [8116/10000] Avg train loss: 0.036140\n",
      "Epoch [8117/10000] Avg train loss: 0.036136\n",
      "Epoch [8118/10000] Avg train loss: 0.036131\n",
      "Epoch [8119/10000] Avg train loss: 0.036127\n",
      "Epoch [8120/10000] Avg train loss: 0.036122\n",
      "Epoch [8121/10000] Avg train loss: 0.036118\n",
      "Epoch [8122/10000] Avg train loss: 0.036113\n",
      "Epoch [8123/10000] Avg train loss: 0.036109\n",
      "Epoch [8124/10000] Avg train loss: 0.036105\n",
      "Epoch [8125/10000] Avg train loss: 0.036100\n",
      "Epoch [8126/10000] Avg train loss: 0.036096\n",
      "Epoch [8127/10000] Avg train loss: 0.036091\n",
      "Epoch [8128/10000] Avg train loss: 0.036087\n",
      "Epoch [8129/10000] Avg train loss: 0.036082\n",
      "Epoch [8130/10000] Avg train loss: 0.036078\n",
      "Epoch [8131/10000] Avg train loss: 0.036074\n",
      "Epoch [8132/10000] Avg train loss: 0.036069\n",
      "Epoch [8133/10000] Avg train loss: 0.036065\n",
      "Epoch [8134/10000] Avg train loss: 0.036060\n",
      "Epoch [8135/10000] Avg train loss: 0.036056\n",
      "Epoch [8136/10000] Avg train loss: 0.036051\n",
      "Epoch [8137/10000] Avg train loss: 0.036047\n",
      "Epoch [8138/10000] Avg train loss: 0.036043\n",
      "Epoch [8139/10000] Avg train loss: 0.036038\n",
      "Epoch [8140/10000] Avg train loss: 0.036034\n",
      "Epoch [8141/10000] Avg train loss: 0.036029\n",
      "Epoch [8142/10000] Avg train loss: 0.036025\n",
      "Epoch [8143/10000] Avg train loss: 0.036021\n",
      "Epoch [8144/10000] Avg train loss: 0.036016\n",
      "Epoch [8145/10000] Avg train loss: 0.036012\n",
      "Epoch [8146/10000] Avg train loss: 0.036007\n",
      "Epoch [8147/10000] Avg train loss: 0.036003\n",
      "Epoch [8148/10000] Avg train loss: 0.035998\n",
      "Epoch [8149/10000] Avg train loss: 0.035994\n",
      "Epoch [8150/10000] Avg train loss: 0.035990\n",
      "Epoch [8151/10000] Avg train loss: 0.035985\n",
      "Epoch [8152/10000] Avg train loss: 0.035981\n",
      "Epoch [8153/10000] Avg train loss: 0.035976\n",
      "Epoch [8154/10000] Avg train loss: 0.035972\n",
      "Epoch [8155/10000] Avg train loss: 0.035968\n",
      "Epoch [8156/10000] Avg train loss: 0.035963\n",
      "Epoch [8157/10000] Avg train loss: 0.035959\n",
      "Epoch [8158/10000] Avg train loss: 0.035954\n",
      "Epoch [8159/10000] Avg train loss: 0.035950\n",
      "Epoch [8160/10000] Avg train loss: 0.035946\n",
      "Epoch [8161/10000] Avg train loss: 0.035941\n",
      "Epoch [8162/10000] Avg train loss: 0.035937\n",
      "Epoch [8163/10000] Avg train loss: 0.035932\n",
      "Epoch [8164/10000] Avg train loss: 0.035928\n",
      "Epoch [8165/10000] Avg train loss: 0.035924\n",
      "Epoch [8166/10000] Avg train loss: 0.035919\n",
      "Epoch [8167/10000] Avg train loss: 0.035915\n",
      "Epoch [8168/10000] Avg train loss: 0.035911\n",
      "Epoch [8169/10000] Avg train loss: 0.035906\n",
      "Epoch [8170/10000] Avg train loss: 0.035902\n",
      "Epoch [8171/10000] Avg train loss: 0.035897\n",
      "Epoch [8172/10000] Avg train loss: 0.035893\n",
      "Epoch [8173/10000] Avg train loss: 0.035889\n",
      "Epoch [8174/10000] Avg train loss: 0.035884\n",
      "Epoch [8175/10000] Avg train loss: 0.035880\n",
      "Epoch [8176/10000] Avg train loss: 0.035876\n",
      "Epoch [8177/10000] Avg train loss: 0.035871\n",
      "Epoch [8178/10000] Avg train loss: 0.035867\n",
      "Epoch [8179/10000] Avg train loss: 0.035862\n",
      "Epoch [8180/10000] Avg train loss: 0.035858\n",
      "Epoch [8181/10000] Avg train loss: 0.035854\n",
      "Epoch [8182/10000] Avg train loss: 0.035849\n",
      "Epoch [8183/10000] Avg train loss: 0.035845\n",
      "Epoch [8184/10000] Avg train loss: 0.035841\n",
      "Epoch [8185/10000] Avg train loss: 0.035836\n",
      "Epoch [8186/10000] Avg train loss: 0.035832\n",
      "Epoch [8187/10000] Avg train loss: 0.035827\n",
      "Epoch [8188/10000] Avg train loss: 0.035823\n",
      "Epoch [8189/10000] Avg train loss: 0.035819\n",
      "Epoch [8190/10000] Avg train loss: 0.035814\n",
      "Epoch [8191/10000] Avg train loss: 0.035810\n",
      "Epoch [8192/10000] Avg train loss: 0.035806\n",
      "Epoch [8193/10000] Avg train loss: 0.035801\n",
      "Epoch [8194/10000] Avg train loss: 0.035797\n",
      "Epoch [8195/10000] Avg train loss: 0.035793\n",
      "Epoch [8196/10000] Avg train loss: 0.035788\n",
      "Epoch [8197/10000] Avg train loss: 0.035784\n",
      "Epoch [8198/10000] Avg train loss: 0.035779\n",
      "Epoch [8199/10000] Avg train loss: 0.035775\n",
      "Epoch [8200/10000] Avg train loss: 0.035771\n",
      "Epoch [8201/10000] Avg train loss: 0.035766\n",
      "Epoch [8202/10000] Avg train loss: 0.035762\n",
      "Epoch [8203/10000] Avg train loss: 0.035758\n",
      "Epoch [8204/10000] Avg train loss: 0.035753\n",
      "Epoch [8205/10000] Avg train loss: 0.035749\n",
      "Epoch [8206/10000] Avg train loss: 0.035745\n",
      "Epoch [8207/10000] Avg train loss: 0.035740\n",
      "Epoch [8208/10000] Avg train loss: 0.035736\n",
      "Epoch [8209/10000] Avg train loss: 0.035732\n",
      "Epoch [8210/10000] Avg train loss: 0.035727\n",
      "Epoch [8211/10000] Avg train loss: 0.035723\n",
      "Epoch [8212/10000] Avg train loss: 0.035719\n",
      "Epoch [8213/10000] Avg train loss: 0.035714\n",
      "Epoch [8214/10000] Avg train loss: 0.035710\n",
      "Epoch [8215/10000] Avg train loss: 0.035706\n",
      "Epoch [8216/10000] Avg train loss: 0.035701\n",
      "Epoch [8217/10000] Avg train loss: 0.035697\n",
      "Epoch [8218/10000] Avg train loss: 0.035693\n",
      "Epoch [8219/10000] Avg train loss: 0.035688\n",
      "Epoch [8220/10000] Avg train loss: 0.035684\n",
      "Epoch [8221/10000] Avg train loss: 0.035680\n",
      "Epoch [8222/10000] Avg train loss: 0.035675\n",
      "Epoch [8223/10000] Avg train loss: 0.035671\n",
      "Epoch [8224/10000] Avg train loss: 0.035667\n",
      "Epoch [8225/10000] Avg train loss: 0.035662\n",
      "Epoch [8226/10000] Avg train loss: 0.035658\n",
      "Epoch [8227/10000] Avg train loss: 0.035654\n",
      "Epoch [8228/10000] Avg train loss: 0.035649\n",
      "Epoch [8229/10000] Avg train loss: 0.035645\n",
      "Epoch [8230/10000] Avg train loss: 0.035641\n",
      "Epoch [8231/10000] Avg train loss: 0.035636\n",
      "Epoch [8232/10000] Avg train loss: 0.035632\n",
      "Epoch [8233/10000] Avg train loss: 0.035628\n",
      "Epoch [8234/10000] Avg train loss: 0.035623\n",
      "Epoch [8235/10000] Avg train loss: 0.035619\n",
      "Epoch [8236/10000] Avg train loss: 0.035615\n",
      "Epoch [8237/10000] Avg train loss: 0.035610\n",
      "Epoch [8238/10000] Avg train loss: 0.035606\n",
      "Epoch [8239/10000] Avg train loss: 0.035602\n",
      "Epoch [8240/10000] Avg train loss: 0.035598\n",
      "Epoch [8241/10000] Avg train loss: 0.035593\n",
      "Epoch [8242/10000] Avg train loss: 0.035589\n",
      "Epoch [8243/10000] Avg train loss: 0.035585\n",
      "Epoch [8244/10000] Avg train loss: 0.035580\n",
      "Epoch [8245/10000] Avg train loss: 0.035576\n",
      "Epoch [8246/10000] Avg train loss: 0.035572\n",
      "Epoch [8247/10000] Avg train loss: 0.035567\n",
      "Epoch [8248/10000] Avg train loss: 0.035563\n",
      "Epoch [8249/10000] Avg train loss: 0.035559\n",
      "Epoch [8250/10000] Avg train loss: 0.035555\n",
      "Epoch [8251/10000] Avg train loss: 0.035550\n",
      "Epoch [8252/10000] Avg train loss: 0.035546\n",
      "Epoch [8253/10000] Avg train loss: 0.035542\n",
      "Epoch [8254/10000] Avg train loss: 0.035537\n",
      "Epoch [8255/10000] Avg train loss: 0.035533\n",
      "Epoch [8256/10000] Avg train loss: 0.035529\n",
      "Epoch [8257/10000] Avg train loss: 0.035524\n",
      "Epoch [8258/10000] Avg train loss: 0.035520\n",
      "Epoch [8259/10000] Avg train loss: 0.035516\n",
      "Epoch [8260/10000] Avg train loss: 0.035512\n",
      "Epoch [8261/10000] Avg train loss: 0.035507\n",
      "Epoch [8262/10000] Avg train loss: 0.035503\n",
      "Epoch [8263/10000] Avg train loss: 0.035499\n",
      "Epoch [8264/10000] Avg train loss: 0.035494\n",
      "Epoch [8265/10000] Avg train loss: 0.035490\n",
      "Epoch [8266/10000] Avg train loss: 0.035486\n",
      "Epoch [8267/10000] Avg train loss: 0.035482\n",
      "Epoch [8268/10000] Avg train loss: 0.035477\n",
      "Epoch [8269/10000] Avg train loss: 0.035473\n",
      "Epoch [8270/10000] Avg train loss: 0.035469\n",
      "Epoch [8271/10000] Avg train loss: 0.035464\n",
      "Epoch [8272/10000] Avg train loss: 0.035460\n",
      "Epoch [8273/10000] Avg train loss: 0.035456\n",
      "Epoch [8274/10000] Avg train loss: 0.035452\n",
      "Epoch [8275/10000] Avg train loss: 0.035447\n",
      "Epoch [8276/10000] Avg train loss: 0.035443\n",
      "Epoch [8277/10000] Avg train loss: 0.035439\n",
      "Epoch [8278/10000] Avg train loss: 0.035435\n",
      "Epoch [8279/10000] Avg train loss: 0.035430\n",
      "Epoch [8280/10000] Avg train loss: 0.035426\n",
      "Epoch [8281/10000] Avg train loss: 0.035422\n",
      "Epoch [8282/10000] Avg train loss: 0.035417\n",
      "Epoch [8283/10000] Avg train loss: 0.035413\n",
      "Epoch [8284/10000] Avg train loss: 0.035409\n",
      "Epoch [8285/10000] Avg train loss: 0.035405\n",
      "Epoch [8286/10000] Avg train loss: 0.035400\n",
      "Epoch [8287/10000] Avg train loss: 0.035396\n",
      "Epoch [8288/10000] Avg train loss: 0.035392\n",
      "Epoch [8289/10000] Avg train loss: 0.035388\n",
      "Epoch [8290/10000] Avg train loss: 0.035383\n",
      "Epoch [8291/10000] Avg train loss: 0.035379\n",
      "Epoch [8292/10000] Avg train loss: 0.035375\n",
      "Epoch [8293/10000] Avg train loss: 0.035371\n",
      "Epoch [8294/10000] Avg train loss: 0.035366\n",
      "Epoch [8295/10000] Avg train loss: 0.035362\n",
      "Epoch [8296/10000] Avg train loss: 0.035358\n",
      "Epoch [8297/10000] Avg train loss: 0.035354\n",
      "Epoch [8298/10000] Avg train loss: 0.035349\n",
      "Epoch [8299/10000] Avg train loss: 0.035345\n",
      "Epoch [8300/10000] Avg train loss: 0.035341\n",
      "Epoch [8301/10000] Avg train loss: 0.035337\n",
      "Epoch [8302/10000] Avg train loss: 0.035332\n",
      "Epoch [8303/10000] Avg train loss: 0.035328\n",
      "Epoch [8304/10000] Avg train loss: 0.035324\n",
      "Epoch [8305/10000] Avg train loss: 0.035320\n",
      "Epoch [8306/10000] Avg train loss: 0.035315\n",
      "Epoch [8307/10000] Avg train loss: 0.035311\n",
      "Epoch [8308/10000] Avg train loss: 0.035307\n",
      "Epoch [8309/10000] Avg train loss: 0.035303\n",
      "Epoch [8310/10000] Avg train loss: 0.035298\n",
      "Epoch [8311/10000] Avg train loss: 0.035294\n",
      "Epoch [8312/10000] Avg train loss: 0.035290\n",
      "Epoch [8313/10000] Avg train loss: 0.035286\n",
      "Epoch [8314/10000] Avg train loss: 0.035281\n",
      "Epoch [8315/10000] Avg train loss: 0.035277\n",
      "Epoch [8316/10000] Avg train loss: 0.035273\n",
      "Epoch [8317/10000] Avg train loss: 0.035269\n",
      "Epoch [8318/10000] Avg train loss: 0.035265\n",
      "Epoch [8319/10000] Avg train loss: 0.035260\n",
      "Epoch [8320/10000] Avg train loss: 0.035256\n",
      "Epoch [8321/10000] Avg train loss: 0.035252\n",
      "Epoch [8322/10000] Avg train loss: 0.035248\n",
      "Epoch [8323/10000] Avg train loss: 0.035243\n",
      "Epoch [8324/10000] Avg train loss: 0.035239\n",
      "Epoch [8325/10000] Avg train loss: 0.035235\n",
      "Epoch [8326/10000] Avg train loss: 0.035231\n",
      "Epoch [8327/10000] Avg train loss: 0.035227\n",
      "Epoch [8328/10000] Avg train loss: 0.035222\n",
      "Epoch [8329/10000] Avg train loss: 0.035218\n",
      "Epoch [8330/10000] Avg train loss: 0.035214\n",
      "Epoch [8331/10000] Avg train loss: 0.035210\n",
      "Epoch [8332/10000] Avg train loss: 0.035205\n",
      "Epoch [8333/10000] Avg train loss: 0.035201\n",
      "Epoch [8334/10000] Avg train loss: 0.035197\n",
      "Epoch [8335/10000] Avg train loss: 0.035193\n",
      "Epoch [8336/10000] Avg train loss: 0.035189\n",
      "Epoch [8337/10000] Avg train loss: 0.035184\n",
      "Epoch [8338/10000] Avg train loss: 0.035180\n",
      "Epoch [8339/10000] Avg train loss: 0.035176\n",
      "Epoch [8340/10000] Avg train loss: 0.035172\n",
      "Epoch [8341/10000] Avg train loss: 0.035168\n",
      "Epoch [8342/10000] Avg train loss: 0.035163\n",
      "Epoch [8343/10000] Avg train loss: 0.035159\n",
      "Epoch [8344/10000] Avg train loss: 0.035155\n",
      "Epoch [8345/10000] Avg train loss: 0.035151\n",
      "Epoch [8346/10000] Avg train loss: 0.035147\n",
      "Epoch [8347/10000] Avg train loss: 0.035142\n",
      "Epoch [8348/10000] Avg train loss: 0.035138\n",
      "Epoch [8349/10000] Avg train loss: 0.035134\n",
      "Epoch [8350/10000] Avg train loss: 0.035130\n",
      "Epoch [8351/10000] Avg train loss: 0.035126\n",
      "Epoch [8352/10000] Avg train loss: 0.035121\n",
      "Epoch [8353/10000] Avg train loss: 0.035117\n",
      "Epoch [8354/10000] Avg train loss: 0.035113\n",
      "Epoch [8355/10000] Avg train loss: 0.035109\n",
      "Epoch [8356/10000] Avg train loss: 0.035105\n",
      "Epoch [8357/10000] Avg train loss: 0.035100\n",
      "Epoch [8358/10000] Avg train loss: 0.035096\n",
      "Epoch [8359/10000] Avg train loss: 0.035092\n",
      "Epoch [8360/10000] Avg train loss: 0.035088\n",
      "Epoch [8361/10000] Avg train loss: 0.035084\n",
      "Epoch [8362/10000] Avg train loss: 0.035079\n",
      "Epoch [8363/10000] Avg train loss: 0.035075\n",
      "Epoch [8364/10000] Avg train loss: 0.035071\n",
      "Epoch [8365/10000] Avg train loss: 0.035067\n",
      "Epoch [8366/10000] Avg train loss: 0.035063\n",
      "Epoch [8367/10000] Avg train loss: 0.035059\n",
      "Epoch [8368/10000] Avg train loss: 0.035054\n",
      "Epoch [8369/10000] Avg train loss: 0.035050\n",
      "Epoch [8370/10000] Avg train loss: 0.035046\n",
      "Epoch [8371/10000] Avg train loss: 0.035042\n",
      "Epoch [8372/10000] Avg train loss: 0.035038\n",
      "Epoch [8373/10000] Avg train loss: 0.035033\n",
      "Epoch [8374/10000] Avg train loss: 0.035029\n",
      "Epoch [8375/10000] Avg train loss: 0.035025\n",
      "Epoch [8376/10000] Avg train loss: 0.035021\n",
      "Epoch [8377/10000] Avg train loss: 0.035017\n",
      "Epoch [8378/10000] Avg train loss: 0.035013\n",
      "Epoch [8379/10000] Avg train loss: 0.035008\n",
      "Epoch [8380/10000] Avg train loss: 0.035004\n",
      "Epoch [8381/10000] Avg train loss: 0.035000\n",
      "Epoch [8382/10000] Avg train loss: 0.034996\n",
      "Epoch [8383/10000] Avg train loss: 0.034992\n",
      "Epoch [8384/10000] Avg train loss: 0.034988\n",
      "Epoch [8385/10000] Avg train loss: 0.034983\n",
      "Epoch [8386/10000] Avg train loss: 0.034979\n",
      "Epoch [8387/10000] Avg train loss: 0.034975\n",
      "Epoch [8388/10000] Avg train loss: 0.034971\n",
      "Epoch [8389/10000] Avg train loss: 0.034967\n",
      "Epoch [8390/10000] Avg train loss: 0.034963\n",
      "Epoch [8391/10000] Avg train loss: 0.034959\n",
      "Epoch [8392/10000] Avg train loss: 0.034954\n",
      "Epoch [8393/10000] Avg train loss: 0.034950\n",
      "Epoch [8394/10000] Avg train loss: 0.034946\n",
      "Epoch [8395/10000] Avg train loss: 0.034942\n",
      "Epoch [8396/10000] Avg train loss: 0.034938\n",
      "Epoch [8397/10000] Avg train loss: 0.034934\n",
      "Epoch [8398/10000] Avg train loss: 0.034929\n",
      "Epoch [8399/10000] Avg train loss: 0.034925\n",
      "Epoch [8400/10000] Avg train loss: 0.034921\n",
      "Epoch [8401/10000] Avg train loss: 0.034917\n",
      "Epoch [8402/10000] Avg train loss: 0.034913\n",
      "Epoch [8403/10000] Avg train loss: 0.034909\n",
      "Epoch [8404/10000] Avg train loss: 0.034905\n",
      "Epoch [8405/10000] Avg train loss: 0.034900\n",
      "Epoch [8406/10000] Avg train loss: 0.034896\n",
      "Epoch [8407/10000] Avg train loss: 0.034892\n",
      "Epoch [8408/10000] Avg train loss: 0.034888\n",
      "Epoch [8409/10000] Avg train loss: 0.034884\n",
      "Epoch [8410/10000] Avg train loss: 0.034880\n",
      "Epoch [8411/10000] Avg train loss: 0.034876\n",
      "Epoch [8412/10000] Avg train loss: 0.034871\n",
      "Epoch [8413/10000] Avg train loss: 0.034867\n",
      "Epoch [8414/10000] Avg train loss: 0.034863\n",
      "Epoch [8415/10000] Avg train loss: 0.034859\n",
      "Epoch [8416/10000] Avg train loss: 0.034855\n",
      "Epoch [8417/10000] Avg train loss: 0.034851\n",
      "Epoch [8418/10000] Avg train loss: 0.034847\n",
      "Epoch [8419/10000] Avg train loss: 0.034843\n",
      "Epoch [8420/10000] Avg train loss: 0.034838\n",
      "Epoch [8421/10000] Avg train loss: 0.034834\n",
      "Epoch [8422/10000] Avg train loss: 0.034830\n",
      "Epoch [8423/10000] Avg train loss: 0.034826\n",
      "Epoch [8424/10000] Avg train loss: 0.034822\n",
      "Epoch [8425/10000] Avg train loss: 0.034818\n",
      "Epoch [8426/10000] Avg train loss: 0.034814\n",
      "Epoch [8427/10000] Avg train loss: 0.034810\n",
      "Epoch [8428/10000] Avg train loss: 0.034805\n",
      "Epoch [8429/10000] Avg train loss: 0.034801\n",
      "Epoch [8430/10000] Avg train loss: 0.034797\n",
      "Epoch [8431/10000] Avg train loss: 0.034793\n",
      "Epoch [8432/10000] Avg train loss: 0.034789\n",
      "Epoch [8433/10000] Avg train loss: 0.034785\n",
      "Epoch [8434/10000] Avg train loss: 0.034781\n",
      "Epoch [8435/10000] Avg train loss: 0.034777\n",
      "Epoch [8436/10000] Avg train loss: 0.034772\n",
      "Epoch [8437/10000] Avg train loss: 0.034768\n",
      "Epoch [8438/10000] Avg train loss: 0.034764\n",
      "Epoch [8439/10000] Avg train loss: 0.034760\n",
      "Epoch [8440/10000] Avg train loss: 0.034756\n",
      "Epoch [8441/10000] Avg train loss: 0.034752\n",
      "Epoch [8442/10000] Avg train loss: 0.034748\n",
      "Epoch [8443/10000] Avg train loss: 0.034744\n",
      "Epoch [8444/10000] Avg train loss: 0.034740\n",
      "Epoch [8445/10000] Avg train loss: 0.034736\n",
      "Epoch [8446/10000] Avg train loss: 0.034731\n",
      "Epoch [8447/10000] Avg train loss: 0.034727\n",
      "Epoch [8448/10000] Avg train loss: 0.034723\n",
      "Epoch [8449/10000] Avg train loss: 0.034719\n",
      "Epoch [8450/10000] Avg train loss: 0.034715\n",
      "Epoch [8451/10000] Avg train loss: 0.034711\n",
      "Epoch [8452/10000] Avg train loss: 0.034707\n",
      "Epoch [8453/10000] Avg train loss: 0.034703\n",
      "Epoch [8454/10000] Avg train loss: 0.034699\n",
      "Epoch [8455/10000] Avg train loss: 0.034695\n",
      "Epoch [8456/10000] Avg train loss: 0.034690\n",
      "Epoch [8457/10000] Avg train loss: 0.034686\n",
      "Epoch [8458/10000] Avg train loss: 0.034682\n",
      "Epoch [8459/10000] Avg train loss: 0.034678\n",
      "Epoch [8460/10000] Avg train loss: 0.034674\n",
      "Epoch [8461/10000] Avg train loss: 0.034670\n",
      "Epoch [8462/10000] Avg train loss: 0.034666\n",
      "Epoch [8463/10000] Avg train loss: 0.034662\n",
      "Epoch [8464/10000] Avg train loss: 0.034658\n",
      "Epoch [8465/10000] Avg train loss: 0.034654\n",
      "Epoch [8466/10000] Avg train loss: 0.034650\n",
      "Epoch [8467/10000] Avg train loss: 0.034645\n",
      "Epoch [8468/10000] Avg train loss: 0.034641\n",
      "Epoch [8469/10000] Avg train loss: 0.034637\n",
      "Epoch [8470/10000] Avg train loss: 0.034633\n",
      "Epoch [8471/10000] Avg train loss: 0.034629\n",
      "Epoch [8472/10000] Avg train loss: 0.034625\n",
      "Epoch [8473/10000] Avg train loss: 0.034621\n",
      "Epoch [8474/10000] Avg train loss: 0.034617\n",
      "Epoch [8475/10000] Avg train loss: 0.034613\n",
      "Epoch [8476/10000] Avg train loss: 0.034609\n",
      "Epoch [8477/10000] Avg train loss: 0.034605\n",
      "Epoch [8478/10000] Avg train loss: 0.034601\n",
      "Epoch [8479/10000] Avg train loss: 0.034597\n",
      "Epoch [8480/10000] Avg train loss: 0.034593\n",
      "Epoch [8481/10000] Avg train loss: 0.034588\n",
      "Epoch [8482/10000] Avg train loss: 0.034584\n",
      "Epoch [8483/10000] Avg train loss: 0.034580\n",
      "Epoch [8484/10000] Avg train loss: 0.034576\n",
      "Epoch [8485/10000] Avg train loss: 0.034572\n",
      "Epoch [8486/10000] Avg train loss: 0.034568\n",
      "Epoch [8487/10000] Avg train loss: 0.034564\n",
      "Epoch [8488/10000] Avg train loss: 0.034560\n",
      "Epoch [8489/10000] Avg train loss: 0.034556\n",
      "Epoch [8490/10000] Avg train loss: 0.034552\n",
      "Epoch [8491/10000] Avg train loss: 0.034548\n",
      "Epoch [8492/10000] Avg train loss: 0.034544\n",
      "Epoch [8493/10000] Avg train loss: 0.034540\n",
      "Epoch [8494/10000] Avg train loss: 0.034536\n",
      "Epoch [8495/10000] Avg train loss: 0.034532\n",
      "Epoch [8496/10000] Avg train loss: 0.034528\n",
      "Epoch [8497/10000] Avg train loss: 0.034523\n",
      "Epoch [8498/10000] Avg train loss: 0.034519\n",
      "Epoch [8499/10000] Avg train loss: 0.034515\n",
      "Epoch [8500/10000] Avg train loss: 0.034511\n",
      "Epoch [8501/10000] Avg train loss: 0.034507\n",
      "Epoch [8502/10000] Avg train loss: 0.034503\n",
      "Epoch [8503/10000] Avg train loss: 0.034499\n",
      "Epoch [8504/10000] Avg train loss: 0.034495\n",
      "Epoch [8505/10000] Avg train loss: 0.034491\n",
      "Epoch [8506/10000] Avg train loss: 0.034487\n",
      "Epoch [8507/10000] Avg train loss: 0.034483\n",
      "Epoch [8508/10000] Avg train loss: 0.034479\n",
      "Epoch [8509/10000] Avg train loss: 0.034475\n",
      "Epoch [8510/10000] Avg train loss: 0.034471\n",
      "Epoch [8511/10000] Avg train loss: 0.034467\n",
      "Epoch [8512/10000] Avg train loss: 0.034463\n",
      "Epoch [8513/10000] Avg train loss: 0.034459\n",
      "Epoch [8514/10000] Avg train loss: 0.034455\n",
      "Epoch [8515/10000] Avg train loss: 0.034451\n",
      "Epoch [8516/10000] Avg train loss: 0.034447\n",
      "Epoch [8517/10000] Avg train loss: 0.034443\n",
      "Epoch [8518/10000] Avg train loss: 0.034439\n",
      "Epoch [8519/10000] Avg train loss: 0.034435\n",
      "Epoch [8520/10000] Avg train loss: 0.034431\n",
      "Epoch [8521/10000] Avg train loss: 0.034426\n",
      "Epoch [8522/10000] Avg train loss: 0.034422\n",
      "Epoch [8523/10000] Avg train loss: 0.034418\n",
      "Epoch [8524/10000] Avg train loss: 0.034414\n",
      "Epoch [8525/10000] Avg train loss: 0.034410\n",
      "Epoch [8526/10000] Avg train loss: 0.034406\n",
      "Epoch [8527/10000] Avg train loss: 0.034402\n",
      "Epoch [8528/10000] Avg train loss: 0.034398\n",
      "Epoch [8529/10000] Avg train loss: 0.034394\n",
      "Epoch [8530/10000] Avg train loss: 0.034390\n",
      "Epoch [8531/10000] Avg train loss: 0.034386\n",
      "Epoch [8532/10000] Avg train loss: 0.034382\n",
      "Epoch [8533/10000] Avg train loss: 0.034378\n",
      "Epoch [8534/10000] Avg train loss: 0.034374\n",
      "Epoch [8535/10000] Avg train loss: 0.034370\n",
      "Epoch [8536/10000] Avg train loss: 0.034366\n",
      "Epoch [8537/10000] Avg train loss: 0.034362\n",
      "Epoch [8538/10000] Avg train loss: 0.034358\n",
      "Epoch [8539/10000] Avg train loss: 0.034354\n",
      "Epoch [8540/10000] Avg train loss: 0.034350\n",
      "Epoch [8541/10000] Avg train loss: 0.034346\n",
      "Epoch [8542/10000] Avg train loss: 0.034342\n",
      "Epoch [8543/10000] Avg train loss: 0.034338\n",
      "Epoch [8544/10000] Avg train loss: 0.034334\n",
      "Epoch [8545/10000] Avg train loss: 0.034330\n",
      "Epoch [8546/10000] Avg train loss: 0.034326\n",
      "Epoch [8547/10000] Avg train loss: 0.034322\n",
      "Epoch [8548/10000] Avg train loss: 0.034318\n",
      "Epoch [8549/10000] Avg train loss: 0.034314\n",
      "Epoch [8550/10000] Avg train loss: 0.034310\n",
      "Epoch [8551/10000] Avg train loss: 0.034306\n",
      "Epoch [8552/10000] Avg train loss: 0.034302\n",
      "Epoch [8553/10000] Avg train loss: 0.034298\n",
      "Epoch [8554/10000] Avg train loss: 0.034294\n",
      "Epoch [8555/10000] Avg train loss: 0.034290\n",
      "Epoch [8556/10000] Avg train loss: 0.034286\n",
      "Epoch [8557/10000] Avg train loss: 0.034282\n",
      "Epoch [8558/10000] Avg train loss: 0.034278\n",
      "Epoch [8559/10000] Avg train loss: 0.034274\n",
      "Epoch [8560/10000] Avg train loss: 0.034270\n",
      "Epoch [8561/10000] Avg train loss: 0.034266\n",
      "Epoch [8562/10000] Avg train loss: 0.034262\n",
      "Epoch [8563/10000] Avg train loss: 0.034258\n",
      "Epoch [8564/10000] Avg train loss: 0.034254\n",
      "Epoch [8565/10000] Avg train loss: 0.034250\n",
      "Epoch [8566/10000] Avg train loss: 0.034246\n",
      "Epoch [8567/10000] Avg train loss: 0.034242\n",
      "Epoch [8568/10000] Avg train loss: 0.034238\n",
      "Epoch [8569/10000] Avg train loss: 0.034234\n",
      "Epoch [8570/10000] Avg train loss: 0.034230\n",
      "Epoch [8571/10000] Avg train loss: 0.034226\n",
      "Epoch [8572/10000] Avg train loss: 0.034222\n",
      "Epoch [8573/10000] Avg train loss: 0.034218\n",
      "Epoch [8574/10000] Avg train loss: 0.034214\n",
      "Epoch [8575/10000] Avg train loss: 0.034210\n",
      "Epoch [8576/10000] Avg train loss: 0.034206\n",
      "Epoch [8577/10000] Avg train loss: 0.034202\n",
      "Epoch [8578/10000] Avg train loss: 0.034198\n",
      "Epoch [8579/10000] Avg train loss: 0.034194\n",
      "Epoch [8580/10000] Avg train loss: 0.034190\n",
      "Epoch [8581/10000] Avg train loss: 0.034186\n",
      "Epoch [8582/10000] Avg train loss: 0.034182\n",
      "Epoch [8583/10000] Avg train loss: 0.034178\n",
      "Epoch [8584/10000] Avg train loss: 0.034174\n",
      "Epoch [8585/10000] Avg train loss: 0.034170\n",
      "Epoch [8586/10000] Avg train loss: 0.034166\n",
      "Epoch [8587/10000] Avg train loss: 0.034163\n",
      "Epoch [8588/10000] Avg train loss: 0.034159\n",
      "Epoch [8589/10000] Avg train loss: 0.034155\n",
      "Epoch [8590/10000] Avg train loss: 0.034151\n",
      "Epoch [8591/10000] Avg train loss: 0.034147\n",
      "Epoch [8592/10000] Avg train loss: 0.034143\n",
      "Epoch [8593/10000] Avg train loss: 0.034139\n",
      "Epoch [8594/10000] Avg train loss: 0.034135\n",
      "Epoch [8595/10000] Avg train loss: 0.034131\n",
      "Epoch [8596/10000] Avg train loss: 0.034127\n",
      "Epoch [8597/10000] Avg train loss: 0.034123\n",
      "Epoch [8598/10000] Avg train loss: 0.034119\n",
      "Epoch [8599/10000] Avg train loss: 0.034115\n",
      "Epoch [8600/10000] Avg train loss: 0.034111\n",
      "Epoch [8601/10000] Avg train loss: 0.034107\n",
      "Epoch [8602/10000] Avg train loss: 0.034103\n",
      "Epoch [8603/10000] Avg train loss: 0.034099\n",
      "Epoch [8604/10000] Avg train loss: 0.034095\n",
      "Epoch [8605/10000] Avg train loss: 0.034091\n",
      "Epoch [8606/10000] Avg train loss: 0.034087\n",
      "Epoch [8607/10000] Avg train loss: 0.034083\n",
      "Epoch [8608/10000] Avg train loss: 0.034079\n",
      "Epoch [8609/10000] Avg train loss: 0.034075\n",
      "Epoch [8610/10000] Avg train loss: 0.034071\n",
      "Epoch [8611/10000] Avg train loss: 0.034068\n",
      "Epoch [8612/10000] Avg train loss: 0.034064\n",
      "Epoch [8613/10000] Avg train loss: 0.034060\n",
      "Epoch [8614/10000] Avg train loss: 0.034056\n",
      "Epoch [8615/10000] Avg train loss: 0.034052\n",
      "Epoch [8616/10000] Avg train loss: 0.034048\n",
      "Epoch [8617/10000] Avg train loss: 0.034044\n",
      "Epoch [8618/10000] Avg train loss: 0.034040\n",
      "Epoch [8619/10000] Avg train loss: 0.034036\n",
      "Epoch [8620/10000] Avg train loss: 0.034032\n",
      "Epoch [8621/10000] Avg train loss: 0.034028\n",
      "Epoch [8622/10000] Avg train loss: 0.034024\n",
      "Epoch [8623/10000] Avg train loss: 0.034020\n",
      "Epoch [8624/10000] Avg train loss: 0.034016\n",
      "Epoch [8625/10000] Avg train loss: 0.034012\n",
      "Epoch [8626/10000] Avg train loss: 0.034008\n",
      "Epoch [8627/10000] Avg train loss: 0.034005\n",
      "Epoch [8628/10000] Avg train loss: 0.034001\n",
      "Epoch [8629/10000] Avg train loss: 0.033997\n",
      "Epoch [8630/10000] Avg train loss: 0.033993\n",
      "Epoch [8631/10000] Avg train loss: 0.033989\n",
      "Epoch [8632/10000] Avg train loss: 0.033985\n",
      "Epoch [8633/10000] Avg train loss: 0.033981\n",
      "Epoch [8634/10000] Avg train loss: 0.033977\n",
      "Epoch [8635/10000] Avg train loss: 0.033973\n",
      "Epoch [8636/10000] Avg train loss: 0.033969\n",
      "Epoch [8637/10000] Avg train loss: 0.033965\n",
      "Epoch [8638/10000] Avg train loss: 0.033961\n",
      "Epoch [8639/10000] Avg train loss: 0.033957\n",
      "Epoch [8640/10000] Avg train loss: 0.033953\n",
      "Epoch [8641/10000] Avg train loss: 0.033950\n",
      "Epoch [8642/10000] Avg train loss: 0.033946\n",
      "Epoch [8643/10000] Avg train loss: 0.033942\n",
      "Epoch [8644/10000] Avg train loss: 0.033938\n",
      "Epoch [8645/10000] Avg train loss: 0.033934\n",
      "Epoch [8646/10000] Avg train loss: 0.033930\n",
      "Epoch [8647/10000] Avg train loss: 0.033926\n",
      "Epoch [8648/10000] Avg train loss: 0.033922\n",
      "Epoch [8649/10000] Avg train loss: 0.033918\n",
      "Epoch [8650/10000] Avg train loss: 0.033914\n",
      "Epoch [8651/10000] Avg train loss: 0.033910\n",
      "Epoch [8652/10000] Avg train loss: 0.033906\n",
      "Epoch [8653/10000] Avg train loss: 0.033903\n",
      "Epoch [8654/10000] Avg train loss: 0.033899\n",
      "Epoch [8655/10000] Avg train loss: 0.033895\n",
      "Epoch [8656/10000] Avg train loss: 0.033891\n",
      "Epoch [8657/10000] Avg train loss: 0.033887\n",
      "Epoch [8658/10000] Avg train loss: 0.033883\n",
      "Epoch [8659/10000] Avg train loss: 0.033879\n",
      "Epoch [8660/10000] Avg train loss: 0.033875\n",
      "Epoch [8661/10000] Avg train loss: 0.033871\n",
      "Epoch [8662/10000] Avg train loss: 0.033867\n",
      "Epoch [8663/10000] Avg train loss: 0.033864\n",
      "Epoch [8664/10000] Avg train loss: 0.033860\n",
      "Epoch [8665/10000] Avg train loss: 0.033856\n",
      "Epoch [8666/10000] Avg train loss: 0.033852\n",
      "Epoch [8667/10000] Avg train loss: 0.033848\n",
      "Epoch [8668/10000] Avg train loss: 0.033844\n",
      "Epoch [8669/10000] Avg train loss: 0.033840\n",
      "Epoch [8670/10000] Avg train loss: 0.033836\n",
      "Epoch [8671/10000] Avg train loss: 0.033832\n",
      "Epoch [8672/10000] Avg train loss: 0.033828\n",
      "Epoch [8673/10000] Avg train loss: 0.033825\n",
      "Epoch [8674/10000] Avg train loss: 0.033821\n",
      "Epoch [8675/10000] Avg train loss: 0.033817\n",
      "Epoch [8676/10000] Avg train loss: 0.033813\n",
      "Epoch [8677/10000] Avg train loss: 0.033809\n",
      "Epoch [8678/10000] Avg train loss: 0.033805\n",
      "Epoch [8679/10000] Avg train loss: 0.033801\n",
      "Epoch [8680/10000] Avg train loss: 0.033797\n",
      "Epoch [8681/10000] Avg train loss: 0.033793\n",
      "Epoch [8682/10000] Avg train loss: 0.033790\n",
      "Epoch [8683/10000] Avg train loss: 0.033786\n",
      "Epoch [8684/10000] Avg train loss: 0.033782\n",
      "Epoch [8685/10000] Avg train loss: 0.033778\n",
      "Epoch [8686/10000] Avg train loss: 0.033774\n",
      "Epoch [8687/10000] Avg train loss: 0.033770\n",
      "Epoch [8688/10000] Avg train loss: 0.033766\n",
      "Epoch [8689/10000] Avg train loss: 0.033762\n",
      "Epoch [8690/10000] Avg train loss: 0.033759\n",
      "Epoch [8691/10000] Avg train loss: 0.033755\n",
      "Epoch [8692/10000] Avg train loss: 0.033751\n",
      "Epoch [8693/10000] Avg train loss: 0.033747\n",
      "Epoch [8694/10000] Avg train loss: 0.033743\n",
      "Epoch [8695/10000] Avg train loss: 0.033739\n",
      "Epoch [8696/10000] Avg train loss: 0.033735\n",
      "Epoch [8697/10000] Avg train loss: 0.033731\n",
      "Epoch [8698/10000] Avg train loss: 0.033728\n",
      "Epoch [8699/10000] Avg train loss: 0.033724\n",
      "Epoch [8700/10000] Avg train loss: 0.033720\n",
      "Epoch [8701/10000] Avg train loss: 0.033716\n",
      "Epoch [8702/10000] Avg train loss: 0.033712\n",
      "Epoch [8703/10000] Avg train loss: 0.033708\n",
      "Epoch [8704/10000] Avg train loss: 0.033704\n",
      "Epoch [8705/10000] Avg train loss: 0.033701\n",
      "Epoch [8706/10000] Avg train loss: 0.033697\n",
      "Epoch [8707/10000] Avg train loss: 0.033693\n",
      "Epoch [8708/10000] Avg train loss: 0.033689\n",
      "Epoch [8709/10000] Avg train loss: 0.033685\n",
      "Epoch [8710/10000] Avg train loss: 0.033681\n",
      "Epoch [8711/10000] Avg train loss: 0.033677\n",
      "Epoch [8712/10000] Avg train loss: 0.033674\n",
      "Epoch [8713/10000] Avg train loss: 0.033670\n",
      "Epoch [8714/10000] Avg train loss: 0.033666\n",
      "Epoch [8715/10000] Avg train loss: 0.033662\n",
      "Epoch [8716/10000] Avg train loss: 0.033658\n",
      "Epoch [8717/10000] Avg train loss: 0.033654\n",
      "Epoch [8718/10000] Avg train loss: 0.033650\n",
      "Epoch [8719/10000] Avg train loss: 0.033647\n",
      "Epoch [8720/10000] Avg train loss: 0.033643\n",
      "Epoch [8721/10000] Avg train loss: 0.033639\n",
      "Epoch [8722/10000] Avg train loss: 0.033635\n",
      "Epoch [8723/10000] Avg train loss: 0.033631\n",
      "Epoch [8724/10000] Avg train loss: 0.033627\n",
      "Epoch [8725/10000] Avg train loss: 0.033623\n",
      "Epoch [8726/10000] Avg train loss: 0.033620\n",
      "Epoch [8727/10000] Avg train loss: 0.033616\n",
      "Epoch [8728/10000] Avg train loss: 0.033612\n",
      "Epoch [8729/10000] Avg train loss: 0.033608\n",
      "Epoch [8730/10000] Avg train loss: 0.033604\n",
      "Epoch [8731/10000] Avg train loss: 0.033600\n",
      "Epoch [8732/10000] Avg train loss: 0.033597\n",
      "Epoch [8733/10000] Avg train loss: 0.033593\n",
      "Epoch [8734/10000] Avg train loss: 0.033589\n",
      "Epoch [8735/10000] Avg train loss: 0.033585\n",
      "Epoch [8736/10000] Avg train loss: 0.033581\n",
      "Epoch [8737/10000] Avg train loss: 0.033577\n",
      "Epoch [8738/10000] Avg train loss: 0.033574\n",
      "Epoch [8739/10000] Avg train loss: 0.033570\n",
      "Epoch [8740/10000] Avg train loss: 0.033566\n",
      "Epoch [8741/10000] Avg train loss: 0.033562\n",
      "Epoch [8742/10000] Avg train loss: 0.033558\n",
      "Epoch [8743/10000] Avg train loss: 0.033554\n",
      "Epoch [8744/10000] Avg train loss: 0.033551\n",
      "Epoch [8745/10000] Avg train loss: 0.033547\n",
      "Epoch [8746/10000] Avg train loss: 0.033543\n",
      "Epoch [8747/10000] Avg train loss: 0.033539\n",
      "Epoch [8748/10000] Avg train loss: 0.033535\n",
      "Epoch [8749/10000] Avg train loss: 0.033531\n",
      "Epoch [8750/10000] Avg train loss: 0.033528\n",
      "Epoch [8751/10000] Avg train loss: 0.033524\n",
      "Epoch [8752/10000] Avg train loss: 0.033520\n",
      "Epoch [8753/10000] Avg train loss: 0.033516\n",
      "Epoch [8754/10000] Avg train loss: 0.033512\n",
      "Epoch [8755/10000] Avg train loss: 0.033509\n",
      "Epoch [8756/10000] Avg train loss: 0.033505\n",
      "Epoch [8757/10000] Avg train loss: 0.033501\n",
      "Epoch [8758/10000] Avg train loss: 0.033497\n",
      "Epoch [8759/10000] Avg train loss: 0.033493\n",
      "Epoch [8760/10000] Avg train loss: 0.033489\n",
      "Epoch [8761/10000] Avg train loss: 0.033486\n",
      "Epoch [8762/10000] Avg train loss: 0.033482\n",
      "Epoch [8763/10000] Avg train loss: 0.033478\n",
      "Epoch [8764/10000] Avg train loss: 0.033474\n",
      "Epoch [8765/10000] Avg train loss: 0.033470\n",
      "Epoch [8766/10000] Avg train loss: 0.033467\n",
      "Epoch [8767/10000] Avg train loss: 0.033463\n",
      "Epoch [8768/10000] Avg train loss: 0.033459\n",
      "Epoch [8769/10000] Avg train loss: 0.033455\n",
      "Epoch [8770/10000] Avg train loss: 0.033451\n",
      "Epoch [8771/10000] Avg train loss: 0.033448\n",
      "Epoch [8772/10000] Avg train loss: 0.033444\n",
      "Epoch [8773/10000] Avg train loss: 0.033440\n",
      "Epoch [8774/10000] Avg train loss: 0.033436\n",
      "Epoch [8775/10000] Avg train loss: 0.033432\n",
      "Epoch [8776/10000] Avg train loss: 0.033429\n",
      "Epoch [8777/10000] Avg train loss: 0.033425\n",
      "Epoch [8778/10000] Avg train loss: 0.033421\n",
      "Epoch [8779/10000] Avg train loss: 0.033417\n",
      "Epoch [8780/10000] Avg train loss: 0.033413\n",
      "Epoch [8781/10000] Avg train loss: 0.033410\n",
      "Epoch [8782/10000] Avg train loss: 0.033406\n",
      "Epoch [8783/10000] Avg train loss: 0.033402\n",
      "Epoch [8784/10000] Avg train loss: 0.033398\n",
      "Epoch [8785/10000] Avg train loss: 0.033394\n",
      "Epoch [8786/10000] Avg train loss: 0.033391\n",
      "Epoch [8787/10000] Avg train loss: 0.033387\n",
      "Epoch [8788/10000] Avg train loss: 0.033383\n",
      "Epoch [8789/10000] Avg train loss: 0.033379\n",
      "Epoch [8790/10000] Avg train loss: 0.033375\n",
      "Epoch [8791/10000] Avg train loss: 0.033372\n",
      "Epoch [8792/10000] Avg train loss: 0.033368\n",
      "Epoch [8793/10000] Avg train loss: 0.033364\n",
      "Epoch [8794/10000] Avg train loss: 0.033360\n",
      "Epoch [8795/10000] Avg train loss: 0.033357\n",
      "Epoch [8796/10000] Avg train loss: 0.033353\n",
      "Epoch [8797/10000] Avg train loss: 0.033349\n",
      "Epoch [8798/10000] Avg train loss: 0.033345\n",
      "Epoch [8799/10000] Avg train loss: 0.033341\n",
      "Epoch [8800/10000] Avg train loss: 0.033338\n",
      "Epoch [8801/10000] Avg train loss: 0.033334\n",
      "Epoch [8802/10000] Avg train loss: 0.033330\n",
      "Epoch [8803/10000] Avg train loss: 0.033326\n",
      "Epoch [8804/10000] Avg train loss: 0.033323\n",
      "Epoch [8805/10000] Avg train loss: 0.033319\n",
      "Epoch [8806/10000] Avg train loss: 0.033315\n",
      "Epoch [8807/10000] Avg train loss: 0.033311\n",
      "Epoch [8808/10000] Avg train loss: 0.033307\n",
      "Epoch [8809/10000] Avg train loss: 0.033304\n",
      "Epoch [8810/10000] Avg train loss: 0.033300\n",
      "Epoch [8811/10000] Avg train loss: 0.033296\n",
      "Epoch [8812/10000] Avg train loss: 0.033292\n",
      "Epoch [8813/10000] Avg train loss: 0.033289\n",
      "Epoch [8814/10000] Avg train loss: 0.033285\n",
      "Epoch [8815/10000] Avg train loss: 0.033281\n",
      "Epoch [8816/10000] Avg train loss: 0.033277\n",
      "Epoch [8817/10000] Avg train loss: 0.033273\n",
      "Epoch [8818/10000] Avg train loss: 0.033270\n",
      "Epoch [8819/10000] Avg train loss: 0.033266\n",
      "Epoch [8820/10000] Avg train loss: 0.033262\n",
      "Epoch [8821/10000] Avg train loss: 0.033258\n",
      "Epoch [8822/10000] Avg train loss: 0.033255\n",
      "Epoch [8823/10000] Avg train loss: 0.033251\n",
      "Epoch [8824/10000] Avg train loss: 0.033247\n",
      "Epoch [8825/10000] Avg train loss: 0.033243\n",
      "Epoch [8826/10000] Avg train loss: 0.033240\n",
      "Epoch [8827/10000] Avg train loss: 0.033236\n",
      "Epoch [8828/10000] Avg train loss: 0.033232\n",
      "Epoch [8829/10000] Avg train loss: 0.033228\n",
      "Epoch [8830/10000] Avg train loss: 0.033225\n",
      "Epoch [8831/10000] Avg train loss: 0.033221\n",
      "Epoch [8832/10000] Avg train loss: 0.033217\n",
      "Epoch [8833/10000] Avg train loss: 0.033213\n",
      "Epoch [8834/10000] Avg train loss: 0.033210\n",
      "Epoch [8835/10000] Avg train loss: 0.033206\n",
      "Epoch [8836/10000] Avg train loss: 0.033202\n",
      "Epoch [8837/10000] Avg train loss: 0.033198\n",
      "Epoch [8838/10000] Avg train loss: 0.033195\n",
      "Epoch [8839/10000] Avg train loss: 0.033191\n",
      "Epoch [8840/10000] Avg train loss: 0.033187\n",
      "Epoch [8841/10000] Avg train loss: 0.033183\n",
      "Epoch [8842/10000] Avg train loss: 0.033180\n",
      "Epoch [8843/10000] Avg train loss: 0.033176\n",
      "Epoch [8844/10000] Avg train loss: 0.033172\n",
      "Epoch [8845/10000] Avg train loss: 0.033168\n",
      "Epoch [8846/10000] Avg train loss: 0.033165\n",
      "Epoch [8847/10000] Avg train loss: 0.033161\n",
      "Epoch [8848/10000] Avg train loss: 0.033157\n",
      "Epoch [8849/10000] Avg train loss: 0.033153\n",
      "Epoch [8850/10000] Avg train loss: 0.033150\n",
      "Epoch [8851/10000] Avg train loss: 0.033146\n",
      "Epoch [8852/10000] Avg train loss: 0.033142\n",
      "Epoch [8853/10000] Avg train loss: 0.033139\n",
      "Epoch [8854/10000] Avg train loss: 0.033135\n",
      "Epoch [8855/10000] Avg train loss: 0.033131\n",
      "Epoch [8856/10000] Avg train loss: 0.033127\n",
      "Epoch [8857/10000] Avg train loss: 0.033124\n",
      "Epoch [8858/10000] Avg train loss: 0.033120\n",
      "Epoch [8859/10000] Avg train loss: 0.033116\n",
      "Epoch [8860/10000] Avg train loss: 0.033112\n",
      "Epoch [8861/10000] Avg train loss: 0.033109\n",
      "Epoch [8862/10000] Avg train loss: 0.033105\n",
      "Epoch [8863/10000] Avg train loss: 0.033101\n",
      "Epoch [8864/10000] Avg train loss: 0.033097\n",
      "Epoch [8865/10000] Avg train loss: 0.033094\n",
      "Epoch [8866/10000] Avg train loss: 0.033090\n",
      "Epoch [8867/10000] Avg train loss: 0.033086\n",
      "Epoch [8868/10000] Avg train loss: 0.033083\n",
      "Epoch [8869/10000] Avg train loss: 0.033079\n",
      "Epoch [8870/10000] Avg train loss: 0.033075\n",
      "Epoch [8871/10000] Avg train loss: 0.033071\n",
      "Epoch [8872/10000] Avg train loss: 0.033068\n",
      "Epoch [8873/10000] Avg train loss: 0.033064\n",
      "Epoch [8874/10000] Avg train loss: 0.033060\n",
      "Epoch [8875/10000] Avg train loss: 0.033057\n",
      "Epoch [8876/10000] Avg train loss: 0.033053\n",
      "Epoch [8877/10000] Avg train loss: 0.033049\n",
      "Epoch [8878/10000] Avg train loss: 0.033045\n",
      "Epoch [8879/10000] Avg train loss: 0.033042\n",
      "Epoch [8880/10000] Avg train loss: 0.033038\n",
      "Epoch [8881/10000] Avg train loss: 0.033034\n",
      "Epoch [8882/10000] Avg train loss: 0.033031\n",
      "Epoch [8883/10000] Avg train loss: 0.033027\n",
      "Epoch [8884/10000] Avg train loss: 0.033023\n",
      "Epoch [8885/10000] Avg train loss: 0.033019\n",
      "Epoch [8886/10000] Avg train loss: 0.033016\n",
      "Epoch [8887/10000] Avg train loss: 0.033012\n",
      "Epoch [8888/10000] Avg train loss: 0.033008\n",
      "Epoch [8889/10000] Avg train loss: 0.033005\n",
      "Epoch [8890/10000] Avg train loss: 0.033001\n",
      "Epoch [8891/10000] Avg train loss: 0.032997\n",
      "Epoch [8892/10000] Avg train loss: 0.032994\n",
      "Epoch [8893/10000] Avg train loss: 0.032990\n",
      "Epoch [8894/10000] Avg train loss: 0.032986\n",
      "Epoch [8895/10000] Avg train loss: 0.032982\n",
      "Epoch [8896/10000] Avg train loss: 0.032979\n",
      "Epoch [8897/10000] Avg train loss: 0.032975\n",
      "Epoch [8898/10000] Avg train loss: 0.032971\n",
      "Epoch [8899/10000] Avg train loss: 0.032968\n",
      "Epoch [8900/10000] Avg train loss: 0.032964\n",
      "Epoch [8901/10000] Avg train loss: 0.032960\n",
      "Epoch [8902/10000] Avg train loss: 0.032957\n",
      "Epoch [8903/10000] Avg train loss: 0.032953\n",
      "Epoch [8904/10000] Avg train loss: 0.032949\n",
      "Epoch [8905/10000] Avg train loss: 0.032945\n",
      "Epoch [8906/10000] Avg train loss: 0.032942\n",
      "Epoch [8907/10000] Avg train loss: 0.032938\n",
      "Epoch [8908/10000] Avg train loss: 0.032934\n",
      "Epoch [8909/10000] Avg train loss: 0.032931\n",
      "Epoch [8910/10000] Avg train loss: 0.032927\n",
      "Epoch [8911/10000] Avg train loss: 0.032923\n",
      "Epoch [8912/10000] Avg train loss: 0.032920\n",
      "Epoch [8913/10000] Avg train loss: 0.032916\n",
      "Epoch [8914/10000] Avg train loss: 0.032912\n",
      "Epoch [8915/10000] Avg train loss: 0.032909\n",
      "Epoch [8916/10000] Avg train loss: 0.032905\n",
      "Epoch [8917/10000] Avg train loss: 0.032901\n",
      "Epoch [8918/10000] Avg train loss: 0.032898\n",
      "Epoch [8919/10000] Avg train loss: 0.032894\n",
      "Epoch [8920/10000] Avg train loss: 0.032890\n",
      "Epoch [8921/10000] Avg train loss: 0.032887\n",
      "Epoch [8922/10000] Avg train loss: 0.032883\n",
      "Epoch [8923/10000] Avg train loss: 0.032879\n",
      "Epoch [8924/10000] Avg train loss: 0.032875\n",
      "Epoch [8925/10000] Avg train loss: 0.032872\n",
      "Epoch [8926/10000] Avg train loss: 0.032868\n",
      "Epoch [8927/10000] Avg train loss: 0.032864\n",
      "Epoch [8928/10000] Avg train loss: 0.032861\n",
      "Epoch [8929/10000] Avg train loss: 0.032857\n",
      "Epoch [8930/10000] Avg train loss: 0.032853\n",
      "Epoch [8931/10000] Avg train loss: 0.032850\n",
      "Epoch [8932/10000] Avg train loss: 0.032846\n",
      "Epoch [8933/10000] Avg train loss: 0.032842\n",
      "Epoch [8934/10000] Avg train loss: 0.032839\n",
      "Epoch [8935/10000] Avg train loss: 0.032835\n",
      "Epoch [8936/10000] Avg train loss: 0.032831\n",
      "Epoch [8937/10000] Avg train loss: 0.032828\n",
      "Epoch [8938/10000] Avg train loss: 0.032824\n",
      "Epoch [8939/10000] Avg train loss: 0.032820\n",
      "Epoch [8940/10000] Avg train loss: 0.032817\n",
      "Epoch [8941/10000] Avg train loss: 0.032813\n",
      "Epoch [8942/10000] Avg train loss: 0.032809\n",
      "Epoch [8943/10000] Avg train loss: 0.032806\n",
      "Epoch [8944/10000] Avg train loss: 0.032802\n",
      "Epoch [8945/10000] Avg train loss: 0.032798\n",
      "Epoch [8946/10000] Avg train loss: 0.032795\n",
      "Epoch [8947/10000] Avg train loss: 0.032791\n",
      "Epoch [8948/10000] Avg train loss: 0.032788\n",
      "Epoch [8949/10000] Avg train loss: 0.032784\n",
      "Epoch [8950/10000] Avg train loss: 0.032780\n",
      "Epoch [8951/10000] Avg train loss: 0.032777\n",
      "Epoch [8952/10000] Avg train loss: 0.032773\n",
      "Epoch [8953/10000] Avg train loss: 0.032769\n",
      "Epoch [8954/10000] Avg train loss: 0.032766\n",
      "Epoch [8955/10000] Avg train loss: 0.032762\n",
      "Epoch [8956/10000] Avg train loss: 0.032758\n",
      "Epoch [8957/10000] Avg train loss: 0.032755\n",
      "Epoch [8958/10000] Avg train loss: 0.032751\n",
      "Epoch [8959/10000] Avg train loss: 0.032747\n",
      "Epoch [8960/10000] Avg train loss: 0.032744\n",
      "Epoch [8961/10000] Avg train loss: 0.032740\n",
      "Epoch [8962/10000] Avg train loss: 0.032736\n",
      "Epoch [8963/10000] Avg train loss: 0.032733\n",
      "Epoch [8964/10000] Avg train loss: 0.032729\n",
      "Epoch [8965/10000] Avg train loss: 0.032726\n",
      "Epoch [8966/10000] Avg train loss: 0.032722\n",
      "Epoch [8967/10000] Avg train loss: 0.032718\n",
      "Epoch [8968/10000] Avg train loss: 0.032715\n",
      "Epoch [8969/10000] Avg train loss: 0.032711\n",
      "Epoch [8970/10000] Avg train loss: 0.032707\n",
      "Epoch [8971/10000] Avg train loss: 0.032704\n",
      "Epoch [8972/10000] Avg train loss: 0.032700\n",
      "Epoch [8973/10000] Avg train loss: 0.032696\n",
      "Epoch [8974/10000] Avg train loss: 0.032693\n",
      "Epoch [8975/10000] Avg train loss: 0.032689\n",
      "Epoch [8976/10000] Avg train loss: 0.032685\n",
      "Epoch [8977/10000] Avg train loss: 0.032682\n",
      "Epoch [8978/10000] Avg train loss: 0.032678\n",
      "Epoch [8979/10000] Avg train loss: 0.032675\n",
      "Epoch [8980/10000] Avg train loss: 0.032671\n",
      "Epoch [8981/10000] Avg train loss: 0.032667\n",
      "Epoch [8982/10000] Avg train loss: 0.032664\n",
      "Epoch [8983/10000] Avg train loss: 0.032660\n",
      "Epoch [8984/10000] Avg train loss: 0.032656\n",
      "Epoch [8985/10000] Avg train loss: 0.032653\n",
      "Epoch [8986/10000] Avg train loss: 0.032649\n",
      "Epoch [8987/10000] Avg train loss: 0.032646\n",
      "Epoch [8988/10000] Avg train loss: 0.032642\n",
      "Epoch [8989/10000] Avg train loss: 0.032638\n",
      "Epoch [8990/10000] Avg train loss: 0.032635\n",
      "Epoch [8991/10000] Avg train loss: 0.032631\n",
      "Epoch [8992/10000] Avg train loss: 0.032627\n",
      "Epoch [8993/10000] Avg train loss: 0.032624\n",
      "Epoch [8994/10000] Avg train loss: 0.032620\n",
      "Epoch [8995/10000] Avg train loss: 0.032617\n",
      "Epoch [8996/10000] Avg train loss: 0.032613\n",
      "Epoch [8997/10000] Avg train loss: 0.032609\n",
      "Epoch [8998/10000] Avg train loss: 0.032606\n",
      "Epoch [8999/10000] Avg train loss: 0.032602\n",
      "Epoch [9000/10000] Avg train loss: 0.032599\n",
      "Epoch [9001/10000] Avg train loss: 0.032595\n",
      "Epoch [9002/10000] Avg train loss: 0.032591\n",
      "Epoch [9003/10000] Avg train loss: 0.032588\n",
      "Epoch [9004/10000] Avg train loss: 0.032584\n",
      "Epoch [9005/10000] Avg train loss: 0.032580\n",
      "Epoch [9006/10000] Avg train loss: 0.032577\n",
      "Epoch [9007/10000] Avg train loss: 0.032573\n",
      "Epoch [9008/10000] Avg train loss: 0.032570\n",
      "Epoch [9009/10000] Avg train loss: 0.032566\n",
      "Epoch [9010/10000] Avg train loss: 0.032562\n",
      "Epoch [9011/10000] Avg train loss: 0.032559\n",
      "Epoch [9012/10000] Avg train loss: 0.032555\n",
      "Epoch [9013/10000] Avg train loss: 0.032552\n",
      "Epoch [9014/10000] Avg train loss: 0.032548\n",
      "Epoch [9015/10000] Avg train loss: 0.032544\n",
      "Epoch [9016/10000] Avg train loss: 0.032541\n",
      "Epoch [9017/10000] Avg train loss: 0.032537\n",
      "Epoch [9018/10000] Avg train loss: 0.032534\n",
      "Epoch [9019/10000] Avg train loss: 0.032530\n",
      "Epoch [9020/10000] Avg train loss: 0.032526\n",
      "Epoch [9021/10000] Avg train loss: 0.032523\n",
      "Epoch [9022/10000] Avg train loss: 0.032519\n",
      "Epoch [9023/10000] Avg train loss: 0.032516\n",
      "Epoch [9024/10000] Avg train loss: 0.032512\n",
      "Epoch [9025/10000] Avg train loss: 0.032508\n",
      "Epoch [9026/10000] Avg train loss: 0.032505\n",
      "Epoch [9027/10000] Avg train loss: 0.032501\n",
      "Epoch [9028/10000] Avg train loss: 0.032498\n",
      "Epoch [9029/10000] Avg train loss: 0.032494\n",
      "Epoch [9030/10000] Avg train loss: 0.032490\n",
      "Epoch [9031/10000] Avg train loss: 0.032487\n",
      "Epoch [9032/10000] Avg train loss: 0.032483\n",
      "Epoch [9033/10000] Avg train loss: 0.032480\n",
      "Epoch [9034/10000] Avg train loss: 0.032476\n",
      "Epoch [9035/10000] Avg train loss: 0.032473\n",
      "Epoch [9036/10000] Avg train loss: 0.032469\n",
      "Epoch [9037/10000] Avg train loss: 0.032465\n",
      "Epoch [9038/10000] Avg train loss: 0.032462\n",
      "Epoch [9039/10000] Avg train loss: 0.032458\n",
      "Epoch [9040/10000] Avg train loss: 0.032455\n",
      "Epoch [9041/10000] Avg train loss: 0.032451\n",
      "Epoch [9042/10000] Avg train loss: 0.032447\n",
      "Epoch [9043/10000] Avg train loss: 0.032444\n",
      "Epoch [9044/10000] Avg train loss: 0.032440\n",
      "Epoch [9045/10000] Avg train loss: 0.032437\n",
      "Epoch [9046/10000] Avg train loss: 0.032433\n",
      "Epoch [9047/10000] Avg train loss: 0.032430\n",
      "Epoch [9048/10000] Avg train loss: 0.032426\n",
      "Epoch [9049/10000] Avg train loss: 0.032422\n",
      "Epoch [9050/10000] Avg train loss: 0.032419\n",
      "Epoch [9051/10000] Avg train loss: 0.032415\n",
      "Epoch [9052/10000] Avg train loss: 0.032412\n",
      "Epoch [9053/10000] Avg train loss: 0.032408\n",
      "Epoch [9054/10000] Avg train loss: 0.032405\n",
      "Epoch [9055/10000] Avg train loss: 0.032401\n",
      "Epoch [9056/10000] Avg train loss: 0.032397\n",
      "Epoch [9057/10000] Avg train loss: 0.032394\n",
      "Epoch [9058/10000] Avg train loss: 0.032390\n",
      "Epoch [9059/10000] Avg train loss: 0.032387\n",
      "Epoch [9060/10000] Avg train loss: 0.032383\n",
      "Epoch [9061/10000] Avg train loss: 0.032380\n",
      "Epoch [9062/10000] Avg train loss: 0.032376\n",
      "Epoch [9063/10000] Avg train loss: 0.032372\n",
      "Epoch [9064/10000] Avg train loss: 0.032369\n",
      "Epoch [9065/10000] Avg train loss: 0.032365\n",
      "Epoch [9066/10000] Avg train loss: 0.032362\n",
      "Epoch [9067/10000] Avg train loss: 0.032358\n",
      "Epoch [9068/10000] Avg train loss: 0.032355\n",
      "Epoch [9069/10000] Avg train loss: 0.032351\n",
      "Epoch [9070/10000] Avg train loss: 0.032348\n",
      "Epoch [9071/10000] Avg train loss: 0.032344\n",
      "Epoch [9072/10000] Avg train loss: 0.032340\n",
      "Epoch [9073/10000] Avg train loss: 0.032337\n",
      "Epoch [9074/10000] Avg train loss: 0.032333\n",
      "Epoch [9075/10000] Avg train loss: 0.032330\n",
      "Epoch [9076/10000] Avg train loss: 0.032326\n",
      "Epoch [9077/10000] Avg train loss: 0.032323\n",
      "Epoch [9078/10000] Avg train loss: 0.032319\n",
      "Epoch [9079/10000] Avg train loss: 0.032316\n",
      "Epoch [9080/10000] Avg train loss: 0.032312\n",
      "Epoch [9081/10000] Avg train loss: 0.032308\n",
      "Epoch [9082/10000] Avg train loss: 0.032305\n",
      "Epoch [9083/10000] Avg train loss: 0.032301\n",
      "Epoch [9084/10000] Avg train loss: 0.032298\n",
      "Epoch [9085/10000] Avg train loss: 0.032294\n",
      "Epoch [9086/10000] Avg train loss: 0.032291\n",
      "Epoch [9087/10000] Avg train loss: 0.032287\n",
      "Epoch [9088/10000] Avg train loss: 0.032284\n",
      "Epoch [9089/10000] Avg train loss: 0.032280\n",
      "Epoch [9090/10000] Avg train loss: 0.032277\n",
      "Epoch [9091/10000] Avg train loss: 0.032273\n",
      "Epoch [9092/10000] Avg train loss: 0.032269\n",
      "Epoch [9093/10000] Avg train loss: 0.032266\n",
      "Epoch [9094/10000] Avg train loss: 0.032262\n",
      "Epoch [9095/10000] Avg train loss: 0.032259\n",
      "Epoch [9096/10000] Avg train loss: 0.032255\n",
      "Epoch [9097/10000] Avg train loss: 0.032252\n",
      "Epoch [9098/10000] Avg train loss: 0.032248\n",
      "Epoch [9099/10000] Avg train loss: 0.032245\n",
      "Epoch [9100/10000] Avg train loss: 0.032241\n",
      "Epoch [9101/10000] Avg train loss: 0.032238\n",
      "Epoch [9102/10000] Avg train loss: 0.032234\n",
      "Epoch [9103/10000] Avg train loss: 0.032231\n",
      "Epoch [9104/10000] Avg train loss: 0.032227\n",
      "Epoch [9105/10000] Avg train loss: 0.032224\n",
      "Epoch [9106/10000] Avg train loss: 0.032220\n",
      "Epoch [9107/10000] Avg train loss: 0.032216\n",
      "Epoch [9108/10000] Avg train loss: 0.032213\n",
      "Epoch [9109/10000] Avg train loss: 0.032209\n",
      "Epoch [9110/10000] Avg train loss: 0.032206\n",
      "Epoch [9111/10000] Avg train loss: 0.032202\n",
      "Epoch [9112/10000] Avg train loss: 0.032199\n",
      "Epoch [9113/10000] Avg train loss: 0.032195\n",
      "Epoch [9114/10000] Avg train loss: 0.032192\n",
      "Epoch [9115/10000] Avg train loss: 0.032188\n",
      "Epoch [9116/10000] Avg train loss: 0.032185\n",
      "Epoch [9117/10000] Avg train loss: 0.032181\n",
      "Epoch [9118/10000] Avg train loss: 0.032178\n",
      "Epoch [9119/10000] Avg train loss: 0.032174\n",
      "Epoch [9120/10000] Avg train loss: 0.032171\n",
      "Epoch [9121/10000] Avg train loss: 0.032167\n",
      "Epoch [9122/10000] Avg train loss: 0.032164\n",
      "Epoch [9123/10000] Avg train loss: 0.032160\n",
      "Epoch [9124/10000] Avg train loss: 0.032157\n",
      "Epoch [9125/10000] Avg train loss: 0.032153\n",
      "Epoch [9126/10000] Avg train loss: 0.032150\n",
      "Epoch [9127/10000] Avg train loss: 0.032146\n",
      "Epoch [9128/10000] Avg train loss: 0.032143\n",
      "Epoch [9129/10000] Avg train loss: 0.032139\n",
      "Epoch [9130/10000] Avg train loss: 0.032135\n",
      "Epoch [9131/10000] Avg train loss: 0.032132\n",
      "Epoch [9132/10000] Avg train loss: 0.032128\n",
      "Epoch [9133/10000] Avg train loss: 0.032125\n",
      "Epoch [9134/10000] Avg train loss: 0.032121\n",
      "Epoch [9135/10000] Avg train loss: 0.032118\n",
      "Epoch [9136/10000] Avg train loss: 0.032114\n",
      "Epoch [9137/10000] Avg train loss: 0.032111\n",
      "Epoch [9138/10000] Avg train loss: 0.032107\n",
      "Epoch [9139/10000] Avg train loss: 0.032104\n",
      "Epoch [9140/10000] Avg train loss: 0.032100\n",
      "Epoch [9141/10000] Avg train loss: 0.032097\n",
      "Epoch [9142/10000] Avg train loss: 0.032093\n",
      "Epoch [9143/10000] Avg train loss: 0.032090\n",
      "Epoch [9144/10000] Avg train loss: 0.032086\n",
      "Epoch [9145/10000] Avg train loss: 0.032083\n",
      "Epoch [9146/10000] Avg train loss: 0.032079\n",
      "Epoch [9147/10000] Avg train loss: 0.032076\n",
      "Epoch [9148/10000] Avg train loss: 0.032072\n",
      "Epoch [9149/10000] Avg train loss: 0.032069\n",
      "Epoch [9150/10000] Avg train loss: 0.032065\n",
      "Epoch [9151/10000] Avg train loss: 0.032062\n",
      "Epoch [9152/10000] Avg train loss: 0.032058\n",
      "Epoch [9153/10000] Avg train loss: 0.032055\n",
      "Epoch [9154/10000] Avg train loss: 0.032051\n",
      "Epoch [9155/10000] Avg train loss: 0.032048\n",
      "Epoch [9156/10000] Avg train loss: 0.032044\n",
      "Epoch [9157/10000] Avg train loss: 0.032041\n",
      "Epoch [9158/10000] Avg train loss: 0.032037\n",
      "Epoch [9159/10000] Avg train loss: 0.032034\n",
      "Epoch [9160/10000] Avg train loss: 0.032030\n",
      "Epoch [9161/10000] Avg train loss: 0.032027\n",
      "Epoch [9162/10000] Avg train loss: 0.032024\n",
      "Epoch [9163/10000] Avg train loss: 0.032020\n",
      "Epoch [9164/10000] Avg train loss: 0.032017\n",
      "Epoch [9165/10000] Avg train loss: 0.032013\n",
      "Epoch [9166/10000] Avg train loss: 0.032010\n",
      "Epoch [9167/10000] Avg train loss: 0.032006\n",
      "Epoch [9168/10000] Avg train loss: 0.032003\n",
      "Epoch [9169/10000] Avg train loss: 0.031999\n",
      "Epoch [9170/10000] Avg train loss: 0.031996\n",
      "Epoch [9171/10000] Avg train loss: 0.031992\n",
      "Epoch [9172/10000] Avg train loss: 0.031989\n",
      "Epoch [9173/10000] Avg train loss: 0.031985\n",
      "Epoch [9174/10000] Avg train loss: 0.031982\n",
      "Epoch [9175/10000] Avg train loss: 0.031978\n",
      "Epoch [9176/10000] Avg train loss: 0.031975\n",
      "Epoch [9177/10000] Avg train loss: 0.031971\n",
      "Epoch [9178/10000] Avg train loss: 0.031968\n",
      "Epoch [9179/10000] Avg train loss: 0.031964\n",
      "Epoch [9180/10000] Avg train loss: 0.031961\n",
      "Epoch [9181/10000] Avg train loss: 0.031957\n",
      "Epoch [9182/10000] Avg train loss: 0.031954\n",
      "Epoch [9183/10000] Avg train loss: 0.031950\n",
      "Epoch [9184/10000] Avg train loss: 0.031947\n",
      "Epoch [9185/10000] Avg train loss: 0.031944\n",
      "Epoch [9186/10000] Avg train loss: 0.031940\n",
      "Epoch [9187/10000] Avg train loss: 0.031937\n",
      "Epoch [9188/10000] Avg train loss: 0.031933\n",
      "Epoch [9189/10000] Avg train loss: 0.031930\n",
      "Epoch [9190/10000] Avg train loss: 0.031926\n",
      "Epoch [9191/10000] Avg train loss: 0.031923\n",
      "Epoch [9192/10000] Avg train loss: 0.031919\n",
      "Epoch [9193/10000] Avg train loss: 0.031916\n",
      "Epoch [9194/10000] Avg train loss: 0.031912\n",
      "Epoch [9195/10000] Avg train loss: 0.031909\n",
      "Epoch [9196/10000] Avg train loss: 0.031905\n",
      "Epoch [9197/10000] Avg train loss: 0.031902\n",
      "Epoch [9198/10000] Avg train loss: 0.031898\n",
      "Epoch [9199/10000] Avg train loss: 0.031895\n",
      "Epoch [9200/10000] Avg train loss: 0.031892\n",
      "Epoch [9201/10000] Avg train loss: 0.031888\n",
      "Epoch [9202/10000] Avg train loss: 0.031885\n",
      "Epoch [9203/10000] Avg train loss: 0.031881\n",
      "Epoch [9204/10000] Avg train loss: 0.031878\n",
      "Epoch [9205/10000] Avg train loss: 0.031874\n",
      "Epoch [9206/10000] Avg train loss: 0.031871\n",
      "Epoch [9207/10000] Avg train loss: 0.031867\n",
      "Epoch [9208/10000] Avg train loss: 0.031864\n",
      "Epoch [9209/10000] Avg train loss: 0.031860\n",
      "Epoch [9210/10000] Avg train loss: 0.031857\n",
      "Epoch [9211/10000] Avg train loss: 0.031854\n",
      "Epoch [9212/10000] Avg train loss: 0.031850\n",
      "Epoch [9213/10000] Avg train loss: 0.031847\n",
      "Epoch [9214/10000] Avg train loss: 0.031843\n",
      "Epoch [9215/10000] Avg train loss: 0.031840\n",
      "Epoch [9216/10000] Avg train loss: 0.031836\n",
      "Epoch [9217/10000] Avg train loss: 0.031833\n",
      "Epoch [9218/10000] Avg train loss: 0.031829\n",
      "Epoch [9219/10000] Avg train loss: 0.031826\n",
      "Epoch [9220/10000] Avg train loss: 0.031823\n",
      "Epoch [9221/10000] Avg train loss: 0.031819\n",
      "Epoch [9222/10000] Avg train loss: 0.031816\n",
      "Epoch [9223/10000] Avg train loss: 0.031812\n",
      "Epoch [9224/10000] Avg train loss: 0.031809\n",
      "Epoch [9225/10000] Avg train loss: 0.031805\n",
      "Epoch [9226/10000] Avg train loss: 0.031802\n",
      "Epoch [9227/10000] Avg train loss: 0.031798\n",
      "Epoch [9228/10000] Avg train loss: 0.031795\n",
      "Epoch [9229/10000] Avg train loss: 0.031792\n",
      "Epoch [9230/10000] Avg train loss: 0.031788\n",
      "Epoch [9231/10000] Avg train loss: 0.031785\n",
      "Epoch [9232/10000] Avg train loss: 0.031781\n",
      "Epoch [9233/10000] Avg train loss: 0.031778\n",
      "Epoch [9234/10000] Avg train loss: 0.031774\n",
      "Epoch [9235/10000] Avg train loss: 0.031771\n",
      "Epoch [9236/10000] Avg train loss: 0.031768\n",
      "Epoch [9237/10000] Avg train loss: 0.031764\n",
      "Epoch [9238/10000] Avg train loss: 0.031761\n",
      "Epoch [9239/10000] Avg train loss: 0.031757\n",
      "Epoch [9240/10000] Avg train loss: 0.031754\n",
      "Epoch [9241/10000] Avg train loss: 0.031750\n",
      "Epoch [9242/10000] Avg train loss: 0.031747\n",
      "Epoch [9243/10000] Avg train loss: 0.031744\n",
      "Epoch [9244/10000] Avg train loss: 0.031740\n",
      "Epoch [9245/10000] Avg train loss: 0.031737\n",
      "Epoch [9246/10000] Avg train loss: 0.031733\n",
      "Epoch [9247/10000] Avg train loss: 0.031730\n",
      "Epoch [9248/10000] Avg train loss: 0.031726\n",
      "Epoch [9249/10000] Avg train loss: 0.031723\n",
      "Epoch [9250/10000] Avg train loss: 0.031720\n",
      "Epoch [9251/10000] Avg train loss: 0.031716\n",
      "Epoch [9252/10000] Avg train loss: 0.031713\n",
      "Epoch [9253/10000] Avg train loss: 0.031709\n",
      "Epoch [9254/10000] Avg train loss: 0.031706\n",
      "Epoch [9255/10000] Avg train loss: 0.031702\n",
      "Epoch [9256/10000] Avg train loss: 0.031699\n",
      "Epoch [9257/10000] Avg train loss: 0.031696\n",
      "Epoch [9258/10000] Avg train loss: 0.031692\n",
      "Epoch [9259/10000] Avg train loss: 0.031689\n",
      "Epoch [9260/10000] Avg train loss: 0.031685\n",
      "Epoch [9261/10000] Avg train loss: 0.031682\n",
      "Epoch [9262/10000] Avg train loss: 0.031679\n",
      "Epoch [9263/10000] Avg train loss: 0.031675\n",
      "Epoch [9264/10000] Avg train loss: 0.031672\n",
      "Epoch [9265/10000] Avg train loss: 0.031668\n",
      "Epoch [9266/10000] Avg train loss: 0.031665\n",
      "Epoch [9267/10000] Avg train loss: 0.031662\n",
      "Epoch [9268/10000] Avg train loss: 0.031658\n",
      "Epoch [9269/10000] Avg train loss: 0.031655\n",
      "Epoch [9270/10000] Avg train loss: 0.031651\n",
      "Epoch [9271/10000] Avg train loss: 0.031648\n",
      "Epoch [9272/10000] Avg train loss: 0.031644\n",
      "Epoch [9273/10000] Avg train loss: 0.031641\n",
      "Epoch [9274/10000] Avg train loss: 0.031638\n",
      "Epoch [9275/10000] Avg train loss: 0.031634\n",
      "Epoch [9276/10000] Avg train loss: 0.031631\n",
      "Epoch [9277/10000] Avg train loss: 0.031627\n",
      "Epoch [9278/10000] Avg train loss: 0.031624\n",
      "Epoch [9279/10000] Avg train loss: 0.031621\n",
      "Epoch [9280/10000] Avg train loss: 0.031617\n",
      "Epoch [9281/10000] Avg train loss: 0.031614\n",
      "Epoch [9282/10000] Avg train loss: 0.031610\n",
      "Epoch [9283/10000] Avg train loss: 0.031607\n",
      "Epoch [9284/10000] Avg train loss: 0.031604\n",
      "Epoch [9285/10000] Avg train loss: 0.031600\n",
      "Epoch [9286/10000] Avg train loss: 0.031597\n",
      "Epoch [9287/10000] Avg train loss: 0.031594\n",
      "Epoch [9288/10000] Avg train loss: 0.031590\n",
      "Epoch [9289/10000] Avg train loss: 0.031587\n",
      "Epoch [9290/10000] Avg train loss: 0.031583\n",
      "Epoch [9291/10000] Avg train loss: 0.031580\n",
      "Epoch [9292/10000] Avg train loss: 0.031577\n",
      "Epoch [9293/10000] Avg train loss: 0.031573\n",
      "Epoch [9294/10000] Avg train loss: 0.031570\n",
      "Epoch [9295/10000] Avg train loss: 0.031566\n",
      "Epoch [9296/10000] Avg train loss: 0.031563\n",
      "Epoch [9297/10000] Avg train loss: 0.031560\n",
      "Epoch [9298/10000] Avg train loss: 0.031556\n",
      "Epoch [9299/10000] Avg train loss: 0.031553\n",
      "Epoch [9300/10000] Avg train loss: 0.031549\n",
      "Epoch [9301/10000] Avg train loss: 0.031546\n",
      "Epoch [9302/10000] Avg train loss: 0.031543\n",
      "Epoch [9303/10000] Avg train loss: 0.031539\n",
      "Epoch [9304/10000] Avg train loss: 0.031536\n",
      "Epoch [9305/10000] Avg train loss: 0.031533\n",
      "Epoch [9306/10000] Avg train loss: 0.031529\n",
      "Epoch [9307/10000] Avg train loss: 0.031526\n",
      "Epoch [9308/10000] Avg train loss: 0.031522\n",
      "Epoch [9309/10000] Avg train loss: 0.031519\n",
      "Epoch [9310/10000] Avg train loss: 0.031516\n",
      "Epoch [9311/10000] Avg train loss: 0.031512\n",
      "Epoch [9312/10000] Avg train loss: 0.031509\n",
      "Epoch [9313/10000] Avg train loss: 0.031506\n",
      "Epoch [9314/10000] Avg train loss: 0.031502\n",
      "Epoch [9315/10000] Avg train loss: 0.031499\n",
      "Epoch [9316/10000] Avg train loss: 0.031495\n",
      "Epoch [9317/10000] Avg train loss: 0.031492\n",
      "Epoch [9318/10000] Avg train loss: 0.031489\n",
      "Epoch [9319/10000] Avg train loss: 0.031485\n",
      "Epoch [9320/10000] Avg train loss: 0.031482\n",
      "Epoch [9321/10000] Avg train loss: 0.031479\n",
      "Epoch [9322/10000] Avg train loss: 0.031475\n",
      "Epoch [9323/10000] Avg train loss: 0.031472\n",
      "Epoch [9324/10000] Avg train loss: 0.031468\n",
      "Epoch [9325/10000] Avg train loss: 0.031465\n",
      "Epoch [9326/10000] Avg train loss: 0.031462\n",
      "Epoch [9327/10000] Avg train loss: 0.031458\n",
      "Epoch [9328/10000] Avg train loss: 0.031455\n",
      "Epoch [9329/10000] Avg train loss: 0.031452\n",
      "Epoch [9330/10000] Avg train loss: 0.031448\n",
      "Epoch [9331/10000] Avg train loss: 0.031445\n",
      "Epoch [9332/10000] Avg train loss: 0.031442\n",
      "Epoch [9333/10000] Avg train loss: 0.031438\n",
      "Epoch [9334/10000] Avg train loss: 0.031435\n",
      "Epoch [9335/10000] Avg train loss: 0.031431\n",
      "Epoch [9336/10000] Avg train loss: 0.031428\n",
      "Epoch [9337/10000] Avg train loss: 0.031425\n",
      "Epoch [9338/10000] Avg train loss: 0.031421\n",
      "Epoch [9339/10000] Avg train loss: 0.031418\n",
      "Epoch [9340/10000] Avg train loss: 0.031415\n",
      "Epoch [9341/10000] Avg train loss: 0.031411\n",
      "Epoch [9342/10000] Avg train loss: 0.031408\n",
      "Epoch [9343/10000] Avg train loss: 0.031405\n",
      "Epoch [9344/10000] Avg train loss: 0.031401\n",
      "Epoch [9345/10000] Avg train loss: 0.031398\n",
      "Epoch [9346/10000] Avg train loss: 0.031395\n",
      "Epoch [9347/10000] Avg train loss: 0.031391\n",
      "Epoch [9348/10000] Avg train loss: 0.031388\n",
      "Epoch [9349/10000] Avg train loss: 0.031384\n",
      "Epoch [9350/10000] Avg train loss: 0.031381\n",
      "Epoch [9351/10000] Avg train loss: 0.031378\n",
      "Epoch [9352/10000] Avg train loss: 0.031374\n",
      "Epoch [9353/10000] Avg train loss: 0.031371\n",
      "Epoch [9354/10000] Avg train loss: 0.031368\n",
      "Epoch [9355/10000] Avg train loss: 0.031364\n",
      "Epoch [9356/10000] Avg train loss: 0.031361\n",
      "Epoch [9357/10000] Avg train loss: 0.031358\n",
      "Epoch [9358/10000] Avg train loss: 0.031354\n",
      "Epoch [9359/10000] Avg train loss: 0.031351\n",
      "Epoch [9360/10000] Avg train loss: 0.031348\n",
      "Epoch [9361/10000] Avg train loss: 0.031344\n",
      "Epoch [9362/10000] Avg train loss: 0.031341\n",
      "Epoch [9363/10000] Avg train loss: 0.031338\n",
      "Epoch [9364/10000] Avg train loss: 0.031334\n",
      "Epoch [9365/10000] Avg train loss: 0.031331\n",
      "Epoch [9366/10000] Avg train loss: 0.031328\n",
      "Epoch [9367/10000] Avg train loss: 0.031324\n",
      "Epoch [9368/10000] Avg train loss: 0.031321\n",
      "Epoch [9369/10000] Avg train loss: 0.031318\n",
      "Epoch [9370/10000] Avg train loss: 0.031314\n",
      "Epoch [9371/10000] Avg train loss: 0.031311\n",
      "Epoch [9372/10000] Avg train loss: 0.031308\n",
      "Epoch [9373/10000] Avg train loss: 0.031304\n",
      "Epoch [9374/10000] Avg train loss: 0.031301\n",
      "Epoch [9375/10000] Avg train loss: 0.031298\n",
      "Epoch [9376/10000] Avg train loss: 0.031294\n",
      "Epoch [9377/10000] Avg train loss: 0.031291\n",
      "Epoch [9378/10000] Avg train loss: 0.031288\n",
      "Epoch [9379/10000] Avg train loss: 0.031284\n",
      "Epoch [9380/10000] Avg train loss: 0.031281\n",
      "Epoch [9381/10000] Avg train loss: 0.031278\n",
      "Epoch [9382/10000] Avg train loss: 0.031274\n",
      "Epoch [9383/10000] Avg train loss: 0.031271\n",
      "Epoch [9384/10000] Avg train loss: 0.031268\n",
      "Epoch [9385/10000] Avg train loss: 0.031264\n",
      "Epoch [9386/10000] Avg train loss: 0.031261\n",
      "Epoch [9387/10000] Avg train loss: 0.031258\n",
      "Epoch [9388/10000] Avg train loss: 0.031254\n",
      "Epoch [9389/10000] Avg train loss: 0.031251\n",
      "Epoch [9390/10000] Avg train loss: 0.031248\n",
      "Epoch [9391/10000] Avg train loss: 0.031244\n",
      "Epoch [9392/10000] Avg train loss: 0.031241\n",
      "Epoch [9393/10000] Avg train loss: 0.031238\n",
      "Epoch [9394/10000] Avg train loss: 0.031235\n",
      "Epoch [9395/10000] Avg train loss: 0.031231\n",
      "Epoch [9396/10000] Avg train loss: 0.031228\n",
      "Epoch [9397/10000] Avg train loss: 0.031225\n",
      "Epoch [9398/10000] Avg train loss: 0.031221\n",
      "Epoch [9399/10000] Avg train loss: 0.031218\n",
      "Epoch [9400/10000] Avg train loss: 0.031215\n",
      "Epoch [9401/10000] Avg train loss: 0.031211\n",
      "Epoch [9402/10000] Avg train loss: 0.031208\n",
      "Epoch [9403/10000] Avg train loss: 0.031205\n",
      "Epoch [9404/10000] Avg train loss: 0.031201\n",
      "Epoch [9405/10000] Avg train loss: 0.031198\n",
      "Epoch [9406/10000] Avg train loss: 0.031195\n",
      "Epoch [9407/10000] Avg train loss: 0.031191\n",
      "Epoch [9408/10000] Avg train loss: 0.031188\n",
      "Epoch [9409/10000] Avg train loss: 0.031185\n",
      "Epoch [9410/10000] Avg train loss: 0.031182\n",
      "Epoch [9411/10000] Avg train loss: 0.031178\n",
      "Epoch [9412/10000] Avg train loss: 0.031175\n",
      "Epoch [9413/10000] Avg train loss: 0.031172\n",
      "Epoch [9414/10000] Avg train loss: 0.031168\n",
      "Epoch [9415/10000] Avg train loss: 0.031165\n",
      "Epoch [9416/10000] Avg train loss: 0.031162\n",
      "Epoch [9417/10000] Avg train loss: 0.031158\n",
      "Epoch [9418/10000] Avg train loss: 0.031155\n",
      "Epoch [9419/10000] Avg train loss: 0.031152\n",
      "Epoch [9420/10000] Avg train loss: 0.031149\n",
      "Epoch [9421/10000] Avg train loss: 0.031145\n",
      "Epoch [9422/10000] Avg train loss: 0.031142\n",
      "Epoch [9423/10000] Avg train loss: 0.031139\n",
      "Epoch [9424/10000] Avg train loss: 0.031135\n",
      "Epoch [9425/10000] Avg train loss: 0.031132\n",
      "Epoch [9426/10000] Avg train loss: 0.031129\n",
      "Epoch [9427/10000] Avg train loss: 0.031125\n",
      "Epoch [9428/10000] Avg train loss: 0.031122\n",
      "Epoch [9429/10000] Avg train loss: 0.031119\n",
      "Epoch [9430/10000] Avg train loss: 0.031116\n",
      "Epoch [9431/10000] Avg train loss: 0.031112\n",
      "Epoch [9432/10000] Avg train loss: 0.031109\n",
      "Epoch [9433/10000] Avg train loss: 0.031106\n",
      "Epoch [9434/10000] Avg train loss: 0.031102\n",
      "Epoch [9435/10000] Avg train loss: 0.031099\n",
      "Epoch [9436/10000] Avg train loss: 0.031096\n",
      "Epoch [9437/10000] Avg train loss: 0.031093\n",
      "Epoch [9438/10000] Avg train loss: 0.031089\n",
      "Epoch [9439/10000] Avg train loss: 0.031086\n",
      "Epoch [9440/10000] Avg train loss: 0.031083\n",
      "Epoch [9441/10000] Avg train loss: 0.031079\n",
      "Epoch [9442/10000] Avg train loss: 0.031076\n",
      "Epoch [9443/10000] Avg train loss: 0.031073\n",
      "Epoch [9444/10000] Avg train loss: 0.031070\n",
      "Epoch [9445/10000] Avg train loss: 0.031066\n",
      "Epoch [9446/10000] Avg train loss: 0.031063\n",
      "Epoch [9447/10000] Avg train loss: 0.031060\n",
      "Epoch [9448/10000] Avg train loss: 0.031056\n",
      "Epoch [9449/10000] Avg train loss: 0.031053\n",
      "Epoch [9450/10000] Avg train loss: 0.031050\n",
      "Epoch [9451/10000] Avg train loss: 0.031047\n",
      "Epoch [9452/10000] Avg train loss: 0.031043\n",
      "Epoch [9453/10000] Avg train loss: 0.031040\n",
      "Epoch [9454/10000] Avg train loss: 0.031037\n",
      "Epoch [9455/10000] Avg train loss: 0.031033\n",
      "Epoch [9456/10000] Avg train loss: 0.031030\n",
      "Epoch [9457/10000] Avg train loss: 0.031027\n",
      "Epoch [9458/10000] Avg train loss: 0.031024\n",
      "Epoch [9459/10000] Avg train loss: 0.031020\n",
      "Epoch [9460/10000] Avg train loss: 0.031017\n",
      "Epoch [9461/10000] Avg train loss: 0.031014\n",
      "Epoch [9462/10000] Avg train loss: 0.031011\n",
      "Epoch [9463/10000] Avg train loss: 0.031007\n",
      "Epoch [9464/10000] Avg train loss: 0.031004\n",
      "Epoch [9465/10000] Avg train loss: 0.031001\n",
      "Epoch [9466/10000] Avg train loss: 0.030998\n",
      "Epoch [9467/10000] Avg train loss: 0.030994\n",
      "Epoch [9468/10000] Avg train loss: 0.030991\n",
      "Epoch [9469/10000] Avg train loss: 0.030988\n",
      "Epoch [9470/10000] Avg train loss: 0.030984\n",
      "Epoch [9471/10000] Avg train loss: 0.030981\n",
      "Epoch [9472/10000] Avg train loss: 0.030978\n",
      "Epoch [9473/10000] Avg train loss: 0.030975\n",
      "Epoch [9474/10000] Avg train loss: 0.030971\n",
      "Epoch [9475/10000] Avg train loss: 0.030968\n",
      "Epoch [9476/10000] Avg train loss: 0.030965\n",
      "Epoch [9477/10000] Avg train loss: 0.030962\n",
      "Epoch [9478/10000] Avg train loss: 0.030958\n",
      "Epoch [9479/10000] Avg train loss: 0.030955\n",
      "Epoch [9480/10000] Avg train loss: 0.030952\n",
      "Epoch [9481/10000] Avg train loss: 0.030949\n",
      "Epoch [9482/10000] Avg train loss: 0.030945\n",
      "Epoch [9483/10000] Avg train loss: 0.030942\n",
      "Epoch [9484/10000] Avg train loss: 0.030939\n",
      "Epoch [9485/10000] Avg train loss: 0.030936\n",
      "Epoch [9486/10000] Avg train loss: 0.030932\n",
      "Epoch [9487/10000] Avg train loss: 0.030929\n",
      "Epoch [9488/10000] Avg train loss: 0.030926\n",
      "Epoch [9489/10000] Avg train loss: 0.030923\n",
      "Epoch [9490/10000] Avg train loss: 0.030919\n",
      "Epoch [9491/10000] Avg train loss: 0.030916\n",
      "Epoch [9492/10000] Avg train loss: 0.030913\n",
      "Epoch [9493/10000] Avg train loss: 0.030910\n",
      "Epoch [9494/10000] Avg train loss: 0.030906\n",
      "Epoch [9495/10000] Avg train loss: 0.030903\n",
      "Epoch [9496/10000] Avg train loss: 0.030900\n",
      "Epoch [9497/10000] Avg train loss: 0.030897\n",
      "Epoch [9498/10000] Avg train loss: 0.030893\n",
      "Epoch [9499/10000] Avg train loss: 0.030890\n",
      "Epoch [9500/10000] Avg train loss: 0.030887\n",
      "Epoch [9501/10000] Avg train loss: 0.030884\n",
      "Epoch [9502/10000] Avg train loss: 0.030880\n",
      "Epoch [9503/10000] Avg train loss: 0.030877\n",
      "Epoch [9504/10000] Avg train loss: 0.030874\n",
      "Epoch [9505/10000] Avg train loss: 0.030871\n",
      "Epoch [9506/10000] Avg train loss: 0.030867\n",
      "Epoch [9507/10000] Avg train loss: 0.030864\n",
      "Epoch [9508/10000] Avg train loss: 0.030861\n",
      "Epoch [9509/10000] Avg train loss: 0.030858\n",
      "Epoch [9510/10000] Avg train loss: 0.030854\n",
      "Epoch [9511/10000] Avg train loss: 0.030851\n",
      "Epoch [9512/10000] Avg train loss: 0.030848\n",
      "Epoch [9513/10000] Avg train loss: 0.030845\n",
      "Epoch [9514/10000] Avg train loss: 0.030841\n",
      "Epoch [9515/10000] Avg train loss: 0.030838\n",
      "Epoch [9516/10000] Avg train loss: 0.030835\n",
      "Epoch [9517/10000] Avg train loss: 0.030832\n",
      "Epoch [9518/10000] Avg train loss: 0.030829\n",
      "Epoch [9519/10000] Avg train loss: 0.030825\n",
      "Epoch [9520/10000] Avg train loss: 0.030822\n",
      "Epoch [9521/10000] Avg train loss: 0.030819\n",
      "Epoch [9522/10000] Avg train loss: 0.030816\n",
      "Epoch [9523/10000] Avg train loss: 0.030812\n",
      "Epoch [9524/10000] Avg train loss: 0.030809\n",
      "Epoch [9525/10000] Avg train loss: 0.030806\n",
      "Epoch [9526/10000] Avg train loss: 0.030803\n",
      "Epoch [9527/10000] Avg train loss: 0.030800\n",
      "Epoch [9528/10000] Avg train loss: 0.030796\n",
      "Epoch [9529/10000] Avg train loss: 0.030793\n",
      "Epoch [9530/10000] Avg train loss: 0.030790\n",
      "Epoch [9531/10000] Avg train loss: 0.030787\n",
      "Epoch [9532/10000] Avg train loss: 0.030783\n",
      "Epoch [9533/10000] Avg train loss: 0.030780\n",
      "Epoch [9534/10000] Avg train loss: 0.030777\n",
      "Epoch [9535/10000] Avg train loss: 0.030774\n",
      "Epoch [9536/10000] Avg train loss: 0.030771\n",
      "Epoch [9537/10000] Avg train loss: 0.030767\n",
      "Epoch [9538/10000] Avg train loss: 0.030764\n",
      "Epoch [9539/10000] Avg train loss: 0.030761\n",
      "Epoch [9540/10000] Avg train loss: 0.030758\n",
      "Epoch [9541/10000] Avg train loss: 0.030754\n",
      "Epoch [9542/10000] Avg train loss: 0.030751\n",
      "Epoch [9543/10000] Avg train loss: 0.030748\n",
      "Epoch [9544/10000] Avg train loss: 0.030745\n",
      "Epoch [9545/10000] Avg train loss: 0.030742\n",
      "Epoch [9546/10000] Avg train loss: 0.030738\n",
      "Epoch [9547/10000] Avg train loss: 0.030735\n",
      "Epoch [9548/10000] Avg train loss: 0.030732\n",
      "Epoch [9549/10000] Avg train loss: 0.030729\n",
      "Epoch [9550/10000] Avg train loss: 0.030725\n",
      "Epoch [9551/10000] Avg train loss: 0.030722\n",
      "Epoch [9552/10000] Avg train loss: 0.030719\n",
      "Epoch [9553/10000] Avg train loss: 0.030716\n",
      "Epoch [9554/10000] Avg train loss: 0.030713\n",
      "Epoch [9555/10000] Avg train loss: 0.030709\n",
      "Epoch [9556/10000] Avg train loss: 0.030706\n",
      "Epoch [9557/10000] Avg train loss: 0.030703\n",
      "Epoch [9558/10000] Avg train loss: 0.030700\n",
      "Epoch [9559/10000] Avg train loss: 0.030697\n",
      "Epoch [9560/10000] Avg train loss: 0.030693\n",
      "Epoch [9561/10000] Avg train loss: 0.030690\n",
      "Epoch [9562/10000] Avg train loss: 0.030687\n",
      "Epoch [9563/10000] Avg train loss: 0.030684\n",
      "Epoch [9564/10000] Avg train loss: 0.030681\n",
      "Epoch [9565/10000] Avg train loss: 0.030677\n",
      "Epoch [9566/10000] Avg train loss: 0.030674\n",
      "Epoch [9567/10000] Avg train loss: 0.030671\n",
      "Epoch [9568/10000] Avg train loss: 0.030668\n",
      "Epoch [9569/10000] Avg train loss: 0.030665\n",
      "Epoch [9570/10000] Avg train loss: 0.030661\n",
      "Epoch [9571/10000] Avg train loss: 0.030658\n",
      "Epoch [9572/10000] Avg train loss: 0.030655\n",
      "Epoch [9573/10000] Avg train loss: 0.030652\n",
      "Epoch [9574/10000] Avg train loss: 0.030649\n",
      "Epoch [9575/10000] Avg train loss: 0.030645\n",
      "Epoch [9576/10000] Avg train loss: 0.030642\n",
      "Epoch [9577/10000] Avg train loss: 0.030639\n",
      "Epoch [9578/10000] Avg train loss: 0.030636\n",
      "Epoch [9579/10000] Avg train loss: 0.030633\n",
      "Epoch [9580/10000] Avg train loss: 0.030629\n",
      "Epoch [9581/10000] Avg train loss: 0.030626\n",
      "Epoch [9582/10000] Avg train loss: 0.030623\n",
      "Epoch [9583/10000] Avg train loss: 0.030620\n",
      "Epoch [9584/10000] Avg train loss: 0.030617\n",
      "Epoch [9585/10000] Avg train loss: 0.030614\n",
      "Epoch [9586/10000] Avg train loss: 0.030610\n",
      "Epoch [9587/10000] Avg train loss: 0.030607\n",
      "Epoch [9588/10000] Avg train loss: 0.030604\n",
      "Epoch [9589/10000] Avg train loss: 0.030601\n",
      "Epoch [9590/10000] Avg train loss: 0.030598\n",
      "Epoch [9591/10000] Avg train loss: 0.030594\n",
      "Epoch [9592/10000] Avg train loss: 0.030591\n",
      "Epoch [9593/10000] Avg train loss: 0.030588\n",
      "Epoch [9594/10000] Avg train loss: 0.030585\n",
      "Epoch [9595/10000] Avg train loss: 0.030582\n",
      "Epoch [9596/10000] Avg train loss: 0.030579\n",
      "Epoch [9597/10000] Avg train loss: 0.030575\n",
      "Epoch [9598/10000] Avg train loss: 0.030572\n",
      "Epoch [9599/10000] Avg train loss: 0.030569\n",
      "Epoch [9600/10000] Avg train loss: 0.030566\n",
      "Epoch [9601/10000] Avg train loss: 0.030563\n",
      "Epoch [9602/10000] Avg train loss: 0.030559\n",
      "Epoch [9603/10000] Avg train loss: 0.030556\n",
      "Epoch [9604/10000] Avg train loss: 0.030553\n",
      "Epoch [9605/10000] Avg train loss: 0.030550\n",
      "Epoch [9606/10000] Avg train loss: 0.030547\n",
      "Epoch [9607/10000] Avg train loss: 0.030544\n",
      "Epoch [9608/10000] Avg train loss: 0.030540\n",
      "Epoch [9609/10000] Avg train loss: 0.030537\n",
      "Epoch [9610/10000] Avg train loss: 0.030534\n",
      "Epoch [9611/10000] Avg train loss: 0.030531\n",
      "Epoch [9612/10000] Avg train loss: 0.030528\n",
      "Epoch [9613/10000] Avg train loss: 0.030525\n",
      "Epoch [9614/10000] Avg train loss: 0.030521\n",
      "Epoch [9615/10000] Avg train loss: 0.030518\n",
      "Epoch [9616/10000] Avg train loss: 0.030515\n",
      "Epoch [9617/10000] Avg train loss: 0.030512\n",
      "Epoch [9618/10000] Avg train loss: 0.030509\n",
      "Epoch [9619/10000] Avg train loss: 0.030506\n",
      "Epoch [9620/10000] Avg train loss: 0.030502\n",
      "Epoch [9621/10000] Avg train loss: 0.030499\n",
      "Epoch [9622/10000] Avg train loss: 0.030496\n",
      "Epoch [9623/10000] Avg train loss: 0.030493\n",
      "Epoch [9624/10000] Avg train loss: 0.030490\n",
      "Epoch [9625/10000] Avg train loss: 0.030487\n",
      "Epoch [9626/10000] Avg train loss: 0.030483\n",
      "Epoch [9627/10000] Avg train loss: 0.030480\n",
      "Epoch [9628/10000] Avg train loss: 0.030477\n",
      "Epoch [9629/10000] Avg train loss: 0.030474\n",
      "Epoch [9630/10000] Avg train loss: 0.030471\n",
      "Epoch [9631/10000] Avg train loss: 0.030468\n",
      "Epoch [9632/10000] Avg train loss: 0.030465\n",
      "Epoch [9633/10000] Avg train loss: 0.030461\n",
      "Epoch [9634/10000] Avg train loss: 0.030458\n",
      "Epoch [9635/10000] Avg train loss: 0.030455\n",
      "Epoch [9636/10000] Avg train loss: 0.030452\n",
      "Epoch [9637/10000] Avg train loss: 0.030449\n",
      "Epoch [9638/10000] Avg train loss: 0.030446\n",
      "Epoch [9639/10000] Avg train loss: 0.030442\n",
      "Epoch [9640/10000] Avg train loss: 0.030439\n",
      "Epoch [9641/10000] Avg train loss: 0.030436\n",
      "Epoch [9642/10000] Avg train loss: 0.030433\n",
      "Epoch [9643/10000] Avg train loss: 0.030430\n",
      "Epoch [9644/10000] Avg train loss: 0.030427\n",
      "Epoch [9645/10000] Avg train loss: 0.030424\n",
      "Epoch [9646/10000] Avg train loss: 0.030420\n",
      "Epoch [9647/10000] Avg train loss: 0.030417\n",
      "Epoch [9648/10000] Avg train loss: 0.030414\n",
      "Epoch [9649/10000] Avg train loss: 0.030411\n",
      "Epoch [9650/10000] Avg train loss: 0.030408\n",
      "Epoch [9651/10000] Avg train loss: 0.030405\n",
      "Epoch [9652/10000] Avg train loss: 0.030402\n",
      "Epoch [9653/10000] Avg train loss: 0.030398\n",
      "Epoch [9654/10000] Avg train loss: 0.030395\n",
      "Epoch [9655/10000] Avg train loss: 0.030392\n",
      "Epoch [9656/10000] Avg train loss: 0.030389\n",
      "Epoch [9657/10000] Avg train loss: 0.030386\n",
      "Epoch [9658/10000] Avg train loss: 0.030383\n",
      "Epoch [9659/10000] Avg train loss: 0.030380\n",
      "Epoch [9660/10000] Avg train loss: 0.030376\n",
      "Epoch [9661/10000] Avg train loss: 0.030373\n",
      "Epoch [9662/10000] Avg train loss: 0.030370\n",
      "Epoch [9663/10000] Avg train loss: 0.030367\n",
      "Epoch [9664/10000] Avg train loss: 0.030364\n",
      "Epoch [9665/10000] Avg train loss: 0.030361\n",
      "Epoch [9666/10000] Avg train loss: 0.030358\n",
      "Epoch [9667/10000] Avg train loss: 0.030354\n",
      "Epoch [9668/10000] Avg train loss: 0.030351\n",
      "Epoch [9669/10000] Avg train loss: 0.030348\n",
      "Epoch [9670/10000] Avg train loss: 0.030345\n",
      "Epoch [9671/10000] Avg train loss: 0.030342\n",
      "Epoch [9672/10000] Avg train loss: 0.030339\n",
      "Epoch [9673/10000] Avg train loss: 0.030336\n",
      "Epoch [9674/10000] Avg train loss: 0.030333\n",
      "Epoch [9675/10000] Avg train loss: 0.030329\n",
      "Epoch [9676/10000] Avg train loss: 0.030326\n",
      "Epoch [9677/10000] Avg train loss: 0.030323\n",
      "Epoch [9678/10000] Avg train loss: 0.030320\n",
      "Epoch [9679/10000] Avg train loss: 0.030317\n",
      "Epoch [9680/10000] Avg train loss: 0.030314\n",
      "Epoch [9681/10000] Avg train loss: 0.030311\n",
      "Epoch [9682/10000] Avg train loss: 0.030308\n",
      "Epoch [9683/10000] Avg train loss: 0.030304\n",
      "Epoch [9684/10000] Avg train loss: 0.030301\n",
      "Epoch [9685/10000] Avg train loss: 0.030298\n",
      "Epoch [9686/10000] Avg train loss: 0.030295\n",
      "Epoch [9687/10000] Avg train loss: 0.030292\n",
      "Epoch [9688/10000] Avg train loss: 0.030289\n",
      "Epoch [9689/10000] Avg train loss: 0.030286\n",
      "Epoch [9690/10000] Avg train loss: 0.030283\n",
      "Epoch [9691/10000] Avg train loss: 0.030279\n",
      "Epoch [9692/10000] Avg train loss: 0.030276\n",
      "Epoch [9693/10000] Avg train loss: 0.030273\n",
      "Epoch [9694/10000] Avg train loss: 0.030270\n",
      "Epoch [9695/10000] Avg train loss: 0.030267\n",
      "Epoch [9696/10000] Avg train loss: 0.030264\n",
      "Epoch [9697/10000] Avg train loss: 0.030261\n",
      "Epoch [9698/10000] Avg train loss: 0.030258\n",
      "Epoch [9699/10000] Avg train loss: 0.030255\n",
      "Epoch [9700/10000] Avg train loss: 0.030251\n",
      "Epoch [9701/10000] Avg train loss: 0.030248\n",
      "Epoch [9702/10000] Avg train loss: 0.030245\n",
      "Epoch [9703/10000] Avg train loss: 0.030242\n",
      "Epoch [9704/10000] Avg train loss: 0.030239\n",
      "Epoch [9705/10000] Avg train loss: 0.030236\n",
      "Epoch [9706/10000] Avg train loss: 0.030233\n",
      "Epoch [9707/10000] Avg train loss: 0.030230\n",
      "Epoch [9708/10000] Avg train loss: 0.030227\n",
      "Epoch [9709/10000] Avg train loss: 0.030223\n",
      "Epoch [9710/10000] Avg train loss: 0.030220\n",
      "Epoch [9711/10000] Avg train loss: 0.030217\n",
      "Epoch [9712/10000] Avg train loss: 0.030214\n",
      "Epoch [9713/10000] Avg train loss: 0.030211\n",
      "Epoch [9714/10000] Avg train loss: 0.030208\n",
      "Epoch [9715/10000] Avg train loss: 0.030205\n",
      "Epoch [9716/10000] Avg train loss: 0.030202\n",
      "Epoch [9717/10000] Avg train loss: 0.030199\n",
      "Epoch [9718/10000] Avg train loss: 0.030196\n",
      "Epoch [9719/10000] Avg train loss: 0.030192\n",
      "Epoch [9720/10000] Avg train loss: 0.030189\n",
      "Epoch [9721/10000] Avg train loss: 0.030186\n",
      "Epoch [9722/10000] Avg train loss: 0.030183\n",
      "Epoch [9723/10000] Avg train loss: 0.030180\n",
      "Epoch [9724/10000] Avg train loss: 0.030177\n",
      "Epoch [9725/10000] Avg train loss: 0.030174\n",
      "Epoch [9726/10000] Avg train loss: 0.030171\n",
      "Epoch [9727/10000] Avg train loss: 0.030168\n",
      "Epoch [9728/10000] Avg train loss: 0.030165\n",
      "Epoch [9729/10000] Avg train loss: 0.030161\n",
      "Epoch [9730/10000] Avg train loss: 0.030158\n",
      "Epoch [9731/10000] Avg train loss: 0.030155\n",
      "Epoch [9732/10000] Avg train loss: 0.030152\n",
      "Epoch [9733/10000] Avg train loss: 0.030149\n",
      "Epoch [9734/10000] Avg train loss: 0.030146\n",
      "Epoch [9735/10000] Avg train loss: 0.030143\n",
      "Epoch [9736/10000] Avg train loss: 0.030140\n",
      "Epoch [9737/10000] Avg train loss: 0.030137\n",
      "Epoch [9738/10000] Avg train loss: 0.030134\n",
      "Epoch [9739/10000] Avg train loss: 0.030131\n",
      "Epoch [9740/10000] Avg train loss: 0.030127\n",
      "Epoch [9741/10000] Avg train loss: 0.030124\n",
      "Epoch [9742/10000] Avg train loss: 0.030121\n",
      "Epoch [9743/10000] Avg train loss: 0.030118\n",
      "Epoch [9744/10000] Avg train loss: 0.030115\n",
      "Epoch [9745/10000] Avg train loss: 0.030112\n",
      "Epoch [9746/10000] Avg train loss: 0.030109\n",
      "Epoch [9747/10000] Avg train loss: 0.030106\n",
      "Epoch [9748/10000] Avg train loss: 0.030103\n",
      "Epoch [9749/10000] Avg train loss: 0.030100\n",
      "Epoch [9750/10000] Avg train loss: 0.030097\n",
      "Epoch [9751/10000] Avg train loss: 0.030094\n",
      "Epoch [9752/10000] Avg train loss: 0.030091\n",
      "Epoch [9753/10000] Avg train loss: 0.030087\n",
      "Epoch [9754/10000] Avg train loss: 0.030084\n",
      "Epoch [9755/10000] Avg train loss: 0.030081\n",
      "Epoch [9756/10000] Avg train loss: 0.030078\n",
      "Epoch [9757/10000] Avg train loss: 0.030075\n",
      "Epoch [9758/10000] Avg train loss: 0.030072\n",
      "Epoch [9759/10000] Avg train loss: 0.030069\n",
      "Epoch [9760/10000] Avg train loss: 0.030066\n",
      "Epoch [9761/10000] Avg train loss: 0.030063\n",
      "Epoch [9762/10000] Avg train loss: 0.030060\n",
      "Epoch [9763/10000] Avg train loss: 0.030057\n",
      "Epoch [9764/10000] Avg train loss: 0.030054\n",
      "Epoch [9765/10000] Avg train loss: 0.030051\n",
      "Epoch [9766/10000] Avg train loss: 0.030047\n",
      "Epoch [9767/10000] Avg train loss: 0.030044\n",
      "Epoch [9768/10000] Avg train loss: 0.030041\n",
      "Epoch [9769/10000] Avg train loss: 0.030038\n",
      "Epoch [9770/10000] Avg train loss: 0.030035\n",
      "Epoch [9771/10000] Avg train loss: 0.030032\n",
      "Epoch [9772/10000] Avg train loss: 0.030029\n",
      "Epoch [9773/10000] Avg train loss: 0.030026\n",
      "Epoch [9774/10000] Avg train loss: 0.030023\n",
      "Epoch [9775/10000] Avg train loss: 0.030020\n",
      "Epoch [9776/10000] Avg train loss: 0.030017\n",
      "Epoch [9777/10000] Avg train loss: 0.030014\n",
      "Epoch [9778/10000] Avg train loss: 0.030011\n",
      "Epoch [9779/10000] Avg train loss: 0.030008\n",
      "Epoch [9780/10000] Avg train loss: 0.030005\n",
      "Epoch [9781/10000] Avg train loss: 0.030002\n",
      "Epoch [9782/10000] Avg train loss: 0.029998\n",
      "Epoch [9783/10000] Avg train loss: 0.029995\n",
      "Epoch [9784/10000] Avg train loss: 0.029992\n",
      "Epoch [9785/10000] Avg train loss: 0.029989\n",
      "Epoch [9786/10000] Avg train loss: 0.029986\n",
      "Epoch [9787/10000] Avg train loss: 0.029983\n",
      "Epoch [9788/10000] Avg train loss: 0.029980\n",
      "Epoch [9789/10000] Avg train loss: 0.029977\n",
      "Epoch [9790/10000] Avg train loss: 0.029974\n",
      "Epoch [9791/10000] Avg train loss: 0.029971\n",
      "Epoch [9792/10000] Avg train loss: 0.029968\n",
      "Epoch [9793/10000] Avg train loss: 0.029965\n",
      "Epoch [9794/10000] Avg train loss: 0.029962\n",
      "Epoch [9795/10000] Avg train loss: 0.029959\n",
      "Epoch [9796/10000] Avg train loss: 0.029956\n",
      "Epoch [9797/10000] Avg train loss: 0.029953\n",
      "Epoch [9798/10000] Avg train loss: 0.029950\n",
      "Epoch [9799/10000] Avg train loss: 0.029947\n",
      "Epoch [9800/10000] Avg train loss: 0.029943\n",
      "Epoch [9801/10000] Avg train loss: 0.029940\n",
      "Epoch [9802/10000] Avg train loss: 0.029937\n",
      "Epoch [9803/10000] Avg train loss: 0.029934\n",
      "Epoch [9804/10000] Avg train loss: 0.029931\n",
      "Epoch [9805/10000] Avg train loss: 0.029928\n",
      "Epoch [9806/10000] Avg train loss: 0.029925\n",
      "Epoch [9807/10000] Avg train loss: 0.029922\n",
      "Epoch [9808/10000] Avg train loss: 0.029919\n",
      "Epoch [9809/10000] Avg train loss: 0.029916\n",
      "Epoch [9810/10000] Avg train loss: 0.029913\n",
      "Epoch [9811/10000] Avg train loss: 0.029910\n",
      "Epoch [9812/10000] Avg train loss: 0.029907\n",
      "Epoch [9813/10000] Avg train loss: 0.029904\n",
      "Epoch [9814/10000] Avg train loss: 0.029901\n",
      "Epoch [9815/10000] Avg train loss: 0.029898\n",
      "Epoch [9816/10000] Avg train loss: 0.029895\n",
      "Epoch [9817/10000] Avg train loss: 0.029892\n",
      "Epoch [9818/10000] Avg train loss: 0.029889\n",
      "Epoch [9819/10000] Avg train loss: 0.029886\n",
      "Epoch [9820/10000] Avg train loss: 0.029883\n",
      "Epoch [9821/10000] Avg train loss: 0.029880\n",
      "Epoch [9822/10000] Avg train loss: 0.029877\n",
      "Epoch [9823/10000] Avg train loss: 0.029874\n",
      "Epoch [9824/10000] Avg train loss: 0.029870\n",
      "Epoch [9825/10000] Avg train loss: 0.029867\n",
      "Epoch [9826/10000] Avg train loss: 0.029864\n",
      "Epoch [9827/10000] Avg train loss: 0.029861\n",
      "Epoch [9828/10000] Avg train loss: 0.029858\n",
      "Epoch [9829/10000] Avg train loss: 0.029855\n",
      "Epoch [9830/10000] Avg train loss: 0.029852\n",
      "Epoch [9831/10000] Avg train loss: 0.029849\n",
      "Epoch [9832/10000] Avg train loss: 0.029846\n",
      "Epoch [9833/10000] Avg train loss: 0.029843\n",
      "Epoch [9834/10000] Avg train loss: 0.029840\n",
      "Epoch [9835/10000] Avg train loss: 0.029837\n",
      "Epoch [9836/10000] Avg train loss: 0.029834\n",
      "Epoch [9837/10000] Avg train loss: 0.029831\n",
      "Epoch [9838/10000] Avg train loss: 0.029828\n",
      "Epoch [9839/10000] Avg train loss: 0.029825\n",
      "Epoch [9840/10000] Avg train loss: 0.029822\n",
      "Epoch [9841/10000] Avg train loss: 0.029819\n",
      "Epoch [9842/10000] Avg train loss: 0.029816\n",
      "Epoch [9843/10000] Avg train loss: 0.029813\n",
      "Epoch [9844/10000] Avg train loss: 0.029810\n",
      "Epoch [9845/10000] Avg train loss: 0.029807\n",
      "Epoch [9846/10000] Avg train loss: 0.029804\n",
      "Epoch [9847/10000] Avg train loss: 0.029801\n",
      "Epoch [9848/10000] Avg train loss: 0.029798\n",
      "Epoch [9849/10000] Avg train loss: 0.029795\n",
      "Epoch [9850/10000] Avg train loss: 0.029792\n",
      "Epoch [9851/10000] Avg train loss: 0.029789\n",
      "Epoch [9852/10000] Avg train loss: 0.029786\n",
      "Epoch [9853/10000] Avg train loss: 0.029783\n",
      "Epoch [9854/10000] Avg train loss: 0.029780\n",
      "Epoch [9855/10000] Avg train loss: 0.029777\n",
      "Epoch [9856/10000] Avg train loss: 0.029774\n",
      "Epoch [9857/10000] Avg train loss: 0.029771\n",
      "Epoch [9858/10000] Avg train loss: 0.029768\n",
      "Epoch [9859/10000] Avg train loss: 0.029765\n",
      "Epoch [9860/10000] Avg train loss: 0.029762\n",
      "Epoch [9861/10000] Avg train loss: 0.029759\n",
      "Epoch [9862/10000] Avg train loss: 0.029756\n",
      "Epoch [9863/10000] Avg train loss: 0.029753\n",
      "Epoch [9864/10000] Avg train loss: 0.029750\n",
      "Epoch [9865/10000] Avg train loss: 0.029747\n",
      "Epoch [9866/10000] Avg train loss: 0.029744\n",
      "Epoch [9867/10000] Avg train loss: 0.029741\n",
      "Epoch [9868/10000] Avg train loss: 0.029738\n",
      "Epoch [9869/10000] Avg train loss: 0.029735\n",
      "Epoch [9870/10000] Avg train loss: 0.029732\n",
      "Epoch [9871/10000] Avg train loss: 0.029729\n",
      "Epoch [9872/10000] Avg train loss: 0.029726\n",
      "Epoch [9873/10000] Avg train loss: 0.029723\n",
      "Epoch [9874/10000] Avg train loss: 0.029720\n",
      "Epoch [9875/10000] Avg train loss: 0.029717\n",
      "Epoch [9876/10000] Avg train loss: 0.029714\n",
      "Epoch [9877/10000] Avg train loss: 0.029711\n",
      "Epoch [9878/10000] Avg train loss: 0.029708\n",
      "Epoch [9879/10000] Avg train loss: 0.029705\n",
      "Epoch [9880/10000] Avg train loss: 0.029702\n",
      "Epoch [9881/10000] Avg train loss: 0.029699\n",
      "Epoch [9882/10000] Avg train loss: 0.029696\n",
      "Epoch [9883/10000] Avg train loss: 0.029693\n",
      "Epoch [9884/10000] Avg train loss: 0.029690\n",
      "Epoch [9885/10000] Avg train loss: 0.029687\n",
      "Epoch [9886/10000] Avg train loss: 0.029684\n",
      "Epoch [9887/10000] Avg train loss: 0.029681\n",
      "Epoch [9888/10000] Avg train loss: 0.029678\n",
      "Epoch [9889/10000] Avg train loss: 0.029675\n",
      "Epoch [9890/10000] Avg train loss: 0.029672\n",
      "Epoch [9891/10000] Avg train loss: 0.029669\n",
      "Epoch [9892/10000] Avg train loss: 0.029666\n",
      "Epoch [9893/10000] Avg train loss: 0.029663\n",
      "Epoch [9894/10000] Avg train loss: 0.029660\n",
      "Epoch [9895/10000] Avg train loss: 0.029657\n",
      "Epoch [9896/10000] Avg train loss: 0.029654\n",
      "Epoch [9897/10000] Avg train loss: 0.029651\n",
      "Epoch [9898/10000] Avg train loss: 0.029648\n",
      "Epoch [9899/10000] Avg train loss: 0.029645\n",
      "Epoch [9900/10000] Avg train loss: 0.029642\n",
      "Epoch [9901/10000] Avg train loss: 0.029639\n",
      "Epoch [9902/10000] Avg train loss: 0.029636\n",
      "Epoch [9903/10000] Avg train loss: 0.029633\n",
      "Epoch [9904/10000] Avg train loss: 0.029630\n",
      "Epoch [9905/10000] Avg train loss: 0.029627\n",
      "Epoch [9906/10000] Avg train loss: 0.029624\n",
      "Epoch [9907/10000] Avg train loss: 0.029621\n",
      "Epoch [9908/10000] Avg train loss: 0.029618\n",
      "Epoch [9909/10000] Avg train loss: 0.029615\n",
      "Epoch [9910/10000] Avg train loss: 0.029612\n",
      "Epoch [9911/10000] Avg train loss: 0.029609\n",
      "Epoch [9912/10000] Avg train loss: 0.029606\n",
      "Epoch [9913/10000] Avg train loss: 0.029603\n",
      "Epoch [9914/10000] Avg train loss: 0.029600\n",
      "Epoch [9915/10000] Avg train loss: 0.029597\n",
      "Epoch [9916/10000] Avg train loss: 0.029594\n",
      "Epoch [9917/10000] Avg train loss: 0.029591\n",
      "Epoch [9918/10000] Avg train loss: 0.029588\n",
      "Epoch [9919/10000] Avg train loss: 0.029585\n",
      "Epoch [9920/10000] Avg train loss: 0.029582\n",
      "Epoch [9921/10000] Avg train loss: 0.029579\n",
      "Epoch [9922/10000] Avg train loss: 0.029576\n",
      "Epoch [9923/10000] Avg train loss: 0.029573\n",
      "Epoch [9924/10000] Avg train loss: 0.029570\n",
      "Epoch [9925/10000] Avg train loss: 0.029567\n",
      "Epoch [9926/10000] Avg train loss: 0.029564\n",
      "Epoch [9927/10000] Avg train loss: 0.029561\n",
      "Epoch [9928/10000] Avg train loss: 0.029558\n",
      "Epoch [9929/10000] Avg train loss: 0.029555\n",
      "Epoch [9930/10000] Avg train loss: 0.029552\n",
      "Epoch [9931/10000] Avg train loss: 0.029549\n",
      "Epoch [9932/10000] Avg train loss: 0.029546\n",
      "Epoch [9933/10000] Avg train loss: 0.029543\n",
      "Epoch [9934/10000] Avg train loss: 0.029540\n",
      "Epoch [9935/10000] Avg train loss: 0.029538\n",
      "Epoch [9936/10000] Avg train loss: 0.029535\n",
      "Epoch [9937/10000] Avg train loss: 0.029532\n",
      "Epoch [9938/10000] Avg train loss: 0.029529\n",
      "Epoch [9939/10000] Avg train loss: 0.029526\n",
      "Epoch [9940/10000] Avg train loss: 0.029523\n",
      "Epoch [9941/10000] Avg train loss: 0.029520\n",
      "Epoch [9942/10000] Avg train loss: 0.029517\n",
      "Epoch [9943/10000] Avg train loss: 0.029514\n",
      "Epoch [9944/10000] Avg train loss: 0.029511\n",
      "Epoch [9945/10000] Avg train loss: 0.029508\n",
      "Epoch [9946/10000] Avg train loss: 0.029505\n",
      "Epoch [9947/10000] Avg train loss: 0.029502\n",
      "Epoch [9948/10000] Avg train loss: 0.029499\n",
      "Epoch [9949/10000] Avg train loss: 0.029496\n",
      "Epoch [9950/10000] Avg train loss: 0.029493\n",
      "Epoch [9951/10000] Avg train loss: 0.029490\n",
      "Epoch [9952/10000] Avg train loss: 0.029487\n",
      "Epoch [9953/10000] Avg train loss: 0.029484\n",
      "Epoch [9954/10000] Avg train loss: 0.029481\n",
      "Epoch [9955/10000] Avg train loss: 0.029478\n",
      "Epoch [9956/10000] Avg train loss: 0.029475\n",
      "Epoch [9957/10000] Avg train loss: 0.029472\n",
      "Epoch [9958/10000] Avg train loss: 0.029469\n",
      "Epoch [9959/10000] Avg train loss: 0.029467\n",
      "Epoch [9960/10000] Avg train loss: 0.029464\n",
      "Epoch [9961/10000] Avg train loss: 0.029461\n",
      "Epoch [9962/10000] Avg train loss: 0.029458\n",
      "Epoch [9963/10000] Avg train loss: 0.029455\n",
      "Epoch [9964/10000] Avg train loss: 0.029452\n",
      "Epoch [9965/10000] Avg train loss: 0.029449\n",
      "Epoch [9966/10000] Avg train loss: 0.029446\n",
      "Epoch [9967/10000] Avg train loss: 0.029443\n",
      "Epoch [9968/10000] Avg train loss: 0.029440\n",
      "Epoch [9969/10000] Avg train loss: 0.029437\n",
      "Epoch [9970/10000] Avg train loss: 0.029434\n",
      "Epoch [9971/10000] Avg train loss: 0.029431\n",
      "Epoch [9972/10000] Avg train loss: 0.029428\n",
      "Epoch [9973/10000] Avg train loss: 0.029425\n",
      "Epoch [9974/10000] Avg train loss: 0.029422\n",
      "Epoch [9975/10000] Avg train loss: 0.029419\n",
      "Epoch [9976/10000] Avg train loss: 0.029416\n",
      "Epoch [9977/10000] Avg train loss: 0.029413\n",
      "Epoch [9978/10000] Avg train loss: 0.029411\n",
      "Epoch [9979/10000] Avg train loss: 0.029408\n",
      "Epoch [9980/10000] Avg train loss: 0.029405\n",
      "Epoch [9981/10000] Avg train loss: 0.029402\n",
      "Epoch [9982/10000] Avg train loss: 0.029399\n",
      "Epoch [9983/10000] Avg train loss: 0.029396\n",
      "Epoch [9984/10000] Avg train loss: 0.029393\n",
      "Epoch [9985/10000] Avg train loss: 0.029390\n",
      "Epoch [9986/10000] Avg train loss: 0.029387\n",
      "Epoch [9987/10000] Avg train loss: 0.029384\n",
      "Epoch [9988/10000] Avg train loss: 0.029381\n",
      "Epoch [9989/10000] Avg train loss: 0.029378\n",
      "Epoch [9990/10000] Avg train loss: 0.029375\n",
      "Epoch [9991/10000] Avg train loss: 0.029372\n",
      "Epoch [9992/10000] Avg train loss: 0.029369\n",
      "Epoch [9993/10000] Avg train loss: 0.029366\n",
      "Epoch [9994/10000] Avg train loss: 0.029364\n",
      "Epoch [9995/10000] Avg train loss: 0.029361\n",
      "Epoch [9996/10000] Avg train loss: 0.029358\n",
      "Epoch [9997/10000] Avg train loss: 0.029355\n",
      "Epoch [9998/10000] Avg train loss: 0.029352\n",
      "Epoch [9999/10000] Avg train loss: 0.029349\n",
      "Epoch [10000/10000] Avg train loss: 0.029346\n"
     ]
    }
   ],
   "source": [
    "set_seed(42)  # Fixing the seed\n",
    "import torch.optim as optim\n",
    "# Proceed with the rest of the setup (loss, optimizer) and training loop as before\n",
    "# Training loop\n",
    "\n",
    "# careful, for minibatch 32, 400 is enough!!!\n",
    "num_epochs = 10000  # You might need more epochs for a deep network\n",
    "losses = []\n",
    "cool_loss = []\n",
    "for epoch in range(num_epochs):\n",
    "    # now we loop through the mini-batches\n",
    "    for inputs, targets in train_loader:\n",
    "        # Forward pass\n",
    "        outputs = model(inputs) # this is f(x;\\theta)\n",
    "        loss = criterion(outputs, targets) # this is \\sum_{t\\in mini batch} \\ell(y_t, f(x_t;\\theta))\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad() # forget the gradients from old steps of Gradient Descent (GD)\n",
    "        loss.backward() # compute the new gradient, \\nabla_\\theta L(\\theta)\n",
    "        optimizer.step() # \\theta_{k+1}=\\theta_k - \\eta \\nabla_\\theta L(\\theta_k).\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    cool_loss.append(np.mean(losses))\n",
    "    losses = []\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] Avg train loss: {np.mean(cool_loss):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1b69b3ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x30313ff40>]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA9UAAAMtCAYAAACRi1JrAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABKYElEQVR4nO3de3xcBZ3w/+8kaZLSS6CUtpSWUgShUK7pgi0iKlhF1IeHXe2KFi/waFdRSh8vdNn1wrpbdlUWd9ciKOjDqtifD7iPu3SR6CIUy4qUgtxBuaSUlNICSUvb3Ob8/mgbmiZpM6fJnJnk/X695kVzzpmZb9KTlE/OmTm5JEmSAAAAAApWkfUAAAAAUK5ENQAAAKQkqgEAACAlUQ0AAAApiWoAAABISVQDAABASqIaAAAAUqrKeoD+yOfz8cILL8SYMWMil8tlPQ4AAABDXJIksWnTppg8eXJUVPR9PLosovqFF16IqVOnZj0GAAAAw8yaNWtiypQpfa4vi6geM2ZMRGz/ZMaOHZvxNAAAAAx1LS0tMXXq1K4e7UtZRPXOU77Hjh0rqgEAACiavb0E2RuVAQAAQEqiGgAAAFIS1QAAAJCSqAYAAICURDUAAACkJKoBAAAgJVENAAAAKYlqAAAASElUAwAAQEqiGgAAAFIS1QAAAJCSqAYAAICURDUAAACkJKoBAAAgJVENAAAAKYlqAAAASElUAwAAQEqiGgAAAFIS1QAAAJCSqAYAAICURDUAAACkJKoBAAAgJVENAAAAKYlqAAAASElUAwAAQEqiGgAAAFIS1QAAAJCSqAYAAICURDUAAACkJKoBAAAgJVENAAAAKYnqAfTPv3oqDrvs1mje0p71KAAAABSBqB4gSZLENxuejIiIE//m9oynAQAAoBhE9QBp7ch3/TlJMhwEAACAohHVA6R2RGXWIwAAAFBkqaJ66dKlMX369KitrY36+vpYsWJFn9t+9KMfjVwu1+N27LHHph4aAAAASkHBUb1s2bJYuHBhXH755bF69eo4/fTT4+yzz47GxsZet//Wt74VTU1NXbc1a9bEuHHj4v3vf/8+Dw8AAABZKjiqr7rqqrjwwgvjoosuihkzZsTVV18dU6dOjWuuuabX7evq6mLSpEldt/vuuy9eeeWV+NjHPrbPw5eaupEjuv7cmffCagAAgKGuoKhua2uLVatWxdy5c7stnzt3bqxcubJfj3H99dfHWWedFdOmTetzm9bW1mhpael2KwdvnDi66893PL4+w0kAAAAohoKiesOGDdHZ2RkTJ07stnzixImxbt26vd6/qakp/vM//zMuuuiiPW63ZMmSqKur67pNnTq1kDEzs+u7fm9t78xuEAAAAIoi1RuV5XK5bh8nSdJjWW9+8IMfxP777x/nnnvuHrdbvHhxNDc3d93WrFmTZsxM/fqJl7IeAQAAgEFWUFSPHz8+KisrexyVXr9+fY+j17tLkiRuuOGGmD9/flRXV+9x25qamhg7dmy3Wzl446QxXX+++f7n41/+66kMpwEAAGCwFRTV1dXVUV9fHw0NDd2WNzQ0xJw5c/Z43zvvvDP+8Ic/xIUXXlj4lGXiE6cf3u3jb9z+ZEaTAAAAUAxVhd5h0aJFMX/+/Jg1a1bMnj07rrvuumhsbIwFCxZExPZTt9euXRs33nhjt/tdf/31ceqpp8bMmTMHZvISVF3V83cU/T01HgAAgPJTcFTPmzcvNm7cGFdccUU0NTXFzJkzY/ny5V3v5t3U1NTjmtXNzc1x8803x7e+9a2BmbpE9XYZrVsfaor3HD85g2kAAAAYbLkkSUr+gsotLS1RV1cXzc3NJf366mc3vBZv/cavuy07etKYuG3hW7IZCAAAgFT626Gp3v2b3lVV9jzN+/F1mzKYBAAAgGIQ1QNoygH7ZT0CAAAARSSqAQAAICVRPcCOmDA66xEAAAAoElE9wH72qZ7X67551fMZTAIAAMBgE9UDbEztiPjq+47ttux///TBeLFlW0YTAQAAMFhE9SD4yJzDeiz7/fPNxR8EAACAQSWqi6Qzn896BAAAAAaYqC6STk0NAAAw5IjqIml8eUvWIwAAADDARHWR/P1tj8d7/nlFbGvvzHoUAAAABoioLqKH17bELx5Zl/UYAAAADBBRPUimjhvZ6/K2Di+uBgAAGCpE9SD55aIzel3emU+KPAkAAACDRVQPkpqqyl6XdyaiGgAAYKgQ1UWWd6QaAABgyBDVRaapAQAAhg5RXWRb2zvjcz99MG572LuAAwAAlDtRPYh+fNGpPZbdcPcz8X9XPR8Lfrgqg4kAAAAYSKJ6EM05YnyPZes3tWYwCQAAAINBVAMAAEBKohoAAABSEtUAAACQkqgGAACAlEQ1AAAApCSqAQAAICVRPcgOHz8q6xEAAAAYJKJ6kB0mqgEAAIYsUT3IrjzvuDjvpEOyHgMAAIBBIKoH2YSxtXHVvBOzHgMAAIBBIKoBAAAgJVFdJL/632dkPQIAAAADTFQXyZjaqh7LXmvtyGASAAAABkrP0mNQVORyPZZ97dbH4g0HjYpDx+0Xc4+dlMFUAAAA7AtRXSS9RfVN9zZ2/fnZK88p5jgAAAAMAKd/F0nPpAYAAKDcieoi6UySrEcAAABggInqIhk/uibrEQAAABhgorqIjpwwOusRAAAAGECiuoieWr856xEAAAAYQKIaAAAAUhLVAAAAkJKoBgAAgJRENQAAAKQkqgEAACAlUQ0AAAApiWoAAABISVQDAABASqIaAAAAUhLVAAAAkJKoBgAAgJRENQAAAKQkqgEAACAlUV1Eo2uqsh4BAACAASSqiyiX9QAAAAAMKFFdIp7Z8FrWIwAAAFAgUV0itrV3Zj0CAAAABRLVRXT4hNF9rqvIOTkcAACg3IjqIvqXD54U7z1hcq/rNDUAAED5EdVFNHXcfvHPHzwpDtl/ZI91mhoAAKD8iOoS4Ug1AABA+RHVJeKK/3gs6xEAAAAokKjOwN//6fE9lt315EvxWmtHBtMAAACQlqjOwJuPHN/r8qTIcwAAALBvRHUJad7anvUIAAAAFEBUl5Dr7vxj1iMAAABQAFFdQto681mPAAAAQAFEdUlxXS0AAIByIqpLSIWmBgAAKCuiuoTkRDUAAEBZEdUAAACQkqguIXkXqgYAACgrorqE/Pi3jVmPAAAAQAFEdUa+9ecn9rr8sMtujXufebm4wwAAAJCKqM7I/zjxkD7XfeDae4o4CQAAAGmJ6gwt/dDJWY8AAADAPhDVGXr3cQdnPQIAAAD7QFQDAABASqIaAAAAUhLVAAAAkJKoztiJU/fPegQAAABSEtUZu+Gjf9Lr8saNW4o8CQAAAIUS1RkbN6q61+Xv+tZdRZ4EAACAQonqErWlrTPrEQAAANgLUQ0AAAApiWoAAABISVQDAABASqIaAAAAUkoV1UuXLo3p06dHbW1t1NfXx4oVK/a4fWtra1x++eUxbdq0qKmpiTe84Q1xww03pBoYAAAASkVVoXdYtmxZLFy4MJYuXRqnnXZaXHvttXH22WfHo48+Goceemiv9/nABz4QL774Ylx//fVxxBFHxPr166Ojo2Ofhx/qGh59Md5xzMSsxwAAAKAPuSRJkkLucOqpp8bJJ58c11xzTdeyGTNmxLnnnhtLlizpsf1tt90Wf/7nfx5PP/10jBs3LtWQLS0tUVdXF83NzTF27NhUj1HKDrvs1j7XPXvlOUWcBAAAgIj+d2hBp3+3tbXFqlWrYu7cud2Wz507N1auXNnrfX7+85/HrFmz4h/+4R/ikEMOiTe+8Y3xuc99LrZu3drn87S2tkZLS0u3GwAAAJSagk7/3rBhQ3R2dsbEid1PSZ44cWKsW7eu1/s8/fTTcffdd0dtbW387Gc/iw0bNsSnPvWpePnll/t8XfWSJUviq1/9aiGjAQAAQNGleqOyXC7X7eMkSXos2ymfz0cul4sf/ehHccopp8S73/3uuOqqq+IHP/hBn0erFy9eHM3NzV23NWvWpBkTAAAABlVBR6rHjx8flZWVPY5Kr1+/vsfR650OPvjgOOSQQ6Kurq5r2YwZMyJJknj++efjyCOP7HGfmpqaqKmpKWQ0AAAAKLqCjlRXV1dHfX19NDQ0dFve0NAQc+bM6fU+p512WrzwwguxefPmrmVPPvlkVFRUxJQpU1KMDAAAAKWh4NO/Fy1aFN/73vfihhtuiMceeywuvfTSaGxsjAULFkTE9lO3L7jggq7tzz///DjwwAPjYx/7WDz66KNx1113xec///n4+Mc/HiNHjhy4z6SMnXJYundFBwAAIFsFX6d63rx5sXHjxrjiiiuiqakpZs6cGcuXL49p06ZFRERTU1M0NjZ2bT969OhoaGiIz3zmMzFr1qw48MAD4wMf+EB87WtfG7jPosydd/Ihce+zL2c9BgAAAAUq+DrVWRjq16nesLk1Zn3tl72uc51qAACA4huU61QzOCr6eOd0AAAASpuoLgGSGgAAoDyJ6hLgSDUAAEB5EtWlQFMDAACUJVFdAhyoBgAAKE+iugQ4/RsAAKA8ieoSIKkBAADKk6guAQ5UAwAAlCdRXQKc/g0AAFCeRDUAAACkJKpLgAPVAAAA5UlUlwCnfwMAAJQnUV0CJDUAAEB5EtUlILeHI9VrXt5SxEkAAAAohKguARV7OFT9Ysu24g0CAABAQUR1CdjTkepv/eqpIk4CAABAIUR1ifvvpzdmPQIAAAB9ENUlriOfZD0CAAAAfRDVJS7R1AAAACVLVAMAAEBKohoAAABSEtUAAACQkqgGAACAlER1iTjswP36XJf3DuAAAAAlSVSXiF8uOiNOnT6u13Unf60h/uY/Hi3yRAAAAOyNqC4RVZUVMbK6std1r25pj+vvfqbIEwEAALA3ohoAAABSEtUlJJf1AAAAABREVAMAAEBKohoAAABSEtUlJJdzAjgAAEA5EdUAAACQkqgGAACAlER1CXHyNwAAQHkR1SXkwtOnZz0CAAAABRDVJWTOG8bH7y4/K+sxAAAA6CdRXWIOGlOT9QgAAAD0k6gGAACAlER1Gfn9869mPQIAAAC7ENVl5H3/8pt4buNrWY8BAADADqK6zDzW1JL1CAAAAOwgqgEAACAlUQ0AAAApieoykyRZTwAAAMBOohoAAABSEtVlxoFqAACA0iGqy8z3Vjyd9QgAAADsIKrLzP2Nr8Zvn96Y9RgAAACEqC5L61q2ZT0CAAAAIaoBAAAgNVENAAAAKYnqEvR/Pn7KHte7VjUAAEBpENUl6Iw3HhTnHHdw1mMAAACwF6K6VOWyHgAAAIC9EdUlak9NnYTzvwEAAEqBqC5RuZxD1QAAAKVOVJeoCk0NAABQ8kR1idrj6d/O/gYAACgJorpEOf0bAACg9IlqAAAASElUl6g9Hah2+jcAAEBpENUlKreHV1VragAAgNIgqkuUl1QDAACUPlFdomqq/NUAAACUOuVWoi4568g+1yVeVA0AAFASRHWJmjCmNn7/lblZjwEAAMAeiOoSNrZ2RNYjAAAAsAeiugw5+RsAAKA0iGoAAABISVQDAABASqK6HDn/GwAAoCSIagAAAEhJVJehm+9/PusRAAAACFFdln77zMtZjwAAAECIagAAAEhNVAMAAEBKohoAAABSEtUAAACQkqgGAACAlEQ1AAAApCSqy9TPVj8fdz75UtZjAAAADGtVWQ9AOpcuezAiIp698pyMJwEAABi+HKkGAACAlEQ1AAAApCSqAQAAICVRDQAAACmJ6hK3/LOnZz0CAAAAfRDVJe6YyWOzHgEAAIA+pIrqpUuXxvTp06O2tjbq6+tjxYoVfW7761//OnK5XI/b448/nnpoAAAAKAUFR/WyZcti4cKFcfnll8fq1avj9NNPj7PPPjsaGxv3eL8nnngimpqaum5HHnlk6qGHmwtmT8t6BAAAAHpRcFRfddVVceGFF8ZFF10UM2bMiKuvvjqmTp0a11xzzR7vN2HChJg0aVLXrbKyMvXQw83omqqsRwAAAKAXBUV1W1tbrFq1KubOndtt+dy5c2PlypV7vO9JJ50UBx98cJx55plxxx137HHb1tbWaGlp6XYDAACAUlNQVG/YsCE6Oztj4sSJ3ZZPnDgx1q1b1+t9Dj744Ljuuuvi5ptvjltuuSWOOuqoOPPMM+Ouu+7q83mWLFkSdXV1XbepU6cWMiYAAAAURarzinO5XLePkyTpsWyno446Ko466qiuj2fPnh1r1qyJb3zjG/GWt7yl1/ssXrw4Fi1a1PVxS0uLsAYAAKDkFHSkevz48VFZWdnjqPT69et7HL3ekze96U3x1FNP9bm+pqYmxo4d2+0GAAAApaagqK6uro76+vpoaGjotryhoSHmzJnT78dZvXp1HHzwwYU8NQAAAJScgk//XrRoUcyfPz9mzZoVs2fPjuuuuy4aGxtjwYIFEbH91O21a9fGjTfeGBERV199dRx22GFx7LHHRltbW/zwhz+Mm2++OW6++eaB/UyGsCTrAQAAAOhVwVE9b9682LhxY1xxxRXR1NQUM2fOjOXLl8e0aduvpdzU1NTtmtVtbW3xuc99LtauXRsjR46MY489Nm699dZ497vfPXCfBQAAAGQglyRJyR8IbWlpibq6umhubh6Wr6/++9sej2t+/cde1z2z5N19vkkcAAAA6fS3Qwt6TTWlp/R/JQIAADB0ieoy8LajJmQ9AgAAAL0Q1WXglOnj+lznQDUAAEB2RDUAAACkJKrLXBm8zxwAAMCQJarLnKQGAADIjqgGAACAlER1mXP2NwAAQHZENQAAAKQkqstc4lXVAAAAmRHVZc7p3wAAANkR1QAAAJCSqAYAAICURDUAAACkJKrLnNdUAwAAZEdUlznv/g0AAJAdUQ0AAAApieoyccX/OLbX5U7/BgAAyI6oLhMfOnVafOy0w7IeAwAAgF2I6jJRWZGL/3nSIT2WO1ANAACQHVFdRno71Ttx/jcAAEBmRHUZyQtoAACAkiKqy0h7Z8+oPu4rt8dP71uTwTQAAACI6jKyrb2z1+Wf/7+/L/IkAAAARIjqsnLEhNFZjwAAAMAuRHUZmbz/yKxHAAAAYBeiGgAAAFIS1QAAAJCSqAYAAICURDUAAACkJKoBAAAgJVENAAAAKYlqAAAASElUl5lZ0w7IegQAAAB2ENVlJulj+Vu/fkdcd9cfizoLAADAcCeqy0w+6T2rn924Jf5u+eNFngYAAGB4E9Vl5mvnzsx6BAAAAHYQ1WXm2Ml1WY8AAADADqIaAAAAUhLVAAAAkJKoBgAAgJRENQAAAKQkqgEAACAlUQ0AAAApiWoAAABISVQDAABASqIaAAAAUhLVAAAAkJKoBgAAgJRENQAAAKQkqgEAACAlUQ0AAAApieoyNLqmKusRAAAACFFdlr7wrqOyHgEAAIAQ1QAAAJCaqC5DuawHAAAAICJENQAAAKQmqgEAACAlUV2Ock4ABwAAKAWiugwdf0hd1iMAAAAQorosnTB1/6xHAAAAIEQ1AAAApCaqy9RH5xyW9QgAAADDnqguU//zpEOyHgEAAGDYE9VlKsl6AAAAAEQ1AAAApCWqAQAAICVRXaZyWQ8AAACAqAYAAIC0RDUAAACkJKoBAAAgJVFdpg4cXZ31CAAAAMOeqC5TUw7YL+sRAAAAhj1RXcbGj67JegQAAIBhTVSXsZzragEAAGRKVJcxTQ0AAJAtUQ0AAAApiWoAAABISVSXsQovqgYAAMiUqAYAAICURDUAAACkJKrLmLO/AQAAsiWqy9ih4/bLegQAAIBhTVSXsa//2Qm9Ll/z8pZ4cM2rxR0GAABgGKrKegDSm1RX22NZPp/E6f9wR0RErPjC22Kqo9kAAACDxpHqIeZzP32w68+Pr9uU4SQAAABDn6guY729Udktq9d2/TlJkiJOAwAAMPyI6jLmzb8BAACylSqqly5dGtOnT4/a2tqor6+PFStW9Ot+v/nNb6KqqipOPPHENE8LAAAAJaXgqF62bFksXLgwLr/88li9enWcfvrpcfbZZ0djY+Me79fc3BwXXHBBnHnmmamHpbvcXi5U7eRvAACAwVVwVF911VVx4YUXxkUXXRQzZsyIq6++OqZOnRrXXHPNHu/3yU9+Ms4///yYPXt26mEpjJdUAwAADK6CorqtrS1WrVoVc+fO7bZ87ty5sXLlyj7v9/3vfz/++Mc/xpe//OV+PU9ra2u0tLR0u9HT3l9TraoBAAAGU0FRvWHDhujs7IyJEyd2Wz5x4sRYt25dr/d56qmn4rLLLosf/ehHUVXVv8tiL1myJOrq6rpuU6dOLWTMYaOiwluVAQAAZCnVG5Xt/lreJEl6fX1vZ2dnnH/++fHVr3413vjGN/b78RcvXhzNzc1dtzVr1qQZc9hz+jcAAMDg6t+h4x3Gjx8flZWVPY5Kr1+/vsfR64iITZs2xX333RerV6+Oiy++OCIi8vl8JEkSVVVVcfvtt8fb3/72HverqamJmpqaQkYDAACAoivoSHV1dXXU19dHQ0NDt+UNDQ0xZ86cHtuPHTs2HnrooXjggQe6bgsWLIijjjoqHnjggTj11FP3bXoAAADIUEFHqiMiFi1aFPPnz49Zs2bF7Nmz47rrrovGxsZYsGBBRGw/dXvt2rVx4403RkVFRcycObPb/SdMmBC1tbU9ljPwnP0NAAAwuAqO6nnz5sXGjRvjiiuuiKamppg5c2YsX748pk2bFhERTU1Ne71mNcXhNdUAAACDK5ckpZ9eLS0tUVdXF83NzTF27Nisxykph112a5/r/uX8k+I9x08u4jQAAABDQ387NNW7fwMAAACiuuxdcuaRfa4r/XMQAAAAypuoLnMVvVwfHAAAgOIQ1WVuT03tQDUAAMDgEtVlbvL+I7MeAQAAYNgS1WXuf550SJ/ryuCN3QEAAMqaqC5zlRVeUw0AAJAVUQ0AAAApieoh4MaPn9Lrcmd/AwAADC5RPQS85Y0H9bo88f7fAAAAg0pUAwAAQEqieghz+jcAAMDgEtVDWM4bgwMAAAwqUT2EOVINAAAwuEQ1AAAApCSqAQAAICVRDQAAACmJagAAAEhJVAMAAEBKohoAAABSEtVD2G+ffjnrEQAAAIY0UT2ELbtvTdYjAAAADGmiGgAAAFIS1QAAAJCSqAYAAICURDUAAACkJKoBAAAgJVENAAAAKYlqAAAASElUAwAAQEqiGgAAAFIS1QAAAJCSqAYAAICURDUAAACkJKoBAAAgJVE9RFw3vz7rEQAAAIYdUT1EzD12UtYjAAAADDuiGgAAAFIS1QAAAJCSqAYAAICURDUAAACkJKqHkPGja7IeAQAAYFgR1UPI4rOPznoEAACAYUVUDyFJ1gMAAAAMM6J6CMknPbP6jsfXx6tb2jKYBgAAYOgT1UNJL4eqP/aD38V5S1cWfxYAAIBhQFQPIUkfJ4A/veG1Ik8CAAAwPIjqIaSXs78BAAAYRKJ6CNHUAAAAxSWqh5De3qgMAACAwSOqh5BTp4/LegQAAIBhRVQPIUdMGJP1CAAAAMOKqAYAAICURDUAAACkJKoBAAAgJVENAAAAKYlqAAAASElUAwAAQEqiGgAAAFIS1QAAAJCSqB4mmpq3Zj0CAADAkCOqh4m5V92V9QgAAABDjqgeJja1dmQ9AgAAwJAjqgEAACAlUQ0AAAApiWoAAABISVQDAABASqIaAAAAUhLVAAAAkJKoBgAAgJRENQAAAKQkqgEAACAlUT2MtHZ0Zj0CAADAkCKqh5HN2zqyHgEAAGBIEdUAAACQkqgeYn580al9rvuLH94fbR35Ik4DAAAwtInqIWbOEePj+Cl1va6799mX4+b7ny/yRAAAAEOXqB6CjjhodJ/rvK4aAABg4IjqISifJFmPAAAAMCyI6iEor6kBAACKQlQPQZoaAACgOET1EJQ4/RsAAKAoRPUQtKemzuWKNwcAAMBQJ6qHoMQJ4AAAAEUhqoegfD7rCQAAAIYHUT0EVVY6xxsAAKAYRPUQtPjso7MeAQAAYFgQ1UPQlAP2y3oEAACAYSFVVC9dujSmT58etbW1UV9fHytWrOhz27vvvjtOO+20OPDAA2PkyJFx9NFHxz/+4z+mHhgAAABKRVWhd1i2bFksXLgwli5dGqeddlpce+21cfbZZ8ejjz4ahx56aI/tR40aFRdffHEcf/zxMWrUqLj77rvjk5/8ZIwaNSo+8YlPDMgnAQAAAFnIJcmermrc06mnnhonn3xyXHPNNV3LZsyYEeeee24sWbKkX49x3nnnxahRo+Jf//Vf+7V9S0tL1NXVRXNzc4wdO7aQcYetwy67tdflf3XOjLjo9MOLPA0AAEB56W+HFnT6d1tbW6xatSrmzp3bbfncuXNj5cqV/XqM1atXx8qVK+OMM87oc5vW1tZoaWnpdgMAAIBSU1BUb9iwITo7O2PixIndlk+cODHWrVu3x/tOmTIlampqYtasWfHpT386Lrrooj63XbJkSdTV1XXdpk6dWsiYAAAAUBSp3qgsl+t+HeQkSXos292KFSvivvvui+985ztx9dVXx0033dTntosXL47m5uau25o1a9KMCQAAAIOqoDcqGz9+fFRWVvY4Kr1+/foeR693N3369IiIOO644+LFF1+Mr3zlK/HBD36w121ramqipqamkNEAAACg6Ao6Ul1dXR319fXR0NDQbXlDQ0PMmTOn34+TJEm0trYW8tQAAABQcgq+pNaiRYti/vz5MWvWrJg9e3Zcd9110djYGAsWLIiI7adur127Nm688caIiPj2t78dhx56aBx99NERsf261d/4xjfiM5/5zAB+GgAAAFB8BUf1vHnzYuPGjXHFFVdEU1NTzJw5M5YvXx7Tpk2LiIimpqZobGzs2j6fz8fixYvjmWeeiaqqqnjDG94QV155ZXzyk58cuM+Cftvba98BAADov4KvU50F16kuXF/Xqf7r9xwTF755epGnAQAAKC+Dcp1qyodwBgAAGHyieog6/9RDsx4BAABgyBPVAAAAkJKoHqL6ejuylza5lBkAAMBAEdXDzHfu/GM8u+G1rMcAAAAYEkT1MDT3H+/KegQAAIAhQVQPQ22d+axHAAAAGBJENQAAAKQkqoeo0TVVWY8AAAAw5InqIWrC2NqsRwAAABjyRPUQVlnR14W1AAAAGAiiegiT1AAAAINLVA9hOVUNAAAwqEQ1AAAApCSqh7CcE8ABAAAGlagGAACAlEQ1AAAApCSqhzJnfwMAAAwqUT2EHTiqOusRAAAAhjRRPYRd/5E/yXoEAACAIU1UD2HHTB6b9QgAAABDmqgGAACAlEQ1AAAApCSqAQAAICVRPcSNqanKegQAAIAhS1QPcd6sDAAAYPCI6iHunz54UtYjAAAADFmieoibOLY26xEAAACGLFENAAAAKYlqAAAASElUAwAAQEqiGgAAAFIS1cNUR2c+6xEAAADKnqgepn666vmsRwAAACh7onqYanp1a9YjAAAAlD1RDQAAACmJagAAAEhJVAMAAEBKohoAAABSEtXDVJL1AAAAAEOAqB4GjpwwOusRAAAAhiRRPQz0dlR6W3tn0ecAAAAYakT1MPXdFc9kPQIAAEDZE9XDQC7rAQAAAIYoUQ0AAAApiWoAAABISVQDAABASqJ6GMh5UTUAAMCgENUAAACQkqgGAACAlET1MNbemc96BAAAgLImqoeBjs6k1+X5pPflAAAA9I+oHgZaOxyRBgAAGAyiehg4/KBRvS7PhbcFBwAA2Beiehj4+p+d0Ovyh9Y2F3kSAACAoUVUDwOT6mrjvr86q8fy//j9CxlMAwAAMHSI6mFi/OiarEcAAAAYckT1MPb0S69lPQIAAEBZE9XD2J1PvhRt3hkcAAAgNVE9zG3r6Mx6BAAAgLIlqoc5F9UCAABIT1QDAABASqIaAAAAUhLVw1wu5wRwAACAtEQ1AAAApCSqh5HzTjqkxzLHqQEAANIT1cPIVfNO7LEsKf4YAAAAQ4aoBgAAgJRE9TC3cXNr1iMAAACULVE9zC29449ZjwAAAFC2RPUw15H3qmoAAIC0RPUwd/P9z2c9AgAAQNkS1cTaV7dmPQIAAEBZEtXEcxtfy3oEAACAsiSqAQAAICVRDQAAACmJagAAAEhJVBOX3fxQLPnPx7IeAwAAoOyIaqLx5S1x7Z1PexdwAACAAolqurS2d2Y9AgAAQFkR1XRJsh4AAACgzIhquiSqGgAAoCCiepg5YUrdHtaqagAAgEKI6mFm2oGj+lznSDUAAEBhRDUAAACkJKrp4kA1AABAYUT1MJPL9b3O6d8AAACFEdXDzDuOmdjnusSxagAAgIKkiuqlS5fG9OnTo7a2Nurr62PFihV9bnvLLbfEO97xjjjooINi7NixMXv27PjFL36RemD2zTnHHZz1CAAAAENGwVG9bNmyWLhwYVx++eWxevXqOP300+Pss8+OxsbGXre/66674h3veEcsX748Vq1aFW9729vive99b6xevXqfh6dwuT2c/+30bwAAgMLkkqSwlDr11FPj5JNPjmuuuaZr2YwZM+Lcc8+NJUuW9Osxjj322Jg3b1586Utf6tf2LS0tUVdXF83NzTF27NhCxqUXh112a6/Ll3/29Dhmsq8vAABAfzu0oCPVbW1tsWrVqpg7d2635XPnzo2VK1f26zHy+Xxs2rQpxo0b1+c2ra2t0dLS0u3G4FvXsjXrEQAAAMpKQVG9YcOG6OzsjIkTu7/Z1cSJE2PdunX9eoxvfvOb8dprr8UHPvCBPrdZsmRJ1NXVdd2mTp1ayJik9PEf3Jf1CAAAAGUl1RuV7f663CRJ9vha3Z1uuumm+MpXvhLLli2LCRMm9Lnd4sWLo7m5ueu2Zs2aNGMCAADAoKoqZOPx48dHZWVlj6PS69ev73H0enfLli2LCy+8MH7605/GWWedtcdta2pqoqamppDRAAAAoOgKOlJdXV0d9fX10dDQ0G15Q0NDzJkzp8/73XTTTfHRj340fvzjH8c555yTblIAAAAoMQWf/r1o0aL43ve+FzfccEM89thjcemll0ZjY2MsWLAgIrafun3BBRd0bX/TTTfFBRdcEN/85jfjTW96U6xbty7WrVsXzc3NA/dZUJDzTjok6xEAAACGhIKjet68eXH11VfHFVdcESeeeGLcddddsXz58pg2bVpERDQ1NXW7ZvW1114bHR0d8elPfzoOPvjgrtsll1wycJ8FBfn6+0/IegQAAIAhoeDrVGfBdaoHXl/Xqn72SqfnAwAADMp1qgEAAIDXiWoAAABISVQDAABASqIaAAAAUhLVAAAAkJKoppvOfMm/GTwAAEDJENV0c/a37ooyuMoaAABASRDVdPPki5ujtSOf9RgAAABlQVQDAABASqIaAAAAUhLVw9SNHz8l6xEAAADKnqgept7yxoP6XJf3RmUAAAD9IqqHsSMmjO51uaYGAADoH1E9jFVV5HpdrqkBAAD6R1TTg9O/AQAA+kdU04OmBgAA6B9RPYzlcr2f/u38bwAAgP4R1cNYPt97PTv9GwAAoH9E9TD2hXcd1etySQ0AANA/onoYO3PGxF6XJ45UAwAA9IuoHuYmjKnpsayPs8IBAADYjage5np7r7LECeAAAAD9IqqHuVz0rOq7ntyQwSQAAADlR1QPcwfvX9tj2ed++mAGkwAAAJQfUT3MjduvOusRAAAAypaoBgAAgJRE9TBXW12Z9QgAAABlS1QPc5e/e0bWIwAAAJQtUT3MTd5/ZNYjAAAAlC1RDQAAACmJaqKi56WqAQAA6AdRTXz/Y6dkPQIAAEBZEtX0eqQ6SZLiDwIAAFBmRDWR76Wff/98c/EHAQAAKDOimsj3UtXtnfkMJgEAACgvopro7O1QNQAAAHslqonOXl4//fMHX4im5q0ZTAMAAFA+RDW9Hqm+8Z7n4u3fuDODaQAAAMqHqCZOPvSAXpdvbe8s8iQAAADlRVQTk+pqsx4BAACgLIlqAAAASElUAwAAQEqiGgAAAFIS1QAAAJCSqGaPnnpxU9YjAAAAlCxRzR694x/vynoEAACAkiWqAQAAICVRzV691tqR9QgAAAAlSVSzV39/2+NZjwAAAFCSRDV79dunX856BAAAgJIkqgEAACAlUU1ERJx38iFZjwAAAFB2RDUAAACkJKrZLsl6AAAAgPIjqokITQ0AAJCGqAYAAICURDUREXHkxNFZjwAAAFB2RDUREXHhm6f3ue6JFzdFkjhBHAAAYHeimoiIqKmqjKvnndjn+uUPrSveMAAAAGVCVNMvN9//fNYjAAAAlBxRTZdcru91Tv8GAADoSVTTL7sm9Yst2+IdV90ZP/jNM5nNAwAAUApENf2y64Hqr//iiXhq/eb4yr8/mt1AAAAAJUBUU7DWjnzWIwAAAJQEUU2X3B5eVO0V1QAAAD2JarqMqNhDVHujMgAAgB5ENV3WvLIl6xEAAADKiqimS+PLfUf1iqc2FHESAACA8iCq6fKmww/MegQAAICyIqrpcsKU/bMeAQAAoKyIarqMG1Xdr+36fjszAACA4UVU02VUTVXc+PFT+lz/zdufiCRJXF4LAABgh6qsB6C0HD+lrs91//xff4jjDul7PQAAwHDjSDXd7O1y1C9uanX6NwAAwA6imm5G1ezl5IW9VTcAAMAwIqrpprpqz7uEpAYAAHidqAYAAICURDUFcfY3AADA60Q1AAAApCSq6eHcEyf3uS5Jknh8XUsRpwEAAChdopoe/v7Pju9z3bMbt8STL24u4jQAAAClS1TTQ01VZZ/rHnz+1eINAgAAUOJENQVZ3fhq1iMAAACUDFENAAAAKYlqAAAASElUAwAAQEqiGgAAAFIS1QAAAJBSqqheunRpTJ8+PWpra6O+vj5WrFjR57ZNTU1x/vnnx1FHHRUVFRWxcOHCtLMCAABASSk4qpctWxYLFy6Myy+/PFavXh2nn356nH322dHY2Njr9q2trXHQQQfF5ZdfHieccMI+D0xxLDnvuKxHAAAAKHkFR/VVV10VF154YVx00UUxY8aMuPrqq2Pq1KlxzTXX9Lr9YYcdFt/61rfiggsuiLq6un0emOL44CmHZj0CAABAySsoqtva2mLVqlUxd+7cbsvnzp0bK1euHLChWltbo6WlpdsNAAAASk1BUb1hw4bo7OyMiRMndls+ceLEWLdu3YANtWTJkqirq+u6TZ06dcAem/47etKYrEcAAAAoaaneqCyXy3X7OEmSHsv2xeLFi6O5ubnrtmbNmgF7bPpvRKU3hwcAANiTqkI2Hj9+fFRWVvY4Kr1+/foeR6/3RU1NTdTU1AzY45FOW0c+6xEAAABKWkGHIqurq6O+vj4aGhq6LW9oaIg5c+YM6GBk70vvPSbrEQAAAEpaQUeqIyIWLVoU8+fPj1mzZsXs2bPjuuuui8bGxliwYEFEbD91e+3atXHjjTd23eeBBx6IiIjNmzfHSy+9FA888EBUV1fHMceItlI25w0HZj0CAABASSs4qufNmxcbN26MK664IpqammLmzJmxfPnymDZtWkRENDU19bhm9UknndT151WrVsWPf/zjmDZtWjz77LP7Nj0AAABkKJckSZL1EHvT0tISdXV10dzcHGPHjs16nGHlsMtu3eP6Z688p0iTAAAAFE9/O9TbOwMAAEBKohoAAABSEtUAAACQkqhmj/76Pd6hHQAAoC+imj06csLorEcAAAAoWaKaPXKtagAAgL6JavaoqnLPu8ial7cUaRIAAIDSI6rZJx//we+yHgEAACAzopp98tT6zVmPAAAAkBlRzV4tescbsx4BAACgJIlq9uritx0RH5g1JesxAAAASo6oZq8qKnIxa9q4PW7T3pmPH/+2MZ7Z8FqRpgIAAMheVdYDMDT8n5XPxtdufSwiIp698pyMpwEAACgOR6rpl/Wbtu1x/X3PvlKkSQAAAEqHqKZf7njipaxHAAAAKDmimn6pyGU9AQAAQOkR1fRLLqeqAQAAdieq6ZdPvuXwrEcAAAAoOaKafnnDQaOzHgEAAKDkiGr65bDxo7IeAQAAoOSIavbZ7Y+si9seWZf1GAAAAEUnqtlnn/jXVVmPAAAAkAlRTb8dMcHrqgEAAHYlqum36+bXZz0CAABASRHV9Nvh3gEcAACgG1ENAAAAKYlqCnLfX52V9QgAAAAlQ1RTkAP2q856BAAAgJIhqilIRS7rCQAAAEqHqKYguZyqBgAA2ElUU7BDx+2X9QgAAAAlQVRTsKpKR6sBAAAiRDUpVOzlFPC//reH498ffKFI0wAAAGRHVFOwvb1Z2b/+93PxmZtWF2cYAACADIlqCvb2oydmPQIAAEBJENUUbOFZR2Y9AgAAQEkQ1RSsdkRl1iMAAACUBFENAAAAKYlqUvmb/3Fs1iMAAABkTlSTyodOnZb1CAAAAJkT1aRSsbfragEAAAwDohoAAABSEtWk9tfvOSbrEQAAADIlqkntwjdPz3oEAACATIlqAAAASElUs0++/mfHZz0CAABAZkQ1++T9s6ZmPQIAAEBmRDUAAACkJKoBAAAgJVHNPvvkWw7vdfkdj68v8iQAAADFJarZZwvPemOvyz/2g9/FYZfdGt+5849FnggAAKA4RDX7bGR15R7XX/mfjxdpEgAAgOIS1QAAAJCSqAYAAICURDUD4t7Lz8x6BAAAgKIT1QyICWNq49r59VmPAQAAUFSimgHzzmMnZT0CAABAUYlqAAAASElUUxTnf/e/47DLbo0XW7ZlPQoAAMCAEdUUxco/boyIiFP/7lcZTwIAADBwRDUD6rKzj856BAAAgKIR1QyoBWe8IesRAAAAikZUM+C+du7MrEcAAAAoClHNgPvwm6ZlPQIAAEBRiGoGRS635/UdnfniDAIAADCIRDWD4kcXntrnuv/vvjUx40u3xZ1PvlTEiQAAAAaeqGZQzDlifJ/rvvB/fx/tnUl84sb7ijgRAADAwBPVDJpbPjUn6xEAAAAGlahm0Jx86AF7XN/a4XXVAABAeRPVAAAAkJKoZlA98tV37nH94+taijQJAADAwBPVDKpRNVXx+Xce1ef6d129Iv6wfnP88aXN8fVfPB5rX91axOkAAAD2TS5JkiTrIfampaUl6urqorm5OcaOHZv1OKRw2GW39mu7ww8aFf/1v986uMMAAADsRX871JFqimLFF97Wr+2efum1QZ4EAABg4IhqimLquP2yHgEAAGDAiWqK5uG9vGkZAABAuRHVFM3omqp46Ctz97rdlraOIkwDAACw70Q1RTWmdkQ8s+Tde9zmmC/9Iv6wflORJgIAAEhPVFN0uVwu/uHPjt/jNmdddVccdtmtMWfJr2Jbe2eRJgMAACiMqCYTH5g1Na6dX7/X7V5o3hYLfriqz/VJksTLr7UN5GgAAAD9JqrJzDuPnRQ3fvyUvW736ydeilXPvRKf/Nf74vfPv9pt3Vd+/kic/DcNcdvD6wZpSgAAgL6JajL1ljceFA986R173e5Pr1kZv3jkxXjfv/wmfv/8q5EkSURE/J97nouIiH+47fFBnRMAAKA3oprM7b9fdTyz5N1x3smH9Gv79/3Lb2L64uXdlj294bW47eGmwRgPAACgT6KakpDL5eKqD5wYyz97er/vM/cf7+z28YIf3j/QYwEAAOyRqKakHDN5bDx2xbti4VlH7nXbJ1/c3GPZV//9kfjS/3s4XtrU2rXstoeb4oa7nxnQOQEAACIicsnOF6eWsJaWlqirq4vm5uYYO3Zs1uNQJNvaO+Nv/uPR+NFvG1Pd/5TDxsWNF54SR//1bRER8fU/Oz7eP2vqQI4IAAAMUf3tUFFNWXjyxU1x3tKVsbm1Y58e5/PvPCp+9+zL8esnXoqvvu/Y+PCbpkVlRS4iIu54Yn0cNLomZh5SNxAjAwAAZUxUM2Rt3Nwa9V/75YA93pfec0zc8cT6WPHUhoiIeM/xB8f5px4aJ0zZP15r64gJY2ojSZLIJ9EV4AAAwNAmqhkWtrV3xj1/3BjfufOP8dtnXi7a837iLYfHdXc9HQvPOjJmTRsXV972WBx3SF081rQp/vLdM2K/6so4cHR1TBhTG5UVufjD+k3xWmtnHD+lLjrzSTz/ytYYUVURh+w/susxk2T78ikHjIxcTrwDAECWBjWqly5dGl//+tejqakpjj322Lj66qvj9NP7ftfmO++8MxYtWhSPPPJITJ48Ob7whS/EggUL+v18oppCbGvvjP/3wNr4ys8fja3tnVmPk8pH5xwWbzhoVCz5z8ejbuSIeOtRE2LmIWPjTw4bF0ccNDqeeHFTvLqlPU6ZPi42b+uI/35mY4yprYqNm9vijKMOirG1I7oe6+XX2mJUTWVsa8tH3X4jejyXo/AAANDToEX1smXLYv78+bF06dI47bTT4tprr43vfe978eijj8ahhx7aY/tnnnkmZs6cGf/rf/2v+OQnPxm/+c1v4lOf+lTcdNNN8ad/+qcD+snAnuyMx+c2vhb/74EXYtnv1sS6lm1Zj8UOZ7zxoHjh1a0x7cD94ogJY+LGe56NLW3bfyly0qH7x6feekT8/vlXo70ziV88si5qR1TGqdPHxW0Pr4sDRlXHjIPHxMuvtUVlLhdTDhgZLzRviw+/aVqMqq6Mza0dMaqmKp58cVPc/9yrMePgMdGRT+KA/UbE3X/YGPkkiTcfMT6ef2VLtGztiBOm7h/3N74SY2qq4uiDx8R+1VXxnw81xcnTDogH1zTHJ95yeIyprYp1LduivSMfL7/WFrUjKmPKASPj+Ve3xuiaqmjvzMcfX3otxtZWxZQDRsa29nzXNptbO6K6siIqKnIxckRlvPDq1oiIqKmqiJZtHTG6pipyuYjNrR0xuW5ktHfmY2t7Z4wcURnNW9tjbO2I2NTaHm84aHR05JNo78hHe2c+XtnSHnUjR8Tm1o7Y1t4Z40fXxLb2zjhoTE3XL03aO/ORy+Xildfa4pUtbTGprjZGjqiMbe35GDeqOipy0fVcuVwudv4T0ZFPojOfRC4XUVNVGZ35JF5r64jN2zpi8i5nXBSqozMfnUkS1ZUV3X65s/sve/I7nrutM7/ja1XZtV1ExJa2zhhVU9W1bWtHPmqqtn+N+5IkSSRJ7HGb3bff01kk7Z35qMzloqIit9dtAYDSN2hRfeqpp8bJJ58c11xzTdeyGTNmxLnnnhtLlizpsf0Xv/jF+PnPfx6PPfZY17IFCxbEgw8+GPfcc0+vz9Ha2hqtra9fEqmlpSWmTp0qqim6be2d0daZj1dea4vnX9ka9z37Sjz/ypb46arnsx4NBl1FLmLnPxB7+5di5IjKrj/v3pK7p+WusbnzzQdzuYjKXC5qqipia3tn5JPXH2v3566syEV1ZUW0d+ajI//6ylwuYkRFRVd479yuuqoi8vkkKitzXfN05JPYtK2ja/aqylzkdsxWkYuoyOUiiYh8kkRFLhdb2jpiW3s+Kity0ZlPorqqIto6tj/PAfuNiI7OJDbt+FzGj66JDZtf/zds5IjK2NreGQfsNyK2teejM5/EyOrtvyCJiBhRmYv2ziRqqirigP2qozNJdvwSIdf1tdy1+3ORiyS2z7Xr1zaJiI2b22J0bdWOX1IkMWLH16m9Mx9VlRUxoiIX+eT1v6POfBJVFbmorMxFkvT8WidJ0mMf2LlkRGXFjl9KvD5XGh07Zsvltn+9d87RmU+ieWt7jBtVHdVVr18BNBcRL29pi6qKiqipqojKilzkk6RrH+rIb//6JTtm7Mxv/9ru+qklSRJVldlcVTSrX7Vk9TuefBJRtdsvrnZ+n2UtSbZ/3+z8X+HtH2//3qrI7dyvts/a0ZlEZcX278nOHT93tv/c6P6JdO7yC8iqylx0dO74Xs29/new8z47P87veP7dH2v7920u8vnY5WfBjl88RtLte3b3/Xv35+z2OSevP+fOx6za8TNg17l2/yvq/hy7Lk96Lutlrojt+0Iut/2xd67rep6dT5wkvQ/en+12setzlJIS2PUjIrufCbs68+gJcfHb934Z3Sz1N6qrCnnQtra2WLVqVVx22WXdls+dOzdWrlzZ633uueeemDt3brdl73znO+P666+P9vb2GDGi5+moS5Ysia9+9auFjAaDonZEZdSOqIyxtSNi2oGj4rQjxkdExNfff8KAPP7Of8hbd/zP+c7/yW58eUuMqa2KNS9vjSdf3BTjR1fH4+s2xajqqqgdURGvbmmP1WtejVXPvdL1WNWVr8dEIXaNg1I2trYqWrYV/u7vh47bL9o68tGyrb3ryPdAqKrIdQu6/qrIRaS4WyYKmXNfX2qRJBEdSRIdu/0d9RbznfkktuZ7Pl+SRLfvgZ3b7W22re2dEe39m3Pn/0zv+j3zypbud941qLsef7ft2ra+fv/2ztd/DgzE2TMvv9a2z49RStbuOJMDgKHliINGZz3CgCkoqjds2BCdnZ0xceLEbssnTpwY69at6/U+69at63X7jo6O2LBhQxx88ME97rN48eJYtGhR18c7j1TDULPziF3tjqN8O/87cWxtREQcPWlsvOOYib3fmZK2t5OAdg/y7b+9z0VHZ77rCEPbjtOJ88nrRyWqKyuiPZ/vOhpSVVERFRXbY6/ryMqO34N35Lef6t25/QG2H4Gp2L62tSMfo6qrorVj+1Hh0TVV0by1PXK7HJHZefSvM5/EfiOqoiOfj/yOIxw7n79lW3u31/D3/Drs9vFuxw3aO5NIkiRqR1RGW2d++1HTilzXkZQk2X60pzOfxH7VVVFVkYvNrR2RJBHt+Xy0bG2Pitz2ZZPqarf/IqyqIqoqK2Jd87aorqqIzny+lzkiNm3riJqqihhVU9V1RHbX542IqKzY/suFilzE+pbWGFM7IrZ1dEZlxfaj6lvaOqO2avuR7taOfHR05mNkdWW0dybRscuR9CSJGDuyKiorcrG1rTPG1FZFa0c+nt2wJaaPHxXNW9sjiSTG1IyIioruR6MjXo/53r6mEdH1d9XakY/aqsrtf0c7jlaNqMxFVUVFtHZ0Ri5yXUeEd/65I5/scqRs+8+lnQcwth9R6n5U7fW/u3wkse9Hg3aehr/zyHzFLmcLvLKlrevn4q46OpPYsLk1phwwstsvf/LJ9n23tT0f2zo6Y3RNVY9fgO38+uX38j06GLJ6a9jdv++K+MQRO36m7NyXd/5M6WuiLF46UbFjP995BHXn98SuP4tGVFZERz7pmq8i1/PneJIkUVlREVU7zmjpTJKuP0fs9n2y6wd9fLqvfx/mXt9fk9dX7P79uuvqnWfa7H44OBfx+lHsHT9n8klE545/L3b+W7H7vrrrX0m3Z+z9j93+Dnf+KYntzxPR8yDzrkfdczt++bzrz6Fd59jxEN3OZNr9sSJeP9OolJTK79RL5X2q9+XlY6WmoKjeafcfdnv7Adjb9r0t36mmpiZqamrSjAZQEvb2P4UjKntfv+spqTtfN7y7morel/fU93Zjdvx3ZPXr2xw0pjx+7h4wqrpf29WN7Dv20zhiwpi9b1SgYyfXDfhjAgDFVdALisaPHx+VlZU9jkqvX7++x9HonSZNmtTr9lVVVXHggQcWOC4AAACUjoKiurq6Ourr66OhoaHb8oaGhpgzZ06v95k9e3aP7W+//faYNWtWr6+nBgAAgHJR8FtfLlq0KL73ve/FDTfcEI899lhceuml0djY2HXd6cWLF8cFF1zQtf2CBQviueeei0WLFsVjjz0WN9xwQ1x//fXxuc99buA+CwAAAMhAwa+pnjdvXmzcuDGuuOKKaGpqipkzZ8by5ctj2rRpERHR1NQUjY2NXdtPnz49li9fHpdeeml8+9vfjsmTJ8c//dM/9fsa1QAAAFCqCr5OdRb6e30wAAAAGAj97dCCT/8GAAAAthPVAAAAkJKoBgAAgJRENQAAAKQkqgEAACAlUQ0AAAApiWoAAABISVQDAABASqIaAAAAUhLVAAAAkJKoBgAAgJRENQAAAKQkqgEAACAlUQ0AAAApiWoAAABISVQDAABASqIaAAAAUhLVAAAAkJKoBgAAgJRENQAAAKQkqgEAACAlUQ0AAAApiWoAAABISVQDAABASqIaAAAAUhLVAAAAkFJV1gP0R5IkERHR0tKS8SQAAAAMBzv7c2eP9qUsonrTpk0RETF16tSMJwEAAGA42bRpU9TV1fW5PpfsLbtLQD6fjxdeeCHGjBkTuVwu63H61NLSElOnTo01a9bE2LFjsx4HerCPUurso5Q6+yilzj5KOSiX/TRJkti0aVNMnjw5Kir6fuV0WRyprqioiClTpmQ9Rr+NHTu2pHcOsI9S6uyjlDr7KKXOPko5KIf9dE9HqHfyRmUAAACQkqgGAACAlET1AKqpqYkvf/nLUVNTk/Uo0Cv7KKXOPkqps49S6uyjlIOhtp+WxRuVAQAAQClypBoAAABSEtUAAACQkqgGAACAlEQ1AAAApCSqAQAAICVRPUCWLl0a06dPj9ra2qivr48VK1ZkPRJD0JIlS+JP/uRPYsyYMTFhwoQ499xz44knnui2TZIk8ZWvfCUmT54cI0eOjLe+9a3xyCOPdNumtbU1PvOZz8T48eNj1KhR8b73vS+ef/75btu88sorMX/+/Kirq4u6urqYP39+vPrqq4P9KTLELFmyJHK5XCxcuLBrmX2UUrB27dr48Ic/HAceeGDst99+ceKJJ8aqVau61ttPyVJHR0f81V/9VUyfPj1GjhwZhx9+eFxxxRWRz+e7trGPUkx33XVXvPe9743JkydHLpeLf/u3f+u2vpj7Y2NjY7z3ve+NUaNGxfjx4+Ozn/1stLW1Dcan3X8J++wnP/lJMmLEiOS73/1u8uijjyaXXHJJMmrUqOS5557LejSGmHe+853J97///eThhx9OHnjggeScc85JDj300GTz5s1d21x55ZXJmDFjkptvvjl56KGHknnz5iUHH3xw0tLS0rXNggULkkMOOSRpaGhI7r///uRtb3tbcsIJJyQdHR1d27zrXe9KZs6cmaxcuTJZuXJlMnPmzOQ973lPUT9fytu9996bHHbYYcnxxx+fXHLJJV3L7aNk7eWXX06mTZuWfPSjH01++9vfJs8880zyy1/+MvnDH/7QtY39lCx97WtfSw488MDkP/7jP5Jnnnkm+elPf5qMHj06ufrqq7u2sY9STMuXL08uv/zy5Oabb04iIvnZz37WbX2x9seOjo5k5syZydve9rbk/vvvTxoaGpLJkycnF1988aB/DfZEVA+AU045JVmwYEG3ZUcffXRy2WWXZTQRw8X69euTiEjuvPPOJEmSJJ/PJ5MmTUquvPLKrm22bduW1NXVJd/5zneSJEmSV199NRkxYkTyk5/8pGubtWvXJhUVFcltt92WJEmSPProo0lEJP/93//dtc0999yTRETy+OOPF+NTo8xt2rQpOfLII5OGhobkjDPO6Ipq+yil4Itf/GLy5je/uc/19lOyds455yQf//jHuy0777zzkg9/+MNJkthHydbuUV3M/XH58uVJRUVFsnbt2q5tbrrppqSmpiZpbm4elM+3P5z+vY/a2tpi1apVMXfu3G7L586dGytXrsxoKoaL5ubmiIgYN25cREQ888wzsW7dum77Y01NTZxxxhld++OqVauivb292zaTJ0+OmTNndm1zzz33RF1dXZx66qld27zpTW+Kuro6+zX98ulPfzrOOeecOOuss7ott49SCn7+85/HrFmz4v3vf39MmDAhTjrppPjud7/btd5+Stbe/OY3x69+9at48sknIyLiwQcfjLvvvjve/e53R4R9lNJSzP3xnnvuiZkzZ8bkyZO7tnnnO98Zra2t3V7CU2xVmT3zELFhw4bo7OyMiRMndls+ceLEWLduXUZTMRwkSRKLFi2KN7/5zTFz5syIiK59rrf98bnnnuvaprq6Og444IAe2+y8/7p162LChAk9nnPChAn2a/bqJz/5Sdx///3xu9/9rsc6+yil4Omnn45rrrkmFi1aFH/5l38Z9957b3z2s5+NmpqauOCCC+ynZO6LX/xiNDc3x9FHHx2VlZXR2dkZf/u3fxsf/OAHI8LPUkpLMffHdevW9XieAw44IKqrqzPdZ0X1AMnlct0+TpKkxzIYSBdffHH8/ve/j7vvvrvHujT74+7b9La9/Zq9WbNmTVxyySVx++23R21tbZ/b2UfJUj6fj1mzZsXf/d3fRUTESSedFI888khcc801ccEFF3RtZz8lK8uWLYsf/vCH8eMf/ziOPfbYeOCBB2LhwoUxefLk+MhHPtK1nX2UUlKs/bEU91mnf++j8ePHR2VlZY/fjKxfv77Hb1FgoHzmM5+Jn//853HHHXfElClTupZPmjQpImKP++OkSZOira0tXnnllT1u8+KLL/Z43pdeesl+zR6tWrUq1q9fH/X19VFVVRVVVVVx5513xj/90z9FVVVV1/5jHyVLBx98cBxzzDHdls2YMSMaGxsjws9Ssvf5z38+LrvssvjzP//zOO6442L+/Plx6aWXxpIlSyLCPkppKeb+OGnSpB7P88orr0R7e3um+6yo3kfV1dVRX18fDQ0N3ZY3NDTEnDlzMpqKoSpJkrj44ovjlltuif/6r/+K6dOnd1s/ffr0mDRpUrf9sa2tLe68886u/bG+vj5GjBjRbZumpqZ4+OGHu7aZPXt2NDc3x7333tu1zW9/+9tobm62X7NHZ555Zjz00EPxwAMPdN1mzZoVH/rQh+KBBx6Iww8/3D5K5k477bQelyN88sknY9q0aRHhZynZ27JlS1RUdP/f9MrKyq5LatlHKSXF3B9nz54dDz/8cDQ1NXVtc/vtt0dNTU3U19cP6ue5R0V+Y7Qhaeclta6//vrk0UcfTRYuXJiMGjUqefbZZ7MejSHmL/7iL5K6urrk17/+ddLU1NR127JlS9c2V155ZVJXV5fccsstyUMPPZR88IMf7PWSBlOmTEl++ctfJvfff3/y9re/vddLGhx//PHJPffck9xzzz3Jcccd5xIbpLLru38niX2U7N17771JVVVV8rd/+7fJU089lfzoRz9K9ttvv+SHP/xh1zb2U7L0kY98JDnkkEO6Lql1yy23JOPHj0++8IUvdG1jH6WYNm3alKxevTpZvXp1EhHJVVddlaxevbrrEsLF2h93XlLrzDPPTO6///7kl7/8ZTJlyhSX1Boqvv3tbyfTpk1Lqqurk5NPPrnrEkcwkCKi19v3v//9rm3y+Xzy5S9/OZk0aVJSU1OTvOUtb0keeuihbo+zdevW5OKLL07GjRuXjBw5MnnPe96TNDY2dttm48aNyYc+9KFkzJgxyZgxY5IPfehDySuvvFKEz5KhZveoto9SCv793/89mTlzZlJTU5McffTRyXXXXddtvf2ULLW0tCSXXHJJcuihhya1tbXJ4Ycfnlx++eVJa2tr1zb2UYrpjjvu6PX/QT/ykY8kSVLc/fG5555LzjnnnGTkyJHJuHHjkosvvjjZtm3bYH76e5VLkiTJ5hg5AAAAlDevqQYAAICURDUAAACkJKoBAAAgJVENAAAAKYlqAAAASElUAwAAQEqiGgAAAFIS1QAAAJCSqAYAAICURDUAAACkJKoBAAAgpf8fr5UUQi2h8GYAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1200x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plot_e = np.linspace(1,  10000, num= 10000)\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.plot(plot_e, np.array(cool_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c0a87dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training metrics:\n",
      "  Accuracy:  1.0000\n",
      "  Precision: 1.0000\n",
      "  Recall:    1.0000\n",
      "  F1 Score:  1.0000\n",
      "\n",
      "Test metrics:\n",
      "  Accuracy:  0.5159\n",
      "  Precision: 0.5238\n",
      "  Recall:    0.3976\n",
      "  F1 Score:  0.4521\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Set model to eval mode\n",
    "model.eval()\n",
    "\n",
    "# Prediction function\n",
    "def get_predictions(loader):\n",
    "    targets = []\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            outputs = model(inputs)\n",
    "            probs = torch.sigmoid(outputs)\n",
    "            preds = (probs > 0.5).float()\n",
    "            targets.extend(labels.numpy())\n",
    "            predictions.extend(preds.numpy())\n",
    "\n",
    "    return np.array(targets).flatten(), np.array(predictions).flatten()\n",
    "\n",
    "# Get predictions\n",
    "train_targets, train_preds = get_predictions(train_loader)\n",
    "test_targets, test_preds = get_predictions(test_loader)\n",
    "\n",
    "# Metrics\n",
    "def print_metrics(y_true, y_pred, label='Set'):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred, zero_division=0)\n",
    "    rec = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    print(f\"\\n{label} metrics:\")\n",
    "    print(f\"  Accuracy:  {acc:.4f}\")\n",
    "    print(f\"  Precision: {prec:.4f}\")\n",
    "    print(f\"  Recall:    {rec:.4f}\")\n",
    "    print(f\"  F1 Score:  {f1:.4f}\")\n",
    "\n",
    "print_metrics(train_targets, train_preds, label='Training')\n",
    "print_metrics(test_targets, test_preds, label='Test')\n",
    "\n",
    "# Confusion Matrices\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "train_cm = confusion_matrix(train_targets, train_preds)\n",
    "test_cm = confusion_matrix(test_targets, test_preds)\n",
    "\n",
    "ConfusionMatrixDisplay(train_cm, display_labels=[0, 1]).plot(ax=axs[0], values_format='d')\n",
    "axs[0].set_title(\"Training Confusion Matrix\")\n",
    "\n",
    "ConfusionMatrixDisplay(test_cm, display_labels=[0, 1]).plot(ax=axs[1], values_format='d')\n",
    "axs[1].set_title(\"Test Confusion Matrix\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3d8859a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test classification accuracy: 51.59%\n",
      "99% CI under null: [46.58%, 56.60%]  Significant? NO\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# === 1. CLASSIFICATION ACCURACY (TEST ONLY) ===\n",
    "test_accuracy = np.mean(test_targets == test_preds)\n",
    "\n",
    "# === 1.1. Confidence interval around 0.5 (null hypothesis) ===\n",
    "n_test = len(test_targets)\n",
    "p_hat = test_accuracy\n",
    "p_0 = 0.5\n",
    "z = norm.ppf(1 - 0.01 / 2)  # two-tailed 99% CI  z  2.576\n",
    "se = np.sqrt(p_0 * (1 - p_0) / n_test)\n",
    "ci_low = p_hat - z * se\n",
    "ci_high = p_hat + z * se\n",
    "significant = \"YES\" if ci_low > 0.5 else \"NO\"\n",
    "\n",
    "print(f\"Test classification accuracy: {test_accuracy:.2%}\")\n",
    "print(f\"99% CI under null: [{ci_low:.2%}, {ci_high:.2%}]  Significant? {significant}\")\n",
    "\n",
    "# === 2. STRATEGY RETURNS (TEST ONLY) ===\n",
    "# Map prediction: 1  long (+1), 0  short (-1)\n",
    "# test_preds: binary predicted labels (0 = short, 1 = long)\n",
    "# test_returns: actual returns aligned with the test set (same shape)\n",
    "\n",
    "# Convert predictions into trading signals: 1  long, 0  short (mapped to +1 / -1)\n",
    "test_signals = 2 * test_preds - 1  # now in {-1, +1}\n",
    "\n",
    "# Compute strategy returns\n",
    "test_strategy_returns = test_signals * test_returns\n",
    "\n",
    "# Plot cumulative returns\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "\n",
    "ax.plot(np.cumsum(test_strategy_returns), label='Strategy (Test)', linestyle='--')\n",
    "ax.plot(np.cumsum(test_returns), label='Asset (Test)', alpha=0.7)\n",
    "\n",
    "ax.set_title('Cumulative Returns on Test Set')\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_ylabel('Cumulative Return')\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLfinanceHW2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
